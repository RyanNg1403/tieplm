{
  "generated_at": "2025-11-14T20:01:09.219904",
  "total_summaries": 62,
  "summaries": [
    {
      "video_id": "Chương 2_GdKIVY6CsTw",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giới thiệu **mô hình máy học tổng quát** (supervised learning pipeline), các mô hình cơ bản (Linear Regression, Logistic Regression, Softmax, Neural Network) và quy trình thiết kế — từ mô hình dự đoán, hàm loss đến việc tối ưu tham số θ. [1][20]\n\n- Các khái niệm sẽ được đề cập: \n  - Mô hình dự đoán dưới dạng ŷ = f_θ(x) (θ là tham số mô hình, x là dữ kiện đầu vào). [1][2]  \n  - Hàm loss L(θ; x, y) (hàm mất mát) để đo sai số giữa ŷ và y. [2][3]  \n  - Bài toán tối ưu hóa để tìm θ sao cho L(θ; x, y) nhỏ nhất (sử dụng thuật toán như gradient descent / Adam). [4][5][19]  \n\n- Ghi chú quan trọng: x và y được xem là các *tham số dữ liệu* (cặp huấn luyện) khi tối ưu hóa hàm loss; trong khi θ là *biến số* cần tối ưu. [3]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Ba công việc chính khi thiết kế mô hình máy học\n- Thiết kế hàm dự đoán: f_θ(x). [4][20]  \n- Thiết kế hàm loss: L(θ; x, y) để đo mức sai khác giữa ŷ và y. [4][20]  \n- Tìm tham số θ tối ưu sao cho L(θ; x, y) nhỏ nhất (bài toán tối ưu). [4][20]\n\n### 2.2. Tối ưu tham số — Gradient Descent (GD)\n- Ý tưởng trực quan: xem L là hàm theo θ (trục θ) và cập nhật θ theo hướng giảm giá trị L. [6]  \n- Dấu đạo hàm/gradient chỉ ra hướng tăng của hàm; để giảm L ta đi **ngược hướng gradient**. [6][7][8]  \n- Công thức cập nhật cơ bản (một chiều hoặc vector):  \n  θ ← θ − α ∇_θ L(θ)  \n  (α là learning rate, ∇_θ L là đạo hàm (gradient) theo θ). [10][15]\n\n- Vấn đề overshoot: nếu gradient quá lớn, cập nhật θ có thể nhảy vượt qua điểm cực tiểu (go past minimum). Vì vậy cần có hệ số α để điều chỉnh bước nhảy. [9][10]\n\n### 2.3. Siêu tham số và điểm dừng\n- Siêu tham số cần khởi tạo: θ₀ (giá trị bắt đầu), learning rate α, ngưỡng dừng ε (hoặc số vòng lặp tối đa). [13][15]  \n- Tham số mặc định thường dùng khi không rõ: α ≈ 10^(−4) (~0.0001) có thể là điểm bắt đầu; kinh nghiệm cho biết α = 0.01 đôi khi đủ; α quá nhỏ sẽ làm quá trình chạy rất chậm. [13][14]  \n- Tiêu chí dừng:\n  - Dừng khi ||∇_θ L|| < ε (đạo hàm đủ nhỏ) — nghĩa là gần điểm cực tiểu. [11][12]  \n  - Hoặc dừng sau một số vòng lặp cố định (max iterations). [12][15]\n\n### 2.4. Vấn đề nhiều điểm cực tiểu và các chiến lược khắc phục\n- Khi L phức tạp có nhiều local minima; kết quả phụ thuộc vào khởi tạo θ₀ (ví dụ minh họa bằng hình viên bi rơi vào hố cục bộ). [16][17]  \n- Giải pháp 1: chạy nhiều lần với các khởi tạo ngẫu nhiên và chọn kết quả tốt nhất (tốn tài nguyên). [17]  \n- Giải pháp 2: sử dụng *momentum* (quán tính) để thoát khỏi hố nông và tiếp tục đi tới hố sâu hơn; nhiều optimizer hiện đại khai thác ý tưởng này. [18]  \n- Adam optimizer: một thuật toán tối ưu phổ biến có khai thác yếu tố tương tự momentum; đã được cài đặt sẵn trong các thư viện như TensorFlow và (trong lời giảng) Python frameworks, rất tiện dùng — thường là lựa chọn mặc định để tìm θ tối ưu. Chi tiết sử dụng sẽ trình bày ở phần thực hành. [19]\n\n### 2.5. Hậu quả thực hành: tập trung vào hai công việc còn lại\n- Do các thư viện hiện nay xử lý tốt bài toán tối ưu (công việc 3) và thường cung cấp Adam, momentum, v.v., nên khi thiết kế mô hình thực tế ta chủ yếu tập trung vào:\n  - Thiết kế hàm dự đoán f_θ(x). [20]  \n  - Thiết kế hàm loss L(θ; x, y) phù hợp bản chất bài toán. [20][21]\n\n### 2.6. Lựa chọn hàm dự đoán và hàm loss theo tính chất y\n- Tùy theo tính chất của y (bài toán hồi quy, phân lớp, hay phi tuyến) mà ta chọn kiến trúc mô hình và dạng hàm loss khác nhau. Ví dụ:\n  - Bài toán hồi quy → loss và mô hình phù hợp với giá trị liên tục. [21]  \n  - Bài toán phân lớp → loss và mô hình phù hợp với phân loại (ví dụ logistic / softmax cho nhiều lớp). [1][21]  \n- Kết luận: thiết kế f_θ và L(·) phải dựa trên mối quan hệ mong muốn giữa x và y. [21][22]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa khái quát: dự đoán giá cổ phiếu hoặc giá nhà — mục tiêu là ŷ ≈ y (dự đoán sát giá thật), dùng hàm loss để định lượng sai số. [2][3]  \n- Ứng dụng thực tế của các optimizer:\n  - Sử dụng Adam (có momentum/biến thể) trong các framework (TensorFlow, Python libs) để tối ưu θ cho mô hình học sâu. [19]  \n- Trường hợp sử dụng quyết định thiết kế:\n  - Nếu y là giá trị liên tục (hồi quy) thì dùng mô hình/hàm loss khác so với bài toán phân lớp; với bài toán phân lớp sẽ chọn mô hình/hàm loss thích hợp cho class labels (ví dụ logistic/softmax). [1][21]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính: Mô hình máy học tổng quát gồm ba bước — thiết kế f_θ(x), chọn hàm loss L(θ; x, y), và tìm θ tối ưu; trong thực hành, phần tối ưu thường được giải quyết bằng các optimizer hiện đại (ví dụ Adam), nên trọng tâm là thiết kế mô hình và hàm loss phù hợp với bài toán. [4][20][19]  \n- Tầm quan trọng: Hiểu rõ vai trò của gradient descent, learning rate, momentum/Adam và tiêu chí dừng giúp thiết kế và huấn luyện mô hình ổn định, tránh overshoot, và xử lý bài toán nhiều cực trị tốt hơn. [9][10][11][18][19]  \n- Liên hệ với các bài giảng khác: Bài này là nền tảng cho các mô-đun sau sẽ triển khai cụ thể các mô hình như Linear Regression, Logistic Regression, Softmax Regression và mạng Neural Network (mạng nơ-ron) — tức là phần kiến trúc mô hình và hàm loss cho từng dạng bài toán sẽ được trình bày tiếp theo trong chương. [1]\n\n(Đã trích dẫn đầy đủ các đoạn gốc video theo từng phần: [1]…[22])",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 0,
          "end_time": 64,
          "text": "ừ ừ Chào các bạn nay thì chúng ta sẽ đến với bài số 2 là máy học tổng quát và một số mô hình cơ bản thì nội dung của buổi hôm nay sẽ bao gồm các phần như sau mô hình máy học tổng quát đây là một trong những cái nội dung quan trọng và đi xuyên suốt trong toàn bộ cái mô hợp này chúng ta sẽ học về cái kiến trúc chung của các cái mô hình máy học có giám sát và từ đó thì chúng ta sẽ phát triển lên các cái mô hình dựa như Linear Regression Logistic Regression, Soft Mark Regression và cuối cùng thì chúng ta sẽ đến cái mô hình học sâu đầu tiên mà chúng ta sẽ học trong mô hình này đó chính là mạng Neural Network thì đối với cái mô hình máy học tổng quát thì chúng ta sẽ nhận cái dự kiện đầu vào đây sẽ là cái thông tin để giúp cho chúng ta đưa ra cái quyết định và quyết định của mình thì nó ở dạng là dự đoán là hàm là một cái giá trị"
        },
        {
          "index": 2,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 47,
          "end_time": 113,
          "text": "đầu tiên mà chúng ta sẽ học trong mô hình này đó chính là mạng Neural Network thì đối với cái mô hình máy học tổng quát thì chúng ta sẽ nhận cái dự kiện đầu vào đây sẽ là cái thông tin để giúp cho chúng ta đưa ra cái quyết định và quyết định của mình thì nó ở dạng là dự đoán là hàm là một cái giá trị y ngã và y ngã này là kết quả của một cái hàm số fθx thì đây là một cái mô hình máy học và fθx thì θ chính là cái tham số của mô hình còn x chính là cái dự kiện đầu vào chúng ta đã nhận được và cái giá trị dự đoán này chúng ta luôn mong muốn nó sắp xỉ với lại cái giá trị thực tế ví dụ như khi chúng ta đoán cái giá của một cái cổ phiếu thì chúng ta mong muốn đoán giá chính xác với lại cái giá trong tương lai hoặc chúng ta dự đoán giá nhà chúng ta mong muốn đoán với cái giá trị thật của cái căn nhà cách chính xác nhất thì để đảm bảo cho cái điều kiện là y ngã sắp xỉ với lại cái y thì chúng ta sẽ phải có một cái hàm nó gọi là hàm loss hàm này gọi là tên tiếng Việt có thể là hàm mất mét"
        },
        {
          "index": 3,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 97,
          "end_time": 164,
          "text": "chúng ta mong muốn đoán với cái giá trị thật của cái căn nhà cách chính xác nhất thì để đảm bảo cho cái điều kiện là y ngã sắp xỉ với lại cái y thì chúng ta sẽ phải có một cái hàm nó gọi là hàm loss hàm này gọi là tên tiếng Việt có thể là hàm mất mét hoặc là hàm lộ lỗi Thì chúng ta có thể có hai cái cách gọi khác nhau thì cái chữ l này là viết tắt của chữ loss rồi và chúng ta có một cái lưu ý đó là nếu như trong cái hàm mô hình máy học thì biến số của mình chính là cái dự kiện đầu vào thì đối với hàm mất mét biến số của mình đó chính là thêta thêta nó sẽ là biến số chứ nó không phải là tham số còn các cái giá trị x và y ở đây thì bình thường theo cái cách ký hiệu trong cái chương trình vô thâu chúng ta hay dùng xy cho biến số nhưng mà trong trường hợp này xy nó chính là tham số tại sao nó gọi là tham số bởi vì x và y trong trường hợp này nó chính là những cái cặp dữ liệu mà mình được sử dụng để huấn luyện"
        },
        {
          "index": 4,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 145,
          "end_time": 215,
          "text": "còn các cái giá trị x và y ở đây thì bình thường theo cái cách ký hiệu trong cái chương trình vô thâu chúng ta hay dùng xy cho biến số nhưng mà trong trường hợp này xy nó chính là tham số tại sao nó gọi là tham số bởi vì x và y trong trường hợp này nó chính là những cái cặp dữ liệu mà mình được sử dụng để huấn luyện chúng ta sẽ đưa vào các cái cặp dữ liệu xy mà mình thu thập được trong thực tế và mình hy vọng là chúng ta sẽ học và tìm ra được một cái hàm mô hình máy học và khi mà cái hàm mô hình máy học này đã học đảm bảo được là cái giá trị mất mát và cái giá trị độ lỗi này là thấp nhất thì sau này chúng ta sẽ sử dụng cái mô hình máy học để đi dự đoán cho những cái mẫu thực tế thế thì 3 cái công việc chính cần phải làm khi thiết kế một cái mô hình đó chính là việc đầu tiên chúng ta sẽ thiết kế một cái mô hình dự đoán tức là thiết kế một cái hàm f thêta x công việc thứ 2 đó là chúng ta sẽ thiết kế cái hàm lỗi hàm lỗi của cái việc dự đoán đó chính là hàm l thêta xy và công việc cuối cùng đó là chúng ta sẽ tìm ra cái tham số thêta để cho cái hàm độ lỗi này là nhỏ nhất"
        },
        {
          "index": 5,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 200,
          "end_time": 266,
          "text": "công việc thứ 2 đó là chúng ta sẽ thiết kế cái hàm lỗi hàm lỗi của cái việc dự đoán đó chính là hàm l thêta xy và công việc cuối cùng đó là chúng ta sẽ tìm ra cái tham số thêta để cho cái hàm độ lỗi này là nhỏ nhất tại vì luôn luôn mong muốn tìm một cái hàm mô hình f thêta x sao cho giá trị dự đoán y ngã sắp xỉ y thì cái việc này nó tương đương với cái việc là chúng ta sẽ có cái hàm độ lỗi là thấp nhất hoặc là cái size số cái mức mát là nhỏ nhất như vậy thì trong 3 cái công việc này thì chúng ta sẽ tìm hiểu cái công việc thứ 3 trước tiên tại sao là như vậy tại vì các cái mô hình về các cái thư viện hiện nay thì đều đã hỗ trợ cho chúng ta tìm cái thêta sao cho cái hàm độ lỗi này nhỏ nhất rồi và chúng ta sẽ sử dụng một cái thuật toán mà sau đây chúng ta sẽ tìm hiểu đó là thuật toán gradient descent đây là một trong những cái thuật toán mà rất là hiệu quả trong cái việc là tìm một cái tham số thêta sao cho cái độ lỗi này là nhỏ nhất"
        },
        {
          "index": 6,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 248,
          "end_time": 315,
          "text": "và chúng ta sẽ sử dụng một cái thuật toán mà sau đây chúng ta sẽ tìm hiểu đó là thuật toán gradient descent đây là một trong những cái thuật toán mà rất là hiệu quả trong cái việc là tìm một cái tham số thêta sao cho cái độ lỗi này là nhỏ nhất và khi cái công việc này mà đã giải quyết rồi thì từ nay trở về sau chúng ta chỉ quan tâm đến 2 cái công việc đầu tiên đó là thiết kế cái hàm mô hình và thiết kế cái hàm lỗi đầu tiên đó chính là chúng ta sẽ vẽ một cái biểu đồ một cái sơ đồ để minh họa cho một cái hàm lỗi L thêta trong trường hợp này XI thì chúng ta sẽ không xem xét nữa tại vì XI là các cái dữ kiện đầu vào đóng góp trong cái việc là hình thành cái hàm lỗi L thêta thì ở đây chúng ta sẽ có cái trục này là trục thêta rồi và chúng ta sẽ chọn một cái hàm lỗi tương đối là đơn giản còn trong trường hợp mà hàm lỗi phức tạp thì chúng ta sẽ bàn thêm sau thì hàm này chúng ta sẽ có một cái nhận định đó là tại một cái vị trí bất kỳ"
        },
        {
          "index": 7,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 300,
          "end_time": 370,
          "text": "thì ở đây chúng ta sẽ có cái trục này là trục thêta rồi và chúng ta sẽ chọn một cái hàm lỗi tương đối là đơn giản còn trong trường hợp mà hàm lỗi phức tạp thì chúng ta sẽ bàn thêm sau thì hàm này chúng ta sẽ có một cái nhận định đó là tại một cái vị trí bất kỳ tại một cái vị trí bất kỳ thêta ở đây thì chúng ta sẽ có một cái nhận định như thế này dấu của cái đạo hàm thì nó sẽ ngược hướng với lại cái điểm cực tiểu thì điểm cực tiểu của chúng ta trong trường hợp này chính là cái điểm này và nếu như tại cái vị trí thêta này chúng ta thấy điểm cực tiểu nó nằm ở bên tay phải tức là lẽ ra chúng ta sẽ phải đi về phía tay phải để tìm đến được cái điểm cực tiểu thì trong trường hợp này đạo hàm nó sẽ hướng như thế nào? thì trong trường hợp này đạo hàm nó sẽ hướng như thế nào? chúng ta thấy tại cái vị trí thêta này thì cái đồ thị của cái hàm số là đang dốc xuống chúng ta thấy tại cái vị trí thêta này thì cái đồ thị của cái hàm số là đang dốc xuống nó đang đổi dốc tức là đạo hàm của mình là âm mà đạo hàm âm tức là hướng của nó sẽ hướng về phía bên tay trái trong khi đó cái điểm cực tiểu thì nó lại nằm bên phía tay phải như vậy ở đây nó đang ngược hướng với lại cái hướng của cái điểm cực tiểu"
        },
        {
          "index": 8,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 347,
          "end_time": 415,
          "text": "chúng ta thấy tại cái vị trí thêta này thì cái đồ thị của cái hàm số là đang dốc xuống chúng ta thấy tại cái vị trí thêta này thì cái đồ thị của cái hàm số là đang dốc xuống nó đang đổi dốc tức là đạo hàm của mình là âm mà đạo hàm âm tức là hướng của nó sẽ hướng về phía bên tay trái trong khi đó cái điểm cực tiểu thì nó lại nằm bên phía tay phải như vậy ở đây nó đang ngược hướng với lại cái hướng của cái điểm cực tiểu bây giờ chúng ta sẽ xét cái tình huống thêta nó nằm ở phía bên tay bên đây thì cái điểm cực tiểu nó nằm ở bên phía tay trái của thêta như vậy thì lẽ ra chúng ta sẽ phải đi về hướng này để hướng đến cái điểm cực tiểu thì trong trường hợp này chúng ta thấy cái hàm của mình nó đang đồng biến nó đang đi lên như vậy đạo hàm của mình sẽ là dương mà đạo hàm dương tức là cái hướng của đạo hàm nó sẽ hướng về hướng này hướng về tay phải như vậy ở trong trường hợp này thì đạo hàm cũng ngược hướng với lại hướng của điểm cực tiểu do đó chúng ta sẽ có một cái bước để cập nhật cái thêta này đó là chúng ta sẽ di chuyển ngược hướng với lại hướng của đạo hàm và nó thể hiện ở cái dấu trừ"
        },
        {
          "index": 9,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 398,
          "end_time": 460,
          "text": "hướng về tay phải như vậy ở trong trường hợp này thì đạo hàm cũng ngược hướng với lại hướng của điểm cực tiểu do đó chúng ta sẽ có một cái bước để cập nhật cái thêta này đó là chúng ta sẽ di chuyển ngược hướng với lại hướng của đạo hàm và nó thể hiện ở cái dấu trừ còn đây là ký hiệu của đạo hàm đây là ký hiệu của đạo hàm và chúng ta sẽ đi ngược hướng thì đây là cái phiên bản đầu tiên thế thì bây giờ nó sẽ có một cái vấn đề nảy sinh đó là nếu như cái giá trị của đạo hàm nó lớn nó quá lớn thì sao ví dụ như chúng ta có cái điểm theta nằm ở đây và tại cái vị trí này cái độ dốc của nó nó rất là lớn do cái giá trị của đạo hàm có thể là một cái vector nó rất là lớn như thế này giá trị đại số của nó nó lớn như thế này và nếu như chúng ta cập nhật chúng ta sẽ lấy cái theta trừ cho cái giá trị đạo hàm này trừ cho cái giá trị đạo hàm này thì đâu đó theta nó sẽ nhảy qua bên đây"
        },
        {
          "index": 10,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 448,
          "end_time": 509,
          "text": "giá trị đại số của nó nó lớn như thế này và nếu như chúng ta cập nhật chúng ta sẽ lấy cái theta trừ cho cái giá trị đạo hàm này trừ cho cái giá trị đạo hàm này thì đâu đó theta nó sẽ nhảy qua bên đây nó sẽ nhảy qua bên đây và như vậy thì nó đã vượt qua cái giá trị điểm cực tiểu như vậy thì ở đây chúng ta gọi là theta theta nó đã bị được cập nhật và đi quá đà nó đi quá cái điểm cực tiểu của mình nó nằm ở đây thế thì để đảm bảo cho cái việc cập nhật nó không có bị đi quá đà thì chúng ta sẽ nhân với một cái hệ số learning rate alpha thì cái này hiểu một cách nôn na đó là đi chậm nhưng mà chắc và lúc này thì chúng ta sẽ có cái công thức cập nhật nó là theta thì sẽ là bằng theta trừ cho alpha nhân cho đạo hàm thì đây là cái công thức cập nhật cho cái việc cập nhật cái cái nam số thay vì nó nhảy quá đà qua đây thì nó sẽ nhảy với một cái đại lượng đủ nhỏ"
        },
        {
          "index": 11,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 498,
          "end_time": 560,
          "text": "thì sẽ là bằng theta trừ cho alpha nhân cho đạo hàm thì đây là cái công thức cập nhật cho cái việc cập nhật cái cái nam số thay vì nó nhảy quá đà qua đây thì nó sẽ nhảy với một cái đại lượng đủ nhỏ rồi sau đó lại nhảy một cái đại lượng đủ nhỏ hướng về cái điểm cực tiểu như vậy bây giờ chúng ta sẽ hãy nảy sinh ra thêm một cái vấn đề tiếp theo đó là cập nhật cho đến khi nào thì dừng cập nhật theta cho đến khi nào thì dừng theta ban đầu nó nằm ở đây và chúng ta sẽ cập nhật cập nhật nhưng mà cập nhật cho đến khi nào thì dừng cập nhật cho đến khi nào thì chúng ta quan sát là khi theta mà càng tiến gần đến cái điểm cực tiểu thì cái độ dốc của hàm này nó sẽ càng càng thấp độ dốc sẽ càng thấp và đỉnh điểm là khi nó và nó khi đạt đến được cái điểm cực tiểu thì cái độ dốc của mình trong trường hợp này nó chính là bằng 0 độ dốc của mình sẽ bằng 0 nhưng mà khi mà tiến được đến 0 này thì cái bước nhảy của nó gần như là rất là thấp"
        },
        {
          "index": 12,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 549,
          "end_time": 607,
          "text": "thì cái độ dốc của mình trong trường hợp này nó chính là bằng 0 độ dốc của mình sẽ bằng 0 nhưng mà khi mà tiến được đến 0 này thì cái bước nhảy của nó gần như là rất là thấp nó gần như là nó không di chuyển do đó thì chúng ta sẽ có một cái gọi là chúng ta sẽ có một cái điểm dừng giải pháp số 1 đó là chúng ta sẽ dừng khi cái độ dốc gần như không còn hoặc là khi đạo hàm nó đủ nhỏ tức là chúng ta sẽ dừng tại cái vị trí như thế này cái độ dốc này nó bé hơn một cái ngưỡng epsilon thì chúng ta sẽ dừng còn cái giải pháp thứ 2 đó chính là chúng ta sẽ dừng khi đạt được một cái số vòng lặp nhất định khi mà chúng ta thấy là cái điểm cái việc mà lặp đi lặp lại này nó tốn quá nhiều nha nó tốn quá nhiều thời gian thì chúng ta sẽ set cố định một cái số lần lặp cố định đủ lớn và khi chúng ta đạt được cái số vòng lặp đó rồi thì chúng ta sẽ dừng như vậy thì chúng ta sẽ có hai cái giải pháp để mà kết thúc cái quá trình cập nhật tham số theta"
        },
        {
          "index": 13,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 599,
          "end_time": 660,
          "text": "và khi chúng ta đạt được cái số vòng lặp đó rồi thì chúng ta sẽ dừng như vậy thì chúng ta sẽ có hai cái giải pháp để mà kết thúc cái quá trình cập nhật tham số theta rồi và như vậy thì các cái bước lặp đi lặp lại lặp đi lặp lại theta thì càng về sau chúng ta sẽ theta nó sẽ càng tiến về cái điểm cực tiểu thì đây chính là cái thuật toán và chúng ta sẽ có một số cái điểm khởi tạo đầu tiên chính là cái điểm theta zero điểm theta zero thứ hai đó là chúng ta sẽ có một cái ngưỡng chúng ta sẽ có một cái ngưỡng để cho biết là khi đạt được đến cái độ dốc như thế nào đó thì chúng ta sẽ kết thúc cái quá trình lặp thì thông thường hai cái tham số này sẽ là các cái siêu tham số ngoài ra thì chúng ta sẽ còn có thêm một cái tham số nữa đó là learning rate tức là cái hệ số mà vừa nãy mình nói đó là chậm và chắc đó hệ số learning rate này khi chúng ta không biết là nó là khoảng bao nhiêu thì thông thường cái tham số mặc định chúng ta có thể sử dụng đó là 10 4 trừ 4"
        },
        {
          "index": 14,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 648,
          "end_time": 710,
          "text": "tức là cái hệ số mà vừa nãy mình nói đó là chậm và chắc đó hệ số learning rate này khi chúng ta không biết là nó là khoảng bao nhiêu thì thông thường cái tham số mặc định chúng ta có thể sử dụng đó là 10 4 trừ 4 tức là khoảng 0.0001 khi chúng ta không biết cái mô hình của mình nó nên sử dụng tham số learning rate alpha là bao nhiêu thì chúng ta chọn cái tham số này tuy nhiên trong cái bài toán này thì chúng ta có một chút kinh nghiệm thì chúng ta có thể chọn cái tham số learning rate nó đủ nhỏ thôi ví dụ như là khoảng 0.01 thôi là được rồi chứ còn nếu mà do cái tham số learning rate alpha này quá nhỏ thì cái thực toán của mình nó sẽ chạy chậm, nó sẽ quá chậm rồi như vậy thì chúng ta sẽ xong cái bước khởi tạo đó là một cái giá trị theta 0 đó là cái điểm bắt đầu của theta learning rate và ngưỡng dừng ngưỡng dừng epsilon thì chúng ta thường cũng chọn một cái ngưỡng đủ nhỏ ví dụ như là 10 4 trừ 5 ví dụ vậy và chúng ta sẽ đến cái quá trình lập"
        },
        {
          "index": 15,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 696,
          "end_time": 760,
          "text": "đó là cái điểm bắt đầu của theta learning rate và ngưỡng dừng ngưỡng dừng epsilon thì chúng ta thường cũng chọn một cái ngưỡng đủ nhỏ ví dụ như là 10 4 trừ 5 ví dụ vậy và chúng ta sẽ đến cái quá trình lập thì cái quá trình lập chúng ta sẽ thực hiện 2 cái công việc thôi đó là cập nhật cái theta theta sẽ là bằng theta trừ cho alpha nhân cho đạo hàm của hàm loss và chúng ta sẽ dừng khi mà cái đạo hàm này đủ nhỏ thì đây là cho cái giải pháp số 1 đây chính là cái giải pháp số 1 còn cho cái giải pháp số 2 thì chúng ta có thể viết 1 cái vòng for for i in 1 cái rank ví dụ như chúng ta cho nó lập 100 lần rồi thì chúng ta chỉ việc cập nhật theta sẽ là bằng theta trừ cho alpha nhân cho đạo hàm là xong thì đây là cái giải pháp thứ 2 đây chính là cái giải pháp thứ 2 và đây là cái giải thuật gradient descent trong trường hợp nếu như"
        },
        {
          "index": 16,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 748,
          "end_time": 809,
          "text": "thì đây là cái giải pháp thứ 2 đây chính là cái giải pháp thứ 2 và đây là cái giải thuật gradient descent trong trường hợp nếu như cái hàm của mình nó phức tạp hơn thì chúng ta sẽ thấy là nó có nhiều cái điểm cực tiểu thì ở đây chúng ta sẽ lấy 1 cái trường hợp đó là chúng ta có 2 cái điểm cực tiểu thì điều gì sẽ xảy ra nếu như chúng ta khởi tạo ngay tại cái vị trí này nếu như chúng ta khởi tạo cái giá trị theta 0 tại đây thì khi giả sử chúng ta nhìn cái này dưới góc độ là 1 cái góc nhìn vật lý chúng ta sẽ có 1 cái viên bi đặt ở đây và khi chúng ta thả cái viên bi này ra nó sẽ từ từ nó rớt xuống khi nó chạm được đến cái điểm cực tiểu của bộ này nó sẽ dừng tại sao nó dừng? tại vì khi chạm được đến cái điểm cực tiểu của bộ này  thì cái đạo hàm của mình nó sẽ sắp xỉ với số 0 mà khi đạo hàm sắp xỉ với 0 thì cái bước nhảy của mình lúc này nó sẽ là"
        },
        {
          "index": 17,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 799,
          "end_time": 860,
          "text": "nó sẽ sắp xỉ với số 0 mà khi đạo hàm sắp xỉ với 0 thì cái bước nhảy của mình lúc này nó sẽ là gần bằng 0 bước nhảy của mình nó sẽ là bằng 0 tức là theta sẽ là bằng theta trừ 0 tức là theta không cập nhật gì nữa tức là nó sẽ đến đây nó sẽ dừng thì dưới cái góc nhìn vật lý nếu như chúng ta có nhiều cái điểm cực tiểu mà dùng cái điểm cực tiểu mà dùng cái phiên bản này thì rõ ràng là không ổn như vậy thì giải pháp đầu tiên đó là chúng ta sẽ chạy nhiều lần và chúng ta sẽ lấy cái giá trị nhỏ nhất ví dụ như chúng ta khởi tạo cái giá trị theta 0 tại đây rồi sau đó chúng ta lại khởi tạo cái giá trị theta 0 tại đây với cái giá trị theta 0 tại đây thì chúng ta sẽ cập nhật và nó sẽ dừng tại đây như vậy thì chúng ta sẽ có 2 cái lần chạy và chúng ta sẽ lấy cái giá trị nào mà nhỏ nhất và chúng ta sẽ lấy cái giá trị nào thì cái điểm yếu của cái phương pháp này đó chính là nó sẽ tốn cái tài nguyên"
        },
        {
          "index": 18,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 849,
          "end_time": 910,
          "text": "và nó sẽ dừng tại đây như vậy thì chúng ta sẽ có 2 cái lần chạy và chúng ta sẽ lấy cái giá trị nào mà nhỏ nhất và chúng ta sẽ lấy cái giá trị nào thì cái điểm yếu của cái phương pháp này đó chính là nó sẽ tốn cái tài nguyên do chúng ta phải lặp đi lặp lại cái quá trình khởi tạo ngẫu nhiên này nhiều rần giải pháp thứ 2 được sử dụng rất là nhiều trong các cái framework hiện nay trong các cái deep learning framework hiện nay đó chính là chúng ta sẽ sử dụng momentum hay còn gọi là quán tính thì khi giả sử như chúng ta khởi tạo tại cái vị trí này tức là tại cái điểm cực tiểu gần cái điểm cực tiểu mà không phải là cái điểm cực tiểu nhỏ nhất thì chúng ta sẽ có cái điểm cực tiểu thì khi chúng ta cập nhật và chúng ta đến cái vị trí này thì nó vẫn còn một cái độ gọi là cái độ quán tính nó sẽ giúp cho chúng ta thoát ra thoát ra khỏi cái vị trí này và để rớt xuống cái vị trí điểm cực tiểu nhỏ hơn thì đây chính là cái giải pháp dùng momentum và một trong những cái thực toán mà có khai thác cái"
        },
        {
          "index": 19,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 899,
          "end_time": 961,
          "text": "và để rớt xuống cái vị trí điểm cực tiểu nhỏ hơn thì đây chính là cái giải pháp dùng momentum và một trong những cái thực toán mà có khai thác cái yếu tố về momentum này chính là thực toán Adam và thực toán Adam thì đã được cài đặt trong các cái thư viện như là TensorFlow và Python cài đặt rất là đầy đủ và sử dụng rất là dễ dàng như vậy thì từ nay về sau nếu như chúng ta nói đến cái việc là tìm cái tham số theta sao cho cái hàm loss L theta nhỏ nhất thì chúng ta sẽ nghĩ ngay đến cái giải pháp đó là sử dụng thực toán Adam này giống như là một cái trick một cái mẹo để cho chúng ta cứ khi mà chúng ta tìm cái giá trị nhỏ nhất của một cái hàm thì chúng ta nghĩ ngay đến cái thực toán Adam thì chi tiết cái cách mà sử dụng thực toán Adam như thế nào thì chúng ta sẽ trình bày trong cái phần thực hành sau như vậy thì với cái mô hình"
        },
        {
          "index": 20,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 949,
          "end_time": 1012,
          "text": "thì chúng ta nghĩ ngay đến cái thực toán Adam thì chi tiết cái cách mà sử dụng thực toán Adam như thế nào thì chúng ta sẽ trình bày trong cái phần thực hành sau như vậy thì với cái mô hình máy học tổng quát này chúng ta nhắc lại 3 cái công việc cần phải thực hiện đó là thiết kế cái hàm dự đoán cái hàm mô hình máy học F theta x chúng ta sẽ phải thiết kế lại cái hàm lỗi L theta xa chúng ta sẽ phải thiết kế lại cái hàm lỗi L theta xa  và chúng ta sẽ phải tìm cái theta sao cho cái hàm lỗi này là nhỏ nhất và chúng ta có một cái chú ý đó là các cái thư viện deep learning hiện tại nó đều đã giải quyết rất tốt cái công việc số 3 này rồi như vậy cái công việc số 3 này chúng ta sẽ không còn quan tâm này nữa và khi dùng thì chúng ta sẽ nghĩ ngay đến cái giải thuật cái thư viện cái hàm đó chính là Adam để mà Adam optimizer nhớ đừng quên đăng ký kênh để nhận thêm video mới nhé  để nhận thêm video mới nhé và như vậy thì từ nay về sau chúng ta chỉ còn giải quyết 2 cái công việc thôi"
        },
        {
          "index": 21,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 998,
          "end_time": 1052,
          "text": "đó chính là Adam để mà Adam optimizer nhớ đừng quên đăng ký kênh để nhận thêm video mới nhé  để nhận thêm video mới nhé và như vậy thì từ nay về sau chúng ta chỉ còn giải quyết 2 cái công việc thôi đó là thiết kế cái hàm dự đoán F theta x và thiết kế cái hàm lỗi L theta xa và thiết kế cái hàm lỗi L theta xa và chúng ta sẽ thiết kế thì tùy theo cái tính chất của y thì tùy theo cái tính chất của y nó phụ thuộc như thế nào với x thì chúng ta sẽ có những cái cách thiết kế khác nhau  ví dụ đối với cái bài toán tiến tính đối với cái bài toán hồi quy thì chúng ta sẽ thiết kế theo một cách khác nhau đối với cái bài toán mà phân lớp chúng ta sẽ thiết kế theo một cái cách khác và đối với những cái bài toán phi tuyến thì chúng ta sẽ thiết kế theo một cái cách khác nữa đó thì vậy tùy vào cái tính chất của cái xa này để chúng ta sẽ thiết kế 2 cái hàm này để chúng ta sẽ thiết kế 2 cái hàm này"
        },
        {
          "index": 22,
          "video_id": "Chương 2_GdKIVY6CsTw",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 1： Mô hình học tổng quát",
          "video_url": "https://youtu.be/GdKIVY6CsTw",
          "start_time": 1048,
          "end_time": 1052,
          "text": "để chúng ta sẽ thiết kế 2 cái hàm này để chúng ta sẽ thiết kế 2 cái hàm này"
        }
      ]
    },
    {
      "video_id": "Chương 2_m8uqtMEg8-E",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giải thích và triển khai **mô hình hồi quy tuyến tính (linear regression)** gồm ba bước chính: (1) thiết kế hàm dự đoán, (2) thiết kế hàm lỗi (loss), (3) tìm tham số θ sao cho loss nhỏ nhất (tối ưu hóa) — với phương pháp gradient descent/ADAM khi cần. [1]\n\n- Các khái niệm sẽ được đề cập:\n  - Mối quan hệ *tuyến tính* giữa input x và output y (đồng biến / nghịch biến). [1][2]\n  - Cách biểu diễn mô hình tuyến tính với tham số θ (θ0 là bias, θ1 là hệ số slope). [2][3]\n  - Hàm lỗi: *mean squared error* (MSE) với hệ số 1/(2n). [4][10]\n  - Tính đạo hàm của loss theo θ0, θ1 và cập nhật bằng gradient descent (và gợi ý dùng ADAM / auto-diff trong framework). [11][13][14]\n\n## 2. Các điểm chính (Main Points)\n\n### A. Khi nào dùng mô hình tuyến tính\n- *Tuyến tính* nghĩa là khi x thay đổi thì y thay đổi theo tỉ lệ (có thể tăng — đồng biến — hoặc giảm — nghịch biến). [1][2]\n\n### B. Hàm dự đoán (Predictor / Hypothesis)\n- Dạng tuyến tính đơn giản với một biến:\n  - f_θ(x) = θ1 * x + θ0  \n  - Ký hiệu θ1 tương tự hệ số a trong y = ax + b, θ0 là bias (tương đương b). [2][3]\n- Vai trò của bias θ0: cho phép đường thẳng dịch chuyển (không bắt buộc đi qua gốc), biểu diễn thành phần không phụ thuộc vào x. [3]\n\n### C. Hàm lỗi (Loss function) — lựa chọn MSE và động lực\n- Chọn hàm lỗi: L(θ) = (1 / (2n)) * Σ_{i=1..n} (f_θ(x_i) - y_i)^2. [4][11]\n- Tại sao không dùng tổng sai số (Σ (f - y))?\n  - Vì các sai số âm và dương có thể triệt tiêu nhau (ví dụ: 3, -2, 5, -6 ⇒ tổng = 0) dẫn tới kết luận sai lệch. [5][6]\n- Tại sao không dùng tổng trị tuyệt đối (Σ |f - y|)?\n  - Mặc dù tránh triệt tiêu dấu, nhưng đạo hàm của |x| không liên tục tại 0 (nhận giá trị ±1), làm quá trình tối ưu bằng gradient khó xử lý. [7]\n- Vì vậy chọn MSE (sai số bình phương) vừa tránh triệt tiêu dấu, vừa có đạo hàm liên tục và đồ thị mượt để dùng gradient-based optimization. [7][4]\n- Tại sao có hệ số 1/(2n)?\n  - Chia trung bình (1/n) để loss không phụ thuộc vào kích thước tập dữ liệu (ví dụ tổng lỗi lớn khi n lớn sẽ gây hiểu nhầm về “độ lớn” lỗi). [8][9]\n  - Hệ số 1/2 để khi lấy đạo hàm, hệ số 2 từ bình phương sẽ triệt tiêu, khiến công thức đạo hàm gọn hơn. [10]\n\n### D. Tính đạo hàm của loss theo các tham số θ\n- Với L(θ0, θ1) = (1 / (2n)) Σ (θ1 x_i + θ0 - y_i)^2, ta có:\n  - ∂L/∂θ0 = (1 / n) Σ (θ1 x_i + θ0 - y_i). [11][12]\n  - ∂L/∂θ1 = (1 / n) Σ (θ1 x_i + θ0 - y_i) * x_i. [13]\n- Phân tích ngắn:\n  - Dùng quy tắc đạo hàm của tổng và quy tắc chuỗi; hệ số 2 từ bình phương bị triệt tiêu bởi 1/2, đạo hàm của θ0 theo θ0 bằng 1. [12][13]\n\n### E. Tối ưu hóa (Gradient Descent) và cập nhật tham số\n- Thuật toán gradient descent (theo video) — khái quát 3 bước:\n  1. Khởi tạo θ0, θ1 (ngẫu nhiên) và siêu tham số: learning rate α (ví dụ 0.01), ngưỡng dừng ε (ví dụ 0.001). [14][15]\n  2. Lặp: tính gradient ∂L/∂θ0, ∂L/∂θ1 rồi cập nhật:\n     - θ0 := θ0 - α * ∂L/∂θ0  \n     - θ1 := θ1 - α * ∂L/∂θ1  [15]\n  3. Dừng khi các đạo hàm (gradient) đủ nhỏ (nhỏ hơn ε) hoặc đạt điều kiện dừng khác. [15][16]\n- Ghi chú: trong thực hành sâu hơn có thể dùng optimizer nâng cao như ADAM; framework deep learning có sẵn công cụ auto-diff để tính đạo hàm và tối ưu tự động. Video nhấn mạnh học cách tính tay để hiểu, sau đó dùng ADAM/framework cho thực nghiệm. [13][14]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa mối quan hệ tuyến tính:\n  - Quan hệ đồng biến (x tăng ⇒ y tăng) hoặc nghịch biến (x tăng ⇒ y giảm). [1][2]\n\n- Ví dụ về sai số triệt tiêu khi dùng tổng (không bình phương):\n  - Sai số mẫu: 3, -2, 5, -6 ⇒ tổng = 0, nhưng mô hình vẫn có sai số đáng kể; điều này cho thấy tổng sai số không hợp lý làm loss. [5][6]\n\n- Ví dụ minh họa về ý nghĩa chia trung bình trong loss:\n  - Nếu tổng lỗi (sum of errors) là 1.000 tỷ nhưng tính trên 1 triệu căn nhà ⇒ trung bình trên 1 căn ~0.001 tỷ (1 triệu) — là con số hợp lý; nếu tổng 1.000 tỷ nhưng chỉ với 10 căn ⇒ trung bình 100 tỷ/căn — quá lớn. Việc chia trung bình giúp so sánh lỗi hợp lý giữa các tập có kích thước khác nhau. [8][9][10]\n\n- Ứng dụng thực tế:\n  - Dự đoán giá nhà (ví dụ minh họa ở trên) — một bài toán điển hình dùng linear regression khi mối quan hệ giữa feature và target xấp xỉ tuyến tính. [8][9]\n\n- Trường hợp sử dụng:\n  - Khi dữ liệu có mối quan hệ tuyến tính rõ ràng giữa biến độc lập và phụ thuộc, và khi ta muốn mô hình đơn giản, dễ diễn giải, linear regression là lựa chọn phù hợp. [1][2]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Ba bước chính khi thiết kế mô hình hồi quy tuyến tính: chọn hàm dự đoán (θ1 x + θ0), chọn hàm lỗi (MSE với 1/(2n)), tối ưu θ bằng gradient descent/ADAM. [1][3][11][15]\n  - Lý do chọn MSE: tránh triệt tiêu dấu, có đạo hàm liên tục, phù hợp cho tối ưu bằng gradient. [5][7][4]\n  - Công thức đạo hàm và cập nhật gradient giúp tìm θ tối ưu; trong thực tế dùng auto-diff và optimizer như ADAM sẽ tiện lợi hơn sau khi hiểu bản chất. [12][13][14]\n\n- Tầm quan trọng:\n  - Linear regression là nền tảng cơ bản cho nhiều mô hình phức tạp hơn trong Deep Learning; hiểu rõ công thức, động lực chọn loss và quá trình tối ưu là bước cần thiết trước khi học các mô hình nâng cao. [1][13][14]\n\n- Liên hệ với các bài giảng khác:\n  - Video liên kết với phần bài học tổng quát về mô hình máy học (khái niệm predictor, loss, gradient descent) và sẽ tiếp tục gợi ý dùng ADAM / framework trong các phần sau. [1][14]\n\n",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 2_m8uqtMEg8-E",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_1： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/m8uqtMEg8-E",
          "start_time": 2,
          "end_time": 62,
          "text": "mô hình tiếp theo chúng ta sẽ tìm hiểu đó chính là mô hình hồi quy tiến tính hay là linear regression thì chúng ta sẽ nhắc lại cái mô hình máy học tổng quát với cái dữ kiện đầu vào x giá trị dự đoán y cả và chúng ta mong muốn sắp xỉ biến đệ giá trị thật thì chúng ta có 3 cái công việc cần phải làm khi thiết kế một cái mô hình đầu tiên đó là thiết kế cái hàm dự đoán hai đó là chúng ta sẽ thiết kế cái hàm độ lỗi và ba đó là đi tìm tham số theta sao cho cái hàm độ lỗi này thành x và công việc này thì đã giải được bằng tập toán gradient descent thế thì ở đây chúng ta có một cái nhấn mạnh đó là tùy vào cái tính chất của cái cặp dữ liệu xy để chúng ta thiết kế hai cái hàm này thế thì chúng ta sẽ xem xét đến cái tình huống đầu tiên đó là giá trị đầu ra y nó có một cái mối quan hệ tuyến tính với cái giá trị đầu vào x thì thế nào gọi là tuyến tính tuyến tính có nghĩa là khi x tăng y S hoặc là khi x thay đổi x tăng"
        },
        {
          "index": 2,
          "video_id": "Chương 2_m8uqtMEg8-E",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_1： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/m8uqtMEg8-E",
          "start_time": 49,
          "end_time": 110,
          "text": "với cái giá trị đầu vào x thì thế nào gọi là tuyến tính tuyến tính có nghĩa là khi x tăng y S hoặc là khi x thay đổi x tăng thì y nó sẽ tăng hoặc là y sẽ giảm ví dụ như đây là cái quan hệ đồng biến chúng ta sẽ có cái mối quan hệ gọi là nghịch biến khi x tăng thì y nó lại có xu hướng là y giảm xuống thì trong trường hợp này nó gọi là mối quan hệ tuyến tính thì bước một đó là chúng ta sẽ thiết kế cái hàm dự đoán thì trong trường hợp mà y có mối quan hệ tuyến tính với x thì chúng ta sẽ có một cái hàm dự đoán trong trường hợp mà y có mối quan hệ tuyến tính với x thì chúng ta có một cái đường thẳng có một cái đường thẳng nó quay như thế này và cái phương trình đường thẳng mà chúng ta đã học hồi xưa đó chính là phương trình y bằng ax cộng b đó là phương trình hồi xưa hồi cấp 2 chúng ta học thì trong trường hợp này chúng ta sẽ sử dụng các hệ thống ký hiệu đó là theta thì thay vì chúng ta để là a thì chúng ta sẽ để là theta 1"
        },
        {
          "index": 3,
          "video_id": "Chương 2_m8uqtMEg8-E",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_1： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/m8uqtMEg8-E",
          "start_time": 99,
          "end_time": 161,
          "text": "đó là phương trình hồi xưa hồi cấp 2 chúng ta học thì trong trường hợp này chúng ta sẽ sử dụng các hệ thống ký hiệu đó là theta thì thay vì chúng ta để là a thì chúng ta sẽ để là theta 1 và b thì chúng ta sẽ để là theta 0 thì vậy chúng ta sẽ có cái công thức cho cái hàm dự đoán f theta x với cái mẫu dữ liệu thứ y ở đây chúng ta sẽ có nhiều mẫu dữ liệu ở đây chúng ta sẽ có xy và yy rồi thì chúng ta sẽ có công thức như thế này thì ở đây đó chính là cái thành phần nó gọi là bias thành phần bias này có cái tác dụng đó là để cho cái giá trị dự đoán không phải lúc nào nó cũng chỉ phụ thuộc vào cái miếng x nó sẽ có những cái trường hợp mà nó sẽ độc lập với miếng x thì nó sẽ biểu diễn bởi cái bias này bias nó sẽ thể hiện cho những cái biểu diễn bởi cái bias này những cái thành phần mà không có phụ thuộc với cái biến đồ vào và với cái cách biểu diễn này thì cái đường thẳng của chúng ta cũng sẽ rất là linh động không nhất thiết nó phải đi qua góc tạo độ"
        },
        {
          "index": 4,
          "video_id": "Chương 2_m8uqtMEg8-E",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_1： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/m8uqtMEg8-E",
          "start_time": 149,
          "end_time": 212,
          "text": "những cái thành phần mà không có phụ thuộc với cái biến đồ vào và với cái cách biểu diễn này thì cái đường thẳng của chúng ta cũng sẽ rất là linh động không nhất thiết nó phải đi qua góc tạo độ nó có thể đi qua những cái đường thẳng bất kỳ nó sẽ đi qua bất kỳ những cái vị trí nào không nhất thiết phải đi qua cái góc tạo độ tạo ra cái sự tự do cho cái đường thẳng của mình bước thứ hai đó là chúng ta sẽ thiết kế cái hàm lỗi thiết kế cái hàm lỗi thì công thức cho cái hàm lỗi trong trường hợp này chúng ta sẽ sử dụng công thức đó là 1 phần 2n nhân cho tổng của y chạy từ 1 đến n của giá trị dự đoán trừ cho cái giá trị thực tế trừ cho cái giá trị thực tế rồi tất cả bình phương thế thì ở đây chúng ta sẽ đặt ra một cái câu hỏi đó là tại sao nó có một cái công thức có vẻ phức tạp như thế này công thức này là trung bình sai số bình phương tại sao nó lại có 1 cái công thức có vẻ phức tạp như thế này thì đầu tiên chúng ta sẽ xét đến những cái phiên bản"
        },
        {
          "index": 5,
          "video_id": "Chương 2_m8uqtMEg8-E",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_1： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/m8uqtMEg8-E",
          "start_time": 198,
          "end_time": 261,
          "text": "tại sao nó có một cái công thức có vẻ phức tạp như thế này công thức này là trung bình sai số bình phương tại sao nó lại có 1 cái công thức có vẻ phức tạp như thế này thì đầu tiên chúng ta sẽ xét đến những cái phiên bản ngây thơ nhất khi chúng ta thiết kế cái hàm lỗi này đó là nếu như chúng ta sử dụng y ngã y trừ cho y thì sao tức là chúng ta sẽ không lấy bình phương mà chúng ta sẽ để là dấu trừ thôi thì nếu chúng ta chọn cái giải pháp này á thì nó sẽ nảy sinh cái vấn đề đó là các cái độ lỗi này khi mà chúng ta tính tổng lại á có khả năng nó sẽ triệt tiên bằng nhau lấy ví dụ với mẫu dữ liệu đầu tiên cái sai số này sai số y ngã trừ y này là bằng 3 với cái mẫu dữ liệu thứ 2 sai số đó là trừ 2 với cái mẫu dữ liệu thứ 3 sai số là bằng 5 và với cái mẫu dữ liệu thứ 4 sai số đó là ví dụ như là trừ 6"
        },
        {
          "index": 6,
          "video_id": "Chương 2_m8uqtMEg8-E",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_1： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/m8uqtMEg8-E",
          "start_time": 249,
          "end_time": 312,
          "text": "với cái mẫu dữ liệu thứ 3 sai số là bằng 5 và với cái mẫu dữ liệu thứ 4 sai số đó là ví dụ như là trừ 6 thì khi chúng ta tính tổng các cái sai số này lại thì chúng ta sẽ thấy như thế nào? tổng 3 trừ 2 cộng 5 trừ 6 thì tổng sai số là bằng 0 như vậy thì với cái việc dùng cái công thức tổng như thế này không có cái bình phương thì nó sẽ dẫn đến là các cái con số âm và dương nó sẽ triệt tiêu bằng nhau dẫn đến là mặc dù cái hệ thống nó đáng sai nhưng mà cuối cùng cái tổng sai số nó là bằng 0 đó là một cái điều vô lý đó là một cái điều vô lý thì đây là cho cái phiên bản đầu tiên phiên bản thứ 2 đó là tại sao chúng ta không sử dụng cái công thức là tổng của trị tiệt đối y ngã y trừ cho y y mà lại sử dụng cái hàm bình phương này thì trong trường hợp này thì nó vẫn thỏa mãn là nếu như chúng ta dùng trị tiệt đối"
        },
        {
          "index": 7,
          "video_id": "Chương 2_m8uqtMEg8-E",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_1： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/m8uqtMEg8-E",
          "start_time": 297,
          "end_time": 362,
          "text": "đó là tại sao chúng ta không sử dụng cái công thức là tổng của trị tiệt đối y ngã y trừ cho y y mà lại sử dụng cái hàm bình phương này thì trong trường hợp này thì nó vẫn thỏa mãn là nếu như chúng ta dùng trị tiệt đối thì khi chúng ta tính tổng đúng không? khi chúng ta tính tổng thì cái tổng sai số của mình lúc này nó sẽ ra một con số rất là lớn như vậy là nó rất là phù hợp về mặt ý nghĩa bây giờ trong trường hợp này là 16 thì đây là một con số rất là lớn thì đây là một con số rất là phù hợp tuy nhiên nó sẽ bị một cái vấn đề đó là sang cái bước số 3 cái việc mà chúng ta tính đạo hàm cho một cái hàm trị tiệt đối đạo hàm cho một cái hàm trị tiệt đối đó là một cái nhận 2 giá trị hoặc là 1 hoặc là trừ 1 như vậy thì nó sẽ là bằng 1 nếu x dương và bằng trừ 1 nếu x âm như vậy thì cái này nó sẽ tạo cho cái việc là cái hàm của chúng ta là 1 nhưng mà tronghCS chúng ta cài đạo hàm của chúng ta nó không liên tục và không liên tục thì dẫn đến là cái quá trình tính toán là sẽ khức tạp hơn"
        },
        {
          "index": 8,
          "video_id": "Chương 2_m8uqtMEg8-E",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_1： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/m8uqtMEg8-E",
          "start_time": 349,
          "end_time": 409,
          "text": "và bằng trừ 1 nếu x âm như vậy thì cái này nó sẽ tạo cho cái việc là cái hàm của chúng ta là 1 nhưng mà tronghCS chúng ta cài đạo hàm của chúng ta nó không liên tục và không liên tục thì dẫn đến là cái quá trình tính toán là sẽ khức tạp hơn do đó thì cái cách thiết kế này nó cũng không phù hợp và như vậy thì từ 2 cái này thì chúng ta sẽ nảy ra đó là chúng ta sẽ dùng 1 công thức đó là tính tổng của các cái sai số bình phương rồi tuy nhiên khi tính tổng các cái sai số bình phương thì tại sao chúng ta lại phải chia trung bình thì nó sẽ nảy sinh một cái vấn đề như thế này nếu như chúng ta không chia trung bình và chúng ta có một cái giá trị độ lỗi ví dụ như chúng ta dự đoán giá nhà với tổng các cái size số của mình đó chính là bằng 1.000 tỷ ví dụ vậy thì câu hỏi đặt ra đó là cái size số 1.000 tỷ này liệu các bạn có dám mua một cái căn nhà mà được dự đoán bởi một cái hệ thống mà có size số là 1.000 tỷ hay không"
        },
        {
          "index": 9,
          "video_id": "Chương 2_m8uqtMEg8-E",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_1： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/m8uqtMEg8-E",
          "start_time": 398,
          "end_time": 460,
          "text": "thì câu hỏi đặt ra đó là cái size số 1.000 tỷ này liệu các bạn có dám mua một cái căn nhà mà được dự đoán bởi một cái hệ thống mà có size số là 1.000 tỷ hay không thì câu trả lời đó chính là không chắc tại vì nếu như 1.000 tỷ này mà chúng ta dự đoán trên 1 triệu căn nhà trên 1 triệu căn nhà thì như vậy trung bình size số trong một căn nhà đó là 0.001 tỷ tức là đây là một con số rất là bé đây là một con số rất là bé nó chỉ khoảng là 1 triệu đồng thôi như vậy thì chúng ta hoàn toàn có thể mua cái căn nhà này đúng không do cái size số rất là bé tuy nhiên nếu tổng số căn nhà trong trường hợp này mà dự đoán là size trên 10 căn nhà thôi thì như vậy là size số trung bình cho một căn nhà trong trường hợp này đó là 100 tỷ thì nếu đoán một cái căn nhà mà size số 100 tỷ thì rõ ràng đây là một con số quá lớn"
        },
        {
          "index": 10,
          "video_id": "Chương 2_m8uqtMEg8-E",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_1： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/m8uqtMEg8-E",
          "start_time": 450,
          "end_time": 510,
          "text": "trên 10 căn nhà thôi thì như vậy là size số trung bình cho một căn nhà trong trường hợp này đó là 100 tỷ thì nếu đoán một cái căn nhà mà size số 100 tỷ thì rõ ràng đây là một con số quá lớn như vậy thì đó là lý do tại sao chúng ta lại phải có chia trung bình, trung bình cộng để khi chúng ta ra được cái hàm chia chúng ta ra được cái giá trị lỗi chúng ta biết được cái lỗi này đó là phù hợp hay không có hợp lý hay không để mà sử dụng ngoài ra ở đây chúng ta sẽ thấy nó có một cái con số 2 tại sao chúng ta lại có cái số 2 ở đây để sau này khi chúng ta tiến hành tính đạo hàm cho cái hàm loss này thì nó sẽ có cái hàm mũ ở đây đúng không thì chúng ta tính đạo hàm thì có số 2 này nó sẽ đem xuống và 2 chia 2 nó sẽ triệt tiêu đi như vậy cái công thức của mình sau này nó sẽ đẹp hơn thì đó là lý do tại sao chúng ta có cái công thức hàm lỗi như trên rồi thì hy vọng là qua các cái phiên bản này chúng ta sẽ hiểu hơn là"
        },
        {
          "index": 11,
          "video_id": "Chương 2_m8uqtMEg8-E",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_1： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/m8uqtMEg8-E",
          "start_time": 499,
          "end_time": 561,
          "text": "có cái công thức hàm lỗi như trên rồi thì hy vọng là qua các cái phiên bản này chúng ta sẽ hiểu hơn là lý do động lực tại sao người ta lại chọn cái hàm lỗi này rồi và sang bước số 3 đó chính là chúng ta sẽ đi tìm theta sao cho cái giá trị l giá hàm giá trị hàm loss là nhỏ nhất và khi này thì chúng ta có công thức là l theta 0 theta 1 theta 0 theta 1 chính là 2 tham số của theta đó là 1 tham số của theta là 1 tham số của theta thì nó sẽ có công thức là bằng trung bình 1 phần 2n nhân cho tổng của cái công thức như sau và cái này chính là cái giá trị y ngã y đó là giá trị dự đoán còn yy ở đây chính là giá trị thực tế rồi và chúng ta sẽ tiến hành đi tính cái đạo hàm này tính đạo hàm của l theo theta 0"
        },
        {
          "index": 12,
          "video_id": "Chương 2_m8uqtMEg8-E",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_1： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/m8uqtMEg8-E",
          "start_time": 551,
          "end_time": 611,
          "text": "còn yy ở đây chính là giá trị thực tế rồi và chúng ta sẽ tiến hành đi tính cái đạo hàm này tính đạo hàm của l theo theta 0 tính đạo hàm của l theo theta 0 tính đạo hàm của l theo theta 0 tính đạo hàm của l theo theta 0  thì chúng ta sẽ thấy là đạo hàm của l này 1 phần 2 n tổng của theta 1 x y cộng cho theta 0 trừ cho y y tất cả bình y chạy từ 1 cho đến n thì đạo hàm của l này thì chúng ta sẽ thấy là đây là hàng số chúng ta sẽ tính đạo hàm của l theo theta 0 thì đối với theta 0 đây là hàng số do đó chúng ta sẽ tính đạo hàm của l theo theta 0 ở dưỡng nguyên 1 phần 2 ener đạo hàm của tổng bằng tổng các cái đạo hàm và đây sẽ là 1 cái hàm hợp đây sẽ là 1 hàm hợp do đó đạo hàm của này thì chúng ta sẽ có là 2 chép cái này xuống đó là theta 1 x y cộng cho theta 0 trừ cho y y sau đó chúng ta sẽ nhân đạo hàm"
        },
        {
          "index": 13,
          "video_id": "Chương 2_m8uqtMEg8-E",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_1： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/m8uqtMEg8-E",
          "start_time": 598,
          "end_time": 660,
          "text": "đây sẽ là 1 hàm hợp do đó đạo hàm của này thì chúng ta sẽ có là 2 chép cái này xuống đó là theta 1 x y cộng cho theta 0 trừ cho y y sau đó chúng ta sẽ nhân đạo hàm của cái phần ruột bên trong đạo hàm của cái phần ruột bên trong theo theta 0 thì đây chính là hàng số đối với theta 0 do đó chúng ta sẽ bỏ và đạo hàm của theta 0 theo theta 0 chính là bằng 1. Như vậy đem cái số 2 này ra ngoài, truyệt tiêu, như vậy thì chúng ta sẽ có cái công thức như trên. Và tương tự như vậy, chúng ta hoàn toàn có thể tính đạo hàng của L theo theta 1 thì bằng trung bình cộng của tổng của theta 1 xa cộng cho theta 0 trừ cho ea tất cả nhân với xa. Thì đây là cái công thức đạo hàng theo theta 1. Thì chúng ta yên tâm đó là với cái bài linear regression này thì chúng ta sẽ còn ngồi tính toán đạo hàng. Nhưng mà như chúng ta có quảng cáo trước đây, các cái deep learning framework nó đã có cái công cụ để giúp cho chúng ta"
        },
        {
          "index": 14,
          "video_id": "Chương 2_m8uqtMEg8-E",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_1： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/m8uqtMEg8-E",
          "start_time": 649,
          "end_time": 710,
          "text": "với cái bài linear regression này thì chúng ta sẽ còn ngồi tính toán đạo hàng. Nhưng mà như chúng ta có quảng cáo trước đây, các cái deep learning framework nó đã có cái công cụ để giúp cho chúng ta tự động tính các cái đạo hàng này và tự động tìm theta để cho cái hàng L là rõ nhất rồi. Và đó đây thì chúng ta tập tập luyện tính đạo hàng thôi để mà sau này chúng ta có thể tiến hành cài đặt và thử nghiệm. Còn cái bước số 3 từ đây trở về sau chúng ta hoàn toàn có thể sử dụng cái dạy thuật ADAM để mà đi tìm giá trị rõ nhất. Rồi, thì ở đây chúng ta sẽ sử dụng cái thuật toán gradient descent đã được học ở trong cái phần về mô hình máy học tổng quát. Chúng ta sẽ có cái bước khởi tạo theta 0 và theta 1 là ngẫu nhiên đồng thời là 2 cái siêu tham số alpha, learning rate alpha và cái tham số dừng epsilon là 2 con số 0. Thì alpha ở đây chúng ta có thể cho là 0.01"
        },
        {
          "index": 15,
          "video_id": "Chương 2_m8uqtMEg8-E",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_1： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/m8uqtMEg8-E",
          "start_time": 697,
          "end_time": 760,
          "text": "theta 0 và theta 1 là ngẫu nhiên đồng thời là 2 cái siêu tham số alpha, learning rate alpha và cái tham số dừng epsilon là 2 con số 0. Thì alpha ở đây chúng ta có thể cho là 0.01 còn epsilon ở đây thì chúng ta có thể cho đó là 0.001 đó là những con số rất là bé và chúng ta sẽ tiến hình lập và lưu ý đó là ở đây chúng ta có 2 tham số theta 0 và theta 1 do đó thì cái bước cập nhật chúng ta sẽ cập nhật trên cả 2 tham số này theta 0 sẽ bằng theta 0 trừ cho alpha nhân cho đạo hàng theta 0 còn đối với theta 1 thì chúng ta sẽ phải tính đạo hàng của hàm đồng này. Thì chúng ta sẽ xét 2 điều kiện dừng đó là khi đạo hàng của hàm loss theo theta 0 và đạo hàng của hàm loss theo theta 1 đủ nhỏ thì chúng ta sẽ kết thúc 1 lần. Thì đây chính là 3 cái bước cho thuật toán linear regression."
        },
        {
          "index": 16,
          "video_id": "Chương 2_m8uqtMEg8-E",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_1： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/m8uqtMEg8-E",
          "start_time": 749,
          "end_time": 760,
          "text": "và đạo hàng của hàm loss theo theta 1 đủ nhỏ thì chúng ta sẽ kết thúc 1 lần. Thì đây chính là 3 cái bước cho thuật toán linear regression."
        }
      ]
    },
    {
      "video_id": "Chương 2_MtJDVr5xHB4",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng  \n  - Giải thích cách tổng quát hóa và *vector hóa* mô hình hồi quy tuyến tính (Linear Regression) để xử lý nhiều biến và nhiều mẫu, và dẫn tới công thức dự đoán, hàm lỗi, đạo hàm (gradient) và thuật toán gradient descent dưới dạng vector/matrix. [1][3][7][13]\n\n- Các khái niệm sẽ được đề cập  \n  - Vector biểu diễn một mẫu (kèm thành phần bias) và ma trận X biểu diễn nhiều mẫu. [1][5]  \n  - Vector tham số θ (theta) gồm θ0 (bias), θ1...θm. [3][14]  \n  - Hàm dự đoán (hypothesis) fθ(x) = θ^T x dưới dạng vector hóa. [3][7]  \n  - Hàm lỗi (cost) là tổng sai số bình phương (MSE dạng 1/(2n) * sum (θ^T x - y)^2) và dạng vector hóa của nó. [4][8][9]  \n  - Đạo hàm (gradient) ∇_θ L và cập nhật bằng gradient descent, cùng điều kiện dừng dựa trên chuẩn của vector gradient. [10][12][13][14]  \n  - Biểu diễn mô hình dưới dạng đồ thị với đầu vào (bias, x1..xm) và trọng số θ0..θm. [15][16]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Biểu diễn một mẫu dưới dạng vector (feature vector) và bias\n- Mỗi mẫu dữ liệu có m biến x1, x2, ..., xm được biểu diễn bằng một vector cột; thành phần đầu tiên là *bias* (thường đặt là 1) để đại diện cho thành phần không phụ thuộc vào các biến đầu vào. [1][2]  \n- Ký hiệu vector mẫu bằng ký tự in đậm (ví dụ x). Thành phần bias là phần tử đầu của vector. [1]\n\n### 2.2. Tập nhiều mẫu — ma trận X và vector nhãn y\n- Khi có n mẫu, ta ghép các vector mẫu (dạng cột) lại thành một ma trận X có kích thước (m+1) x n (m+1 do thêm thành phần bias). [5]  \n- Nhãn của toàn bộ n mẫu được ghi thành vector hàng y có kích thước 1 x n (hay xem như vector ngang). [6]  \n- Kết quả: X ∈ R^{(m+1)×n}, y ∈ R^{1×n}. [5][6]\n\n### 2.3. Hàm dự đoán (hypothesis) dạng vector hóa\n- Với tham số θ ∈ R^{(m+1)×1}, dự đoán cho từng mẫu i là θ^T x_i (scalar). Khi áp dụng cho toàn bộ tập n mẫu, ta có vector dự đoán (1 x n): y_hat = θ^T X. [3][7]  \n- Lưu ý hướng/mũi của nhân ma trận: θ^T X cho ra vector hàng chứa n giá trị dự đoán. [7][8]\n\n### 2.4. Hàm lỗi (cost) dạng tổng, dạng vector hóa\n- Hàm lỗi cho một mẫu: (θ^T x - y)^2. [4]  \n- Hàm lỗi cho toàn bộ n mẫu (MSE dạng phổ biến trong bài giảng):  \n  J(θ) = (1 / (2n)) * sum_{i=1..n} (θ^T x_i - y_i)^2. [4][9]  \n- Dạng vector hóa tương đương (như diễn giải trong bài):  \n  J(θ) = (1 / (2n)) * (θ^T X - y) (θ^T X - y)^T, nơi (θ^T X - y) là vector hàng 1×n và nhân với chuyển vị cho ra scalar bằng tổng các sai số bình phương. [7][8][9]\n\n### 2.5. Đạo hàm (gradient) của hàm lỗi theo θ — trực giác và công thức\n- Vì θ là vector, ta dùng ký hiệu ∇ (Nabla) để chỉ gradient — một vector gồm các đạo hàm theo từng thành phần θ0..θm. [10][14]  \n- Bước suy luận (các bước trong video): khi tính đạo hàm của (1/2n) * (θ^T X - y)(...) thì hằng số 2 triệt tiêu với 1/2, phần hằng y (không phụ thuộc θ) có đạo hàm bằng 0, và đạo hàm của θ^T x theo θ cho kết quả chứa x. [11][12]  \n- Công thức gradient (theo mô tả trong video):  \n  ∇_θ L(θ) = (1 / n) * X * (θ^T X - y)^T.  \n  (Video trình bày ở dạng: 1/n × X × (θ^T X - y)^T—tương đương biểu diễn gradient bằng ma trận-vector theo hướng đã dùng trong bài.) [13][11][12]\n\n### 2.6. Gradient descent (cập nhật tham số) và điều kiện dừng\n- Khởi tạo θ (vector) ngẫu nhiên; chọn hai siêu tham số nhỏ: learning rate α (alpha) và ngưỡng dừng ε (epsilon). [12][13]  \n- Quy tắc cập nhật (vector): θ := θ - α * ∇_θ L(θ). [12][13]  \n- Điều kiện dừng: dừng khi chuẩn (độ lớn) của vector gradient ∇_θ L(θ) đủ nhỏ (≤ ε). Gradient ở đây là vector các đạo hàm thành phần (∂L/∂θ0, ∂L/∂θ1, ..., ∂L/∂θm). [14][13]\n\n### 2.7. Tóm tắt vector hóa tổng quát\n- Việc rút thừa θ ra khi nhân θ với từng x_i dẫn tới biểu diễn y_hat = θ^T X; việc nhân hai vector (vector hàng × vector cột) thu được tổng sai số bình phương. Đây là mục tiêu của vector hóa để thực hiện tính toán hiệu quả trên toàn bộ tập dữ liệu. [7][8][9][13]\n\n### 2.8. Biểu diễn mô hình dưới dạng đồ thị (graph)\n- Mô hình có các nút đầu vào: bias, x1, x2, ..., xm; mỗi cạnh từ đầu vào tới nút tổng có trọng số tương ứng θ0, θ1, ..., θm. Tổng có nhiệm vụ cộng các tích (θi * xi) để cho ra giá trị dự đoán. Độ dài/cạnh tượng trưng cho trọng số (weight) trong minh hoạ của bài giảng. [15][16]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa cụ thể trong video: bài toán dự đoán giá nhà  \n  - Features: x1 = diện tích, x2 = số phòng, ..., xm = khoảng cách tới trung tâm. Nhãn y là giá nhà. Mỗi mẫu là một vector gồm bias và các x_i. [2]  \n- Ứng dụng/Trường hợp sử dụng: mô hình Linear Regression tổng quát cho bài toán hồi quy với nhiều biến đầu vào; video cũng thông báo rằng phần tiếp theo sẽ trình bày cài đặt bằng hai cách: vectorized và non-vectorized (không vector hóa). [14]  \n- Ghi chú: video chỉ nêu ví dụ dự đoán giá nhà làm minh hoạ; triển khai cụ thể (code) sẽ ở phần tiếp theo. [2][14]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính  \n  - Ta đã chuyển từ biểu diễn scalar/single-sample sang biểu diễn vector/matrix để xử lý nhiều biến và nhiều mẫu: mẫu → vector (có bias), bộ mẫu → ma trận X, θ → vector tham số. [1][5][3]  \n  - Hàm dự đoán dạng vector: y_hat = θ^T X. [7]  \n  - Hàm lỗi (MSE) và dạng vector hóa: J(θ) = (1/(2n)) (θ^T X - y)(θ^T X - y)^T. [9]  \n  - Gradient ∇_θ L(θ) được biểu diễn dưới dạng vector/matrix và dùng cho cập nhật gradient descent θ := θ - α∇_θ L. Dừng khi chuẩn của gradient đủ nhỏ. [11][12][13][14]  \n  - Mô hình có thể trực quan hoá như một đồ thị đơn giản với input nodes, weights (θ) và một nút tổng để ra dự đoán. [15][16]\n\n- Tầm quan trọng của nội dung  \n  - Vector hóa làm phép toán trên toàn bộ tập dữ liệu hiệu quả hơn, dễ áp dụng cho tối ưu hóa bằng ma trận, và là nền tảng để mở rộng sang các mô hình phức tạp hơn. (Nội dung này được nhấn mạnh qua toàn bộ phần giảng). [7][13]\n\n- Liên hệ với các bài giảng khác / phần tiếp theo  \n  - Video báo trước sẽ thực hiện cài đặt Linear Regression bằng hai phương pháp: vectorized và không vectorized, và sẽ tiếp tục từ phần lý thuyết đã trình bày. [14]  \n  - Mô tả đồ thị của mô hình cũng là bước kết nối tới cách biểu diễn và lan truyền thông tin trong các mô hình phức tạp hơn. [15][16]\n\n---\n\nGhi chú: Tất cả nội dung trên được tóm tắt trực tiếp từ các đoạn trong video theo thứ tự thời gian: [1]...[16].",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 2_MtJDVr5xHB4",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_2： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/MtJDVr5xHB4",
          "start_time": 0,
          "end_time": 59,
          "text": "và chúng ta sẽ làm quen sẽ dần làm quen với cái việc đó là chúng ta tổng quá khóa và vector hóa cái mô hình máy học thì để tổng quá khóa và vector hóa thì chúng ta sẽ đưa đến các cái khái niệm sử dụng các cái vector và ma trận thì ở đây chúng ta sẽ có 2 cái vector đối với dữ liệu là một mẫu tức là gồm nhiều biến x1, x1, x2, xm thì ở đây chúng ta sẽ cho một cái ví dụ đây là một cái mẫu dữ liệu được ký hiệu bởi một cái vector và vector này chúng ta lưu ý là ký hiệu bởi một cái ký tự viết in đậm và viết thừa viết in đậm nhưng mà ít nhỏ thành phần đầu tiên đó chính là thành phần bias đó chính là đại diện cho tất cả những cái dữ liệu gì mà mô hình của mình nó độc lập với các cái biến đầu vào"
        },
        {
          "index": 2,
          "video_id": "Chương 2_MtJDVr5xHB4",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_2： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/MtJDVr5xHB4",
          "start_time": 49,
          "end_time": 112,
          "text": "thành phần đầu tiên đó chính là thành phần bias đó chính là đại diện cho tất cả những cái dữ liệu gì mà mô hình của mình nó độc lập với các cái biến đầu vào các cái biến đầu vào của mình nó không có còn là một biến đầu vào nữa mà nó có thể còn nhiều biến đầu vào lấy ví dụ bài toán dự đoán giá nhà thì x1 này của mình nó có thể sẽ là diện tích x2 này của mình có thể là số phòng và xm này có thể là khoảng cách đến trung tâm thì đây chính là các cái biến số để giúp cho chúng ta dự đưa ra được cái dự đoán cái giá trị y cái nhãn của một cái mẫu dữ liệu của mình nó sẽ là cái giá trị y và các cái biến số để giúp cho mình dự đoán cái nhãn này đó chính là x1 x2 cho đến xm như vậy chúng ta đã tổng quát khóa cho cái trường hợp là nhiều biến nhưng mà lưu ý là mới chỉ có một mẫu dữ liệu thôi"
        },
        {
          "index": 3,
          "video_id": "Chương 2_MtJDVr5xHB4",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_2： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/MtJDVr5xHB4",
          "start_time": 100,
          "end_time": 161,
          "text": "và các cái biến số để giúp cho mình dự đoán cái nhãn này đó chính là x1 x2 cho đến xm như vậy chúng ta đã tổng quát khóa cho cái trường hợp là nhiều biến nhưng mà lưu ý là mới chỉ có một mẫu dữ liệu thôi với một mẫu dữ liệu thôi thì trong cái phần tiếp theo chúng ta sẽ tổng quát phá cho cái tình huống là nhiều mẫu dữ liệu và cái tham số của cái mô hình của mình mô hình dự đoán của mình trong trường hợp này đó chính là một cái vector theta bao gồm nhiều thành phần thì theta 0, theta 1 và theta m thì tương ứng theta 0 sẽ được nhân với bias theta 1 sẽ được nhân với x1 và theta m sẽ nhân với xm như vậy thì lúc này hàm dự đoán của mình f theta x nó sẽ được viết bằng tích vô hướng của theta và x tích vô hướng của theta và x và khi nhân tích vô hướng thì nó sẽ lấy từng thành phần nhân với nhau xong rồi cộng lại và hàm lỗi trong trường hợp này nó sẽ là lấy theta x trừ y tất cả bình"
        },
        {
          "index": 4,
          "video_id": "Chương 2_MtJDVr5xHB4",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_2： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/MtJDVr5xHB4",
          "start_time": 149,
          "end_time": 210,
          "text": "và khi nhân tích vô hướng thì nó sẽ lấy từng thành phần nhân với nhau xong rồi cộng lại và hàm lỗi trong trường hợp này nó sẽ là lấy theta x trừ y tất cả bình đây chính là cái giá trị dự đoán hay còn gọi là y nghẽ và giá trị dự đoán này mình mong muốn sắp xử bê thì chúng ta sẽ lấy cái thằng này trừ nhau và bình phương đó chính là cho trường hợp một mẫu dữ liệu một mẫu dữ liệu còn trong trường hợp mà dữ liệu của mình là toàn bộ n mẫu thì mình sẽ vector hóa nó dưới dạng như thế này chúng ta sẽ ký hiệu bằng một cái ma trận chúng ta sẽ ký hiệu bằng một cái ma trận trong đó các cái cột của cái ma trận này nó tương ứng là một cái vector biểu diễn trong một mẫu vector của một mẫu và vector này nó sẽ thể hiện dưới dạng là dạng cột các vector này thể hiện ở dưới dạng cột và các cái cột này ráp lại với nhau"
        },
        {
          "index": 5,
          "video_id": "Chương 2_MtJDVr5xHB4",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_2： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/MtJDVr5xHB4",
          "start_time": 199,
          "end_time": 260,
          "text": "của một mẫu và vector này nó sẽ thể hiện dưới dạng là dạng cột các vector này thể hiện ở dưới dạng cột và các cái cột này ráp lại với nhau thì nó sẽ tạo ra thành một cái ma trận và cái chỉ số ở phía trên nó tương ứng là cái chỉ số thứ tự của mẫu như vậy thì nếu như chúng ta có n mẫu và từng cái x1 này x2 xn này nó là bao gồm m biến như vậy thì cái ma trận x này nó sẽ thuộc nó sẽ có cái kích thước đó là m cộng 1 nhân với lại n tại sao nó lại có cái mẫu này đó chính là do cái thành phần bias m biến thêm một cái thành phần bias nữa nó là m cộng 1 rồi đối với cái nhãn thì chúng ta nhãn của toàn bộ n mẫu thì nó sẽ kí hiệu bằng một cái ma trận y trong đó mẫu giá trị y1,y2,yn chính là cái nhãn của mình"
        },
        {
          "index": 6,
          "video_id": "Chương 2_MtJDVr5xHB4",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_2： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/MtJDVr5xHB4",
          "start_time": 249,
          "end_time": 309,
          "text": "rồi đối với cái nhãn thì chúng ta nhãn của toàn bộ n mẫu thì nó sẽ kí hiệu bằng một cái ma trận y trong đó mẫu giá trị y1,y2,yn chính là cái nhãn của mình và chẳng khác miesz left magic stomach y an chúng ta sẽ có y sẽ thuộc một cái ma trận kích thước đó là 1 nhân cho n. Hay còn gọi đây là một cái vector mà vector dạng nằm ngang. Còn x trong trường hợp này nó chính là một cái ma trận. Số dòng của mình nó sẽ là m cộng 1. Và số cột của mình trong trường hợp này nó chính là n. Rồi và như vậy thì cái hàm dự đoán của mình đó chính là chúng ta sẽ lấy cái tham số theta nhân cho từng mẫu dữ liệu theta nhân với x1, theta x2, theta xn. Và khi này thì chúng ta sẽ giống như là rút thừa số chung vậy đó. Chúng ta sẽ rút cái thừa số chung theta ra."
        },
        {
          "index": 7,
          "video_id": "Chương 2_MtJDVr5xHB4",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_2： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/MtJDVr5xHB4",
          "start_time": 296,
          "end_time": 358,
          "text": "chúng ta sẽ lấy cái tham số theta nhân cho từng mẫu dữ liệu theta nhân với x1, theta x2, theta xn. Và khi này thì chúng ta sẽ giống như là rút thừa số chung vậy đó. Chúng ta sẽ rút cái thừa số chung theta ra. Rồi sau đó đưa các cái x1, x2 vào bên trong cái dấu ngoặt. Và toàn bộ cái x1, x2 cho đến xn này nó chính là ma trận x. Như vậy thì chúng ta sẽ có cái công thức cho cái hàm dự đoán đó là theta chuyển vị nhau với x. Đối với hàm độ lỗi thì chúng ta cũng hoàn toàn làm tương tự như vậy. Hàm độ lỗi thì chúng ta cần phải lấy cái giá trị dự đoán trừ cho giá trị thực tế. Thì giá trị dự đoán trong trường hợp này trong trường hợp này đó chính là y ngã. Và cái theta chuyển vị nhân với x này thì nó sẽ là một cái vector dạng nằm ngang. Theta chuyển vị nhân với x nó là một cái vector dạng nằm ngang."
        },
        {
          "index": 8,
          "video_id": "Chương 2_MtJDVr5xHB4",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_2： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/MtJDVr5xHB4",
          "start_time": 348,
          "end_time": 410,
          "text": "Và cái theta chuyển vị nhân với x này thì nó sẽ là một cái vector dạng nằm ngang. Theta chuyển vị nhân với x nó là một cái vector dạng nằm ngang. Và y của mình nó cũng chính là một cái vector dạng nằm ngang. Giống như ở đây. Là 1 nhân n. Tại vì chúng ta có n mẫu nên chúng ta sẽ có n với giá trị dự đoán. Đây chính là y ngã. Đây chính là y. Và lấy hai cái này trừ cho nhau. Và lấy hai cái này trừ cho nhau. Thì chúng ta sẽ có theta chuyển vị nhân x trừ cho y. Thì kết quả của mình nó cũng sẽ ra một cái vector dạng nằm ngang như thế này. Và để tính tổng trung bình cộng của các cái sai số bình phương. Đúng không? Thì chúng ta sẽ lấy các cái vector nằm ngang này nhân với nhau. Rồi sau đó cộng lại. Thì đây chính là hai cái vector nhân với nhau. Thì đây chính là theta chuyển vị nhân với x trừ y. Và cái vector. Vector dạng dọc như thế này."
        },
        {
          "index": 9,
          "video_id": "Chương 2_MtJDVr5xHB4",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_2： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/MtJDVr5xHB4",
          "start_time": 398,
          "end_time": 463,
          "text": "Thì đây chính là hai cái vector nhân với nhau. Thì đây chính là theta chuyển vị nhân với x trừ y. Và cái vector. Vector dạng dọc như thế này. Đây chính là theta chuyển vị nhân với x trừ y. Tất cả chuyển vị thì nó sẽ tạo thành một cái vector dạng nằm dọc. Và khi nhân hai cái này lại với nhau. Khi nhân thì chúng ta sẽ nhân từng phần tử. Nhân từng phần tử. Rồi. Sau đó cộng lại. Thì bản chất đây chính là sai số bình phương. Sai số. Đây chính là cái sai số trong một bổ nè. Một thứ hai nè. Một thứ ba nè. Một thứ n. Đây chính là các cái sai số. Và khi chúng ta nhân theo cái kiểu là hai cái vector nằm ngang nhân với vector nằm dọc. Thì nó sẽ tạo ra tổng cái sai số bình phương. Sau đó chúng ta sẽ chia chung mình cộng. Và để tính đạo hàm cho hàm nỗi thì ở trong trường hợp này chúng ta lưu ý. Theta của mình nó không còn là một cái tham số dạng scalar mà nó là một vector."
        },
        {
          "index": 10,
          "video_id": "Chương 2_MtJDVr5xHB4",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_2： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/MtJDVr5xHB4",
          "start_time": 448,
          "end_time": 510,
          "text": "Sau đó chúng ta sẽ chia chung mình cộng. Và để tính đạo hàm cho hàm nỗi thì ở trong trường hợp này chúng ta lưu ý. Theta của mình nó không còn là một cái tham số dạng scalar mà nó là một vector. Do đó chúng ta phải dùng một cái kí hiệu là Napla. Thì bản chất đây chính là một cái vector gradient. Vector gradient. Vector đạo hàm. Cho một cái vector. Thì chúng ta tương tự như vậy. Chúng ta tính đạo hàm cho cái hàm ở trên. Thì một phần hai nè. Chúng ta kéo xuống. Và cái số hai này. Thì ở đây nó sẽ tương đương là theta chuyển vị trừ y. Tất cả bình phương. Đúng không? Thì cái bình phương đó chúng ta tính đạo hàm. Thì nó sẽ triệt tiêu đi con số hai. Rồi cái thành phần này chúng ta sẽ đem xuống dưới. Và đạo hàm bên trong cái ruột này. Theo cái theta. Đạo hàm của cái này theo theta thì y. Đối với theta nó chính là hàng số. Do đó chúng ta sẽ xem như thằng này đạo hàm là bằng 0. Đạo hàm của theta x."
        },
        {
          "index": 11,
          "video_id": "Chương 2_MtJDVr5xHB4",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_2： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/MtJDVr5xHB4",
          "start_time": 499,
          "end_time": 559,
          "text": "Theo cái theta. Đạo hàm của cái này theo theta thì y. Đối với theta nó chính là hàng số. Do đó chúng ta sẽ xem như thằng này đạo hàm là bằng 0. Đạo hàm của theta x. Theo biến theta. Thì chúng ta sẽ triệt tiêu đi cái này. Chúng ta sẽ có x. Do đó đạo hàm của theta x nó chính là x. Còn nguyên cái thành phần này. Chúng ta sẽ đem xuống dưới. Nếu như chúng ta nhìn ở dưới dạng góc độ là đại số. Tuy nhiên cái cách này nó không chính thống. Nó không có đúng. Về mặt ký hiệu. Nhưng mà nếu chúng ta nhìn cái thao tác nhân này. Đó là theta chuyển vị x trừ y. Tất cả bình phương. Thì khi lấy đạo hàm của nó ra. Thì chúng ta sẽ đem cái số 2 xuống. Rồi cái thành phần này chúng ta sẽ kéo xuống. Đó là theta chuyển vị x trừ y. Rồi. Sau đó chúng ta sẽ nhân đạo hàm. Của cái phần ruột. Đạo hàm của phần ruột theo theta. Thì thành phần này là bỏ. Thành phần này. Nó sẽ còn là x."
        },
        {
          "index": 12,
          "video_id": "Chương 2_MtJDVr5xHB4",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_2： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/MtJDVr5xHB4",
          "start_time": 550,
          "end_time": 611,
          "text": "Rồi. Sau đó chúng ta sẽ nhân đạo hàm. Của cái phần ruột. Đạo hàm của phần ruột theo theta. Thì thành phần này là bỏ. Thành phần này. Nó sẽ còn là x. Cái số 2 này nó sẽ triệt tiêu cái này. Nó sẽ bị giữ mất. Như vậy nó sẽ còn là x. Nhưng cho theta chuyển vị x trừ y. Thì đây là cái chứng minh cho cái công thức đạo hàm này. Và khi chúng ta đã tính được đạo hàm rồi. Thì cái thuật toán gradient descent. Rất là đơn giản. Đó là chúng ta sẽ khởi tạo ngẫu nhiên cái vector. Lưu ý lúc này theta của mình. Nó là phần ruột. Thì nó sẽ là 1 vector rồi. Tại vì cái x của mình. Nó bao gồm m biến. Theta của mình. Thì nó sẽ là bao gồm m cộng 1 thành phần. Theta. Nó sẽ là bao gồm theta 0, theta 1. Cho đến theta m. m cộng 1 thành phần. Và các cái thành phần này chúng ta sẽ khởi tạo ngẫu nhiên. Alpha và 2 siêu tham số alpha và epsilon. Cũng là khởi tạo các cái con số rất là nhỏ."
        },
        {
          "index": 13,
          "video_id": "Chương 2_MtJDVr5xHB4",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_2： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/MtJDVr5xHB4",
          "start_time": 601,
          "end_time": 661,
          "text": "m cộng 1 thành phần. Và các cái thành phần này chúng ta sẽ khởi tạo ngẫu nhiên. Alpha và 2 siêu tham số alpha và epsilon. Cũng là khởi tạo các cái con số rất là nhỏ. Chúng ta sẽ lập. Và theta sẽ được cập nhật bằng theta trừ cho alpha. Thì đạo hàm gradient của L theo theta. Nó sẽ có công thức là 1 phần n x của theta chuyển bí x trừ y. Do đó thì chúng ta chép nó qua đây. Và chúng ta sẽ có cái công thức cập nhật. Rồi. Điều kiện dừng. Đó là nếu như cái giá trị size tổng. Cái giá trị size số này. Cái độ lớn của cái vector này. Đủ nhỏ. Thì chúng ta sẽ dừng lập. Thì chúng ta lưu ý là cái nét la của L theo theta. Nó là cái vector gradient. Hay nói cách khác. Nó là. Bằng các cái đạo hàm thành phần. Theo theta 0. Đạo hàm của L theo theta 1."
        },
        {
          "index": 14,
          "video_id": "Chương 2_MtJDVr5xHB4",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_2： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/MtJDVr5xHB4",
          "start_time": 649,
          "end_time": 711,
          "text": "Hay nói cách khác. Nó là. Bằng các cái đạo hàm thành phần. Theo theta 0. Đạo hàm của L theo theta 1. Theo theta m. Thì đây là 1 cái vector. Do đó chúng ta hoàn toàn có thể sử dụng cái giá trị độ lớn của cái vector này. Để làm cái điều kiện dừng. Khi mà cái giá trị độ lớn của cái vector đạo hàm này. Của vector gradient này mà đủ nhỏ. Thì chúng ta sẽ kết thúc cùng lập. Thì đây chính là. Cái tổng quát hóa và vector hóa. Cho cái mô hình. Linear regression. Và trong cái phần tiếp theo thì chúng ta sẽ tiến hành cài đặt bằng. 2 cái phương pháp mà vector hóa. Và. Cả cái phương pháp mà không vector hóa. Và cuối cùng cho cái phần linear regression này. Chúng ta sẽ biểu diễn. Cái mô hình của mình dưới dạng là. Đồ thị. Đầu vào chúng ta sẽ có cái thành phần là bias. Rồi các cái biến. X1, X2 cho đến Xm."
        },
        {
          "index": 15,
          "video_id": "Chương 2_MtJDVr5xHB4",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_2： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/MtJDVr5xHB4",
          "start_time": 699,
          "end_time": 759,
          "text": "Chúng ta sẽ biểu diễn. Cái mô hình của mình dưới dạng là. Đồ thị. Đầu vào chúng ta sẽ có cái thành phần là bias. Rồi các cái biến. X1, X2 cho đến Xm. Và tương ứng. Từng cái đầu vào này chúng ta sẽ có các cái tham số. Theta 0, Theta 1, Theta 2 và Theta m. Và khi từng cái thành phần này nhân vô. Chúng ta sẽ qua một cái hàm tính tổng. Tại sao lại tổng. Tại vì tổng. Của từng các cái tích này. Đúng không. X1, Theta 0. X1, Theta 1. X2, Theta 2. Xm, Theta m. Sau đó chúng ta cộng lại. Chúng ta sẽ ra được cái giá trị dự đoán. Và đây thì là cái dạng viết dưới dạng là vector hóa. Như vậy thì với một cái đồ thị này thì chúng ta. Có thể hiểu được cách mà chúng ta lan truyền thông tin. Và cái độ dài của cái cạnh này. Nó tương ứng nó gọi là. Trọng số. Trọng số của cái thông tin."
        },
        {
          "index": 16,
          "video_id": "Chương 2_MtJDVr5xHB4",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2a_2： Mô hình hồi quy tuyến tính (Linear Regression)",
          "video_url": "https://youtu.be/MtJDVr5xHB4",
          "start_time": 750,
          "end_time": 782,
          "text": "Và cái độ dài của cái cạnh này. Nó tương ứng nó gọi là. Trọng số. Trọng số của cái thông tin. Thông tin này đưa vào. Có trọng số là Theta 0. Thông tin này đưa vào. Có trọng số là Theta 1. Mỗi cái này nó sẽ có một cái trọng số. Rồi nó tổng. Cái này nó sẽ gọi là tổng hợp thông tin. Tổng hợp thông tin để đưa ra cái giá trị dự đoán. Thì đó là toàn bộ cái nội dung của phần Linear Direction."
        }
      ]
    },
    {
      "video_id": "Chương 2_sPoJ8VS7nLc",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Cài đặt mô hình Linear Regression bằng 3 cách khác nhau: (1) dùng tham số rời rạc theta0, theta1; (2) phiên bản vector hóa gom theta thành một vector/matrix; (3) sử dụng thư viện Keras để tận dụng tự động tính đạo hàm và tiện lợi khi huấn luyện. [1]\n\n- Các khái niệm sẽ được đề cập: sinh dữ liệu mẫu (có noise), hiện thực hoá gradient descent (khởi tạo tham số, learning rate alpha, điều kiện dừng epsilon), công thức đạo hàm loss theo từng tham số, phiên bản vector hóa với ma trận X và vector theta, xử lý lỗi/kiểm tra debug khi viết code, và lợi ích của dùng Keras (tự tính đạo hàm). [1][2]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Tổng quan ba phiên bản cài đặt\n- Phiên bản 1 — tham số rời (theta0, theta1) được xử lý như các biến độc lập trong code (khởi tạo và cập nhật từng biến). [1]  \n- Phiên bản 2 — vector hóa: gom theta0 và theta1 vào một biến theta (vector/ma trận) và dùng phép toán ma trận để tính gradient và cập nhật. [1][13]  \n- Phiên bản 3 — sử dụng Keras: Keras tự động tính toán đạo hàm và cập nhật tham số, giúp tiết kiệm thao tác tính đạo hàm thủ công. [1][2]\n\n### 2.2. Sinh dữ liệu mẫu (data generation)\n- Ban đầu tác giả có ví dụ với một đường thẳng (ví dụ -6x + 10) và thêm nhiễu để dữ liệu không nằm chính xác trên đường thẳng. [2]  \n- Sau đó chương trình được chỉnh để mô phỏng y = 3x + 8 với noise ~ Normal(mean=0, std=2), x lấy từ 1 đến 10 với bước 0.5. [3]  \n- Để tăng tính thực tế, tác giả thay đổi standard deviation (std) của noise lên 4 để giao động lớn hơn. (ví dụ: std = 4) [4]\n\n### 2.3. Cài đặt thuật toán huấn luyện — phiên bản tham số rời\n- Khởi tạo tham số: ví dụ gán trực tiếp theta0 = -123, theta1 = 456 (có thể dùng random nhưng ở ví dụ gán cứng để đơn giản). [5]  \n- Hyperparameters: learning rate alpha thường chọn nhỏ (ví dụ alpha = 0.01) và ngưỡng dừng epsilon (ví dụ epsilon = 0.001). [5][6]  \n- Công thức cập nhật (gradient descent, dạng tường minh):\n  - Dự đoán: y_pred = theta1 * x + theta0. [8][12]  \n  - Đạo hàm của loss theo theta0: trung bình của (y_pred - y) (tức 1/n * sum(y_pred - y)). [6][7]  \n  - Đạo hàm của loss theo theta1: trung bình của (y_pred - y) * x (tức 1/n * sum((y_pred - y) * x)). [7][8]  \n  - Cập nhật:\n    - theta0 := theta0 - alpha * (1/n * sum(y_pred - y)). [6][8]  \n    - theta1 := theta1 - alpha * (1/n * sum((y_pred - y) * x)). [7][8]\n  (Các phép trung bình được tính bằng np.mean / np.sum trong code). [7][8]\n\n- Điều kiện dừng: nếu trị tuyệt đối của cả hai đạo hàm (gradient components) đều nhỏ hơn epsilon thì dừng vòng lặp (break). [9]\n\n### 2.4. Kết quả và trực quan hóa (phiên bản tham số rời)\n- Sau chạy thuật toán, kết quả ước lượng xấp xỉ với đường sinh dữ liệu: theta0 ≈ 7.7 (gần 8), theta1 ≈ 2.97 (gần 3). Điều này cho thấy thuật toán học được tham số từ các điểm mẫu mà không biết trước công thức y = 3x + 8. [10][11]  \n- Trực quan hóa: vẽ các điểm dữ liệu (scatter) và vẽ đường mô hình y = theta1 * x + theta0 để kiểm tra mức khớp; đường thẳng đi xuyên qua đám mây điểm cho thấy mô hình khớp tốt. [11][12]\n\n### 2.5. Phiên bản vector hóa (vectorized implementation)\n- Gom tham số: theta được lưu dưới dạng một vector/ma trận kích thước 2x1 (chứa bias/theta0 và hệ số theta1). [13][14]  \n- Ma trận X: được xây dựng với hàng đầu tiên là các giá trị bias (1s) và hàng thứ hai là các giá trị x; tức X có dạng:\n  - hàng 1: ones (bias)\n  - hàng 2: x values\n  (phải chuyển x từ vector sang dạng ma trận để ghép hàng). [15][16][17]\n\n- Biểu diễn đạo hàm (gradient) theo vector:\n  - gradient (rad) = (1/n) * X * (X^T * theta - y^T)  (dùng phép nhân ma trận / dot) — trong lời giảng tác giả mô tả công thức này bằng việc dùng dot/transpose để nhân ma trận và trừ y. [14][18][19]\n  - Cập nhật theta bằng: theta := theta - alpha * rad. [14][15]\n\n- Triển khai code lưu ý:\n  - Cần convert/reshape x thành ma trận đúng kích thước rồi concat/hstack hàng ones và hàng x; lỗi thường gặp do không gói tuple khi dùng cat/concatenate — cần truyền (row1, row2) vào hàm concat. [16][20]  \n  - Sau sửa lỗi concat, kết quả theta thu được tương đương với phiên bản tham số rời (ví dụ theta0 ≈ 7.7, theta1 ≈ 2.9) và mô hình trực quan tương tự. [21]\n\n### 2.6. Phiên bản sử dụng Keras\n- Lợi ích: Keras tự tính toán đạo hàm (autograd/automatic differentiation), nên không cần triển khai công thức đạo hàm thủ công; tiết kiệm công sức và giảm khả năng lỗi khi viết gradient. [1][2]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa cụ thể trong video:\n  - Sinh dữ liệu từ hàm tuyến tính y = 3x + 8 với thêm noise (normal, mean=0, std ban đầu 2, sau đó tăng lên 4 để tăng độ giao động), x từ 1 đến 10 bước 0.5; dùng dữ liệu này để huấn luyện linear regression. [3][4]  \n  - Chạy gradient descent (phiên bản tham số rời) với alpha = 0.01, epsilon = 0.001, khởi tạo theta0/theta1 (ví dụ gán cố định) và quan sát theta hội tụ ~ (7.7, 2.97). [5][6][10]\n\n- Ứng dụng/thực tế:\n  - Minh hoạ quy trình cơ bản của supervised learning: sinh/thu thập dữ liệu, thiết kế model (linear), định nghĩa loss, tối ưu bằng gradient descent, kiểm tra kết quả bằng trực quan hóa. (Ý này được thể hiện qua toàn bộ ví dụ trong video). [2][11][12]\n\n- Trường hợp sử dụng:\n  - Dùng để dạy/làm hiểu linear regression cơ bản, kiểm tra cách hiện thực hoá gradient descent thủ công, so sánh với cách vector hóa và thư viện (Keras) để thấy lợi ích của việc vector hóa và tự động hóa đạo hàm. [1][13][2]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Bài giảng hướng dẫn cài đặt Linear Regression theo 3 cách: tham số rời, vector hóa, và dùng Keras. [1]  \n  - Minh hoạ bằng dữ liệu tổng hợp y = 3x + 8 (có noise) và thực hiện gradient descent với công thức đạo hàm rõ ràng: đạo hàm theo theta0 là trung bình của (y_pred - y), theo theta1 là trung bình của (y_pred - y)*x; cập nhật theta := theta - alpha * gradient. [7][8][6]  \n  - Phiên bản vector hóa dùng ma trận X (hàng 1 = ones, hàng 2 = x) và tính gradient bằng phép nhân ma trận (rad = (1/n) * X * (X^T * theta - y^T)) để cập nhật theta. [15][18][14]  \n  - Keras giúp tự tính đạo hàm và đơn giản hoá quá trình huấn luyện. [1][2]\n\n- Tầm quan trọng:\n  - Hiểu cả cách hiện thực thủ công (để nắm rõ nguyên lý) và cách vector hóa (để tối ưu hiệu năng) là nền tảng quan trọng trước khi sử dụng thư viện cao cấp như Keras. Việc biết cách debug/kiểm tra (ví dụ lỗi concat/reshape) cũng rất cần thiết khi triển khai thực tế. [5][20][21]\n\n- Liên hệ với các bài giảng khác:\n  - (Video đề cập rằng đây là phần cài đặt trong Chương 2; việc nắm linear regression cơ bản sẽ liên quan đến bài sau về mô hình hóa và tối ưu hoá nâng cao — ý này được ngụ ý qua vị trí bài trong chương, không có chi tiết thêm trong các chunk). [1]\n\n---\n\nGhi chú: các giá trị, công thức và kết quả in/plot trong phần tóm tắt đều lấy trực tiếp từ nội dung bài giảng và các đoạn code/ket quả mà giảng viên trình bày trong video. [2][3][4][6][10][11][21]",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 1,
          "end_time": 61,
          "text": "Trong vòng tiếp theo thì chúng ta sẽ tiến hành cài đặt thuật bán Linear Ration Và chúng ta sẽ cài bằng 3 phiên bản Phiên bản đầu tiên đó là chúng ta sẽ dùng các tham số Theta như là những cái biến rời đạt là Theta 0, Theta 1 ở đây Trong cái phiên bản dạng vector hóa thì chúng ta sẽ gom tất cả những cái tham số Theta 0, Theta 1 này vào chung một cái biến đó là Theta Thì cái việc này nó sẽ giúp cho cái chương trình của mình nó nhìn nó gọn hơn Và cái phiên bản số 3 đó là chúng ta sẽ sử dụng thư viện Keras Thì cái phiên bản cài đặt sử dụng thư viện Keras Nó sẽ giúp cho chúng ta tiết kiệm được rất nhiều cái công sức trong cái việc đó là tính đạo hàm Chúng ta sẽ không cần phải ngồi tính toán các cái giá trị đạo hàm một cách thường minh"
        },
        {
          "index": 2,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 45,
          "end_time": 112,
          "text": "Thì cái phiên bản cài đặt sử dụng thư viện Keras Nó sẽ giúp cho chúng ta tiết kiệm được rất nhiều cái công sức trong cái việc đó là tính đạo hàm Chúng ta sẽ không cần phải ngồi tính toán các cái giá trị đạo hàm một cách thường minh Mà Keras nó sẽ tự tính toán và tự tính cái đạo hàm này cho chúng ta luôn Thì đầu tiên chúng ta sẽ cài đặt với cái phiên bản là tham số rời đạt Đối với cái phiên bản tham số rời đạt thì ở đây chúng ta sẽ có Khởi tạo một cái đoạn code Đoạn code để tạo sinh ra cái dữ liệu mẫu Thì chúng ta sẽ chạy thử cái đoạn code ở đây Và như chúng ta thấy thì Ở đây là cái phương trình đường thẳng mà mình cho trước Đó là trừ 6x cộng 10 Và để tăng thêm cái tính thật Tức là mình thêm cái một cái đoạn lượng nhiễu Để cho cái các cái điểm của mình nó đừng có đi thẳng tốc Mà nó sẽ giao động xung quanh một cái đường thẳng của mình thôi"
        },
        {
          "index": 3,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 99,
          "end_time": 160,
          "text": "Và để tăng thêm cái tính thật Tức là mình thêm cái một cái đoạn lượng nhiễu Để cho cái các cái điểm của mình nó đừng có đi thẳng tốc Mà nó sẽ giao động xung quanh một cái đường thẳng của mình thôi Thì ở đây chúng ta sẽ có thêm là cái noise Là bằng random Theo cái phân bố là normal Với tâm Với cái min là bằng 0 Và độ lịch chuẩn của mình là 2 Rồi x của mình là cái giá trị từ 1 cho đến 10 Với cái mức nhảy là 0.5 Như vậy thì để gần để tạo ra các cái dữ liệu Mà gần với lại cái mô hình ở đây Thì chúng ta sẽ tạo ra một cái phương trình đường thẳng Dù như là y là bằng 3x cộng cho 8 Tức là một cái dạng hàm đồng ý Và cái điểm ở đây thì chúng ta sẽ mô phỏng nó là bằng các cái điểm màu xanh Thì chúng ta sẽ sửa lại chương trình này một chút xíu"
        },
        {
          "index": 4,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 145,
          "end_time": 210,
          "text": "Dù như là y là bằng 3x cộng cho 8 Tức là một cái dạng hàm đồng ý Và cái điểm ở đây thì chúng ta sẽ mô phỏng nó là bằng các cái điểm màu xanh Thì chúng ta sẽ sửa lại chương trình này một chút xíu Để cho cái điểm mình generate ra ở đây nó nhìn nó giống hơn một chút Ở đây sẽ là 3x cộng cho 8 Rồi Các cái điểm ở đây thì là các điểm màu xanh rồi Thì chúng ta muốn cho cái giao động này nó nhìn có vẻ nó lớn hơn Thì chúng ta có thể để ở đây là 4 Standard Dependent xe là bằng 4 Chúng ta thấy là cái độ rộng của nó Và các cái giao động của cái điểm x này của mình nó lớn hơn Sau với lại cái Standard Dependent xe là bằng 4  Cái giao động của nó là 4 Rồi xe mũi tiếp theo thì chúng ta sẽ tiến hành cài đặt cái thập toán huấn luyện Dựa trên cái phiên bản là tham số rời đẹp"
        },
        {
          "index": 5,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 199,
          "end_time": 262,
          "text": "Rồi xe mũi tiếp theo thì chúng ta sẽ tiến hành cài đặt cái thập toán huấn luyện Dựa trên cái phiên bản là tham số rời đẹp Thì chúng ta sẽ cài đặt cái bước đầu tiên đó chính là khởi tạo tham số thê ta 0 và thê ta 1 là ngỗ nhiên Thì để cài đặt cái này thì chúng ta sẽ hiện thực hóa Cái chương trình ở đây Thê ta 0 Rồi ngỗ nhiên thì chúng ta có thể cho cái giá trị ví dụ như là trừ 123 Rồi thê ta 1 sẽ là bằng 456 Chúng ta có thể sử dụng hàm random nhưng mà tuy ở đây chúng ta sẽ gắn trực tiếp cái giá trị luôn để cho nó đơn giản Tiếp theo đó là chúng ta sẽ khởi tạo các cái giá trị alpha và epsilon Rồi thì cái giá trị alpha Right Thông thường đó là những cái giá trị bé chúng ta sẽ để là alpha bằng 0.01 và epsilon sẽ lập để 0.001"
        },
        {
          "index": 6,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 251,
          "end_time": 311,
          "text": "Rồi thì cái giá trị alpha Right Thông thường đó là những cái giá trị bé chúng ta sẽ để là alpha bằng 0.01 và epsilon sẽ lập để 0.001 Rồi tiếp theo thì chúng ta sẽ cài đặt cái bộng lập Chúng ta sẽ tiến hành cài đặt cái bộng lập Và cập nhật lại cái cái tham số thê ta 0 dựa trên cái công thức này Thê ta 0 là bằng thê ta 2 không trừ alpha nhân cho đạo hàm của hàm loss theo theta 0 mà theta 0 thì nó được tính bởi cái công thức ở đây còn đạo hàm của theta, đạo hàm của hàm loss theo theta 1 thì chúng ta sẽ có cái công thức ở đây rồi ý nghĩa của cái công thức này đó là gì đạo hàm của hàm loss theo theta 0 nó sẽ là bằng trung bình cộng 1,1 nè, của tổng nè là trung bình cộng của ở đây chính là"
        },
        {
          "index": 7,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 298,
          "end_time": 359,
          "text": "ý nghĩa của cái công thức này đó là gì đạo hàm của hàm loss theo theta 0 nó sẽ là bằng trung bình cộng 1,1 nè, của tổng nè là trung bình cộng của ở đây chính là giá trị dự đoán trừ cho giá trị thực tế ở đây thì cũng thương tượng như vậy đó là trung bình cộng của giá trị dự đoán trừ cho giá trị thực tế lưu ý là chúng ta sẽ có thêm 1 cái thành phần nhân với x ở đây nữa khác cái công thức ở bên đây 1 chút xíu, đó là có cái thành phần này rồi thì chúng ta sẽ tiến hành cài đặt ok ok                     Số 1, y, 2 hãy đi ngang xem theta 0 bằng theta 0 trừ cho alpha nhân với lại trung bình cộng ở đây chúng ta lấy là trung bình cộng thì nó sẽ là np.min"
        },
        {
          "index": 8,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 349,
          "end_time": 412,
          "text": "trừ cho alpha nhân với lại trung bình cộng ở đây chúng ta lấy là trung bình cộng thì nó sẽ là np.min rồi và giá trị dự đoán là 0.7  dùng dòng dòng để đoán tức là x nhân với lại theta 1 cộng cho theta 0 trừ cho y rồi thương tượng như vậy theta 1 thì sẽ là bằng theta 1 trừ cho alpha và ở đây khi chúng ta thực hiện cái bên trong cái hành np.min thì chúng ta phải có chú ý là chúng ta phải nhân thêm cái thành phần nữa là x ở đây rồi sau đó thì điều kiện dừng đó là nếu như trị tiệt đối của cái đạo hàm hàm loss theo theta 0 và trị tiệt đối của đạo hàm của hàm loss theo theta 1"
        },
        {
          "index": 9,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 399,
          "end_time": 460,
          "text": "sau đó thì điều kiện dừng đó là nếu như trị tiệt đối của cái đạo hàm hàm loss theo theta 0 và trị tiệt đối của đạo hàm của hàm loss theo theta 1 mà bé hơn một cái ngữ thì chúng ta sẽ dùng git abs rồi trị tiệt đối của cái đạo hàm thì chúng ta sẽ copy các giá trị mà chúng ta đã tính ở trên này bé hơn epsilon thì chúng ta sẽ copy kỹ thuật ở đây rồi bé hơn epsilon thì chúng ta sẽ break rồi và bây giờ thì mình sẽ tiến hành chạy thử cái chương trình này rồi may quá không có lỗi thế thì ở đây chúng ta sẽ xem coi theta 1       theta 0 là bao nhiêu chúng ta sẽ in ra là print"
        },
        {
          "index": 10,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 448,
          "end_time": 510,
          "text": "mình sẽ tiến hành chạy thử cái chương trình này rồi may quá không có lỗi thế thì ở đây chúng ta sẽ xem coi theta 1       theta 0 là bao nhiêu chúng ta sẽ in ra là print theta 0 theta 0 rồi ở đây sẽ là theta 1 rồi và cái giá trị mà mình tính ra được đó là theta 0 là bằng 7,7 theta 0 là bằng 7,7  nếu mà chúng ta so vào cái công thức gốc ở đây thì chúng ta thấy là 7,7 nó gần với lại cái con số 8 theta 1 nó sẽ ra là 2,97 nó sẽ gần với lại cái con số 3 thì rõ ràng chúng ta thấy là trong cái thuật toán này mình không hề sử dụng cái công thức tường minh của cái model là"
        },
        {
          "index": 11,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 500,
          "end_time": 562,
          "text": "nó sẽ gần với lại cái con số 3 thì rõ ràng chúng ta thấy là trong cái thuật toán này mình không hề sử dụng cái công thức tường minh của cái model là y bằng 3x cộng 8 và mọi thứ chỉ được tính toán dựa trên các cái điểm lấy mẫu này  chúng ta không hề biết trước cái công thức này nhưng sau khi huấn luyện xong thì các cái giá trị theta 0 và theta 1 nó đều sắp xỉ với lại cái công thức mà chúng ta đã chọn ban đầu ở đây vậy thì chúng ta sẽ đến cái bước thứ 3 đó là chúng ta sẽ trực quan hóa cái mô hình này thì để trực quan hóa mô hình này thì chúng ta chỉ việc là copy cái đoạn cốt ở trên đây đồng thời đó là chúng ta sẽ vẽ thêm cái hàm mô hình dự đoán cho từng cái giá trị x đầu vào của mình sẽ là x rồi giá trị dự đoán của mình nó sẽ là x nhân với lại theta 1 cộng cho theta 0"
        },
        {
          "index": 12,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 550,
          "end_time": 611,
          "text": "đầu vào của mình sẽ là x rồi giá trị dự đoán của mình nó sẽ là x nhân với lại theta 1 cộng cho theta 0 rồi và ở đây chúng ta sẽ không có thêm số là b o tại vì ở đây ở dòng trên á là chúng ta đang vẽ dưới dạng điểm còn ở bên dưới chúng ta đang muốn vẽ cái mô hình dưới dạng đường thì chúng ta sẽ không có cái thêm số này rồi sau khi vẽ xong thì chúng ta thấy là cái đường thẳng mô hình của mình nó đi xuyên qua cái đám mây điểm của cái dữ liệu mẫu điều đó cho thấy đó là cái mô hình của mình nó rất là khớp nó rất là khớp rồi bây giờ chúng ta sẽ tiến hành qua cái bước cài đặt tiếp theo cái phiên bản cài đặt tiếp theo đó chính là phiên bản vector hóa thì trong cái phiên bản vector hóa này thì cái tham số theta của mình nó sẽ được khởi tạo ngẫu nhiên"
        },
        {
          "index": 13,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 596,
          "end_time": 663,
          "text": "rồi bây giờ chúng ta sẽ tiến hành qua cái bước cài đặt tiếp theo cái phiên bản cài đặt tiếp theo đó chính là phiên bản vector hóa thì trong cái phiên bản vector hóa này thì cái tham số theta của mình nó sẽ được khởi tạo ngẫu nhiên và theta này bản chất nó chính là một cái bộ các cái giá trị theta 0 và theta 1 rồi và chúng ta sẽ có cái công thức trực tiếp cho cái việc là tính đạo em của hàm loss theo theta ở đây rồi thì chúng ta sẽ copy cái chương trình ở phía trên chúng ta sẽ copy cái chương trình ở phía trên lại chọn gọi đem xuống đây và thay vì chúng ta tính chúng ta để 2 cái giá trị theta 0 và theta 1 là 2 giá trị rà ràng thì chúng ta sẽ gom nó lại thành một biến theta và chúng ta lưu ý là trong cái trường hợp này thì theta của mình nó sẽ là một cái ma trận kích thước là 2 như 1 cái đó thì khi khởi tạo ở đây chúng ta sẽ để là np.arrays"
        },
        {
          "index": 14,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 648,
          "end_time": 708,
          "text": "thì chúng ta sẽ gom nó lại thành một biến theta và chúng ta lưu ý là trong cái trường hợp này thì theta của mình nó sẽ là một cái ma trận kích thước là 2 như 1 cái đó thì khi khởi tạo ở đây chúng ta sẽ để là np.arrays và chúng ta sẽ gom nó lại thành 1 biến theta 2 như 1 thì khi khởi tạo ở đây chúng ta sẽ để là np.arrays chúng ta phải để nó là 3 trộn 2 x 1, trong đó giá trị đầu tiên là 1, 2, 3, và giá trị tiếp theo sẽ là 4, 5, 6. Rồi, và chúng ta sẽ bỏ 2 giá trị, thì 8 không thì 1 ở đây. Ở trong công thức cập nhật này, chúng ta cũng sẽ sửa lại, đó là theta, bằng theta trừ cho alpha, nhau. Ở đây chúng ta sẽ là nhân với lại x, nhân cho theta chuyển vị nhân x, trừ y. Vì để thực hiện được cái này, để thực hiện được cái này, thì chúng ta sẽ phải có thêm cái biến x, cái ma trận x. X của mình, x của mình sẽ là,"
        },
        {
          "index": 15,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 696,
          "end_time": 760,
          "text": "thì chúng ta sẽ phải có thêm cái biến x, cái ma trận x. X của mình, x của mình sẽ là, hàng đầu tiên sẽ là các giá trị bias. Hàng ở dưới chính là các giá trị x1, x2. Rồi, và khi có x này rồi, thì chúng ta mới có thể thực hiện được cái công thức, tính đạo hàm ở đây. Và chúng ta sẽ dùng cái này, ở trong cái biến, nó đặt tên là biến rad, tương ứng là gradient. Tại vì, ở đây chúng ta tính đạo hàm, theo 1 cái vector, thì chúng ta phải dùng từ là gradient, đạo hàm theo từng biến, thì nó sẽ là derivative. Còn đạo hàm theo vector, thì chúng ta phải dùng là rad. Rồi, ở đây sẽ là nhân với rad, và trong đó rad, thì sẽ được tính bằng cái công thức, mà chúng ta phân tỏ ở đây. Mình muốn tính cái đó,"
        },
        {
          "index": 16,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 750,
          "end_time": 811,
          "text": "Rồi, ở đây sẽ là nhân với rad, và trong đó rad, thì sẽ được tính bằng cái công thức, mà chúng ta phân tỏ ở đây. Mình muốn tính cái đó, thì chúng ta sẽ phải có 1 cái ma trận, là ma trận x. X ở đây, nó sẽ là, hàng đầu tiên là các cái con số 1, là np.once, và số chiều của nó, nó sẽ là 1. Và số hàng, số dòng sẽ là 1, và số hàng, sẽ là, tương ứng là cái số phần tử của mình, thì mình có thể để ra là, là lệnh của, là, y, tức là số phần tử. Rồi, đây là cái hàng đầu tiên, sang cái hàng thứ 2, đó chính là x. Nhưng mà lưu ý, x ở đây á, ban đầu á, nó là 1 vector, chúng ta phải convert cái x này, về cái dạng là ma trận, có số hàng là 1, và số cột sẽ là số phần tử. Vậy đó, ở đây chúng ta phải là reset."
        },
        {
          "index": 17,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 801,
          "end_time": 858,
          "text": "chúng ta phải convert cái x này, về cái dạng là ma trận, có số hàng là 1, và số cột sẽ là số phần tử. Vậy đó, ở đây chúng ta phải là reset. Số hàng là 1, số dòng, nếu như chúng ta muốn khai báo thường minh, thì chúng ta để là len y cũng được, nhưng mà nó hơi dài, thì ở đây mình có thêm 1 cái mẹo, đó là chúng ta sẽ để là trừ 1, tức là chương trình nó sẽ tự tính, nó sẽ tự tính cái số cột của mình là bao nhiêu, dựa trên cái số phần tử x ban đầu. Rồi, thì chúng ta sẽ phải convert, sẽ phải nối 2 cái hàng này lại với nhau, nối 2 cái hàng này lại với nhau, và trong, chúng ta sẽ sử dụng thư viện đó là minpy.convert, minpy.convert, minpy.convert, minpy.convert, rồi, ở đây thì chúng ta sẽ hiện thực hóa cái công thức cho đạo hàm, đó là, 1 phần n thì chúng ta sẽ làm 1 chia cho len của y, chia cho len của y,"
        },
        {
          "index": 18,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 849,
          "end_time": 910,
          "text": "ở đây thì chúng ta sẽ hiện thực hóa cái công thức cho đạo hàm, đó là, 1 phần n thì chúng ta sẽ làm 1 chia cho len của y, chia cho len của y, rồi, nhân với lại x, rồi, nhân với lại x, chúng ta sẽ nhân với lại thành phần đầu tiên, đó chính là x, đó chính là x, rồi thành phần thứ 2 sẽ là 1 cái, kết quả, theta chuyển vị x chữ y, chuyển vị, rồi, nhân với x này, nhân với lại 1 cái thành phần, chuyển vị, thì bên trong này á, nó sẽ là, x ở đây chúng ta sẽ là chấm dot, để tương ứng là nhân vai trận, chấm dot là nhân vai trận, rồi, theta, chuyển vị là chấm t, rồi, nhân với lại x, chấm dot x, rồi sau đó chúng ta sẽ trừ cho y, thì ở trong đây là y hoa,"
        },
        {
          "index": 19,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 899,
          "end_time": 960,
          "text": "chuyển vị là chấm t, rồi, nhân với lại x, chấm dot x, rồi sau đó chúng ta sẽ trừ cho y, thì ở trong đây là y hoa, nhưng mà, trong cái đoạn code ở đây, thì y của mình, là viết y thường, rồi, rồi, và chúng ta sẽ tính ở đây, sau khi tính xong thì chúng ta sẽ phải cập nhật lại cái đạo hàm này thêm 1 lần nữa, sau khi tính xong thì chúng ta sẽ phải cập nhật lại cái đạo hàm này thêm 1 lần nữa, sẽ tính lại cái đạo hàm này thêm 1 lần nữa, và, để lấy cái phần tử đầu tiên, thì sẽ là, rath, 0, 0, là phần tử đầu tiên, của theta, thành phần thứ 2, sẽ là, 1, 0,  ở đây thì có cái lỗi,"
        },
        {
          "index": 20,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 950,
          "end_time": 1010,
          "text": "1, 0,  ở đây thì có cái lỗi, ở đây chúng ta sẽ có cái lỗi, đó là, chỉ phải home integer, scalar can be converted to scalar next, ok, bây giờ mình sẽ, khi có cái lỗi xảy ra, thì mình sẽ cùng thử in ra các cái giá trị, để xem coi nó có đúng như cái ý đồ của mình hay không, đầu tiên, à, còn cat này, ok, được rồi, ở đây chúng ta sẽ phải gói, cái hàng số 1 này, với lại cái x này, rồi, gói nó lại, tức là, hàng số 1, là cái giá trị này, hàng x là giá trị này, và chúng ta sẽ gói nó lại trong 1 cái bộ tuple, cái dấu bảo hoạch này, rồi sau đó chúng ta mới truyền vào bên trong phần cat này,"
        },
        {
          "index": 21,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 999,
          "end_time": 1051,
          "text": "là cái giá trị này, hàng x là giá trị này, và chúng ta sẽ gói nó lại trong 1 cái bộ tuple, cái dấu bảo hoạch này, rồi sau đó chúng ta mới truyền vào bên trong phần cat này, rồi sau đó chúng ta mới truyền vào bên trong phần cat này, ok, đã chạy được rồi, và bây giờ chúng ta sẽ cùng kiểm tra xem, cái giá trị thê ta của mình, sau khi chạy xong, nó có giá trị là bao nhiêu, thì nếu chúng ta so với lại, các cái giá trị đã được trend, trước đây, thì chúng ta thấy là giá trị nó giống nhau, thê ta 0 là 7,7, và thê ta 1 là bằng 2,9, 7,7, 2,9, như vậy là nó rất hớp, nó rất hớp với lại cái cách cài đặt, sử dụng với các tham số rồi này, và tương tự như vậy thì chúng ta cũng sẽ trực quan hóa, và không được tùy, nếu như kết quả nó, là giống như lại cái môn ở đây, là giống như lại cái môn ở đây,"
        },
        {
          "index": 22,
          "video_id": "Chương 2_sPoJ8VS7nLc",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_1： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPoJ8VS7nLc",
          "start_time": 1049,
          "end_time": 1051,
          "text": "là giống như lại cái môn ở đây, là giống như lại cái môn ở đây,"
        }
      ]
    },
    {
      "video_id": "Chương 2_sPqwytzfxqM",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: hướng dẫn cách cài đặt mô hình Linear Regression bằng Keras (sử dụng API của TensorFlow/Keras), tận dụng tính năng tự động tính đạo hàm và các optimizer để huấn luyện, và triển khai đầy đủ các phương thức cần thiết cho một model (build/view/train/save/load/summary/predict). [1][2][3]\n\n- Các khái niệm sẽ được đề cập:\n  - Cách định nghĩa kiến trúc model bằng Keras functional/API (Input, Dense, Model). [3][4][8]\n  - Các phương thức chuẩn của mô hình: build, view/summary, train (fit), save, load, predict. [2][3][13][14][15]\n  - Cấu hình optimizer (SGD, Adam) và learning rate, hàm loss (MSE). [10][11][12]\n  - Khái niệm epoch và theo dõi history (loss qua các epoch). [17][18]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Vì sao dùng Keras (auto-differentiation & optimizer)\n- Dùng Keras giúp tránh phải tự tay tính đạo hàm vì Keras hỗ trợ tự động tính gradient và cung cấp các optimizer để huấn luyện mô hình. [1]\n\n### 2.2. Kiến trúc lớp và API cần dùng\n- Sử dụng Keras functional/API để định nghĩa model: cần import lớp Input và Dense, và cuối cùng đóng gói bằng đối tượng Model. [3][4][8]  \n  - Tạo Input layer với shape phù hợp với dữ liệu đầu vào (ví dụ trong bài là một biến đầu vào => shape là vectơ 1 chiều). [5][6]  \n  - Tạo Dense layer (fully-connected) làm output, chỉ định số units (ở đây units = 1 vì dự báo một giá trị). [6]  \n  - Lưu ý: biến input là đối tượng (biến) được tạo trước, và khi khai báo Dense cần truyền đúng biến input đó làm input của layer tiếp theo. [7][8]\n\n### 2.3. Cấu hình activation cho Linear Regression\n- Đối với Linear Regression không dùng activation (tức output là phép biến đổi tuyến tính, *no activation*). [6][7]\n\n### 2.4. Đóng gói model và phương thức build\n- Sau khi khai báo input và output cần đóng gói bằng Model(input, output). [8]  \n- Phương thức build của model cần được gọi/thiết lập với thông tin về kích thước input (ví dụ data chỉ có 1 biến đầu vào => build với input dim = 1). [15][16]\n\n### 2.5. Chọn optimizer và learning rate\n- Có thể dùng Stochastic Gradient Descent (SGD) làm optimizer; nếu chưa biết chọn gì thì có thể dùng learning rate = 0.01 làm giá trị mặc định. [10][11]  \n- Adam là một lựa chọn optimizer phổ biến khác (được nhắc tới như một tùy chọn). [11]\n\n### 2.6. Compile model: optimizer + loss\n- Sau khi đóng gói model, gọi model.compile(...) truyền optimizer và hàm loss. Ví dụ trong bài sử dụng loss = MSE (mean squared error). [12]\n\n### 2.7. Huấn luyện (fit) và history\n- Gọi model.fit(x, y, epochs=...) để huấn luyện; hàm trả về đối tượng history chứa giá trị loss qua từng epoch. [12][13]  \n- Cần chỉ định số epochs: một epoch = một lần duyệt hết toàn bộ dữ liệu huấn luyện; để train nhiều lần qua cùng dữ liệu, tăng số epoch (ví dụ 500 epochs). [17][18]\n\n### 2.8. Lưu và tải model\n- Sau khi huấn luyện có thể lưu mô hình xuống file (model.save) để tránh phải train lại từ đầu; sau đó dùng keras.models.load_model(path) để load lại. [2][3][13][14]\n\n### 2.9. Summary và Predict\n- Gọi model.summary() để xem kiến trúc mô hình (các layer, số tham số). [3][14]  \n- Dự đoán với model.predict(inputs) — khi predict không có nhãn (y) đi kèm, chỉ cần dữ liệu đầu vào. [14][15]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ triển khai Linear Regression với Keras (tóm tắt các bước thực hiện, theo video):\n  1. Import từ keras: Input, Dense, Model (và optimizer cần thiết). [3][4][8][10]  \n  2. Tạo input layer với shape phù hợp (ví dụ shape = 1 cho một biến đầu vào). [5][6]  \n  3. Tạo output = Dense(units=1, activation=None) nối đầy đủ với input. [4][6][7]  \n  4. Đóng gói Model(inputs=input, outputs=output) và gọi build với thông số kích thước input nếu cần. [8][15][16]  \n  5. Compile model với optimizer (ví dụ SGD với lr=0.01) và loss = mse. [11][12]  \n  6. Gọi model.fit(x, y, epochs=NUM_EPOCHS) và thu history để xem loss qua thời gian. [12][13][17]  \n  7. Lưu model (model.save) nếu cần, hoặc load bằng load_model để sử dụng lại. [13][14]  \n  8. Dùng model.predict(x_test) để sinh dự đoán trên input mới. [14][15]\n\n- Ví dụ thực nghiệm trong bài:\n  - Huấn luyện với epochs = 500 (ví dụ) và quan sát loss giảm trong quá trình train: loss khởi điểm khoảng 1000, sau một thời gian giảm về ~300 rồi tiếp tục giảm xuống 48, 23, 21 và cuối cùng khoảng ~10 khi hoàn thành 500 epoch. Chương trình huấn luyện trong ví dụ chạy rất nhanh. [17][18][19]\n\n- Ứng dụng/Trường hợp sử dụng:\n  - Linear Regression đơn giản để dự đoán một biến liên tục từ một đầu vào (demo minh họa quy trình xây dựng/huấn luyện/lưu/tải/predict bằng Keras). [6][15]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Sử dụng Keras functional/API để hiện thực Linear Regression giúp tận dụng auto-differentiation và các optimizer có sẵn, giảm công sức tự tính đạo hàm và tối ưu. [1][3]  \n  - Quy trình chuẩn gồm: định nghĩa Input/Dense, đóng gói thành Model, build (cung cấp kích thước input), compile (optimizer + loss), fit (với epochs), lưu/load model, summary và predict. [3][8][12][13][14][15]  \n  - Cấu hình phổ biến: optimizer như SGD (lr ≈ 0.01) hoặc Adam; loss phổ biến cho regression là MSE; đối với linear regression không dùng activation. [11][12][6][7]\n\n- Tầm quan trọng:\n  - Nắm vững cách cài đặt cơ bản này là nền tảng để triển khai các mô hình phức tạp hơn trong các bài học tiếp theo, vì các thao tác build/compile/fit/save/load/predict là những thao tác lặp lại trong hầu hết pipeline huấn luyện mô hình. [1][2][3]\n\n- Liên hệ với các bài giảng khác:\n  - Nội dung này là phần hiện thực hóa (implementation) của kiến thức về mô hình tuyến tính và huấn luyện (đã/ sẽ được đề cập trong chương), và sẽ được tái sử dụng cho các mô hình phức tạp hơn trong các phần sau. [1][2]\n\n–––––\nGhi chú: Các trích dẫn [1], [2], ... tham chiếu trực tiếp tới các đoạn (chunk) tương ứng trong video với timestamps đã cung cấp.",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 0,
          "end_time": 65,
          "text": "rồi bây giờ chúng ta sẽ qua cái phiên bản cài đặt tiếp theo quan trọng nhất và nó sẽ được sử dụng xuyên suốt cho cái mô hợp này của mình đó là chúng ta sẽ sử dụng thư viện tên sạc vô và keras thì với cái cách cài đặt mà sử dụng keras nó sẽ giúp cho chúng ta đỡ phải đi tính đạo hàng tại vì nó đã hỗ trợ cho mình cái việc tính đạo hàng và các thư viện liên quan đến tối ưu hóa optimizer để hỗ trợ để huấn luyện thì ở đây chúng ta sẽ có một cái bộ khung cài đặt giờ chúng ta sẽ phải tuân thủ để từ nay về sau cái việc mà cài đặt nó sẽ đi theo đúng cái fan như thế này đối tượng của mình nó sẽ được cài đặt bằng một cái dạng là quán và mai mô đồ chúng ta sau này chúng ta có thể đổi nó thành cái tên của cái môn của mình ít chính là cái con sắt cơ Nếu như chúng ta muốn có những ký khởi tạo bầu bào ban đầu hoặc không thì ta sẽ vì con rồi nên phương thức tiếp theo rất quan trọng"
        },
        {
          "index": 2,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 44,
          "end_time": 110,
          "text": "mình nó sẽ được cài đặt bằng một cái dạng là quán và mai mô đồ chúng ta sau này chúng ta có thể đổi nó thành cái tên của cái môn của mình ít chính là cái con sắt cơ Nếu như chúng ta muốn có những ký khởi tạo bầu bào ban đầu hoặc không thì ta sẽ vì con rồi nên phương thức tiếp theo rất quan trọng và bắt buộc phải có đã chúng lập phương thức view phương thức view này để cho chúng ta biết kiến trúc của khí mùa này như thế nào phương thức tiếp theo rất là quan trọng đó chính là phương thức trên dùng để huấn luyện mô hình với cái data trên của mình phương thức theo đó là chúng ta sẽ lưu cái mô hình để xong dưới file để sau này nếu như cái quá trình train mô hình nó tốn rất nhiều thời gian có thể lên đến vài tiếng hoặc là vài ngày thì cái việc lưu mô hình này sẽ giúp cho chúng ta tái sử dụng được mô hình về sau và đi kèm với phương thức sell thì chắc chắn nó sẽ có cái phương thức gọi là phương thức load để load cái mô hình này lên từ file rồi chúng ta sẽ tóm tắt mô hình thông qua cái phương"
        },
        {
          "index": 3,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 95,
          "end_time": 160,
          "text": "được mô hình về sau và đi kèm với phương thức sell thì chắc chắn nó sẽ có cái phương thức gọi là phương thức load để load cái mô hình này lên từ file rồi chúng ta sẽ tóm tắt mô hình thông qua cái phương thức là xong rồi cho biết là kiến trúc mô hình này bao gồm các lớp nào và số tham số của từng lớp ra sao và cuối cùng đó chính là phương thức predict chúng ta sẽ đưa ra cái dự đoán dựa trên cái input đầu vào rồi bây giờ chúng ta sẽ copy toàn bộ cái nội dung ở đây và chúng ta sẽ hiện thực hóa nó cho cái mô hình là linear regression thì đầu tiên đó là mymodel chúng ta sẽ để là linear regression rồi để mà có thể hiện thực hóa được cho cái phương thức build thì chúng ta sẽ phải form keras.layout rồi chúng ta sẽ import cái lớp input lớp input chính là"
        },
        {
          "index": 4,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 148,
          "end_time": 210,
          "text": "form keras.layout rồi chúng ta sẽ import cái lớp input lớp input chính là lớp input chính là cái lớp đầu tiên này ừ ừ rồi tiếp theo đó là để tính ra được cái nốt xong rồi thì chúng ta sẽ phải có một cái lớp nữa là lớp dance cái dance có nghĩa là kết nối đầy đủ tất cả những cái dự kiện đầu vào sẽ được kết nối đầy đủ với lại cái nốt đầu ra đó là lý do mà nó tại sao nó đặt tên là dance rồi ừ ừ ừ ừ ừ ừ ừ ừ ừ ừ                    ừ ừ ừ ừ ừ ừ       kể đặt cái phương thức bưu và để cho tổng quát thì cái bưu này chúng ta sẽ phải cho biết số cái input"
        },
        {
          "index": 5,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 199,
          "end_time": 262,
          "text": "ừ ừ ừ ừ ừ ừ       kể đặt cái phương thức bưu và để cho tổng quát thì cái bưu này chúng ta sẽ phải cho biết số cái input đen sân của mình là bao nhiêu thì trong cái trường hợp này trong cái ví dụ này input đen sân này của mình em là bằng một nhưng mà một cách tổng quát đúng không một cách tổng quát thì chúng ta nên để tham số hóa để tham số hóa cho m này là input đen sân ừ ừ ừ ừ     rồi chúng ta sẽ tạo cái chúng ta sẽ đi lần được từ trái qua phải đầu tiên sẽ là tạo cái lớp input này tạo lớp input vậy là input rồi nó sẽ cho biết là cái shape tức là cái kích thước của nó là bao nhiêu thì xếp ở đây là quy định là input tìm phải tức là nó chỉ là một cái vector thôi rồi và chúng"
        },
        {
          "index": 6,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 240,
          "end_time": 311,
          "text": "này tạo lớp input vậy là input rồi nó sẽ cho biết là cái shape tức là cái kích thước của nó là bao nhiêu thì xếp ở đây là quy định là input tìm phải tức là nó chỉ là một cái vector thôi rồi và chúng ta sẽ trả về đây sẽ là input trả về một cái tên là input tiếp theo đó là chúng ta sẽ tính cái output output của mình nó là cái kết quả của cái phép biến đổi là dense đây chính là output đây là chính là cái giá trị dự đoán này rồi output dự đoán là kết quả của phép biến đổi dense kết nối đầy đủ thì trong cái dense này thì chúng ta phải chỉ cho cái phương thức này nó biết đó là output của mình nó sẽ có bao nhiêu nốt thì trong trường hợp này chúng ta chỉ có duy nhất một nốt chúng ta chỉ có duy nhất một nốt cho ra chúng ta sẽ để là 1 rồi activation activation gì ở đây"
        },
        {
          "index": 7,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 293,
          "end_time": 364,
          "text": "chúng ta phải chỉ cho cái phương thức này nó biết đó là output của mình nó sẽ có bao nhiêu nốt thì trong trường hợp này chúng ta chỉ có duy nhất một nốt chúng ta chỉ có duy nhất một nốt cho ra chúng ta sẽ để là 1 rồi activation activation gì ở đây chúng ta chỉ tính toán tiến tính mà không có cái Activate nào gửi chúng ta sẽ đại tì thousand là và giật ở Yeah Rồi và trong khi em ví tích thì cái công thức của cái cái tham số app play son này nó sẽ lạo hành xe coi rồi chúng ta có sử dụng 3S không thì ở đây ta sẽ có sử dụng cái 저희가 suy để là là trục rồi và đây là cái lớp biến đổi và cái lớp biến đổi này nó phải cho biết cái những chuyện cái layer đầu vào nó là gì vậy đó ở đây cái cú pháp của mình sẽ ra để mở qua tìm tôi lưu ý ở đây chính là cái biến input này biến input này chính là cái lớp cái biến input này chứ nó không phải là module và cái class tên này input hoa chứ gì đây nó phải là cái biến mà đã tạo ra ở bước trước đó"
        },
        {
          "index": 8,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 345,
          "end_time": 424,
          "text": "layer đầu vào nó là gì vậy đó ở đây cái cú pháp của mình sẽ ra để mở qua tìm tôi lưu ý ở đây chính là cái biến input này biến input này chính là cái lớp cái biến input này chứ nó không phải là module và cái class tên này input hoa chứ gì đây nó phải là cái biến mà đã tạo ra ở bước trước đó rồi cuối cùng chúng ta sẽ đóng gói chúng ta sẽ đóng gói toàn bộ cái input và cái output này thì để đóng gói để đóng gói thì chúng ta sẽ sử dụng một cái đối tượng nó tên là module import from keras import module rồi và"
        },
        {
          "index": 9,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 394,
          "end_time": 484,
          "text": ""
        },
        {
          "index": 10,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 424,
          "end_time": 513,
          "text": "ở đây nó sẽ không biết tên sau lâu là gì không ở đây thì nó sẽ phải lại import rô hét tia rồi và chúng ta sẽ sử dụng là stochastic gradient design"
        },
        {
          "index": 11,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 495,
          "end_time": 558,
          "text": "ở đây nó sẽ không biết tên sau lâu là gì không ở đây thì nó sẽ phải lại import rô hét tia rồi và chúng ta sẽ sử dụng là stochastic gradient design và chúng ta sẽ sử dụng là stochastic gradient design tuy nhiên ở đây thì nó sẽ có một số cái tuy nhiên ở đây thì nó sẽ có một số cái tuy nhiên ở đây thì nó sẽ có một số cái mục số KKKI bài giờ khác biểu nhiên là Adam mục số KKKI bài giờ khác biểu nhiên là Adam mục số KKKI bài giờ khác biểu nhiên là Adam trong rạp mặt định nếu chúng ta không biết gì hết trong rạp mặt định nếu chúng ta không biết gì hết trong rạp mặt định nếu chúng ta không biết gì hết về cattle hiện ra thấy xin quả rồi tự nhiên đây về cattle hiện ra thấy xin quả rồi tự nhiên đây thì by default chúng ta có thể sử dụng là 0.01 rồi đồng thời là chúng ta sẽ phải cho cái model của mình nhận biết cho cái model của mình nhận biết cái optimizer này"
        },
        {
          "index": 12,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 548,
          "end_time": 611,
          "text": "rồi đồng thời là chúng ta sẽ phải cho cái model của mình nhận biết cho cái model của mình nhận biết cái optimizer này chúng ta sẽ truyền vào cái optimizer và phải cho nó biết cái hàm loss thì ở đây là trong keras nó cũng hỗ trợ cho mình luôn các cái hàm loss phổ biến thì ở đây chúng ta sẽ sử dụng là mse rồi sau khi model của mình đã được đóng gói đúng không? đã được compile với cái thu thức xin lỗi với các cái hàm số optimizer và hàm loss thì chúng ta sẽ sell.model.fit chúng ta sẽ fit is trend và is trend ok ok                                 Sẽ trả cái tham số các cái quá trình huấn luyện Nó có những cái tham số nào"
        },
        {
          "index": 13,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 600,
          "end_time": 660,
          "text": "ok                                 Sẽ trả cái tham số các cái quá trình huấn luyện Nó có những cái tham số nào giá trị loss nó chẳng như thế nào nó gọi là history thì nó sẽ trả ra đây rồi để sau khi trend xong thì chúng ta sẽ lưu cái model này xuống đúng không? thì chúng ta sẽ phải có thêm một cái tham số đó là cái model là cái đường dẫn đến cái file model của mình rồi và hàm này nó sẽ không dVIE greatest Nó sẽ ch등 evil thì chúng ta sẽ để theo, SDK nó sẽ để thêm và đây sẽ là kẻ lệnh chúng ta sẽ lại trả ra là"
        },
        {
          "index": 14,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 648,
          "end_time": 710,
          "text": "và đây sẽ là kẻ lệnh chúng ta sẽ lại trả ra là  là cái phương thức nữa của Keras đó là Keras.model chúng ta sẽ import phương thức là load model rồi ở đây sẽ là load model và chúng ta sẽ truyền cái đường dẫn vào model bạn sau khi xong thì nó xong nó sẽ trả vào cái biến cell.model này rồi summary thì cell.model.summary rồi để dự đoán thì chúng ta sẽ phải có một cái biến đầu vào đó là istech chúng ta sẽ không có istech tại vì khi dự đoán mà chúng ta đâu có cái nhãn của cái kết quả trả về đâu chúng ta chỉ có cái input đầu vào thôi để istech và đây sẽ là cell.model"
        },
        {
          "index": 15,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 693,
          "end_time": 761,
          "text": "rồi để dự đoán thì chúng ta sẽ phải có một cái biến đầu vào đó là istech chúng ta sẽ không có istech tại vì khi dự đoán mà chúng ta đâu có cái nhãn của cái kết quả trả về đâu chúng ta chỉ có cái input đầu vào thôi để istech và đây sẽ là cell.model model.predict istech rồi bây giờ chúng ta sẽ chạy thử may quá không có lỗi bước tiếp theo chúng ta sẽ khởi tạo mô hình và huấn luyện với cái dữ liệu mẫu thì mô hình ở đây sẽ là linear impression và chúng ta sẽ đưa vào một cái biến đó là link text rồi tiếp theo đó là chúng ta sẽ gọi cái hàm build là hàm build và chúng ta sẽ gọi hàm build là hàm build và chúng ta sẽ gọi hàm build học các cú và phải có một cái tham số Đó là en custation n Christ chấm collaborations của bạn vào trung inter lab đẹp đải vì sao ở trong kể dữ liệu này chúng ta chỉ có duy nhất một cái"
        },
        {
          "index": 16,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 740,
          "end_time": 810,
          "text": "học các cú và phải có một cái tham số Đó là en custation n Christ chấm collaborations của bạn vào trung inter lab đẹp đải vì sao ở trong kể dữ liệu này chúng ta chỉ có duy nhất một cái biến đầu vào có jakieś sách không tính thật là chiếcrogen màng đại vì trong cái lớp đen ở trên at nó đã có cái tham số là cho biết là có sử dụng bài app hay không rồi, và đó input điên vào đây chỉ là cái dữ liệu thô ban đầu, phần tính cái thành phần bài app, ta phải chú ý cho đó ha rồi ở đây build sẽ là bằng 1 và chúng ta sẽ tiến hành là trend và chúng ta sẽ truyền vào x và y x và y chính là cái cặp dữ liệu rồi invalid argument, compile optimizer, rồi thì ở đây chúng ta sẽ cùng xem lại keras.optimizer"
        },
        {
          "index": 17,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 799,
          "end_time": 859,
          "text": "invalid argument, compile optimizer, rồi thì ở đây chúng ta sẽ cùng xem lại keras.optimizer ok, ở đây thiếu cái tham số, ở đây là cái tham số là này chúng ta biết size nội chúng ta rồi đây chúng ta chạy lại rồi, ở đây khi chúng ta trend thì chúng ta phải cho cái chương trình nó biết là trend với bao nhiêu epoch thì ở đây với 1 trên 1 epoch thì cái hàm loss của mình, cái giá trị loss của mình nó trả ra là đến 1000 thì rõ ràng đây là một giá trị rất là lớn, thế thì chúng ta sẽ phải tham số hóa để phá phá cái số lượng 5 epoch. epoch là sao? tức là giả sử như dữ liệu chúng ta có 100 mẫu"
        },
        {
          "index": 18,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 849,
          "end_time": 918,
          "text": "để phá phá cái số lượng 5 epoch. epoch là sao? tức là giả sử như dữ liệu chúng ta có 100 mẫu thì một epoch có nghĩa là chúng ta train hết một việc 100 mẫu dữ liệu này và nếu như chúng ta muốn train đi train lại nhiều lần thì chúng ta sẽ phải khai báo cho nó biết là số lần mà chúng ta muốn train đi train lại cái dữ liệu này. Ví dụ như chúng ta muốn train đi train lại là 500 lần thì chúng ta sẽ để là epoch bằng 500 và đây chúng ta mô đồ chấm viết thì chúng ta sẽ phải có là epoch là bài num epoch. Rồi và ở đây train thì chúng ta sẽ truyền ra là 500 và cũng may mắn đó là chương trình của mình nó train rất là nhanh và các bạn cũng sẽ thấy là cái loss ban đầu là nó là 1.000 đúng không? thì sau khi train một lúc sau ban đầu là 300"
        },
        {
          "index": 19,
          "video_id": "Chương 2_sPqwytzfxqM",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_2： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/sPqwytzfxqM",
          "start_time": 890,
          "end_time": 937,
          "text": "là bài num epoch. Rồi và ở đây train thì chúng ta sẽ truyền ra là 500 và cũng may mắn đó là chương trình của mình nó train rất là nhanh và các bạn cũng sẽ thấy là cái loss ban đầu là nó là 1.000 đúng không? thì sau khi train một lúc sau ban đầu là 300 rồi nó giảm xuống còn 48, 23, 21 và khi kết thúc thực toán chạy hết 500 epoch thì chúng ta sẽ có 1.000 epoch. Và khi kết thúc thực toán chạy hết 500 epoch thì chúng ta sẽ có 1.000 epoch. Và khi kết thúc thực toán chạy hết 500 epoch. thì nó giảm xuống còn là khoảng 10 thôi từ 300 xuống còn 10 thì nó giảm xuống còn là khoảng 10 thôi từ 300 xuống còn 10"
        }
      ]
    },
    {
      "video_id": "Chương 2_CqnM7BT7oSU",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng:\n  - Minh họa cách cài đặt và kiểm tra một mô hình *linear regression* (hàm tuyến tính đơn giản) bằng cách truy xuất tham số (theta), trực quan hóa đường hồi quy, lưu/mở mô hình và thực hiện dự đoán; đồng thời so sánh cách cài đặt “mỗi tham số một” với dạng *vector hóa* và nêu lợi thế của Keras trong việc tự động hóa đạo hàm và cập nhật tham số. [1][2][5][9][10][11]\n\n- Các khái niệm sẽ được đề cập:\n  - Truy xuất trọng số từ lớp của mô hình (getweight) và cách tách *weight* / *bias*. [1][2][3]  \n  - Cấu trúc tham số theta = (theta0, theta1) cho bài toán hồi quy tuyến tính một biến. [3][4]  \n  - Trực quan hóa đường hồi quy và đánh giá sai số do noise. [5]  \n  - Lưu và tải mô hình, thao tác với dữ liệu đầu vào khi predict. [6][7][8][9]  \n  - Vấn đề mở rộng khi triển khai cập nhật từng tham số và giải pháp vector hóa; lợi ích của Keras (framework tự tính đạo hàm). [9][10][11]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Truy xuất tham số (weights & bias) từ mô hình\n- Để xem tham số theta, cần gọi một phương thức lấy trọng số như depth.getweight và trả về self.model.layer[...] — ta bỏ qua layer 0 (input) và quan sát layer 1 (lớp tính toán). [1]\n- Khi lấy weight của lớp fully-connected, kiến trúc khung làm tách riêng thành phần trọng số (weights) và bias, nên ta nhận được hai mảng con: một mảng trọng số và một mảng bias. [2][3]\n\n### 2.2. Cấu trúc của các tham số trong bài toán một feature\n- Do mô hình ở ví dụ chỉ có một feature, mảng trọng số chỉ chứa một phần tử duy nhất (tương ứng theta1). [3]\n- Giá trị trọng số (ví dụ ~3.13) là tham số cho kết nối đầy đủ *không bao gồm* bias; bias (ví dụ ~6.6) được lưu riêng. [3]\n- Ta xây dựng theta dưới dạng:\n  - theta0 = bias\n  - theta1 = weight (cho feature)\n  - Tóm tắt toán học: ŷ = theta0 + theta1 * x  (hồi quy tuyến tính cơ bản). [3][4]\n\n### 2.3. Trích xuất và hiển thị theta\n- Trong code, sau khi lấy w (weights array), ta ánh xạ thành các thành phần theta (ví dụ gán w[?] vào theta1 và bias vào theta0), sau đó in ra theta0, theta1 để kiểm tra. [2][4]\n- (Ghi chú thực thi) đôi khi cần thêm các bước padding/reshape (ví dụ “thêm ... 0.0”) để phù hợp định dạng mảng trước khi in/visualize. [4]\n\n### 2.4. Trực quan hóa và nhận xét về kết quả học\n- Sau training và trích xuất theta, vẽ đường thẳng hồi quy trên scatter plot của dữ liệu; đường thẳng học được thường xuyên xuyên qua “đám mây điểm” mặc dù các tham số có thể hơi khác so với giá trị tham chiếu do noise trong dữ liệu. [5]\n- Ví dụ: tham số 3 và 6 gần khớp với giá trị thực (ví dụ 3.8) nhưng không đạt chính xác vì có nhiễu (noise). Mô hình vẫn nắm bắt đúng dạng tuyến tính tổng quát. [5]\n\n### 2.5. Lưu mô hình và gọi hàm predict\n- Trước khi dùng predict, cần lưu mô hình vào một đường dẫn (ví dụ “my_model...”) để có thể tải lại sau. [6][7]\n- Khi predict, phải đảm bảo dạng dữ liệu đầu vào đúng; không thể truyền một scalar đơn lẻ mà cần truyền mảng (array) có shape phù hợp — nếu truyền sai sẽ gây lỗi. [8]\n- Ví dụ thực thi: truyền x = 7 (dưới dạng array) vào model.predict => kết quả ≈ 27–28 (trong demo giá trị in ra là khoảng 28.76). Đây tương ứng với dự đoán trên đường thẳng đã học. [7][8][9]\n\n### 2.6. Hạn chế của cách “một tham số — một biến” và giải pháp vector hóa\n- Phiên bản cài đặt đầu tiên (mỗi tham số quản lý riêng) có nhược điểm lớn: nếu mô hình có hàng triệu tham số, ta sẽ phải triển khai và cập nhật từng tham số một, rất bất tiện và không khả thi trên quy mô lớn. [9]\n- Giải pháp: *vector hóa* — đóng gói tất cả tham số trong một biến (vector/matrix) để xử lý hiệu quả hàng loạt. Tuy nhiên, cách vector hóa đòi hỏi tự suy ra và triển khai công thức đạo hàm (gradient) một cách tường minh. [10]\n\n### 2.7. Lợi ích của Keras / framework deep learning\n- Khi dùng Keras (hoặc “Deep Learning Traveler” như bài giảng nhắc), ta chỉ cần định nghĩa kiến trúc: kích thước đầu vào, phép biến đổi, activation, có dùng bias hay không, loss function, optimizer... Framework sẽ tự động tính đạo hàm và cập nhật tham số cho ta, không cần hiện thực tay các công thức gradient. [10][11]\n- Từ bài Logistic trở đi, giảng viên sẽ dùng cách cài đặt với Keras để đơn giản hóa việc xây dựng mô hình và tận dụng tính tự động hóa tính đạo hàm. [11][12]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa trong video:\n  - Truy xuất w và bias từ layer 1 của mô hình đã train, in theta0/theta1, vẽ đường thẳng hồi quy trên dữ liệu (một feature). [1][2][3][4][5]\n  - Lưu mô hình ra file, sau đó tải và predict với input x = 7 (phải truyền dưới dạng array), kết quả predict ≈ 28.7, khớp với suy đoán trực quan từ đường hồi quy. [6][7][8][9]\n\n- Ứng dụng thực tế / trường hợp sử dụng:\n  - Minh họa cơ bản cho hồi quy tuyến tính một biến — nền tảng cho bài toán hồi quy phức tạp hơn và cho việc hiểu cách framework quản lý tham số. [5][9]\n  - Bài học mở rộng: khi số chiều và số tham số lớn (mô hình phức tạp, millions of parameters), cần vector hóa và dùng framework (Keras) để tự động hóa việc tính gradient và cập nhật. [9][10][11]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Ta đã thực hiện việc trích xuất trọng số và bias từ layer tính toán (bỏ qua input layer), ánh xạ thành theta0, theta1 và trực quan hóa đường hồi quy learnt từ dữ liệu có noise. [1][2][3][4][5]\n  - Đã trình bày quy trình lưu/tải mô hình và cách gọi predict (lưu ý dạng đầu vào phải là array, không phải scalar), ví dụ dự đoán x=7 → ~28.7. [6][7][8][9]\n  - Nêu hạn chế của cách quản lý tham số “mỗi tham số một” khi mở rộng và khuyến nghị dùng vector hóa; đồng thời nhấn mạnh lợi ích của Keras (framework tự động tính đạo hàm và cập nhật) để đơn giản hóa việc triển khai mô hình phức tạp. [9][10][11]\n\n- Tầm quan trọng của nội dung:\n  - Hiểu rõ cách truy xuất và cấu trúc tham số (weight vs bias), cùng với thực hành lưu/predict, là bước căn bản cần thiết trước khi chuyển sang các mô hình lớn hơn. Việc nhận ra giới hạn của cài đặt thủ công dẫn tới việc sử dụng Keras để scale tốt hơn. [2][3][9][11]\n\n- Liên hệ với các bài giảng khác:\n  - Giảng viên thông báo từ bài Logistic trở đi sẽ sử dụng cách cài đặt bằng Keras để tận dụng tự động hóa tính đạo hàm và đơn giản hóa việc xây dựng mô hình. [11][12]\n\n(Thông tin trên được trích trực tiếp từ các đoạn trong video: [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12].)",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 2_CqnM7BT7oSU",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_3： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/CqnM7BT7oSU",
          "start_time": 0,
          "end_time": 59,
          "text": "Rồi bây giờ tiếp theo thì chúng ta sẽ cùng xem xem cái tham số theta của mình. Thế thì muốn xem cái tham số theta thì mình sẽ phải cung cấp thêm cho nó một hành nữa. Một cái phương thức nữa đó là depth.getweight. Rồi và chúng ta sẽ return là self.model.layer. Thì layer số 0 là input. Mình sẽ không xem cái layer đó mà mình sẽ xem cái layer số 1 chính là cái lớp đen. Input này thì mình sẽ không xem cái này mà mình sẽ xem từ cái số 1 cho đi chính là cái lớp đen. Rồi và getweight. Rồi. Chúng ta sẽ phải chạy lại và phải train lại hành này. Cũng may đó là. Thế chứ."
        },
        {
          "index": 2,
          "video_id": "Chương 2_CqnM7BT7oSU",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_3： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/CqnM7BT7oSU",
          "start_time": 52,
          "end_time": 110,
          "text": "Rồi. Chúng ta sẽ phải chạy lại và phải train lại hành này. Cũng may đó là. Thế chứ. Chương trình của mình nó chạy khá là nhanh. Rồi. Bây giờ là lin.getweight. Thì weight này mình sẽ để là w ở đây đi ha. Rồi. Thì chúng ta sẽ thấy là. Khi chạy với 500 một lập. Thì. Ở đây nó có 2. Nó sẽ có một cái array. Trong đó chúng ta có thể quan sát được nhanh. Đó là có 2 cái array con. Thì do cái kiến trúc của. Của cái kê rác nó tổng quát hơn. Nên. Nó sẽ tách cái thành phần bias."
        },
        {
          "index": 3,
          "video_id": "Chương 2_CqnM7BT7oSU",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_3： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/CqnM7BT7oSU",
          "start_time": 100,
          "end_time": 160,
          "text": "Thì do cái kiến trúc của. Của cái kê rác nó tổng quát hơn. Nên. Nó sẽ tách cái thành phần bias. Và cái thành phần trọng số của cái phép. Fully connected. Cái phép kết nối đầy đủ. Riêng. Thì cái bar này. 3.13. Đó chính là cái tham số. Cho cái. Cho cái phần kết nối đầy đủ này. Không bao gồm bias. Không bao gồm bias. Và bias. Thì nó sẽ là. Nó lưu trong một cái bộ array riêng. Thì là 6.6 nè. Là cái bộ. Tham số cho cái bias này. Còn. 3.13. Chính là cái bộ tham số. Cho cái vùng gọi lại. Cho cái phần gọi lại. Thì do ở đây là chúng ta. Chỉ có duy nhất. Một cái. Feature thôi. Nên cái array này của chúng ta. Nó cũng sẽ có duy nhất một cái tham số thôi. Rồi bây giờ chúng ta sẽ lấy cái thành phần theta. Không."
        },
        {
          "index": 4,
          "video_id": "Chương 2_CqnM7BT7oSU",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_3： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/CqnM7BT7oSU",
          "start_time": 150,
          "end_time": 210,
          "text": "Feature thôi. Nên cái array này của chúng ta. Nó cũng sẽ có duy nhất một cái tham số thôi. Rồi bây giờ chúng ta sẽ lấy cái thành phần theta. Không. Nó sẽ là bằng w. 1. Đó chính là cái thành phần bên phải nè. Rồi. Không. Và. Theta 1. Nó sẽ là. W. Không. Tức là cái thành phần array này. Và nó sẽ phải thêm. Ờ. Chuyên súc vô bên. Hai cái. Cái cái phần nữa. Nữa. Hai cái hoạt nữa. Là 0.0. Rồi. Bây giờ chúng ta sẽ in ra. Theta 0. 2 chồng. Theta 0. Theta 1. Rồi. Giá trị đương nhiên. Nó sẽ giống với những gì chúng ta nhìn. Và bây giờ chúng ta sẽ trực quan hóa. Chúng ta sẽ cùng trực quan hóa."
        },
        {
          "index": 5,
          "video_id": "Chương 2_CqnM7BT7oSU",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_3： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/CqnM7BT7oSU",
          "start_time": 202,
          "end_time": 258,
          "text": "Rồi. Giá trị đương nhiên. Nó sẽ giống với những gì chúng ta nhìn. Và bây giờ chúng ta sẽ trực quan hóa. Chúng ta sẽ cùng trực quan hóa. Nó sẽ giống với những gì chúng ta nhìn. Chúng ta sẽ trực quan hóa. Tuy nhiên là khi chúng ta quan sát. Thì chúng ta thấy. Cái giá trị 3. Và 6 này á. Nó cũng đã khá khớp. Nó đã khá khớp. Với lại cái con số là 3. 8 ở đây. Và sợi dĩ tại sao nó không đạt được đến. Giá trị là 3. 8. Là vì nó có cái đại liệu noise. Rồi. Theta 0. Và theta 1. Thì chúng ta sẽ thấy cái đường thẳng của mình. Nó sẽ giống như thế này.  Nó cũng đi xuyên qua. Mặc dù. Cái tham số. Hai cái tham số này. Nó có khác đôi chút. So với lại. Hai cái phiên bản trên. Nhưng mà cái model của mình. Nó vẫn học được về. Đúng cái. Dạng đường thẳng. Đi xuyên qua cái đám mây điểm ở đây. Tiếp theo. Thì chúng ta sẽ. Thử sử dụng các cái phương thức. Ví dụ như là phương thức predict."
        },
        {
          "index": 6,
          "video_id": "Chương 2_CqnM7BT7oSU",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_3： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/CqnM7BT7oSU",
          "start_time": 249,
          "end_time": 309,
          "text": "Đúng cái. Dạng đường thẳng. Đi xuyên qua cái đám mây điểm ở đây. Tiếp theo. Thì chúng ta sẽ. Thử sử dụng các cái phương thức. Ví dụ như là phương thức predict. Trước khi sử dụng phương thức predict. Thì chúng ta sẽ. Lưu cái model này xuống. Và chúng ta sẽ. Đi xuyên qua.  Rồi. Chúng ta sẽ truyền vô một cái đường dẫn. Ví dụ như là. Mây. Model. . . . . . . . .  . .    .   . . .  . . . . . . . ."
        },
        {
          "index": 7,
          "video_id": "Chương 2_CqnM7BT7oSU",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_3： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/CqnM7BT7oSU",
          "start_time": 300,
          "end_time": 363,
          "text": ". . . . . . . . . . . . . . . . . .   . .         là lưu mô đồ chấm lốt từ một cái mô đồ đã được lưu trước đó là mai mô đồ rồi sau đó thì chúng ta sẽ cùng predict ví dụ như chúng ta tính cái giá trị là tại 7 đi thì chúng ta giống lên giống lên thì chiếu qua bên đây đâu đó nó phải ra là 27 28 gì đấy thì nó mới đúng bây giờ chúng ta sẽ truyền"
        },
        {
          "index": 8,
          "video_id": "Chương 2_CqnM7BT7oSU",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_3： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/CqnM7BT7oSU",
          "start_time": 340,
          "end_time": 408,
          "text": "cùng predict ví dụ như chúng ta tính cái giá trị là tại 7 đi thì chúng ta giống lên giống lên thì chiếu qua bên đây đâu đó nó phải ra là 27 28 gì đấy thì nó mới đúng bây giờ chúng ta sẽ truyền vô giá trị là 7 rồi nó báo sai ở cái dòng này ở đây nó sẽ không thể truyền vào cái giá trị Scala mà chúng ta phải truyền vào giá trị dạng 5B array rồi ở đây sẽ có một cái array giá trị là 7 ừ ừ ở đây mình nên trả ra là predict rồi sau đó chúng ta sẽ in qua predicted value of x vào 7 rồi chúng ta sẽ để là track"
        },
        {
          "index": 9,
          "video_id": "Chương 2_CqnM7BT7oSU",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_3： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/CqnM7BT7oSU",
          "start_time": 399,
          "end_time": 459,
          "text": "predicted value of x vào 7 rồi chúng ta sẽ để là track rồi như vậy thì nó sẽ là 28 đúng như cái hồi nãy chúng ta dự đoán đúng không nếu giá trị 7 nè chiếu lên trên cái đường thẳng này sau đó chiếu qua đây thì nó sẽ ra giá trị là khoảng 27 28 thì nó ra với cái mô đồ của mình cho ra là hai mươi tám bảy sáu như vậy thì qua cái demo này chúng ta đã tiến hành cài đặt cái mô mini medicine với ba phiên bản phiên bản đầu tiên đó chính là phiên bản tham số rồi ra thì cái phiên có một cái điểm yếu đó chính là chúng ta sẽ phải đi triển khai cho từng tham số thì điều gì xảy ra nếu như mô hình của mình nó lên đến hàng triệu tham số tức là chúng ta sẽ phải cập nhật cái này"
        },
        {
          "index": 10,
          "video_id": "Chương 2_CqnM7BT7oSU",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_3： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/CqnM7BT7oSU",
          "start_time": 449,
          "end_time": 510,
          "text": "đó chính là chúng ta sẽ phải đi triển khai cho từng tham số thì điều gì xảy ra nếu như mô hình của mình nó lên đến hàng triệu tham số tức là chúng ta sẽ phải cập nhật cái này hàng triệu lần tức là một triệu tham số thì chúng ta sẽ phải có một triệu dòng cập nhật như thế này rất là bất tiện do đó thì chúng ta phải chuyển sang cái dạng thứ hai đó là dạng vector hóa vector hóa này thì mọi tham số nó sẽ được đóng gói trong một cái biến thay tay tuy nhiên thì cái cách làm này nó lại có một cái điểm yếu đó là chúng ta phải đi tính cái công thức chúng ta sẽ phải đi tính cái công thức đạo hàm một cách tương minh trong khi đó với cái phiên bản mà dùng Keras thì chúng ta có thể quan sát thấy ở trong cái mã nguồn của mình không hề có một cái bước nào đi tính đạo hàm hết mà mình chỉ quy định cho nó cái kiến trúc là đầu vào kích thước bao nhiêu thực hiện cái phép biến đổi gì, activation là gì rồi rồi có sử dụng bias hay không kết thúc rồi mình quy ước cho nó là sử dụng độ lỗi là gì"
        },
        {
          "index": 11,
          "video_id": "Chương 2_CqnM7BT7oSU",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_3： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/CqnM7BT7oSU",
          "start_time": 498,
          "end_time": 552,
          "text": "cái kiến trúc là đầu vào kích thước bao nhiêu thực hiện cái phép biến đổi gì, activation là gì rồi rồi có sử dụng bias hay không kết thúc rồi mình quy ước cho nó là sử dụng độ lỗi là gì thậm chí chúng ta cũng không cần phải cài lại cái độ lỗi nó cũng đã có một số cái độ lỗi phổ biến rồi như MSC, cross entropy, v.v rồi chúng ta cũng sẽ chỉ cho nó biết là cái optimizer là gì và chuyện còn lại là cái Deep Learning Traveler nó sẽ tự tính toán đạo hàm sẽ tự cập nhật cho mình thì đây chính là cái điểm lợi của cái việc là dùng Keras và từ nay trở về sau từ bài Logistics trở đi thì chúng ta sẽ sử dụng cái cách kè đặt này cho nó đơn giản và cái việc tính đạo hàm nó đã được cái Deep Learning Traveler ngầm thực hiện cho chúng ta rồi và chúng ta chỉ tập trung vào cái việc là xây dựng cái mô hình mà thôi"
        },
        {
          "index": 12,
          "video_id": "Chương 2_CqnM7BT7oSU",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 2b_3： Cài đặt mô hình linear regression",
          "video_url": "https://youtu.be/CqnM7BT7oSU",
          "start_time": 549,
          "end_time": 552,
          "text": "và chúng ta chỉ tập trung vào cái việc là xây dựng cái mô hình mà thôi"
        }
      ]
    },
    {
      "video_id": "Chương 2_T2xJmTiRM5o",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- **Mục tiêu chính của bài giảng**: Giới thiệu mô hình **Logistic Regression (mô hình hồi quy luận lý)** cho bài toán phân lớp nhị phân, từ việc biểu diễn biên phân tách tuyến tính đến thiết kế hàm dự đoán liên tục và hàm mất mát phù hợp để huấn luyện bằng Gradient Descent. [1][2][3][4][5]\n\n- **Các khái niệm sẽ được đề cập**:\n  - Bài toán phân lớp nhị phân, *linearly separable* và biểu diễn biên phân tách bằng tham số θ. [1][2][3]\n  - Hạn chế của hàm dự đoán dạng rời rạc (step function) và lý do cần hàm liên tục. [4][5]\n  - Hàm *sigmoid* (logistic) làm nhiệm vụ chuyển giá trị thực về (0,1) và định nghĩa mô hình fθ(x)=σ(θ^T x). [6][22][23]\n  - Hàm mất mát tiêu biểu là *binary cross-entropy* (BCE) và so sánh với MSE. [11][12][20][21]\n  - Vectorization (biểu diễn ma trận/véc-tơ) để tính toán trên toàn bộ mẫu. [8][9][10][21][22]\n\n(Thông tin trích dẫn theo các đoạn video tương ứng.) [1][2][3][4][5][6][8][9][10][11][12][20][21][22][23]\n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Bài toán phân lớp nhị phân và biểu diễn biên phân tách\n- Ta xét dữ liệu đầu vào X (ví dụ 2 chiều X1, X2) với nhãn Y ∈ {0,1}, mục tiêu phân tách hai lớp (màu xanh / cam). Nếu dữ liệu là *linearly separable*, tồn tại một đường thẳng phân chia hai lớp. Phương trình đường thẳng có dạng A·X1 + B·X2 + C = 0, hay quy về tham số θ: θ0 + θ1·X1 + θ2·X2 = 0. [1][2][3]\n\n- Quy ước: nếu θ0 + θ1·X1 + θ2·X2 ≥ 0 thì gán nhãn 1, ngược lại nhãn 0 (định nghĩa bằng step/threshold). Tuy nhiên hàm này là **không liên tục** (discontinuous), gây khó khăn khi cần tính đạo hàm để tối ưu θ bằng Gradient Descent. [3][4][5]\n\n  (Xem minh họa đường phân tách và phân chia nửa mặt phẳng.) [2][3][4]\n\n### 2.2. Tại sao cần hàm sigmoid (liên tục)?\n- Domain của θ^T x là (-∞, +∞) trong khi nhãn mong muốn là {0,1}. Để “ép” đầu ra vào khoảng (0,1) và có hàm khả vi, ta dùng hàm sigmoid:  \n  σ(x) = 1 / (1 + e^{-x}). Hàm này ánh xạ từ (-∞, +∞) → (0,1) và có đồ thị dạng chữ S, là hàm liên tục và khả vi. [5][6][7][23]\n\n- Định nghĩa mô hình (hypothesis):  \n  fθ(x) = σ(θ^T x) (sigmoid áp dụng lên tích vô hướng θ^T x). [8][22][23]\n\n### 2.3. Tham số (θ) và bias; vectorization\n- Tham số θ gồm θ0 (bias) và θ1..θm tương ứng các đặc trưng X1..Xm. Với nhiều đặc trưng, θ^T x = θ0·1 + θ1·x1 + ... + θm·xm. [8]\n\n- Vector hóa:  \n  - Một mẫu x là một vectơ cột; tập n mẫu tạo thành ma trận X; θ là vectơ cột (θ0..θm). [9]  \n  - Khi áp dụng sigmoid lên một vector (θ^T X) thì phép tính thực hiện *element-wise* trên từng phần tử, kết quả là vector các ŷ (predictions). Sigmoid có thể áp dụng lên vector để thu được vector ŷ. [10][11]\n\n### 2.4. Hàm mất mát: Binary Cross-Entropy (BCE)\n- Với 1 mẫu, hàm mất mát (loss) dùng cho Logistic Regression là Binary Cross-Entropy:  \n  L(y, ŷ) = − [ y·log(ŷ) + (1−y)·log(1−ŷ) ]. [11][20][21]\n\n- Tính chất kiểm tra:  \n  - Nếu dự đoán đúng (ví dụ y=1 và ŷ→1) thì L→0 (log(1)=0). [13][14]  \n  - Nếu dự đoán sai nặng (ví dụ y=1 nhưng ŷ→0) thì log(ŷ)→−∞ và L→+∞, tức là mất mát lớn — mô phỏng “hình phạt” mạnh cho dự đoán sai. [15][16]\n\n### 2.5. Tại sao không dùng MSE (Mean Squared Error) cho phân lớp?\n- MSE: L(θ) = (1/2n) Σ (ŷ − y)^2 là phù hợp cho hồi quy tuyến tính, nhưng với bài toán phân lớp nhị phân, MSE trừng phạt sai số quá nhẹ (ví dụ y=1, ŷ=0 → lỗi = 1) so với BCE (có thể trở nên rất lớn khi ŷ≈0). Điều này ảnh hưởng đến biên độ đạo hàm và tốc độ cập nhật θ khi dùng Gradient Descent. [12][16][17]\n\n- Vì BCE có độ dốc lớn khi dự đoán rất sai, gradient lớn dẫn đến cập nhật nhanh hơn, giúp huấn luyện hiệu quả hơn so với MSE cho bài toán phân lớp. (So sánh đồ thị hàm mất mát và hệ quả lên đạo hàm/learning step.) [17][18][19]\n\n### 2.6. Hàm mất mát vector hóa cho toàn dataset\n- Với n mẫu, ta tính loss trung bình (mean) trên từng phần tử:  \n  J(θ) = (1/n) Σ L(y^{(i)}, ŷ^{(i)}) = (1/n) Σ [ −y^{(i)} log(ŷ^{(i)}) − (1−y^{(i)}) log(1−ŷ^{(i)}) ]  \n  (ký hiệu: BCE = binary cross-entropy). [20][21][22]\n\n- Quy trình: tính z = θ^T X (cho tất cả mẫu), áp dụng σ element-wise để có ŷ (vector), rồi tính loss theo công thức BCE từng phần tử và lấy trung bình. [10][21][22]\n\n### 2.7. Huấn luyện bằng Gradient Descent\n- Ta tối ưu θ bằng Gradient Descent: θ ← θ − α ∇_θ J(θ). Vì J(θ) được lựa chọn là BCE kết hợp sigmoid, hàm khả vi nên gradient có thể tính được và cập nhật thực hiện ổn định; độ lớn gradient được điều chỉnh nhờ cấu trúc BCE nên huấn luyện thường nhanh hơn so với dùng MSE cho phân lớp. [5][11][17][18][19][20]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- **Ví dụ minh họa 2 chiều**:  \n  - Dữ liệu gồm hai lớp (xanh/cam) trên mặt phẳng (X1, X2). Nếu dữ liệu *linearly separable*, tồn tại một đường thẳng phân chia hai lớp; biểu diễn đường thẳng bằng θ0 + θ1·X1 + θ2·X2 = 0. Nếu dùng step function thì gán nhãn theo dấu của θ^T x; nhưng thay bằng sigmoid ta có xác suất thuộc lớp 1 là σ(θ^T x). [1][2][3][4][6][22]\n\n- **Vectorization (nhiều mẫu)**:  \n  - Tập hợp nhiều mẫu thành ma trận X, θ^T X cho ra vector z, áp dụng sigmoid element-wise để thu được vector dự đoán ŷ cho toàn bộ mẫu; sau đó tính BCE trên từng phần tử và lấy trung bình để có J(θ). Đây là hình thức tính toán hiệu quả khi huấn luyện trên batch/full dataset. [9][10][11][21][22]\n\n- **Trường hợp sử dụng thực tế**:\n  - Bất kỳ bài toán phân lớp nhị phân nào cần xác suất dự đoán (ví dụ: phân loại spam/ham, dự đoán bệnh có/không) có thể dùng Logistic Regression như một mô hình cơ bản hoặc baseline. Video minh họa khái quát các bước xây dựng và tối ưu mô hình này. [1][2][6][22]\n\n---\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Logistic Regression chuyển từ phân tách tuyến tính (θ^T x) sang đầu ra xác suất bằng hàm sigmoid σ(θ^T x), cho phép giá trị trong (0,1) và khả vi để tối ưu. [2][3][6][22][23]\n  - Hàm mất mát phù hợp là *binary cross-entropy* (BCE) vì nó trừng phạt mạnh các dự đoán sai nặng và dẫn đến gradient lớn hơn, giúp cập nhật tham số nhanh và hiệu quả khi dùng Gradient Descent. MSE không phù hợp bằng do trừng phạt nhẹ hơn và gây cập nhật chậm. [11][12][16][17][18][19][20]\n  - Vectorization (ma trận X và vectơ θ) và tính element-wise của sigmoid giúp tính toán hiệu quả trên toàn bộ tập dữ liệu. [9][10][21]\n\n- Tầm quan trọng:\n  - Logistic Regression là mô hình nền tảng cho phân lớp nhị phân, cung cấp nền tảng cho hiểu các mô hình phức tạp hơn (ví dụ khi kết hợp với regularization, hoặc mở rộng sang mạng neural). Việc hiểu rõ hàm sigmoid, BCE và lý do chọn chúng là thiết yếu cho học sâu. [5][6][11][20][22]\n\n- Liên hệ với các bài giảng khác:\n  - Nội dung này liên quan chặt chẽ đến các khái niệm về hàm dự đoán, hàm mất mát và tối ưu (Gradient Descent) đã xuất hiện trong phần tổng quát về mô hình và tối ưu hóa (tham khảo phần giới thiệu mô hình và gradient descent). [1][5][18]\n\n(Toàn bộ nội dung tóm tắt dựa trên các đoạn trích từ video: [1]…[23].)",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 1,
          "end_time": 61,
          "text": "Moodle Logistic Direction thì cũng được phát triển từ Moodle học tổng quát thì chúng ta nhắc lại đầu vào là chúng ta sẽ có cái dữ liệu X và đầu ra thì chúng ta sẽ có cái dữ liệu Y và tùy vào cái tính chất của cái cặp dữ liệu XI này nè, để mình sẽ thiết kế các cái hàm Moodle dự đoán FθX và hàm độ lỗi dự đoán LθXY còn cái công việc số 3 đó là tìm Theta so với hàm độ lỗi nhỏ nhất này thì chúng ta cũng đã có cái công cụ đó là tục toán Radian Descent Đối với cái Moodle Logistic Direction thì chúng ta sẽ phải đi giải quyết một cái bài toán, trong đó chúng ta sẽ phải phân ra làm 2 lớp, xanh và cam ở đây 2 cái X1 và X2 nó chính là cái đặc trưng đầu vào trong trường hợp này chúng ta sẽ lấy mặt phẳng 2 chiều và chúng ta sẽ phải phân tách 2 cái tập điểm xanh và 1 cam này ra làm 2 phần"
        },
        {
          "index": 2,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 49,
          "end_time": 110,
          "text": "cái đặc trưng đầu vào trong trường hợp này chúng ta sẽ lấy mặt phẳng 2 chiều và chúng ta sẽ phải phân tách 2 cái tập điểm xanh và 1 cam này ra làm 2 phần và trong trường hợp này thì cái dữ liệu của mình nó gọi là phân tách được một cách tiến tính hay còn gọi là Linear Separable thì ở đây chúng ta sẽ có được một cái đường thẳng tách ra làm 2 thì theo như cái kiến thức toán tấp 2, tấp 3 mà chúng ta đã học thì với cái phương trình đường thẳng này chúng ta có thể biết nó dưới dạng là AX1 cộng cho B X2 cộng cho C bằng 0 và tất cả những cái điểm nào mà nằm trên cái đường thẳng này thì khi thế vào cái điểm X1, X2 nằm trên đường thẳng này, thế vào thì chúng ta sẽ có cái giá trị là bằng 0 còn bây giờ chúng ta sẽ làm quen với lại cái bộ tham số đó là Theta 1, X1 cộng cho Theta 2, X2 và cộng cho cái thành phần 3"
        },
        {
          "index": 3,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 97,
          "end_time": 159,
          "text": "còn bây giờ chúng ta sẽ làm quen với lại cái bộ tham số đó là Theta 1, X1 cộng cho Theta 2, X2 và cộng cho cái thành phần 3 là Theta 0 thì cái này nếu như những cái điểm nào mà nằm trên cái đường này thì nó sẽ là bằng 0 còn những cái điểm nào mà nằm về phía bên trên nằm về phía bên trên ví dụ như ở đây thì chúng ta thế vô thì nó sẽ ra cái giá trị là lớn hơn 0 còn những cái điểm nào mà nằm dưới như vậy chúng ta thế vào nó sẽ là 1 điểm nằm nếu như vậy thì dựa trên cái quan sát cũng như là cái kiến thức mà toán cấp 3 cấp 2, cấp 3 mà chúng ta đã học được thì chúng ta sẽ thiết kế cái hàm dự đoán thiết kế cái hàm dự đoán bằng cái dạng như trên đó là F Theta X 1, X2 với X là cái dự kiện đầu vào là 2 cái đặc trưng đầu vào nó sẽ là bằng 1 tức là cái nhãn Y này nè là bằng 1"
        },
        {
          "index": 4,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 149,
          "end_time": 210,
          "text": "với X là cái dự kiện đầu vào là 2 cái đặc trưng đầu vào nó sẽ là bằng 1 tức là cái nhãn Y này nè là bằng 1 nếu Theta 0 Theta 0 cộng cho Theta 1, X1 cộng cho Theta 2, X2 Theta 0, X1 cộng cho Theta 1, X1, cộng cho Theta X2 lớn hơn bằng 0 tức là nó thuộc về 1 nửa cái mặt phẳng này tức là nó thuộc về 1 nửa cái mặt phẳng này thì nó sẽ được kén giá trị là 1 thì nó sẽ được kén giá trị là 1 và nó sẽ được kén bằng 0 cái nhãn dự đoán của mình nó sẽ được kén bằng 0 cái nhãn dự đoán của mình nó sẽ được kén bằng 0 nếu như Theta 0 cộng cho Theta 1, X1 cộng Theta 2, X2 nó bé hơn 0 tức là nó nằm về 1 nửa phía bên này thì nếu như chúng ta thiết kế cái hàm dự đoán như thế này thì điều gì sẽ xảy ra điều gì sẽ xảy ra điều gì sẽ xảy ra đó là hàm này là hàm không liên tục hàm này là hàm không liên tục hàm này là hàm không liên tục mà hàm không liên tục thì sau này"
        },
        {
          "index": 5,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 201,
          "end_time": 261,
          "text": "đó là hàm này là hàm không liên tục hàm này là hàm không liên tục hàm này là hàm không liên tục mà hàm không liên tục thì sau này khi chúng ta đến cái bước số 3 cái đạo hàm, chúng ta tính đạo hàm nó sẽ rất là khó do đó thì chúng ta phải cố gắng thiết kế cái hàm F Theta X do đó thì chúng ta phải cố gắng thiết kế cái hàm F Theta X sao cho nó phải là 1 cái hàm liên tục thì chúng ta sẽ tìm cách thiết kế bằng cách dựa trên cái quan sát đó là cái miền giá trị của cái Theta 0 đó là tức là cái miền giá trị của cái giá trị của Theta 0 của cái phép tính này nó sẽ thuộc cái đoạn là từ trừ vô cùng cho đến cộng vô cùng trong khi đó cái giá trị mà mình mong muốn dự đoán nó sẽ nhận 2 giá trị là 0 và 1 mà mình mong muốn dự đoán nó sẽ nhận 2 giá trị là 0 và 1 trong khi đó cái miền giá trị của Theta 0 cộng cho Theta 1 X1 cộng cho Theta 2 X2 nó là trừ vô cùng cho đến cộng vô cùng thì để ép cho cái giá trị này"
        },
        {
          "index": 6,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 246,
          "end_time": 311,
          "text": "dự đoán nó sẽ nhận 2 giá trị là 0 và 1 mà mình mong muốn dự đoán nó sẽ nhận 2 giá trị là 0 và 1 trong khi đó cái miền giá trị của Theta 0 cộng cho Theta 1 X1 cộng cho Theta 2 X2 nó là trừ vô cùng cho đến cộng vô cùng thì để ép cho cái giá trị này cái tổng này về cái giá trị từ 0 cho đến 1 thì chúng ta sẽ sử dụng cái hàm nó gọi là hàm sigmoid hàm sigmoid nó sẽ có cái công thức như sau sigmoid của X ở đây là chúng ta sẽ biết X thường là bằng 1 phần 1 cộng cho E mũ trừ X và cái dạng đồ thị hàm số của cái hàm sigmoid nó sẽ có cái dạng như sau của cái hàm sigmoid nó sẽ có cái dạng như sau  của cái hàm sigmoid nó sẽ có cái dạng như sau rồi từ trừ vô cùng cho đến cộng vô cùng với cái giá trị đầu vào của mình là từ trừ vô cùng cho đến cộng vô cùng thì qua cái hàm sigmoid thì nó sẽ ép về cái miền giá trị nó sẽ ép về cái miền giá trị là từ 0 cho đến 1 là từ 0 cho đến 1"
        },
        {
          "index": 7,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 299,
          "end_time": 352,
          "text": "từ trừ vô cùng cho đến cộng vô cùng thì qua cái hàm sigmoid thì nó sẽ ép về cái miền giá trị nó sẽ ép về cái miền giá trị là từ 0 cho đến 1 là từ 0 cho đến 1 thì đây là cái sơ đồ cái đồ thị của cái hàm sigmoid và như vậy thì từ cái giá trị đầu vào Theta 0 Theta 1 X1 Theta 2 X2 Theta 3 X3 Theta 4 X5 Theta 6 X7  Theta 9 X10 Theta 11 X12 Theta 13 X13 Theta 14 X15 Theta 16 X17 Theta 18 X20 Theta 19 X21 Theta 19 X22 Theta 19 X23 Theta 19 X24 Theta 19 X25"
        },
        {
          "index": 8,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 350,
          "end_time": 410,
          "text": "Theta 19 X25 Theta 20 X26 Theta 21 X27 Theta 22 X28 Theta 19 X30  Theta 23 X37  Theta 24 X39  Theta 24 X41 Theta 24 X42  Theta 24 X44 Theta 25 X46 còn 1 đó chính là cái thành phần bias và tham số của mình đó sẽ là theta theta sẽ bao gồm theta 0 theta 1, theta 2 cho đến theta m nó tương ứng với lại cái x tàu bào như vậy cái hàm dự đoán của mình nó sẽ là viết gọn lại f theta x sẽ bằng sigma y của theta chuyển vị nhau x và đối với cái việc mà vector hóa nhưng mà cho cái dữ liệu mà toàn mẫu"
        },
        {
          "index": 9,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 399,
          "end_time": 461,
          "text": "của theta chuyển vị nhau x và đối với cái việc mà vector hóa nhưng mà cho cái dữ liệu mà toàn mẫu tức là tập hợp tất cả những cái mẫu dữ liệu của cái tập dữ liệu của mình thì đây là 1 mẫu nè đây là 1 mẫu mẫu thứ nhất mẫu thứ 2 và đây là mẫu thứ n mỗi cái mẫu này nó sẽ là dữ diện dữ diện như vậy là 1 cái cột và tập hợp tất cả cái cột này nó sẽ tạo thành 1 cái ma trận và 1 cái ma trận và theta nó sẽ là 1 cái vector theta 0, theta 1 cho đến theta m như vậy cái hàm dự đoán của mình nó sẽ được viết gọn lại cũng cùng 1 cái công thức như trên nếu như công thức ở trên đây công thức ở đây đó là theta chuyển vị nhau với x x này là 1 mẫu thì qua cái công thức bên đây"
        },
        {
          "index": 10,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 448,
          "end_time": 510,
          "text": "nếu như công thức ở trên đây công thức ở đây đó là theta chuyển vị nhau với x x này là 1 mẫu thì qua cái công thức bên đây x này nó là n mẫu tức là toàn bộ toàn bộ các cái mẫu dữ liệu của mình và theta chuyển vị nhân viếc trong trường hợp này nó chính là 1 vector dạng nằm ngang nó sẽ là 1 cái vector trong đó từng cái phần tử ở đây nó sẽ là các cái giá trị y ngã dự đoán các cái giá trị dự đoán thì ở đây chúng ta lưu ý đó là cái hàm sigmoid ở đây nó không phải là tính cho 1 giá trị scaler mà nó sẽ tính cho 1 cái vector và kết quả của cái phép sigmoid này kết quả của cái phép mà biến đổi xích môi, cái hàm xích môi này trên cái vector này nó sẽ ra một cái vector nó sẽ ra một cái vector và từng cái phần tử trong đây nó tương ứng chính là xích môi của cái phần tử ở phía trên"
        },
        {
          "index": 11,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 499,
          "end_time": 562,
          "text": "trên cái vector này nó sẽ ra một cái vector nó sẽ ra một cái vector và từng cái phần tử trong đây nó tương ứng chính là xích môi của cái phần tử ở phía trên phần tử này qua hàm xích môi nó sẽ tính ra cái giá trị ở đây như vậy ở đây nó sẽ là tính element-wise tức là tính trên từng phần tử và xích môi của một vector nằm ngang nó sẽ ra một cái vector nằm ngang và chúng ta sẽ qua cái bước thứ 2 đó là chúng ta sẽ thiết kế cái hàm lỗi và trong trường hợp này thì cái y giá trị thực tế là nó sẽ nhận 2 giá trị là 1 y bằng 1 hoặc là y bằng 0 tương ứng là 2 cái phần lớp của mình thì đối với cái hàm lỗi cho cái trường hợp mà 1 mẫu dữ liệu và không có vector hóa không vector hóa nghĩa là chúng ta sẽ tính trên từng cái phần tử riêng biệt thay vì tính hàng loạt và thì ở đây chúng ta sẽ có cái công thức hàm độ lỗi như trên thì ở đây chúng ta sẽ đặt một câu hỏi là tại sao cái công thức của cái hàm lỗi này"
        },
        {
          "index": 12,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 550,
          "end_time": 610,
          "text": "riêng biệt thay vì tính hàng loạt và thì ở đây chúng ta sẽ có cái công thức hàm độ lỗi như trên thì ở đây chúng ta sẽ đặt một câu hỏi là tại sao cái công thức của cái hàm lỗi này nó có vẻ phức tạp quá tại sao cái công thức này nó có vẻ phức tạp nó có hàm lock rồi 1 trừ y nhưng cho lock của 1 trừ y ngã thì cái hàm này nó quá phức tạp tại sao chúng ta không sử dụng chính cái hàm min square mse của cái hàm cho cái phần linear regressor đó là công thức l theta là bằng 1 phần 2n trung bình cộng của y ngã trừ cho y tất cả bình phương tại sao chúng ta không dùng cái công thức này mà lại sử dụng cái công thức ở trên rồi thì bây giờ trước tiên chúng ta phải kiểm tra xem cái công thức ở trên nó có cái tính đúng đắn hay không thế thì yêu cầu đặt ra"
        },
        {
          "index": 13,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 600,
          "end_time": 667,
          "text": "mà lại sử dụng cái công thức ở trên rồi thì bây giờ trước tiên chúng ta phải kiểm tra xem cái công thức ở trên nó có cái tính đúng đắn hay không thế thì yêu cầu đặt ra đối với cái hàm lỗi đó là nếu như chúng ta dự đoán đúng là hàm lỗi này nó giống như WAAN nếu mà đúng thì hàm số lỗi của mình là phải bằng 0 Vì đó thì cái lỗi của mình là phải bằng MARL và nếu chúng ta đánh sai thì cái lỗi của mình nó Phải lớn hơn không thì biết chúng ta sẽ xét thử một cái trường hợp nếu y của mình đá là bằng 1 giá trị thực tế này nhưng cái giá trị dự đoán của mình nó lại bằng không rồi bây giờ chúng ta sẽ xét t shutter Tet Hệ Đoán avoid này là đáng đúng ngôn ra anh cũng bằng 1 y đi thì khi chúng ta Thế một cái k weld hãy đăng b __ hãy đăng no Pista thức này chúng ta thế vô công thức này thì cái last của mình trong trường hợp này nó sẽ là giá trị bằng bao nhiêu bằng trường rồi y bằng một y bằng một diễn nguyên kéo một xuống lốc y lốc y chính là lốc của lốc y ngã ở đây là y ngã nha thì y ngã là bằng một chúng ta đang dự đoán đúng rồi"
        },
        {
          "index": 14,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 648,
          "end_time": 712,
          "text": "trị bằng bao nhiêu bằng trường rồi y bằng một y bằng một diễn nguyên kéo một xuống lốc y lốc y chính là lốc của lốc y ngã ở đây là y ngã nha thì y ngã là bằng một chúng ta đang dự đoán đúng rồi 1 trừ y trong trường hợp này 1 trừ cho một tính là bằng không do đó cái phần còn lại là chúng ta không cần tính nữa như vậy nó sẽ là bằng trừ lốc của một mà trừ của lốc của một thì trong cái biểu đồ trong cái đồ thị của hàm lóc đây là hàm lóc x giá trị tại đây là bằng một thì lốc của một tại vị trí này nó tương ứng nó sẽ là bằng không như vậy trong trường hợp mà đoán đúng trong trường hợp đoán đúng à à Exhale nếu cho cái trường hợp i bằng không và I nhà bạn không cứ dọc là cũng đáng đúng"
        },
        {
          "index": 15,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 702,
          "end_time": 760,
          "text": "Exhale nếu cho cái trường hợp i bằng không và I nhà bạn không cứ dọc là cũng đáng đúng nhưng mà 의 trường hợp mà i bằng không thì các bạn thấy ốp chúng ta cũng sẽ ra được giá trị sai số l vakum alle chúng ta sẽ xem trong cái trường hợp chúng ta sẽ xem trong cái trường hợp đó là aç lẽ chúng ta đã sai Ừế chúng ta đến sai Y bằng một Y bằng 1 và y bằng Disk toolbox đồng caffeine go y bằng 1 hoặc là y bằng 1 và 1 cái thì y bằng freue đó là chúng ta sẽ xem trong trường hợp đó là conversa bằng nhãy det iai hãy書 ở web pří công đoán sai Empress Big Business career art y ngã dự đoán là bằng 0 thì thế vô cái công thức chúng ta sẽ thế vô cái công thức ở trên đây thì nó sẽ ra như thế nào y bằng 1 loss của mình nó sẽ là bằng trừ 1 diễn quyền nhân cho lốc của y ngã, y ngã của mình trong trường hợp này là bằng 0 rồi 1 trừ y 1 trừ y sẽ là bằng 1 trừ 1 tức là bằng 0, giới lánh phần sau chúng ta bỏ qua như vậy nó sẽ là bằng trừ"
        },
        {
          "index": 16,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 749,
          "end_time": 810,
          "text": "lốc của y ngã, y ngã của mình trong trường hợp này là bằng 0 rồi 1 trừ y 1 trừ y sẽ là bằng 1 trừ 1 tức là bằng 0, giới lánh phần sau chúng ta bỏ qua như vậy nó sẽ là bằng trừ lốc 0 và chúng ta thấy là với cái đồ thị hàm số này thì khi x của mình mà tiến về 0 thì cái hàm lốc giá trị của lốc nó sẽ tiến về trừ vô cùng, nó sẽ tiến về trừ vô cùng do đó thì nó sẽ là bằng trừ của trừ vô cùng, tức là bằng cộng vô cùng hay nói cách khác, đó là nếu bán sai thì cái mức mát của chúng ta đó là chúng ta sẽ mất nguyên 1 cái căn nhà tức là chính là 1 cái giá trị rất là lớn rồi bây giờ chúng ta sẽ thử thử nghiệm trên cái giá trị msi, trên cái công thức mà mean square ra như trên đây, thì nếu như dự đoán sai nếu như dự đoán đúng thì tự nhiên cái lỗi nó cũng sẽ bằng 0, thế vô chúng ta cũng sẽ có bằng 0, nhưng nếu chúng ta đáng sai nếu chúng ta đáng sai, tức là chúng ta sẽ có y"
        },
        {
          "index": 17,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 799,
          "end_time": 861,
          "text": "như trên đây, thì nếu như dự đoán sai nếu như dự đoán đúng thì tự nhiên cái lỗi nó cũng sẽ bằng 0, thế vô chúng ta cũng sẽ có bằng 0, nhưng nếu chúng ta đáng sai nếu chúng ta đáng sai, tức là chúng ta sẽ có y trừ cho y ngã tất cả mình đúng không, thì y mà trừ y ngã mình tức là bằng 1 trừ 0 tất cả mình nó sẽ là bằng 1 như vậy nếu như dùng công thức msi này thì cái sự trừng phạt này nó quá bé so với lại cái công thức của hàm loss ở đây cái này quá bé, còn cái này là rất là lớn thì cái việc lớn bé này nó sẽ ảnh hưởng như thế nào khi chúng ta có cái hàm mất mát mà lớn thì cái việc mà cập nhật cái đạo hàm cái việc tính đạo hàm theo thê ta nếu như mà đạo hàm này nó sẽ là hàm lân biến hoặc là nabla của l theo thê ta thì khi mà cái giá trị này có cái độ dốc có cái giá trị nó lớn tức là cái độ dốc của cái hàm l này nó lớn"
        },
        {
          "index": 18,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 848,
          "end_time": 913,
          "text": "nếu như mà đạo hàm này nó sẽ là hàm lân biến hoặc là nabla của l theo thê ta thì khi mà cái giá trị này có cái độ dốc có cái giá trị nó lớn tức là cái độ dốc của cái hàm l này nó lớn thì khi đó đạo hàm của mình nó sẽ lớn ngược lại nếu như cái means where ra này cái giá trị của mình nó nhỏ thì khi đó tính đạo hàm cái độ dốc của cái hàm cái đạo hàm của mình nó sẽ nhỏ thì dẫn đến là cái bước cập nhật của mình nó sẽ chậm thì dẫn đến là cái bước cập nhật của mình nó sẽ chậm nghĩa là sao chúng ta có cái công thức thê ta là bằng thê ta trừng cho alpha nhân cho đạo hàm của loss theo thê ta thì nếu như cái độ dốc của cái hàm l này ví dụ như chúng ta có 2 cái hàm đây là hàm thứ nhất hàm l1 và hàm thứ 2 cả 2 hàm này thì trong đó cái hàm l1 chúng ta thấy có cái độ dốc rất là lớn"
        },
        {
          "index": 19,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 899,
          "end_time": 961,
          "text": "hàm l1 và hàm thứ 2 cả 2 hàm này thì trong đó cái hàm l1 chúng ta thấy có cái độ dốc rất là lớn thì khi đó cái đạo hàm cái giá trị đạo hàm của mình nó sẽ lớn còn cái hàm l2 cái độ dốc của mình nó tuy thoải thoải do đó thì cái đạo hàm của nó bé thì nếu như cái độ dốc mà lớn thì cái bước nhảy của mình nó sẽ lớn dẫn đến đó là cái việc cập nhật thê ta nó sẽ nhanh dẫn đến đó là cái việc cập nhật thê ta nó sẽ nhanh cái việc cập nhật thê ta nhanh cái việc cập nhật thê ta nhanh do đó thì chúng ta sẽ sử dụng công thức trừ của y lớp y ngã cộng cho 1 trừ y lớp 1 trừ y ngã thì nó sẽ giúp cho cái việc huấn luyện cái việc huấn luyện sẽ thực hiện rất là nhanh nhanh hơn so với cái việc là dùng cái công thức min square ở đây, thì đó là lý do tại sao mình lại đi sử dụng cái công thức min square rồi, bây giờ"
        },
        {
          "index": 20,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 950,
          "end_time": 1010,
          "text": "nhanh hơn so với cái việc là dùng cái công thức min square ở đây, thì đó là lý do tại sao mình lại đi sử dụng cái công thức min square rồi, bây giờ chúng ta sẽ qua cái công thức cho cái trường hợp mà nhiều mẫu và có vector hóa thì cũng tương tự như vậy, với từng mẫu dữ liệu chúng ta ghép lại thì chúng ta sẽ có 1 cái ma trận x và cái nhãn y của cái dữ liệu nó sẽ là 1 cái vector dạng làm nghe tham số của mình là theta 0, theta 1 và theta m thì khi đó cái hàm lỗi của mình nó sẽ có cái công thức đó là 1 phần 2 1 phần n binary, cái chữ bce này là viết tắt của chữ binary cross entropy thì đây chính là cái công thức mà hồi nãy mình đã liệt kê mình đã trình bày đó là bằng y trừ của y"
        },
        {
          "index": 21,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 1002,
          "end_time": 1061,
          "text": "thì đây chính là cái công thức mà hồi nãy mình đã liệt kê mình đã trình bày đó là bằng y trừ của y nhân cho góc của y ngã cộng cho 1 trừ y nhân cho góc của 1 trừ y y ngã đây chính là cái công thức binary cross entropy và lưu ý là chúng ta sẽ tính trên từng phần tử nghĩa là sao khi chúng ta tính cái sigmoid của theta x chúng ta sẽ có chuỗi các cái phần tử dạng vector dạng làm nghe đây là y ngã còn cái y của mình nó cũng sẽ có 1 cái chuỗi các cái phần tử tạo thành 1 cái vector làm nghe và chúng ta sẽ đi tính toán trên 2 cái giá trị tính cái độ lỗi trên 2 cái giá trị y ngã này bằng cách đó thì chúng ta sẽ có 1 cái vector nào ngang như thế này là nó sẽ lấy từng phần tử ở đây ra từng cái phần tử của y ngã với từng phần tử của y, thế vào công thức này"
        },
        {
          "index": 22,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 1048,
          "end_time": 1109,
          "text": "và chúng ta sẽ đi tính toán trên 2 cái giá trị tính cái độ lỗi trên 2 cái giá trị y ngã này bằng cách đó thì chúng ta sẽ có 1 cái vector nào ngang như thế này là nó sẽ lấy từng phần tử ở đây ra từng cái phần tử của y ngã với từng phần tử của y, thế vào công thức này để tính, rồi sau đó nó lại cộng trung bình lại, nó sẽ cộng hết, cộng trung bình nó sẽ thực hiện trên từng phần tử của cái y ngã và y này để mà tính ra cái hàm lỗi rồi, và cái dạng độ thị của cái hàm Logistic Regression của mình thì nó sẽ, nó cũng tương tự như cái hàm Linear Regression nếu như Linear Regression chúng ta đến cái bức tổng này là xong đúng không, thì chúng ta sẽ qua tiếp một cái phép biến độ nữa là hàm Sigma, Sigma sau khi thực hiện cái phép tổng này, thì chúng ta sẽ có cái công thức y ngã là bằng fθx là bằng Sigma của θ chuyển vị nhân x, θ chuyển vị nhân x chính là cái kết quả sau khi thực hiện cái này qua cái hàm Sigma thì nó sẽ ra cái y ngã"
        },
        {
          "index": 23,
          "video_id": "Chương 2_T2xJmTiRM5o",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3a： Mô hình hồi quy luận lý (Logistic Regression)",
          "video_url": "https://youtu.be/T2xJmTiRM5o",
          "start_time": 1099,
          "end_time": 1123,
          "text": "fθx là bằng Sigma của θ chuyển vị nhân x, θ chuyển vị nhân x chính là cái kết quả sau khi thực hiện cái này qua cái hàm Sigma thì nó sẽ ra cái y ngã rồi, trong đó cái công thức của Sigma thì nó sẽ là bằng 1 1 phần 1 cộng cho e 1 trừ x thì đây chính là cái dạng độ thị của hàm Mô hình Logistic Regression"
        }
      ]
    },
    {
      "video_id": "Chương 2_jl9v7IDMTsk",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: hướng dẫn **cài đặt mô hình Logistic Regression** bằng thư viện Keras, tận dụng tính năng tự động tính đạo hàm (automatic differentiation) để không phải tính tay gradient và cập nhật tham số. [1]  \n- Các khái niệm sẽ được đề cập: cách **tạo dữ liệu hai lớp** (two-class synthetic data), kiến trúc mạng với **input 2 chiều** và **Dense output 1 node** với activation *sigmoid*, việc **build/compile/fit** model trong Keras, và theo dõi *training/validation loss*. [1][2][7][9][11]\n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 Tạo tập dữ liệu (Data generation)\n- Dữ liệu được mô phỏng là hai tập điểm (red/blue) sinh ngẫu nhiên xung quanh hai tâm khác nhau; input feature là 2 chiều (x1, x2). [1][2]  \n- Cụ thể, các điểm đỏ (red) được random xung quanh một tâm (được nhắc trong video là tọa độ \"1 1\") và các điểm xanh (blue) random quanh tâm khác (được nhắc là \"5 1\"); hai tập này có thể **tách được bởi một đường thẳng** (linearly separable). [4][3]  \n- Tạo thêm tập *validation* bằng cùng công thức như tập train nhưng thêm hậu tố *val* (validation data). [4][5]  \n- Nhãn y: red → label = 1; blue → label = 0. [5]\n\n### 2.2 Kiến trúc mô hình (Model architecture)\n- **Input layer**: nhận vector 2 chiều (M = 2, tương ứng x1 và x2). [2][6][13]  \n- **Output layer**: một lớp Dense (fully-connected) duy nhất với **1 node**, hàm kích hoạt **sigmoid**, và sử dụng **bias = True**. [7][8]  \n- Khi gọi model.summary(), sẽ thấy input dimension = 2, lớp Dense với tổng số tham số = 3 (2 trọng số + 1 bias). Giải thích: tổng parameters = số input features + bias = 2 + 1 = 3. [13][14]\n\n### 2.3 Triển khai với Keras — build/compile/fit\n- Tái sử dụng khung chương trình từ bài Linear Regression: các phương thức như *build*, *train*, *plot*, *summary*, *predict*, *gateway* được dùng lại; trọng tâm cần viết lại là *build* và *train*. [5][6]  \n- Trong phương thức *build*: tạo Input layer (với shape tương ứng input_dim), tạo Dense(1, activation='sigmoid', use_bias=True) và nối output với input; sau đó đóng gói thành Model(inputs, outputs). [6][7][8]  \n- Trong phương thức *train*: khởi tạo optimizer là **Stochastic Gradient Descent (SGD)** với learning rate = **0.01** (được khai báo qua tham số tương tự numing rate trong video). [9]  \n- Chọn hàm loss phù hợp cho bài toán phân lớp nhị phân: **Binary Crossentropy**, khởi tạo bằng đối tượng tf.keras.losses.BinaryCrossentropy(). [10]  \n- Gọi model.compile(optimizer=..., loss=...) và sau đó model.fit(train_data, validation_data=..., epochs=...). Ví dụ trong video đặt epochs = 5 ở bước demo ban đầu. [9][10][11]\n\n### 2.4 Lưu trữ kết quả huấn luyện và vấn đề tên biến\n- Kết quả huấn luyện được trả về dưới dạng *history object* và được gán vào biến (video dùng tên đại diện là his = log_red.rk ... theo trình bày). [14]  \n- Trong quá trình triển khai có gặp lỗi liên quan đến tên biến/tham số (ví dụ: tên is_train, is_val, num_epoch, chính tả epochs), cần kiểm tra và sửa tên tham số trước khi build lại và train lại. [15][16]\n\n### 2.5 Quan sát quá trình huấn luyện và hiện tượng overfitting\n- Khi huấn luyện quan sát thấy training loss giảm (ví dụ các giá trị loss giảm từ 3 → 2 → 1 → 0), cho thấy mô hình đang học theo hướng mong muốn. [16][17]  \n- Thường thấy **training loss < validation loss**; validation loss cao hơn có thể là dấu hiệu của *overfitting* (mô hình phù hợp quá với dữ liệu train và không khái quát tốt trên validation). [17][18]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa trong video: tạo hai tập điểm đỏ và xanh, trực quan hóa scatter plot cho thấy hai cụm dữ liệu có thể tách bằng một đường thẳng; dùng logistic regression (Dense + sigmoid) để phân lớp. [3][4]  \n- Mô tả output của model.summary(): input shape = (2,), Dense layer với tổng parameters = 3 (2 weights + 1 bias). Đây là ví dụ minh họa rõ ràng cách tính số tham số cho mô hình đơn giản. [13][14]  \n- Quá trình train (ví dụ epochs ban đầu = 5, trong thử nghiệm có đặt num_epoch = 500) và lưu lịch sử training vào biến history để vẽ loss/accuracy theo epoch. [11][15]  \n- Ứng dụng thực tế: bài toán phù hợp cho **binary classification** khi dữ liệu tương đối *linearly separable*; phương pháp này là nền tảng để hiểu các mô hình phân lớp phức tạp hơn. (Video nhấn mạnh tính phân tách theo đường thẳng của dữ liệu ví dụ). [3][4]\n\n---\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính: bài giảng trình bày cách cài đặt logistic regression bằng Keras — từ tạo dữ liệu synthetic hai lớp, xây dựng mạng (Input 2 chiều → Dense(1, sigmoid)), compile với optimizer SGD (lr=0.01) và loss = BinaryCrossentropy, đến việc fit model và theo dõi training/validation loss. [1][2][6][9][10][11]  \n- Tầm quan trọng: sử dụng Keras giúp giảm gánh nặng tính toán đạo hàm tay (autodiff), cho phép tập trung vào thiết kế dữ liệu và pipeline huấn luyện; theo dõi validation loss rất quan trọng để phát hiện overfitting. [1][9][17]  \n- Liên hệ với các bài giảng khác: phương pháp và khung chương trình (build/plot/summary/predict) được tái sử dụng từ bài Linear Regression trước đó; những khái niệm về số tham số, trainable vs total parameters sẽ có mở rộng khi học các mô hình phức tạp như CNN trong các bài tiếp theo. [5][13][14]\n\n--- \n\nGhi chú: Tất cả nội dung trên đều dựa trực tiếp trên các đoạn trích từ video theo thứ tự thời gian. Các chi tiết về tên biến hay lỗi chính tả trong video (ví dụ is_train / is_val, num_epoch / epochs, \"1 năm\"/\"5 1\") được trình bày nguyên văn theo ngữ cảnh đã xuất hiện trong các chunk. [15][16][4]",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 2_jl9v7IDMTsk",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_1： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/jl9v7IDMTsk",
          "start_time": 0,
          "end_time": 60,
          "text": "trong phần này thì chúng ta sẽ tiến hành cài đặt cái mô hình Logistic Direction và chúng ta sẽ sử dụng thư viện Keras thì với thư viện Keras nó sẽ giúp cho chúng ta không cần phải tính đạo hàm của hàm loss một cách tương minh tức là ngầm bên trong Keras thì nó vẫn tính đạo hàm nhưng mà nó sẽ giúp cho chúng ta không phải ngồi tính toán lại các cái công thức các tương minh để Keras nó sẽ tự tính nó sẽ tự tính đạo hàm nó sẽ tự update các cái tham số cho mình thì để minh họa và mô phỏng cho cái mô hình Logistic Direction thì chúng ta sẽ sử dụng 2 cái tập điểm xanh và 1 cam tự tạo giống như trên đây miễn là sao chúng ta có thể tách 2 cái tập điểm này ra bằng 1 cái bùi vỏ cách thức để tạo ra cái tập dữ liệu này đó chính là chúng ta sẽ sử dụng 2 cái tâm và với 2 tâm này thì chúng ta sẽ random random giao động xung quanh này với 1 cái hàm nhỉu"
        },
        {
          "index": 2,
          "video_id": "Chương 2_jl9v7IDMTsk",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_1： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/jl9v7IDMTsk",
          "start_time": 49,
          "end_time": 110,
          "text": "cách thức để tạo ra cái tập dữ liệu này đó chính là chúng ta sẽ sử dụng 2 cái tâm và với 2 tâm này thì chúng ta sẽ random random giao động xung quanh này với 1 cái hàm nhỉu và lưu ý đó là input feature cho cái tập data này sẽ là 2 chiều đó là 2 cái tạo độ x1 và x2 trong không gian còn y của mình nó chính là cái thể hiện cho cái màu sắc của các cái data point ở đây các cái điểm dữ liệu ở đây ở bên dưới đó chính là cái mô hình Logistic Direction ở dạng đô thị trong đó thì ở đây sẽ là cái input layer và trong trường hợp này thì M của mình chính là bằng 2 tương ứng là 2 cái tạo độ trong không gian của mình và ở đây thì chúng ta sẽ có 1 cái hàm kích hoạt Activation 1 cái hàm kích hoạt là hàm sigmoid và cả 2 cái thằng này thì nó sẽ được đặt tên"
        },
        {
          "index": 3,
          "video_id": "Chương 2_jl9v7IDMTsk",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_1： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/jl9v7IDMTsk",
          "start_time": 99,
          "end_time": 163,
          "text": "và ở đây thì chúng ta sẽ có 1 cái hàm kích hoạt Activation 1 cái hàm kích hoạt là hàm sigmoid và cả 2 cái thằng này thì nó sẽ được đặt tên đó là Dense kết nối đầy đủ rồi bây giờ chúng ta sẽ cùng tiến hành cài đặt cho cái logistic direction thì cũng tương tự như linear direction đầu tiên chúng ta sẽ tạo ra các dữ liệu mẫu thì ở đây chúng ta sẽ có n sample chính là số mẫu cho một loại điểm chúng ta sẽ generate ra dữ liệu trend và dữ liệu test và dữ liệu validation thì trong cái ví dụ là này thì chúng ta sẽ có thêm cái sự tham gia của tập dữ liệu validation và ở đây chúng ta sẽ có 2 điểm là red point và blue point tức là điểm màu đỏ và điểm màu xanh và đối với cái điểm màu đỏ thì nó sẽ xoay xung quanh cái điểm có tọa độ là 1 năm thì chúng ta sẽ cùng theo dõi cái hình minh họa cho các cái điểm"
        },
        {
          "index": 4,
          "video_id": "Chương 2_jl9v7IDMTsk",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_1： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/jl9v7IDMTsk",
          "start_time": 146,
          "end_time": 209,
          "text": "và ở đây chúng ta sẽ có 2 điểm là red point và blue point tức là điểm màu đỏ và điểm màu xanh và đối với cái điểm màu đỏ thì nó sẽ xoay xung quanh cái điểm có tọa độ là 1 năm thì chúng ta sẽ cùng theo dõi cái hình minh họa cho các cái điểm thì đối với các cái điểm màu đỏ thì nó sẽ giao động xung quanh cái điểm có tọa độ là 1 năm tầm 1 năm như vậy chúng ta có cái tâm đây và chúng ta sẽ random xung quanh cái điểm 1 năm này thì có cái điểm màu đỏ đây đối với cái điểm màu xanh thì chúng ta sẽ random xung quanh cái điểm có tọa độ là 5 1 tâm đó đây chúng ta sẽ random noise xung quanh cái này và như vậy thì 2 tập điểm màu đỏ và màu xanh này thì đều có thể tách ra được bởi một cái đường thảo tương tự như vậy cho cái điểm màu xanh này cho cái tập dữ liệu validation thì chúng ta cũng sẽ sử dụng cái công thức etrain công thức etrain nhưng mà chúng ta sẽ để thêm cái hậu tố đó là val tức là validation"
        },
        {
          "index": 5,
          "video_id": "Chương 2_jl9v7IDMTsk",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_1： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/jl9v7IDMTsk",
          "start_time": 198,
          "end_time": 261,
          "text": "tương tự như vậy cho cái điểm màu xanh này cho cái tập dữ liệu validation thì chúng ta cũng sẽ sử dụng cái công thức etrain công thức etrain nhưng mà chúng ta sẽ để thêm cái hậu tố đó là val tức là validation còn y thì nó sẽ bao gồm đối với tập dữ liệu etrain thì cái phần đầu red void nó sẽ có cái y tương ứng nhãn là 1 và cái phần blue void thì cái phần nhãn của mình nó tương ứng sẽ là 0 rồi và bước tiếp theo thì chúng ta sẽ chạy lại rồi nó sẽ ra các cái tập điểm như thế này cũng có thể chia tách được ra bởi 1 cái đường thẳng đối với kích thuận tài toán huấn luyện thì như đã đề cập tức là chúng ta sẽ sử dụng cái thư viện Keras và trong cái bài linear regression thì chúng ta có 1 cái bộ khung chương trình thì ở đây chúng ta cũng sẽ sử dụng lại cái bộ khung đó tuy nhiên ở đây chúng ta tái sử dụng lại các cái phương án của chúng ta phương thức là cell, plot, summary, predict và gateway"
        },
        {
          "index": 6,
          "video_id": "Chương 2_jl9v7IDMTsk",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_1： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/jl9v7IDMTsk",
          "start_time": 247,
          "end_time": 307,
          "text": "và trong cái bài linear regression thì chúng ta có 1 cái bộ khung chương trình thì ở đây chúng ta cũng sẽ sử dụng lại cái bộ khung đó tuy nhiên ở đây chúng ta tái sử dụng lại các cái phương án của chúng ta phương thức là cell, plot, summary, predict và gateway chúng ta sẽ phải viết lại cái phương thức build và trend thì đối với cái phần phương thức build thì chúng ta cũng sẽ phải có 1 cái lớp đầu tiên đó chính là cái lớp input lớp input rồi input và chúng ta sẽ phải truyền cho nó cái set của đầu vào và set này thì đó cũng tương tự như linear regression nó sẽ có cái tham số đó là input in và có thêm dấu phẩy ở đây để hàm ý đó là cái set này nó sẽ thể là cho những kiện đầu vào là vector chứ không phải là 1 vai trận vector này gồm có input in chiều rồi nó sẽ trả về 1 cái biến đó là input"
        },
        {
          "index": 7,
          "video_id": "Chương 2_jl9v7IDMTsk",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_1： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/jl9v7IDMTsk",
          "start_time": 295,
          "end_time": 359,
          "text": "cái set này nó sẽ thể là cho những kiện đầu vào là vector chứ không phải là 1 vai trận vector này gồm có input in chiều rồi nó sẽ trả về 1 cái biến đó là input tiếp theo đó là output thì cái output của mình á output mình sẽ làm 1 cái lớp biến đổi là cái nối đầy đủ là dense trong đó nó chỉ có duy nhất 1 node chúng ta sẽ có duy nhất 1 node đầu ra và cái hàm activation của mình sẽ là hàm sigmoid activation của mình sẽ là hàm sigmoid rồi và ở đây chúng ta có cái thành phần bias có cái thành phần bias rồi output sẽ là bằng dense và đầu ra của mình sẽ là 1 node activation thì chúng ta sẽ để là bằng bằng sigmoid"
        },
        {
          "index": 8,
          "video_id": "Chương 2_jl9v7IDMTsk",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_1： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/jl9v7IDMTsk",
          "start_time": 349,
          "end_time": 408,
          "text": "activation thì chúng ta sẽ để là bằng bằng sigmoid rồi use bias chúng ta sẽ để là bằng true và lưu ý là chúng ta mới chỉ tạo cho cái lớp output chúng ta phải truyền phải truyền cái lớp đầu vào cho nó đó là input rồi và tiếp theo đó là chúng ta sẽ đóng gói cái input và cái output này lại có input và output đóng gói nó lại vào 1 cái biến tên là model và cái biến model này thì sẽ trả cho 1 cái thu thức đó là cell.node rồi thì ở đây chúng ta sẽ không cần phải trả gì hết thì ở đây chúng ta sẽ không cần phải trả gì hết  cái phương thức build này chúng ta sẽ không cần phải trả gì hết ở phương thức trend"
        },
        {
          "index": 9,
          "video_id": "Chương 2_jl9v7IDMTsk",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_1： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/jl9v7IDMTsk",
          "start_time": 397,
          "end_time": 461,
          "text": "cho 1 cái thu thức đó là cell.node rồi thì ở đây chúng ta sẽ không cần phải trả gì hết thì ở đây chúng ta sẽ không cần phải trả gì hết  cái phương thức build này chúng ta sẽ không cần phải trả gì hết ở phương thức trend thì chúng ta sẽ cần phải khởi tạo cái optimizer ats.kerast .optimizer . ở đây tương tự chúng ta vẫn sử dụng cái stochastic gradient descent và chúng ta sẽ phải truyền tham số đầu vào là numing rate là bằng 0.01 0.01 thì tương lai thì cái numing rate này chúng ta cũng hoàn toàn có thể tham số hóa nó nhưng mà thôi ở đây chúng ta sẽ tạm thời là cổ để tiếp theo đó là cell.model .compile .compile chúng ta sẽ truyền vào cái optimizer chúng ta sẽ truyền vào cái optimizer là bằng vt rồi đồng thời"
        },
        {
          "index": 10,
          "video_id": "Chương 2_jl9v7IDMTsk",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_1： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/jl9v7IDMTsk",
          "start_time": 452,
          "end_time": 509,
          "text": ".compile .compile chúng ta sẽ truyền vào cái optimizer chúng ta sẽ truyền vào cái optimizer là bằng vt rồi đồng thời loss function thì chúng ta sẽ sử dụng là tf. lúc trước thì chúng ta sử dụng là mean square error đúng không thì chúng ta có thể sử dụng là binary cross entropy để khai báo cho cái binary cross entropy thì chúng ta sẽ khai báo như sau là tf.kerast . loss . rồi binary cross entropy rồi và lưu ý là nó phải phải tạo dưới dạng là một cái đối tượng cho đó ở đây chúng ta phải để thêm là dấu mẫu trong mặt rồi và bây giờ thì mình sẽ tiến hành trend là cell.model .fit dữ liệu is trend"
        },
        {
          "index": 11,
          "video_id": "Chương 2_jl9v7IDMTsk",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_1： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/jl9v7IDMTsk",
          "start_time": 499,
          "end_time": 560,
          "text": "rồi và bây giờ thì mình sẽ tiến hành trend là cell.model .fit dữ liệu is trend is trend is validation is validation và is validation thì và is validation thì và thì chúng ta sẽ đóng gói trong cái phương thức là validation data vậy chúng ta phải đóng gói nó lại chứ không phải là truyền đời như thằng is trend là is trend được validation thay thai đây rồi và ở đây thì chúng ta sẽ có thêm cái tham số là số lượng epoch thì ở đây sẽ để là n epoch n epoch của mình sẽ là bằng 5 epoch rồi như vậy thì chúng ta đã cài xong cái phương thức game mô hình là logistic direction và hai cái"
        },
        {
          "index": 12,
          "video_id": "Chương 2_jl9v7IDMTsk",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_1： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/jl9v7IDMTsk",
          "start_time": 548,
          "end_time": 611,
          "text": "sẽ là bằng 5 epoch rồi như vậy thì chúng ta đã cài xong cái phương thức game mô hình là logistic direction và hai cái các cái phương thức như là cell load summary predict gateway là chúng ta sẽ tái sử dụng lại các cái mô hình đa số nó cũng sẽ tái sử dụng lại như vậy chủ yếu là chúng ta sẽ tiến hành cài đặt cái phương thức build và phương thức trend và bây giờ chúng ta sẽ chạy thử xem có lỗi gì không mà ngày hôm nay không có lỗi giờ chúng ta sẽ tiến hành khởi tạo build mô hình và xem coi cái kiến trúc mô hình của mình nó như thế nào đây sẽ là logistic regression rồi chúng ta sẽ khởi tạo là logistic regression rồi build thì ở đây chúng ta sẽ phải truyền vào cái tham số là input dimension thì như đã đề cập hồi nãy á tức là ở đây"
        },
        {
          "index": 13,
          "video_id": "Chương 2_jl9v7IDMTsk",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_1： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/jl9v7IDMTsk",
          "start_time": 600,
          "end_time": 660,
          "text": "rồi build thì ở đây chúng ta sẽ phải truyền vào cái tham số là input dimension thì như đã đề cập hồi nãy á tức là ở đây dimension đầu vào của chúng ta sẽ có 2 thành phần là x1 và x2 do đó ở đây thì chúng ta sẽ để ở đây tham số là 2 rồi và tấm tắt logistic regression.summary rồi thì ở đây chúng ta sẽ thấy là nó sẽ có input nè đầu vào của mình là 2 và đương nhiên là không có tham số nào lớp tiếp theo là lớp dense và số tham số của mình là 3 tại sao lại là 3 tại vì nó sẽ có 2 cái thành phần đầu vào và đồng thời là có thêm 1 cái thành phần bias nên cái số tham số của mình sẽ là 3"
        },
        {
          "index": 14,
          "video_id": "Chương 2_jl9v7IDMTsk",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_1： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/jl9v7IDMTsk",
          "start_time": 650,
          "end_time": 711,
          "text": "tại vì nó sẽ có 2 cái thành phần đầu vào và đồng thời là có thêm 1 cái thành phần bias nên cái số tham số của mình sẽ là 3 và output của mình nó sẽ ra là 1, 1 lốt như vậy tổng số tham số sẽ là 3 và số tham số có thể truyền được trong trường hợp này là 3 thì trong 1 số cái mô hình phức tạp hơn như CNN thì nó sẽ có tình huống là trainable model nó sẽ ít hơn so với total parameters tổng số thành số là vì nó sẽ đóng bao 1 số phần và nó sẽ train 1 số phần thì cái đó chúng ta sẽ đến cái bài đó chúng ta sẽ nói sau rồi tiếp theo thì chúng ta sẽ tiến hành train cái mô hình của mình thì lưu ý là trong trường hợp này mô hình này mình sẽ có trả về cái quá trình train cái dữ liệu của cái quá trình này  và chúng ta sẽ để cái đối tượng trên là his và bằng log red chấm rk chúng ta sẽ truyền truyền vào is train"
        },
        {
          "index": 15,
          "video_id": "Chương 2_jl9v7IDMTsk",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_1： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/jl9v7IDMTsk",
          "start_time": 695,
          "end_time": 760,
          "text": "mô hình này mình sẽ có trả về cái quá trình train cái dữ liệu của cái quá trình này  và chúng ta sẽ để cái đối tượng trên là his và bằng log red chấm rk chúng ta sẽ truyền truyền vào is train is train rồi is van is van lưu ý là tham số num epoch bằng 500 rồi ở đây nó không hiểu is train là gì thì chúng ta sẽ lên đây xem cái đặt tên biến là đúng hay chưa is train là phải viết khoa chữ x và chữ y rồi is train is van là is van rồi ở đây là n epoch lưu ý là tham số num epoch là is van là is van"
        },
        {
          "index": 16,
          "video_id": "Chương 2_jl9v7IDMTsk",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_1： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/jl9v7IDMTsk",
          "start_time": 754,
          "end_time": 808,
          "text": "rồi ở đây là n epoch lưu ý là tham số num epoch là is van là is van rồi chúng ta sẽ xem trong hàm train num epoch rồi ở đây thì cái hàm fit này có thể là cái tham số của mình số epoch của mình mình để viz size lựu chính tả epoch, square và không có chữ n rồi bây giờ mình sẽ phải build lại từ đầu rồi đã bắt đầu train rồi bây giờ mọi người thấy là below đang từ 3 giảm xuống còn 2 giảm xuống còn 1 giảm xuống còn 0 bẻ mây tức là nó đang đi theo đúng hướng như mình mong muốn"
        },
        {
          "index": 17,
          "video_id": "Chương 2_jl9v7IDMTsk",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_1： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/jl9v7IDMTsk",
          "start_time": 798,
          "end_time": 856,
          "text": "rồi bây giờ mọi người thấy là below đang từ 3 giảm xuống còn 2 giảm xuống còn 1 giảm xuống còn 0 bẻ mây tức là nó đang đi theo đúng hướng như mình mong muốn rồi thì chút nữa chúng ta sẽ ờm và lưu ý là chúng ta sẽ thấy là có 2 is thrown vềここ là 6 đại lượng là loss của thằng trend và validation loss thì cái loss của tập trend nó sẽ thường nó sẽ thấp hơn trong trường hợp này nó còn thấp hơn cả cái validation loss thì cũng đúng thôi tại vì nó sẽ có cái hiện tượng gọi là overfitting và loss cho tập trend thì thường nó sẽ thấp hơn so với validate validate thường cái dữ liệu của mình nó sẽ mới hơn nó sẽ không có lặp lại trên cái tập trend do đó thì loss của thằng validation sẽ cao hơn so với loss của tập trend"
        },
        {
          "index": 18,
          "video_id": "Chương 2_jl9v7IDMTsk",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_1： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/jl9v7IDMTsk",
          "start_time": 842,
          "end_time": 856,
          "text": "validate thường cái dữ liệu của mình nó sẽ mới hơn nó sẽ không có lặp lại trên cái tập trend do đó thì loss của thằng validation sẽ cao hơn so với loss của tập trend"
        }
      ]
    },
    {
      "video_id": "Chương 2_istYhrhklqs",
      "summary": "## 1. Giới thiệu (Introduction)\n- Mục tiêu chính của bài giảng: hướng dẫn cách trực quan hóa kết quả huấn luyện mô hình *logistic regression* đã cài đặt bằng Keras, bao gồm:\n  - vẽ đồ thị loss (train/validation) từ history; [1]\n  - trực quan hóa **decision boundary** (đường phân tách) của mô hình nhị phân bằng cách trích xuất tham số (theta) từ weights và vẽ đường thẳng tương ứng; [3]\n  - cách lấy weights/bias từ model (get_weights / indexing) và sử dụng để tính tọa độ đường thẳng; [7][8]\n  - nhận xét về hiện tượng overfitting: loss trên training thường thấp hơn validation; [2][12]\n- Các khái niệm sẽ được đề cập: history.loss & history.val_loss, legend/colors cho đồ thị, phương trình đường thẳng của logistic classifier (Theta0 + Theta1*x1 + Theta2*x2 = 0), biến đổi sang dạng x2 = m*x1 + b để vẽ; [1][2][3][5]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 Trực quan hóa loss (train vs validation)\n- Sử dụng object/biến vlt.Loss (từ history) để truy xuất giá trị loss và val_loss và vẽ đồ thị; đặt legend để phân biệt; [1]\n- Gán màu: màu xanh cho loss của tập train, màu cam cho validation; sử dụng legend/labels để giải thích; [2]\n- Quan sát thường thấy: loss trên tập train thường thấp hơn loss trên tập validation (hiện tượng overfitting / gap giữa train và val); đây là điều cần quan sát khi đánh giá mô hình; [2][12]\n\n### 2.2 Công thức đường ranh (decision boundary) cho logistic regression\n- Phương trình tổng quát của đường biên trong không gian 2 chiều (với bias):  \n  Theta0 + Theta1 * x1 + Theta2 * x2 = 0. [3][4]\n- Ý nghĩa về dấu của biểu thức: các điểm thuộc một lớp (ví dụ màu xanh, y=1) sẽ cho giá trị biểu thức > 0, các điểm lớp kia (màu cam, y=0) sẽ cho giá trị < 0; các điểm nằm đúng trên đường sẽ cho giá trị = 0. Điều này giúp phân biệt hai vùng không gian theo giá trị tuyến tính của model. [4]\n\n### 2.3 Chuyển sang dạng y = m*x + b để vẽ\n- Để vẽ đường thẳng trên mặt phẳng (x1, x2), chuyển đổi phương trình trên về dạng x2 = f(x1):  \n  Từ Theta0 + Theta1*x1 + Theta2*x2 = 0, ta có  \n  Theta2 * x2 = -Theta0 - Theta1 * x1  \n  => x2 = - (Theta1 / Theta2) * x1 - (Theta0 / Theta2). [5][13]\n- Kỹ thuật vẽ: chọn hai giá trị cho x1 (ví dụ x1 = -1 và x1 = 6), tính x2 theo công thức trên cho mỗi x1, được hai điểm; nối hai điểm đó để tạo đường thẳng (decision boundary). [6][9][13]\n\n### 2.4 Lấy tham số (theta) từ model (get_weights)\n- Gọi hàm get_weights của Keras model để lấy weights và bias; bias thường nằm trong array cuối cùng của kết quả get_weights. [7]\n- Cách index cụ thể: có thể lấy theta0 từ bias (phần tử bias cuối), theta1 và theta2 từ ma trận weight (ví dụ lấy hàng/cột tương ứng theo cách model lưu trữ). Video mô tả việc lấy phần tử đầu/ở hàng thứ 2 tùy cấu trúc array để xác định theta1, theta2. [8]\n- Sau khi có theta0, theta1, theta2, dùng công thức ở mục 2.3 để sinh tọa độ vẽ. [8][6]\n\n### 2.5 Vẽ đường thẳng và điểm dữ liệu (implement plotting)\n- Chọn mảng x1 gồm hai giá trị (ví dụ [-1, 6]) rồi áp dụng: x2 = - (theta1/theta2) * x1 - (theta0/theta2) để có hai x2 tương ứng; [9][10]\n- Dùng plt.plot (matplotlib) để vẽ đường thẳng lên cùng biểu đồ các điểm dữ liệu (scatter). Video cho thấy thao tác plt.plot để hiển thị đường thẳng cùng dữ liệu; [10][11]\n- Khi vẽ kết hợp với đồ thị loss, dùng legend / màu sắc để phân biệt các thành phần (data points, decision boundary, loss curves). [1][2][11]\n\n### 2.6 Ghi nhớ về cài đặt Keras cho logistic regression\n- Mô hình logistic regression được triển khai bằng Keras, kế thừa các phương thức như call, load, summary, predict; cách gọi các hàm này tương tự như trong bài linear regression; cần truyền đường dẫn file khi load model đã lưu; [11][12]\n- Video nhắc lại: ngoài việc training, cần trực quan hóa cả giá trị loss của quá trình train và validation để đánh giá mô hình; [12]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n- Ví dụ minh họa trong video:\n  - Chọn x1 = -1 và x1 = 6; tính x2 cho từng x1 bằng công thức x2 = - (theta1/theta2)*x1 - (theta0/theta2); vẽ hai điểm và nối để có decision boundary. [9][10][13]\n  - Vẽ đồ thị loss (train và validation) sử dụng history (vlt.Loss) và phân biệt bằng màu xanh (train) và màu cam (validation); quan sát validation loss thường nằm trên training loss. [1][2]\n- Ứng dụng thực tế:\n  - Dùng cách này để trực quan hóa giới hạn phân lớp của mô hình logistic 2D, giúp hiểu vùng dự đoán cho mỗi lớp và phát hiện khi model phân lớp không đúng mong muốn (ví dụ do overfitting hoặc weights chưa phù hợp). [4][2]\n- Trường hợp sử dụng:\n  - Debug và kiểm tra mô hình classification đơn giản bằng cách nhìn trực tiếp decision boundary so với dữ liệu scatter; [11]\n  - Kiểm tra training dynamics qua đồ thị loss train/val để quyết định điều chỉnh hyperparameters hoặc thêm regularization khi val_loss > train_loss. [12]\n\n## 4. Kết luận (Conclusion)\n- Tóm tắt các ý chính:\n  - Vẽ đồ thị loss train/validation từ history giúp phát hiện overfitting/hiệu năng huấn luyện; [1][2][12]\n  - Decision boundary của logistic regression 2D được cho bởi Theta0 + Theta1*x1 + Theta2*x2 = 0; biến đổi sang x2 = - (Theta1/Theta2)*x1 - (Theta0/Theta2) để vẽ; [3][5][13]\n  - Cần lấy đúng theta từ model (get_weights, chú ý bias ở array cuối và indexing của weights) rồi tính hai điểm để plot đường thẳng bằng plt.plot; [7][8][9][10][11]\n  - Việc triển khai model trong Keras hỗ trợ các phương thức quen thuộc (call, load, summary, predict), việc load model cần truyền đường dẫn file lưu; [11][12]\n- Tầm quan trọng: những bước này là thiết yếu để trực quan hóa và kiểm thử một mô hình phân lớp tuyến tính — giúp đánh giá trực quan hiệu quả phân lớp, phát hiện lỗi và quyết định điều chỉnh mô hình. [4][2][12]\n- Liên hệ với các bài giảng khác: cách gọi các hàm và logic cài đặt tương tự phần *Linear Regression* đã học trước đó (call, load, summary, predict), phần trực quan hóa loss và weights áp dụng trực tiếp cho logistic regression. [11][12]\n\n(Phần tóm tắt trên dựa hoàn toàn vào các đoạn dữ liệu được cung cấp trong video: [1] … [13].)",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 2_istYhrhklqs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_2： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/istYhrhklqs",
          "start_time": 0,
          "end_time": 60,
          "text": "Rồi bây giờ trước tiên thì chúng ta sẽ visualize cái hàm loss như thế nào Ở đây sẽ là vlt.Loss Và để truy xuất vô cái loss thì chúng ta sẽ để là history Rồi để đây sẽ là loss Và tương tự như vậy chúng ta sẽ có val loss Rồi legend thì chúng ta sẽ phải vẽ vlt.Loss Rồi legend thì chúng ta sẽ phải vẽ vlt.Loss Rồi Rồi RLoss"
        },
        {
          "index": 2,
          "video_id": "Chương 2_istYhrhklqs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_2： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/istYhrhklqs",
          "start_time": 43,
          "end_time": 111,
          "text": "Rồi legend thì chúng ta sẽ phải vẽ vlt.Loss Rồi legend thì chúng ta sẽ phải vẽ vlt.Loss Rồi Rồi RLoss Về validation Vlt.Loss Vlt.Loss Thì cái màu xanh nó là tương ứng cho cái loss của thập train Đây chúng ta sẽ để là train V audit Và validation thì chúng ta sẽ để là camo Chúng ta madam trong vì ta để màu cam Thì tại đây chúng ta sẽ thấy là whole validation nó nằm ở phía trên So với lại cái tập trên Rồi tiếp theo thì chúng ta sẽ tiến hành trực quan hóa cái momen Rồi chúng ta sẽ tiến hành là chỉ quan mô hình Và chúng ta sẽ tiến hành trực quan hóa môn Mình Vì có thể sẽ copy cái đoạn code để vẽ các ngôn KO Ober là mína mùa n 그때 vàoKS days sau."
        },
        {
          "index": 3,
          "video_id": "Chương 2_istYhrhklqs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_2： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/istYhrhklqs",
          "start_time": 95,
          "end_time": 160,
          "text": "Rồi tiếp theo thì chúng ta sẽ tiến hành trực quan hóa cái momen Rồi chúng ta sẽ tiến hành là chỉ quan mô hình Và chúng ta sẽ tiến hành trực quan hóa môn Mình Vì có thể sẽ copy cái đoạn code để vẽ các ngôn KO Ober là mína mùa n 그때 vàoKS days sau.  các cái data point để chúng ta clear đi cho nó gọn rồi vẽ lại các dữ liệu ban đầu rồi bây giờ chúng ta sẽ phải vẽ cái mô hình này thế thì làm sao chúng ta có thể vẽ được cái mô hình này thì chúng ta sẽ phải quay qua bên đây để xem coi cái phương trình của cái đường thẳng này đó là gì thì nếu mà thông thường thì phương trình cho cái model này nó sẽ ở dạng đó là Theta 0 cộng cho Theta 1 x 1 cộng cho Theta 2 x 2 và cái phương trình đường thẳng này thì nó sẽ là bằng 0"
        },
        {
          "index": 4,
          "video_id": "Chương 2_istYhrhklqs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_2： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/istYhrhklqs",
          "start_time": 149,
          "end_time": 210,
          "text": "Theta 0 cộng cho Theta 1 x 1 cộng cho Theta 2 x 2 và cái phương trình đường thẳng này thì nó sẽ là bằng 0 tất cả những cái điểm màu xanh ví dụ ở đây trong trường hợp này là tương ứng y là bằng 1 và ở đây thì tương ứng là y bằng 0 thì tất cả những cái điểm màu xanh ví dụ ở đây trong trường hợp này là tương ứng y là bằng 1 và ở đây thì tương ứng là y bằng 0 và những cái điểm mà nằm về cùng phía màu xanh nó sẽ khiến cho cái bộ giá trị này lớn hơn 0 còn nếu như với những cái điểm màu cam tức là cho cái nhạc y bằng 0 thì nó sẽ làm cho Theta 0 cộng cho Theta x 1 cộng cho Theta x 2 sẽ bằng 0 còn những cái điểm nào x 1 x 2 nào mà nằm trên cái đường thẳng này thì khi thế vô nó sẽ đoạt có giá trị bằng 0 do đó để trực quan hóa cái mô hình của mình thì chúng ta sẽ phải đi vẽ cái phương trình đường thẳng này tuy nhiên để vẽ được cái phương trình đường thẳng này thì chúng ta sẽ phải dùng một cái trick đó là chúng ta sẽ đưa về cái dạng phương trình là giống như thời xưa là y bằng x cộng b"
        },
        {
          "index": 5,
          "video_id": "Chương 2_istYhrhklqs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_2： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/istYhrhklqs",
          "start_time": 198,
          "end_time": 260,
          "text": "tuy nhiên để vẽ được cái phương trình đường thẳng này thì chúng ta sẽ phải dùng một cái trick đó là chúng ta sẽ đưa về cái dạng phương trình là giống như thời xưa là y bằng x cộng b thì ở đây là chúng ta không có cái khái niệm y và x giống như là cái hồ cấp 2 mà chúng ta sẽ phải đưa về cái khái niệm y của mình chính là cái x 2 của mình như vậy thì ở đây mình sẽ phải đưa về cái dạng công thức là x 2 bằng cái gì đấy của cái x 1 vậy thì chúng ta sẽ chuyển vé thôi theta 2 x 2 đúng không theta 2 x 2 sẽ là bằng trừng theta 0 cộng cho theta 1 x 1 rồi và cùng chia 2 vé cho theta 2 thì lúc đó chúng ta sẽ có công thức đó là x 2 sẽ là bằng trừng theta 1 phần theta 2 x 1 x 1 x 1 theta 2 x 2"
        },
        {
          "index": 6,
          "video_id": "Chương 2_istYhrhklqs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_2： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/istYhrhklqs",
          "start_time": 242,
          "end_time": 306,
          "text": "rồi và cùng chia 2 vé cho theta 2 thì lúc đó chúng ta sẽ có công thức đó là x 2 sẽ là bằng trừng theta 1 phần theta 2 x 1 x 1 x 1 theta 2 x 2  theta 0 chia chào theta 2 và dựa trên cái công thức này thì khi chúng ta vẽ lên trên đường thị chúng ta sẽ pick ra các cái giá trị x 1 ví dụ như pick ra một cái giá trị x 1 ở đây pick ra cái giá trị x 1 ở đây thì chúng ta thế vào chúng ta sẽ có cái x 2 và từ đó chúng ta sẽ vẽ được 2 cái điểm này và khi có 2 điểm này rồi thì nối lại thì chúng ta sẽ có cái đường thị rồi như vậy thì ở đây chúng ta sẽ phải tìm coi cái theta 0 theta 1 theta 2 nó là cái gì thì muốn vậy thì chúng ta sẽ phải gọi cái hàm get way được ok ok ok"
        },
        {
          "index": 7,
          "video_id": "Chương 2_istYhrhklqs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_2： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/istYhrhklqs",
          "start_time": 298,
          "end_time": 359,
          "text": "thì muốn vậy thì chúng ta sẽ phải gọi cái hàm get way được ok ok ok ok ok ok ok ok ok ok ok ok    kea ok ok okay ok ok ok okay ok ok ok  ok ok ok ok okay ok ok ok ok ok ok ok   ok ok ok ok ok dos ok ok ok ok ok ok 0. ấy thì nó tương ứng nó tương ứng là cái tham số cho 2 cái thành phần x1 và x2 cái thành phần bias nó sẽ nằm trong cái array cuối thành phần bias nó sẽ nằm trong array cuối, như vậy thì mình sẽ có cái theta 0"
        },
        {
          "index": 8,
          "video_id": "Chương 2_istYhrhklqs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_2： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/istYhrhklqs",
          "start_time": 349,
          "end_time": 408,
          "text": "cái thành phần bias nó sẽ nằm trong cái array cuối thành phần bias nó sẽ nằm trong array cuối, như vậy thì mình sẽ có cái theta 0 là thành phần cuối đúng không, như vậy nó sẽ là 1 w1 rồi, và lấy ra cái tổ tử rồi, theta 1 thì nó sẽ là bằng w và cho cái array chúng ta sẽ lấy cái array đầu tiên ra array w0 và với cái w0 thì chúng ta sẽ lấy phần tử đầu tiên và tương tự như vậy cho theta 2, thì chúng ta sẽ lấy là w0, tức là cái array này và lấy phần tử thứ ở hàng thứ 2 đúng không, như vậy là 1, 0 rồi, và chúng ta sẽ có theta 0, theta 1, theta 2 và bây giờ chúng ta sẽ lần được vẽ"
        },
        {
          "index": 9,
          "video_id": "Chương 2_istYhrhklqs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_2： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/istYhrhklqs",
          "start_time": 400,
          "end_time": 459,
          "text": "1, 0 rồi, và chúng ta sẽ có theta 0, theta 1, theta 2 và bây giờ chúng ta sẽ lần được vẽ thì chúng ta sẽ thế các cái giá trị là tại trừ 1 đi thế giá trị x1 tại trừ 1 và thế giá trị x1 tại giá trị là 6 1, thì chúng ta sẽ có 2 x2 cái giá trị là trừ 1 và 6 đây chính là cái x1 đây chính là x1 và tương ứng cái điểm tọa độ trục x2 tương ứng của nó đó thì chúng ta sẽ dựa trên cái công thức này chúng ta sẽ dựa trên công thức là x2 là bằng trừ theta 1 chia cho theta 2 x1 trừ cho theta 0 chia cho theta 2 như vậy thì ở đây mình sẽ có một cái mảng rồi trừ theta 1 chia cho theta 2"
        },
        {
          "index": 10,
          "video_id": "Chương 2_istYhrhklqs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_2： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/istYhrhklqs",
          "start_time": 452,
          "end_time": 500,
          "text": "rồi trừ theta 1 chia cho theta 2 dân cho x1 x1 trong trường hợp này của mình nó chính là trừ 1 rồi chúng ta copy à quên chúng ta còn một thành phần nữa là trừ theta 0 chia cho theta 2 trừ cho theta 0 chia cho theta 2 và sau đó chúng ta sẽ copy cái công thức này x2 cho cái khi chúng ta thế với cái điểm thứ hai đó là điểm 6 rồi ở đây sẽ là nhân xấu rồi ở đây là plt.plot"
        },
        {
          "index": 11,
          "video_id": "Chương 2_istYhrhklqs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_2： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/istYhrhklqs",
          "start_time": 497,
          "end_time": 562,
          "text": "rồi ở đây là plt.plot đó thì khi chúng ta vẽ lên trên khi chúng ta vẽ lên trên thì chúng ta vẽ lên trên thì thì chúng ta xemτά nó là điểm này erf chúng ta xemtion allez chúng ta xem CAT is chúng ta phải vẽ lên trên đây khi chúng ta sẵn có chứ không là bằng kè đây là loại chicken chúng ta vẽ lên Grey     dung kỳ ở đây     không có đúng  Đừng để ch fömer x2 vậy là chúng ta thấy rằng chúng ta đã có x1 ký theo này sẽ nói về various thì chúng ta đã tiến hành sử dụng Keras cài đặt cho cái mô hình Logistics Reaction và kế thừa được những cái phương thức như là Cell, Load, Summary Predict thì cái cách gọi những cái hàm này nó cũng hoàn toàn"
        },
        {
          "index": 12,
          "video_id": "Chương 2_istYhrhklqs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_2： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/istYhrhklqs",
          "start_time": 548,
          "end_time": 611,
          "text": "cài đặt cho cái mô hình Logistics Reaction và kế thừa được những cái phương thức như là Cell, Load, Summary Predict thì cái cách gọi những cái hàm này nó cũng hoàn toàn tương tự như cái bài Linear Reaction Cell thì chúng ta chỉ cần truyền cái đường dẫn vào file, Load thì chúng ta cũng phải đưa cái đường dẫn của cái file mà nó đã lưu và đồng thời trong cái bài này thì chúng ta có thêm một cái phần là trực quan hóa cái kết quả của các cái giá trị loss trong cái quá trình trend và validation thì đối với cái phần loss của hàm trend thì lúc nào nó cũng có cái giá trị loss thấp hơn do đó có cái hiện tượng Logo Kitting và validation thì thường là có cái loss nó sẽ cao hơn so với hàm trend và để trực quan hóa cho cái mô hình thì chúng ta sẽ phải xác lập cái phương thức, cái phương trình để xác lập cái phương thức của cái đường thẳng là Theta 0 cộng Theta 1 x 1 Theta 2 x 2 bằng 0 và chúng ta sẽ chuyển đổi nó"
        },
        {
          "index": 13,
          "video_id": "Chương 2_istYhrhklqs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 3b_2： Cài đặt mô hình logistic regression",
          "video_url": "https://youtu.be/istYhrhklqs",
          "start_time": 599,
          "end_time": 645,
          "text": "thì chúng ta sẽ phải xác lập cái phương thức, cái phương trình để xác lập cái phương thức của cái đường thẳng là Theta 0 cộng Theta 1 x 1 Theta 2 x 2 bằng 0 và chúng ta sẽ chuyển đổi nó về cái dạng là x 2 là bằng trừ Theta 1 chi cho Theta 2 x 1 trừ cho Theta 0 x 2 để đưa về cái dạng là Theta 0 Theta 2 chi cho Theta 2 để đưa về cái dạng quen thuộc giống hồi xưa đó là y bằng x cộng b thì một cái thành phần x 1 là chúng ta sẽ pick ra giá trị là 1 chúng ta sẽ pick ra cái giá trị là trừ 1 x 1  rồi sau đó chúng ta sẽ pick ra giá trị là 6 và thế nó vào thì chúng ta sẽ có cái tọa độ thương hiếm của x 2 thì đây là cái cách thức để mà mình trực quan hóa cho bài Logistic Direction"
        }
      ]
    },
    {
      "video_id": "Chương 2_G4lcEPrfETo",
      "summary": "## 1. Giới thiệu (Introduction)\n- Mục tiêu chính: Giải thích mô hình Softmax Regression (hay Multinomial Logistic Regression) như một mở rộng của logistic regression cho bài toán phân lớp nhiều nhãn (K > 2), cách biểu diễn nhãn, hàm dự đoán (Softmax) và hàm lỗi (Cross-Entropy). [1][3][8][14]  \n- Các khái niệm sẽ được đề cập: biểu diễn nhãn (one-hot vs binary coding), mở rộng từ logistic đến K lớp, vấn đề của phép argmax, định nghĩa hàm Softmax và tính chất của nó, ví dụ số, và hàm lỗi Cross-Entropy (một mẫu và toàn bộ tập). [1][2][3][5][8][9][14][17]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Biểu diễn nhãn: one-hot và binary coding\n- Khi nhãn y là đơn nhãn (một mẫu chỉ thuộc đúng một lớp trong tập C có K phần tử), ta dùng vector one-hot để biểu diễn y. [1][2]  \n- Trong trường hợp đa nhãn (một mẫu có thể thuộc nhiều lớp), vẫn dùng vector 0/1 nhưng gọi là *binary coding* (không gọi là one-hot). [2]\n\n### 2.2. Từ logistic regression sang Softmax regression\n- Logistic regression cho K=2 tìm một đường thẳng (hoặc siêu phẳng) phân tách hai lớp; đường thẳng này biểu diễn bởi tham số θ, ví dụ với x=(x1,x2) phương trình đường là θ0 + θ1 x1 + θ2 x2 = 0. [3]  \n- Với K>2, ta có thể tưởng tượng K bộ tham số (mỗi bộ tạo một đường phân lớp riêng) — tương đương K logistic classifiers — và với mỗi điểm ta sẽ quyết định lớp bằng cách so sánh các xác suất đầu ra của các lớp này (chọn lớp có xác suất cao nhất). Ở đây bài toán xét ở chế độ *đơn nhãn* (single-label): mỗi điểm chỉ gán cho đúng một trong K lớp. [3][4]\n\n### 2.3. Vấn đề của argmax (hàm Max) và động lực dùng Softmax\n- Quy tắc chọn lớp bằng argmax (hàm Max) trên các giá trị đầu ra làm cho hàm quyết định không khả vi (non-differentiable), gây khó khăn khi tối ưu hóa bằng gradient-based methods như Gradient Descent. [5][6]  \n- Giải pháp: thay argmax bằng hàm Softmax — một \"soft\" version của max — để tạo ra các xác suất có thể tính đạo hàm và dùng được trong tối ưu hóa. [6][7]\n\n### 2.4. Hàm dự đoán: định nghĩa Softmax và mô tả mô hình\n- Gọi vector đầu vào cho lớp là G = (G1, G2, ..., GK) = Θ^T x (ví dụ Θ^T x cho toàn bộ K lớp). Hàm dự đoán được định nghĩa là:\n  f_θ(x) = Softmax(G) = Softmax(Θ^T x). [8]  \n- Thành phần i của vector dự đoán ŷ (y_hat) được cho bởi công thức Softmax:\n  ŷ_i = exp(G_i) / sum_{j=1..K} exp(G_j). [9][8]\n\n### 2.5. Tính chất của Softmax và ví dụ số\n- Các giá trị ŷ_i do Softmax cho ra thỏa mãn: 0 < ŷ_i < 1 cho mọi i và sum_i ŷ_i = 1 — tức là tạo ra một *phân phối xác suất* trên K lớp. [10][11][12]  \n- Softmax là hàm liên tục và khả vi; vìđược xây dựng từ hàm exp nên đạo hàm của nó có dạng dễ xử lý, thuận tiện cho việc tính gradient trong các framework Deep Learning. [11][12][13]  \n- Ví dụ số minh họa: nếu G = [1, 2, 3], thì\n  - ŷ_1 = e^1 / (e^1 + e^2 + e^3),  \n  - ŷ_2 = e^2 / (e^1 + e^2 + e^3),  \n  - ŷ_3 = e^3 / (e^1 + e^2 + e^3).  \n  Từ đó thấy trực quan tính chất dương và tổng bằng 1. [9][10]\n\n### 2.6. Hàm lỗi: Cross-Entropy\n- Với một mẫu (single sample) có nhãn thực y (vector one-hot) và dự đoán ŷ, hàm lỗi được dùng là Cross-Entropy (negative log-likelihood): L(y, ŷ) = - sum_{i=1..K} y_i * log(ŷ_i). (giới thiệu và dùng trong video) [14][15]  \n- Trường hợp dự đoán chính xác (ŷ trùng với y one-hot), nếu lớp đúng có y_i = 1 và ŷ_i = 1 thì thành phần tương ứng trong loss = 0, nên tổng loss = 0 (ví dụ tính đặt vào công thức cho thấy). [15][16]  \n- Trường hợp dự đoán sai và mô hình gán xác suất rất nhỏ cho lớp đúng (ŷ_i ≈ 0), term -log(ŷ_i) → +∞, do đó loss có thể rất lớn; điều này cũng dẫn tới gradient lớn (cập nhật tham số nhanh hơn khi lỗi lớn). [16][17]\n\n### 2.7. Hàm lỗi trên toàn bộ tập dữ liệu và vector hóa\n- Với N mẫu, loss toàn tập thường lấy trung bình các loss từng mẫu (vẫn có dấu trừ ở trước trong công thức Cross-Entropy). Với vector hóa, công thức viết gọn là trung bình của Cross-Entropy giữa y (thực tế) và Softmax(Θ^T x) (dùng cho từng mẫu rồi lấy trung bình). [17][18][19]  \n- Các framework Deep Learning hiện đại cung cấp sẵn hàm Softmax và hàm Cross-Entropy (kèm gradient), nên thông thường ta sử dụng các thư viện đó thay vì tự khai triển đạo hàm thủ công. [13]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n- Minh họa trực quan: hình minh họa với ba lớp (tam giác, tròn, dấu X)—mỗi lớp tương ứng một phân vùng do các đường phân lớp tạo ra; có vùng ranh giới mà argmax của các giá trị tuyến tính có thể gây bất định (vùng \"khu vực khó phân\") khi dùng phân lớp cứng; Softmax giúp chuyển sang xác suất mềm để học dễ dàng hơn. [4][5][6]  \n- Ví dụ số cụ thể: G = [1,2,3] dẫn tới ŷ tính bằng công thức exp/định thức, cho thấy tính chất xác suất của Softmax. [9][10]  \n- Ứng dụng thực tế: bài toán phân lớp nhiều lớp (multi-class single-label) trong machine learning / deep learning, nơi cần dự đoán một trong K nhãn (ví dụ phân loại ảnh nhiều lớp). Softmax + Cross-Entropy là cặp tiêu chuẩn cho các bài toán này. [4][8][14]  \n- Trường hợp sử dụng: khi nhãn là single-label dùng one-hot + Softmax; khi multi-label (thuộc nhiều lớp cùng lúc) dùng binary coding (và thường dùng sigmoid + binary cross-entropy cho từng nhãn, tuy video chỉ nhắc phân biệt biểu diễn). [2]\n\n## 4. Kết luận (Conclusion)\n- Tóm tắt: Softmax Regression là mở rộng tự nhiên của logistic regression cho K>2, biểu diễn nhãn bằng one-hot (single-label) hoặc binary coding (multi-label), sử dụng Softmax để tạo phân phối xác suất đầu ra khả vi, và Cross-Entropy để đo lỗi và huấn luyện bằng gradient-based methods. [1][2][3][8][14]  \n- Tầm quan trọng: Softmax + Cross-Entropy là thành phần cốt lõi cho các mô hình phân lớp nhiều lớp trong Deep Learning vì 1) cho ra xác suất chuẩn (sum=1), 2) khả vi và có đạo hàm dễ tính (= thuận tiện để tối ưu hóa), và 3) tương thích tốt với các framework hiện đại. [11][12][13][14]  \n- Liên hệ với bài giảng khác: mô hình này là bước mở rộng trực tiếp từ logistic regression (K=2) sang K lớp; việc tính đạo hàm và tối ưu hóa sẽ thường được ủy thác cho các Deep Learning Framework trong các bài sau. [3][13]\n\nGhi chú: Tất cả nội dung trên tóm tắt trực tiếp theo các đoạn trong video: [1]…[19].",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 1,
          "end_time": 61,
          "text": "đối với mô hình softbox regression thì chúng ta cũng sẽ phát triển từ cái mô hình logistic regression thì đầu tiên là chúng ta sẽ xem cái điều kiện đó là nhãn của dữ liệu y gỗn của dữ liệu y nó thuộc một cái tập C trong đó C này thì cái số lượng phần tử K của nó số lượng phần tử K là lớn hơn 2 đối với mô hình logistic regression thì K của mình là bằng 2 trong trường hợp mà nhiều hơn 2 phần lớp thì chúng ta sẽ sử dụng cái mô hình softbox và ở đây chúng ta sẽ thấy có 3 cái tập điểm thì chúng ta hy vọng rằng là cái output y này của mình nếu như mà nó chỉ có gán duy nhất một nhãn một nhãn duy nhất y của mình nó chỉ có thể là có một nhãn thì chúng ta sẽ sử dụng cái vector nó gọi là one hot nó gọi là one hot còn trong trường hợp mà đa nhãn"
        },
        {
          "index": 2,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 51,
          "end_time": 109,
          "text": "y của mình nó chỉ có thể là có một nhãn thì chúng ta sẽ sử dụng cái vector nó gọi là one hot nó gọi là one hot còn trong trường hợp mà đa nhãn tức là y của mình nó có thể vừa thuộc một lớp có thể thuộc hai lớp có thể thuộc ba lớp thì chúng ta cũng sẽ sử dụng cái vector biểu diễn dạng 0 1 như thế này nhưng lúc này nó không còn gọi là vector one hot nữa mà nó gọi là binary coding rồi thì đây là cái cách để biểu diễn cái y trong trường hợp mà nó có một nhãn hoặc nó có nhiều nhãn và đối với cái mô hình mà phân góp dì phân mà chúng ta đã học trước đây chúng ta sử dụng môn logistic regression thì cái việc mà tìm ra được một cái bộ tham số theta nó tương đương với cái việc là chúng ta tìm ra được một cái đường thẳng tìm ra được một cái đường thẳng để phân tách hai cái tập điểm này ra làm hai và cái đường thẳng này nó được tạo bởi cái tham số theta với cái công thức đó là"
        },
        {
          "index": 3,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 97,
          "end_time": 162,
          "text": "là chúng ta tìm ra được một cái đường thẳng tìm ra được một cái đường thẳng để phân tách hai cái tập điểm này ra làm hai và cái đường thẳng này nó được tạo bởi cái tham số theta với cái công thức đó là ví dụ trong trường hợp này là x x1 x2 ha thì cái công thức cho cái phương trình đường thẳng này đó chính là theta 0 cộng cho theta 1 x1 cộng cho theta 2 x2 là bằng 0 thì đây là cái phương trình đường thẳng và đại diện cho đường thẳng này đó sẽ là cái bộ tham số theta thì bây giờ áp dụng cái mô hình logistic nếu chúng ta mở rộng mô hình logistic ca cái mô hình logistic cho ca lớp trong ví dụ này chúng ta lấy là ca bằng 3 ha thì với dự kiện đầu vào x chúng ta sẽ có  mình ngồi với lại một cái x Contin проект dig n missions con gì mổ qua kẹo xét môi 3 muchísimo  thằng đấy hoàn hảo là một cái l bridge"
        },
        {
          "index": 4,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 149,
          "end_time": 217,
          "text": "x Contin проект dig n missions con gì mổ qua kẹo xét môi 3 muchísimo  thằng đấy hoàn hảo là một cái l bridge giống   cwr  truy rất d conoc 것ди  able? Chúng ta sẽ học ra một mùi Logistics thứ 2 là 1 xám và chúng ta sẽ có một đường phân lớp. Và với mùi Logistics thứ K thì chúng ta sẽ có một đường phân lớp ở đây. Và ở đây chúng ta sẽ đưa ra quyết định là rốt cuộc nó sẽ thuộc về lớp tam giác, lớp tròn hay là lớp X. Với một lưu ý đó là trong trường hợp này chúng ta sẽ sử dụng là đơn nhãn. Trong trường hợp này chúng ta là đơn nhãn tức là một điểm, một cái điểm thì nó chỉ được gán vào duy nhất một trong ba lớp tròn tam giác và X."
        },
        {
          "index": 5,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 191,
          "end_time": 256,
          "text": "Và ở đây chúng ta sẽ đưa ra quyết định là rốt cuộc nó sẽ thuộc về lớp tam giác, lớp tròn hay là lớp X. Với một lưu ý đó là trong trường hợp này chúng ta sẽ sử dụng là đơn nhãn. Trong trường hợp này chúng ta là đơn nhãn tức là một điểm, một cái điểm thì nó chỉ được gán vào duy nhất một trong ba lớp tròn tam giác và X. Thì bây giờ làm sao chúng ta chọn ra được nó thuộc về lớp tròn tam giác hay X. Thì chúng ta sẽ chọn cái mô hình trong ca cái mô hình này thì chúng ta sẽ chọn cái mô hình nào mà có cái xác xuất đầu ra. Là cao nhất tức là y ngã 1, y ngã 2 cho đến y ngã K. Thì giá trị nào cao nhất thì chúng ta sẽ nói nó thuộc về phân lớp đó. Tuy nhiên nếu mà làm như thế này thì nó sẽ nảy sinh ra một số cái vấn đề. Và cái vấn đề đó là nếu như cái điểm của mình nó nằm ở trong cái khu vực mà nó sẽ nằm trong cái tam giác này."
        },
        {
          "index": 6,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 247,
          "end_time": 313,
          "text": "Và cái vấn đề đó là nếu như cái điểm của mình nó nằm ở trong cái khu vực mà nó sẽ nằm trong cái tam giác này. Thì nó sẽ nói là không thuộc tam giác. Đối với cái đường màu đen thì nó sẽ nói là không phải là hình tròn. Và đối với cái dấu x, đối với cái đường màu xanh thì nó sẽ nói không thuộc cái dấu x. Như vậy thì rốt cuộc cái điểm này nó sẽ thuộc về lớp nào. Và tương tự như vậy nó sẽ ở cái vùng nhiễu dương này. Thì nó nói rằng là nó sẽ thuộc về lớp tam giác. Cái này sẽ nói về lớp x. Như vậy thì chúng ta sẽ... Kết luận cái điểm này thuộc về lớp nào. Thì đó chính là cái vùng mà gây nhật nhạt khó khăn cho chúng ta. Và để giải quyết vấn đề này thì như chúng ta đã nói chúng ta sẽ có 3 cái giá trị này. Thì chúng ta có thể sử dụng cái mô hình đó là... Chúng ta sẽ gọi cái hàm Max của các cái giá trị Y này."
        },
        {
          "index": 7,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 299,
          "end_time": 362,
          "text": "Và để giải quyết vấn đề này thì như chúng ta đã nói chúng ta sẽ có 3 cái giá trị này. Thì chúng ta có thể sử dụng cái mô hình đó là... Chúng ta sẽ gọi cái hàm Max của các cái giá trị Y này. I woman Max. Ờ... Tuy nhiên... Nếu mà chúng ta dùng cái hàm Max này á. Thì nó sẽ nảy sinh ra một cái vấn đề đó là... Hàm Max này đó là một cái hàm không khó tính đạo hàm. Nó sẽ là một cái hàm khó tính đạo hàm. Và hàm khó tính đạo hàm thì cái bước số 3 của chúng ta khi mà... Chúng ta dùng cái giải thuật Radian Descent nó cũng sẽ khó tính. Như vậy thì giải pháp của mình trong trường hợp này đó chính là mô hình Softmax. Thay vì dùng hàm Max thì chúng ta sẽ sử dụng một cái hàm gọi là hàm Softmax. Ờ... Thì chúng ta ý tưởng đó là... Bỏ hết tất cả các cái nốt Sigma ở đây. Mà chúng ta sẽ thay nó bằng một cái hàm duy nhất đó là hàm Softmax."
        },
        {
          "index": 8,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 347,
          "end_time": 410,
          "text": "Thay vì dùng hàm Max thì chúng ta sẽ sử dụng một cái hàm gọi là hàm Softmax. Ờ... Thì chúng ta ý tưởng đó là... Bỏ hết tất cả các cái nốt Sigma ở đây. Mà chúng ta sẽ thay nó bằng một cái hàm duy nhất đó là hàm Softmax. Với các cái dữ liệu đầu vào là G1, G2, GK. Đầu ra sẽ lần lượt tướng là Y1, Y2 cho đến YK. Và cái công thức để thiết kế cho cái hàm dự đoán nó cũng rất là đơn giản. Đó là Fθx sẽ bằng Softmax của Theta chuyển vị nhân bếp. Bình thường ở đây... Ở hàm Sigma thì ở đây chúng ta sẽ bỏ đi và thay bằng một cái hàm Softmax. Rồi... Và... Nếu như chúng ta đặt G... G là bằng Theta x. Tức là đây nè. Là cái vector G bao gồm các cái thành phần G1 cho đến G2, GK. Thì khi đó cái công thức của cái Softmax... Nó sẽ có công thức là như sau. Softmax G sẽ là bằng Y ngã. Tức là giá trị dự đoán. Với cái công thức của cái thành phần thứ Y... Của cái Y ngã ha. Y ngã Y..."
        },
        {
          "index": 9,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 400,
          "end_time": 459,
          "text": "Nó sẽ có công thức là như sau. Softmax G sẽ là bằng Y ngã. Tức là giá trị dự đoán. Với cái công thức của cái thành phần thứ Y... Của cái Y ngã ha. Y ngã Y... Sẽ là bằng... Y ngã Y. Mũ... G... Y... Chia cho tổng... Của K chạy từ 1 cho đến K lớn. E mũ... G mũ K. Thì cái công thức này... Nhìn có vẻ phức tạp. Tuy nhiên chúng ta có thể... Đưa ra một số cái con số để chúng ta có thể tính thử. Ví dụ như G của mình... Nó sẽ là bằng... Giá trị là... 1... 2... 3... Mình cố tình đưa ra cái con số này... Để cho nó... Nó... Tăng liên tục và nó dễ tính. Thì... Cái giá trị... Y ngã của mình... Nó sẽ là một cái vector. Nó sẽ là một cái vector. Trong đó... Cái thành phần đầu tiên của mình... Thành phần đầu tiên của mình nó sẽ là... E mũ Y. Đúng không? Thì ở đây Y trong trường hợp... Phần thần tử đầu tiên nó sẽ là... E mũ 1. Chia cho..."
        },
        {
          "index": 10,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 449,
          "end_time": 509,
          "text": "Thành phần đầu tiên của mình nó sẽ là... E mũ Y. Đúng không? Thì ở đây Y trong trường hợp... Phần thần tử đầu tiên nó sẽ là... E mũ 1. Chia cho... Tổng... Của các cái E mũ... K. Tức là... E mũ 1... Cộng cho... E mũ 2... Cộng cho... E mũ 3. Tương tự như vậy... Cái thành phần thứ 2... Của vector... Dự đoán Y ngã... Nó sẽ là... E mũ 2... Chia cho tổng... E mũ 1... Cộng cho... E mũ 2... Cộng cho... E mũ 3. Và thành phần cuối cùng... Nó sẽ là... E mũ 3... Chia cho... E mũ 1... Cộng cho... E mũ 2... Cộng cho... E mũ 3. Như vậy thì chúng ta có thể thấy... Cái tính chất... Nó có một cái tính chất... Đó là gì? Các cái giá trị Y... Ngã Y này... Nó đều là những cái con số lớn hơn 0... Và bé hơn 1... Đó là những con số lớn hơn 0... Tại vì E... Là một cái hàm... Mà giá trị... Miền giá trị của nó là... Là dương... Đó... Tất cả các phần tử này đều là dương..."
        },
        {
          "index": 11,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 498,
          "end_time": 561,
          "text": "Và bé hơn 1... Đó là những con số lớn hơn 0... Tại vì E... Là một cái hàm... Mà giá trị... Miền giá trị của nó là... Là dương... Đó... Tất cả các phần tử này đều là dương... Rồi... Tổng... Tất cả các cái Y ngã Y này... Thì nó sẽ là bằng... 1... Ví dụ như trong ví dụ này... Thì chúng ta thấy là... Tổng của nó sẽ là bằng... E mũ 1... Cộng cho... E mũ 2... Cộng cho... E mũ 3... Tất cả chia cho... Cái mũ số chung... Đó là E... Mũ 1... Cộng cho E mũ 2... Cộng cho E mũ 3... Thì cái này nó sẽ là bằng 1... Như vậy thì... Cái hàm sốt mắt nó có một cái tính chất khá hay... Đó là... Nó sẽ mát... Các cái giá trị... Z1, Z2, Z3... ZK... Thuộc... Cái giải giá trị là từ... Trừ vô cùng... Cộng vô cùng... Nó sẽ ép... Cái giá trị Z này... Đúng không? Trên cái miền giá trị là từ... Trừ vô cùng... Về cái giá trị... E... Nó sẽ thuộc cái đoạn là từ..."
        },
        {
          "index": 12,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 550,
          "end_time": 610,
          "text": "Nó sẽ ép... Cái giá trị Z này... Đúng không? Trên cái miền giá trị là từ... Trừ vô cùng... Về cái giá trị... E... Nó sẽ thuộc cái đoạn là từ... 0... Cho đến 1... Và tổng... Tất cả các cái giá trị E này... Thì đều bằng hộp... Thì đây chính là một cái... Không gian... Xác xúc... Tại vì trong cái không gian xác xúc... Thì mỗi một cái phần tử này nè... Mỗi một cái giá trị dự đoán này nè... Nó là cái xác xúc... Để thuộc về một cái lớp số 1... Cái này sẽ là xác xúc... Thuộc về cái lớp số 2... Và cái này sẽ là cái xác xúc... Thuộc về lớp số 2... Và các cái xác xúc này... Đều tuân theo tính chất... Từng cái phần tử này... Từng cái xác xúc này... Nó đều là những con số từ 0 cho đến 1... Và tổng tất cả các cái biến cố... Tất cả tổng tất cả các cái xác xúc... Nó đều là... Nó sẽ là bằng 1... Thì đây là một cái tính chất rất là... Của số mắt... Đồng thời... Cái số mắt của mình... Nó là một cái hàm liên tục... Và đạo hàm của nó... Cũng sẽ tính rất là dễ dàng..."
        },
        {
          "index": 13,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 599,
          "end_time": 660,
          "text": "Nó đều là... Nó sẽ là bằng 1... Thì đây là một cái tính chất rất là... Của số mắt... Đồng thời... Cái số mắt của mình... Nó là một cái hàm liên tục... Và đạo hàm của nó... Cũng sẽ tính rất là dễ dàng... Tại sao mình nói có thể tính dễ dàng là vì... Các bạn thấy là ở trong đây... Nó có sử dụng các cái hàm là E mũ... Mà chúng ta biết rồi... E mũ X... Đạo hàm... Nó cũng chính là... Bằng E mũ X... Thì chính là cái tính chất này... Nên khi chúng ta... Tính đạo hàm của số mắt... Thì nó sẽ ra cái công thức rất là đẹp... Tuy nhiên thì trong cái phạm vi... Của... Cái bước 2... Và bước 1 và bước 2... Thì chúng ta sẽ không tính... Đạo hàm của số mắt... Và như cũng đã giới thiệu thì... Cái số mắt... Nó cũng đã được hỗ trợ... Tính đạo hàm... Thông qua các cái thư viện của... Deep Learning Framework rồi... Đó thì ở đây mình sẽ không có... Đề cập đến chi tiết... Mà sẽ sử dụng các cái Deep Learning Framework... Về sau... Rồi... Và đối với cái bước số 2... Tức là cái bước để mà... Thiết kế cái hàm lỗi... Thì... Đối với cái trường hợp mà 1 mẫu..."
        },
        {
          "index": 14,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 648,
          "end_time": 710,
          "text": "Mà sẽ sử dụng các cái Deep Learning Framework... Về sau... Rồi... Và đối với cái bước số 2... Tức là cái bước để mà... Thiết kế cái hàm lỗi... Thì... Đối với cái trường hợp mà 1 mẫu... Thì chúng ta sẽ có cái công thức... Hàm lỗi... Cái công thức này nó gọi là... Cross... Entropy... Thì thực ra công thức này là công thức... Dạng tổng quát... Của cái Minority... Cross Entropy... Với... Cái Y... Của mình... Sẽ là 1 cái... Y ngã của mình... Nó sẽ là 1 cái vector... Dự đoán... Còn Y của mình... Thì nó sẽ là cái vector... Ờ... Dữ liệu thực tế... Thì bây giờ mình sẽ lấy... Một cái... Trường hợp... Ví dụ... Đó là Y của mình... Là... Có 3 thành phần thôi... Đó là... 0... 1... 0... Tức là... Y này nó đang nói là... Cái mẫu dữ liệu của mình... Nó đang thuộc về 1 cái lớp thứ 2..."
        },
        {
          "index": 15,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 699,
          "end_time": 760,
          "text": "Đó là Y của mình... Là... Có 3 thành phần thôi... Đó là... 0... 1... 0... Tức là... Y này nó đang nói là... Cái mẫu dữ liệu của mình... Nó đang thuộc về 1 cái lớp thứ 2... Cái lớp thứ 2... Ví dụ đây là Y... Ờ... Trong trường hợp này là... 0... 1... 0... Y trong trường hợp này đó là... 1... 0... 0... Y trong trường hợp này... Đó là... 0... 0...  1... Thì ở đây hàm ý đó là... Chúng ta đang... Cái nhãn của mình nó là cái... Nhãn tâm giác... Nhãn tâm giác... Rồi... Đây là giá trị thực tế... Còn giá trị dự đoán... Y nghẽ... Nếu như cái y nghẽ này của mình... Mà khớp với lại giá trị dự đoán... 0... 1... 0... Thì khi chúng ta thế vào cái công thức này... Nó sẽ là bằng... 0... Nhau với lại lớp 0... Coi dấu trừ ở đằng trước nữa... Trong cái công thức này thì nó thiếu cái dấu trừ nha..."
        },
        {
          "index": 16,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 749,
          "end_time": 810,
          "text": "Thì khi chúng ta thế vào cái công thức này... Nó sẽ là bằng... 0... Nhau với lại lớp 0... Coi dấu trừ ở đằng trước nữa... Trong cái công thức này thì nó thiếu cái dấu trừ nha... Nó sẽ có cái dấu trừ ở đằng trước... Cộng cho 1... Lốc 1... Cộng cho 0... Lốc 0... Thì 0... Trị tiêu... 1 lốc 1... Tức là... Lốc của 1 đó là 0... Do đó thì cái công thức này... Lỗi của mình nó sẽ là bằng 0... Trong trường hợp mà đoán đúng... Đây là đoán đúng... Trong trường hợp mà đoán sai... Tức là Y... Là bằng... 0... 1... 0... Nhưng mà cái y nghẽ của mình... Thì nó lại là bằng... 1... 0... 1 đi... À... 1... 0... 0 đi... Thì lúc này... Cái loss của mình... Cái loss của mình nó sẽ là bằng trừ của... 0... Lốc 1... Cộng cho... 1 lốc 0... Cộng cho... 0..."
        },
        {
          "index": 17,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 799,
          "end_time": 859,
          "text": "0... Lốc 1... Cộng cho... 1 lốc 0... Cộng cho... 0... Lốc 0... Thì rõ ràng là... 0... Như với mấy của mình... 0... 0... Như với mấy của mình... 0... Và lúc này... Thì... Thì cái loss của mình... Nó sẽ là bằng trừ... Của... 1... Lốc 0... Thì... Trong cái bài... Cross entropy... Chúng ta biết rồi... Lốc 0... Nó chính là bằng trừ cuối cùng... Do đó thì... Trừ của trừ... Nó sẽ ra là cộng cuối cùng... Tức là ra 1 cái con số... Vô cùng lớn... Và sở dĩ có cái con số vô cùng lớn... Nó sẽ giúp cho... Cái đạo hàm của mình... Đã lớn... Đạo hàm lớn... Thì cái việc cập nhật tham số... Nó sẽ nhanh hơn... Nên là nhắc lại cái ký ký nét cũ... Rồi... Thì đây là cho trường hợp... 1 mẫu dữ liệu... Đối với trường hợp... Mà toàn mẫu dữ liệu... Tức là chúng ta... X, Y... Đầu vào... Thì chúng ta... Đó sẽ là 1... Tập hợp... Các cái mẫu... Nhiều mẫu... Ở đây chúng ta có... N mẫu nè... Có N mẫu... Chúng ta sẽ tính trung bình cộng... Lưu ý là có dấu trừ... Trước..."
        },
        {
          "index": 18,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 849,
          "end_time": 910,
          "text": "Thì chúng ta... Đó sẽ là 1... Tập hợp... Các cái mẫu... Nhiều mẫu... Ở đây chúng ta có... N mẫu nè... Có N mẫu... Chúng ta sẽ tính trung bình cộng... Lưu ý là có dấu trừ... Trước... Và với từng mẫu... Thì chúng ta lại tính... Cái công thức... Cho... Từng cái phần tử... Của cái vector... Y... K... Và Y ngã K... Đó... Chúng ta... Với cái phần... Với cái mẫu thứ Y... Đúng không? Chúng ta sẽ có 1 cái vector... Với cái mẫu dữ liệu dự đoán... Y ngã... Chúng ta sẽ có vector... Rồi... Qua cái công thức... Cross entropy này... Ờ... Thực hiện... 11Y... Tức là thực hiện trên từng phần tử... Chúng ta sẽ ra... 1 cái giá trị lỗi... Và... Chúng ta sẽ... Tính trung bình cộng... Tất cả các cái lỗi này... Trên... Toàn bộ N mẫu... Thì đây là cái công thức... Ờ... Toàn mẫu... Và... Không có vector hóa... Đối với trường hợp mà... Nhiều mẫu... Toàn mẫu... Mà có vector hóa... Thì chúng ta sẽ có công thức... Rất là gọn như sau... Đó là bằng trung bình cộng... Của... Cross entropy..."
        },
        {
          "index": 19,
          "video_id": "Chương 2_G4lcEPrfETo",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4a： Mô hình hồi quy Softmax (SoftmaxRegression)",
          "video_url": "https://youtu.be/G4lcEPrfETo",
          "start_time": 898,
          "end_time": 936,
          "text": "Không có vector hóa... Đối với trường hợp mà... Nhiều mẫu... Toàn mẫu... Mà có vector hóa... Thì chúng ta sẽ có công thức... Rất là gọn như sau... Đó là bằng trung bình cộng... Của... Cross entropy... Của... Cross entropy... Và... Đầu vào của mình... Nó sẽ là... Softmax của... Theta chuyển vị... Như X... Và đây là giá trị thực tế... Y... Đây là đoán... Đây là giá trị thực... Và chúng ta sẽ... Thực hiện cái phép... Cross entropy... Trên element Y... Trên từng phần tử... Rồi sau đó tính trung bình cộng lại..."
        }
      ]
    },
    {
      "video_id": "Chương 2_G71D3dacAds",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Hướng dẫn cài đặt mô hình **softmax regression** (đa lớp phân loại với k > 2), xây dựng, huấn luyện và trực quan hóa kết quả trên dữ liệu 2 chiều với k = 4 lớp. [1][2][11]  \n- Các khái niệm sẽ được đề cập: kiến trúc mô hình (input gồm X1, X2 và bias; một lớp Fully Connected; activation = softmax), cách tạo dữ liệu nhiều lớp (các tâm lớp và sinh mẫu quanh tâm), one-hot encoding cho nhãn, định nghĩa hàm loss (categorical cross-entropy), phương thức build và train (fit) của mô hình, lựa chọn optimizer và learning rate, trực quan hóa biên phân lớp bằng lưới và phép dự đoán. [1][2][3][4][6][10][15]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 Kiến trúc mô hình Softmax Regression\n- Mô hình chỉ có một lớp Fully Connected (dense) nhận input gồm hai feature X1, X2 và thêm thành phần bias; output là một vector k chiều (k = 4 trong bài) và activation cuối cùng là **softmax** để thu được xác suất phân lớp. [1][2][7][19]  \n- Khác với linear/logistic regression (nhị phân), softmax regression cho ra k output tương ứng k lớp. [2][19]\n\n### 2.2 Tạo dữ liệu huấn luyện\n- Dữ liệu được sinh dựa trên một số điểm tâm S1, S2, S3, S4 (tọa độ các tâm được nêu trong video: S1 là \"10-2\", S2 là \"28\", S3 là \"128\", S4 là \"20\" — nguyên văn từ transcript) và mỗi tâm tương ứng một lớp; với mỗi tâm sinh ra các điểm xung quanh theo noise (min = 0, stddev ≈ 1.5) và số mẫu mỗi lớp là 50. [2][3]  \n- Sau khi gom các điểm từ các tâm lại, feature X là các điểm 2 chiều (x, y) và nhãn Y ban đầu là dạng scalar 0,1,2,3 (mỗi nhãn lặp n_sample = 50 lần). [3][4]\n\n### 2.3 One-hot encoding cho nhãn\n- Để dùng hàm loss dạng cross-entropy cho phân lớp đa lớp, nhãn scalar được chuyển sang dạng **one-hot encoding**: ví dụ nhãn 0 → [1, 0, 0, 0], nhãn 2 → [0, 0, 1, 0]. [4][5]\n\n### 2.4 Cài đặt lớp (class) SoftmaxRegression: phương thức build\n- Hàm build nhận các tham số: input_dim (số chiều input), thêm tham số mới output_dim (hay output beam, tức k) cho softmax regression. [6]  \n- Trong build: tạo layer dense với units = output_dim, activation = softmax, use_bias = True; input shape tương ứng input_dim (ở bài là 2). Sau khi build, đóng gói input và output thành một đối tượng model trả về. [7][8][19]\n\n### 2.5 Huấn luyện (train / fit)\n- Phương thức train/fit cấu hình: số epoch có thể lớn hơn (ví dụ đề xuất 1000 epoch) do độ phức tạp mô hình tăng; optimizer có thể dùng SGD (stochastic gradient descent) hoặc Adam (nhanh hơn); learning rate ví dụ = 0.01. [8][9]  \n- Biên dịch model với optimizer và loss là **categorical cross-entropy** (ở transcript đề cập dưới tên categorical entropy / categorical gross entropy), rồi gọi model.fit(x_train, y_train_onehot, epochs = num_epoch). [9][10]\n\n### 2.6 Kích thước tham số (model params)\n- Ví dụ cấu hình khi input_dim = 2 và output_dim = 4: số tham số được báo là 12. Giải thích: mỗi output unit có weights cho 2 input + 1 bias → (2 + 1) * 4 = 12. (Cách biểu diễn và con số này được kiểm tra trong model.summary). [11][12]\n\n### 2.7 Hành vi loss trong huấn luyện\n- Khi huấn luyện trên bộ dữ liệu này, loss ban đầu khá lớn (~3) và giảm dần theo epoch (về ~2, 1, 0.1, cuối cùng có thể xuống ~0.05 tùy cấu hình và số epoch). Huấn luyện sẽ lâu hơn so với các mô hình đơn giản hơn do nhiều tham số và số lớp output lớn hơn. [13][14]\n\n### 2.8 Trực quan hóa biên phân lớp (decision boundaries)\n- Cách trực quan hóa: tạo một grid (lưới) trên miền x1 ∈ [x1_min, x1_max] và x2 ∈ [x2_min, x2_max] (ở ví dụ, grid chạy từ −8 tới 17 cho cả x1 và x2), với mỗi điểm trên lưới gọi hàm predict(model, x_grid) để lấy output softmax. [15][16][17]  \n- Dùng np.argmax trên vector output 4 chiều để lấy chỉ số lớp có xác suất cao nhất, map chỉ số này sang màu và ký hiệu, rồi tô màu từng ô lưới để hiển thị vùng thuộc về từng lớp. Kết quả cho thấy softmax regression tách các vùng thành 4 vùng màu tương ứng khá hợp lý. [18][20]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa cụ thể trong video:  \n  - Sinh 4 cụm điểm 2D quanh 4 tâm (mỗi tâm 50 mẫu), xây dựng softmax regression (input_dim=2, output_dim=4), build → compile (optimizer + categorical cross-entropy) → fit (nhiều epoch). Theo dõi loss giảm từ ~3 xuống ~0.05. [2][3][4][9][13][14]  \n  - Trực quan hóa bằng grid từ −8 tới 17 cho cả trục x1 và x2, dự đoán nhãn cho từng điểm grid bằng model.predict và np.argmax, vẽ các vùng màu cho mỗi lớp. Kết quả phân vùng phù hợp với cụm dữ liệu. [15][16][17][18][20]\n\n- Ứng dụng thực tế / trường hợp sử dụng (những ý trong video liên quan):  \n  - Softmax regression là mô hình cơ bản cho phân loại đa lớp (multiclass classification) — dùng để phân loại các dữ liệu có nhiều hơn hai nhãn. (Trong bài, áp dụng trên dữ liệu 2 chiều để minh họa trực quan). [1][2][19]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính: Video trình bày cài đặt đầy đủ một lớp softmax regression: kiến trúc (FC + softmax), sinh dữ liệu đa lớp, chuyển nhãn sang one-hot, xây dựng model (build), compile với optimizer và loss = categorical cross-entropy, huấn luyện (fit) với số epoch lớn, và trực quan hóa biên phân lớp bằng grid + predict + np.argmax. [1][2][4][6][9][15][18]  \n- Tầm quan trọng: Softmax regression là mở rộng tự nhiên của logistic regression cho bài toán phân lớp nhiều lớp; hiểu cách cài đặt, huấn luyện và trực quan hóa giúp nắm vững tiền đề cho các mô hình phân loại phức tạp hơn. [2][19]  \n- Liên hệ với các bài giảng khác: Build và train của softmax regression sử dụng cùng khuôn khổ chương trình đã dùng cho linear regression và logistic regression; điểm khác chính là output_dim > 1 và activation = softmax, cùng với loss phù hợp (categorical cross-entropy). [1][6][19]\n\nGhi chú: Các con số, tên biến và các giá trị (k = 4, n_sample = 50, noise std ≈ 1.5, grid từ −8 tới 17, learning rate = 0.01, epoch đề xuất ≈ 1000, loss giảm từ ~3 xuống ~0.05) đều được mô tả trong nội dung video gốc. [2][3][4][8][9][13][14][17]",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 0,
          "end_time": 60,
          "text": "tiếp theo thì chúng ta sẽ tiến hành cài đặt cái mô hình SoftPath Regression thì đối với mô hình SoftPath Regression thì chúng ta sẽ phải phân lớp với cái số phân lớp là lớn hơn 2 thì trong trường hợp này số phân lớp của mình chọn đó chính là bằng 4 và ở đây thì chúng ta sẽ có một cái mô hình dưới dạng là đồ thị của SoftPath Regression thì cũng tương tự như 2 mô hình Linear Regression và Logistic Regression chúng ta sẽ có cái lớp Input trong đó thì cái lớp Input nó sẽ bao gồm 2 cái feature là X1, X2 và kèm theo một cái thành phần là Bias và cái lớp Output của mình thì nhìn có vẻ lớn nhìn thì có vẻ lớn nhưng thật ra ở đây chúng ta chỉ có duy nhất một cái lớp gọi là lớp Fully Connected và cái SoftPath này thì nó... nó chính là cái Activation của mình đó chính là cái hàm Activation"
        },
        {
          "index": 2,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 45,
          "end_time": 111,
          "text": "nhìn thì có vẻ lớn nhưng thật ra ở đây chúng ta chỉ có duy nhất một cái lớp gọi là lớp Fully Connected và cái SoftPath này thì nó... nó chính là cái Activation của mình đó chính là cái hàm Activation và Dance ở đây thì nó sẽ khác so với Linear Regression và Logistic Regression là Output của nó là nó đầu ra có đến k cái Output nó sẽ k Output ở trong trường hợp này k của mình chính là bằng 4 như vậy thì tiếp theo thì chúng ta sẽ tiến hành cài đặt cái mô hình SoftPath Regression thì cũng tương tự như vậy đó như vậy đầu tiên chúng ta sẽ tiến hành tạo cái dữ liệu và chúng ta sẽ có trước cái đoạn code để tạo dữ liệu thì ý tưởng tạo dữ liệu thì chúng ta cũng sẽ dựa trên một số cái điểm gọi là điểm tâm S1 là có tọa độ là 10-2 S2 có tọa độ là 28 S3 có tọa độ là 128"
        },
        {
          "index": 3,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 95,
          "end_time": 162,
          "text": "thì ý tưởng tạo dữ liệu thì chúng ta cũng sẽ dựa trên một số cái điểm gọi là điểm tâm S1 là có tọa độ là 10-2 S2 có tọa độ là 28 S3 có tọa độ là 128 S4 có tọa độ là 20 và với mỗi cái tâm này nó sẽ tương ứng với lại một cái phần lớp và với mỗi tâm ví dụ S1 chúng ta sẽ generate ra các cái điểm xoay xung quanh cái tâm này với cái noise của mình đó là min của mình là 0 và độ lực chuẩn sẽ là 1.5 và số mẫu của mình sẽ là 50 cho mỗi class rồi sau đó thì chúng ta gom toàn bộ các cái điểm này ra ra là 0.5 và số mẫu của mình sẽ gom đến tất cả những điểm tâm 1, tt2, tt3 và tt4 để tạo thành cái feature about x y, nhãn y thì nó tương ứng chính là các cái nhãn 0, 1, 2, 3 và với mỗi cái giá trị 0 này thì chúng ta sẽ nhân với n sample tức là nhân với 50 lần"
        },
        {
          "index": 4,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 149,
          "end_time": 209,
          "text": "y, nhãn y thì nó tương ứng chính là các cái nhãn 0, 1, 2, 3 và với mỗi cái giá trị 0 này thì chúng ta sẽ nhân với n sample tức là nhân với 50 lần 50 giá trị 0 cho cái class số 1 50 giá trị 1 cho cái class số 2 50 giá trị 2 50 giá trị 3 cho cái class số 3 50 giá trị 4 cho cái class số 3 50 giá trị 4 cho cái class số 4 và để mà có thể sử dụng được các cái độ đo về hàm loss như là cross entropy thì cái y của mình ban đầu nó ở dạng nhãn nó sẽ được tình vật về cái dạng là one hot encoding chúng ta sẽ sử dụng cái hàm sau để đưa nó về cái dạng one hot encoding one hot encoding có nghĩa là sao tức là ví dụ như cái nhãn của mình là 0 thì khi đưa về one hot encoding nó sẽ có cái dạng như sau đó là 1 0 0 0 tức là ví dụ như cái nhãn của mình đó là 2"
        },
        {
          "index": 5,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 199,
          "end_time": 264,
          "text": "nó sẽ có cái dạng như sau đó là 1 0 0 0 tức là ví dụ như cái nhãn của mình đó là 2 thì nó sẽ đưa về cái dạng one hot encoding đó là 0 0 1 0 thì đây là one hot encoding rồi và chúng ta sẽ chạy lại cái đoạn code này rồi tương tự như vậy thì mỗi một cái code này sẽ có 1 hàm loss như thế này  nó sẽ có 1 cái điểm nó sẽ có 1 cái màu rồi thì ở đây chúng ta sẽ thấy nó có do cái yếu tố ngộ nhiên thì nó sẽ có 1 vài điểm nó sẽ hơi giao thoa nhưng mà cái này thì cũng không ảnh hưởng nhiều đến cái việc nguồn lấu liệt ở phần cài đặt cho cái thuật toán ở phần cài đặt cho thuật toán thì chúng ta sẽ sử dụng cái bộ khung chương trình tương tự như linear direction và logistic direction thì chúng ta sẽ chủ yếu cài đặt cho 2 cái phương thức đó là build và trend"
        },
        {
          "index": 6,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 250,
          "end_time": 311,
          "text": "thì chúng ta sẽ sử dụng cái bộ khung chương trình tương tự như linear direction và logistic direction thì chúng ta sẽ chủ yếu cài đặt cho 2 cái phương thức đó là build và trend thì đối với cái build á thì đầu vào của mình á nó sẽ có cái input dimension input beam và cho cái software direction riêng cho software direction thì cái chúng ta sẽ phải có thêm 1 cái tham số nữa đó là cái tham số output beam hay nói cách hết đó chính là cái tham số k của mình đó chính là cái tham số k của mình vậy thì ở đây chúng ta sẽ tiến hành lại cái tham số k của mình  để mình bổ sung thêm 1 cái tham số nữa đó là output beam rồi và cũng tương tự ha chúng ta sẽ có input là bằng input rồi shape là bằng input"
        },
        {
          "index": 7,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 299,
          "end_time": 362,
          "text": "rồi và cũng tương tự ha chúng ta sẽ có input là bằng input rồi shape là bằng input vậy đầu vào của mình sẽ là input beam nó là 1 cái vector rồi về phần output thì nó chỉ là kết quả của 1 cái phép biến đổi kết nối đầy đủ là dense và đầu ra của mình bình thường mình để là 1 thì bây giờ đầu ra của mình nó chính là output output beam rồi activation thì mình sẽ phải để hàm đó là sumax rồi use bias thì chúng ta sẽ để là bằng true rồi và ở đây là chúng ta mới chỉ khởi tạo cho cái nấp biến đổi chúng ta sẽ phải truyền đầu vào cho nó"
        },
        {
          "index": 8,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 351,
          "end_time": 413,
          "text": "use bias thì chúng ta sẽ để là bằng true rồi và ở đây là chúng ta mới chỉ khởi tạo cho cái nấp biến đổi chúng ta sẽ phải truyền đầu vào cho nó chính là cái đối tượng tên là input rồi bây giờ chúng ta sẽ đóng gói cái input và output lại vào 1 cái đối tượng tên là model và chúng ta sẽ trả về cell.model và hàm này thì chúng ta sẽ không có trả về kết quả gì hết rồi đối với cái phương thức trend thì chúng ta cũng sẽ có cái số epoch là ví dụ như chúng ta vì cái mô hình này nó phức tạp hơn nên cái số epoch của chúng ta có thể phải cho cái con số nó lớn hơn như là 1000 epoch rồi và tương tự như vậy optimizer sẽ làm cho nó lớn hơn bằng tf.keras.optimizer"
        },
        {
          "index": 9,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 397,
          "end_time": 459,
          "text": "có thể phải cho cái con số nó lớn hơn như là 1000 epoch rồi và tương tự như vậy optimizer sẽ làm cho nó lớn hơn bằng tf.keras.optimizer và chúng ta cũng sẽ sử dụng stochastic gradient sense tuy nhiên nếu chúng ta muốn thì chúng ta cũng có thể sử dụng adam nó sẽ nhanh hơn rồi learning rate chúng ta bằng 0.01 rồi bây giờ chúng ta sẽ cell.model.compile để tích hợp cái optimizer này vào rồi chúng ta sẽ đồng thời cũng khai báo cái hàm loss thì ở đây lúc trước thì chúng ta sử dụng là mse ở đây chúng ta có sẽ sử dụng là"
        },
        {
          "index": 10,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 449,
          "end_time": 512,
          "text": "rồi chúng ta sẽ đồng thời cũng khai báo cái hàm loss thì ở đây lúc trước thì chúng ta sử dụng là mse ở đây chúng ta có sẽ sử dụng là categorical entropy entropy And gcategorical entropy  entropy và để trend thì chúng ta sẽ để là cell.model.fit dữ liệu x trend và etrend rồi số epoch thì chúng ta sẽ để là epoch bằng num epoch bằng num epoch  rồi như vậy là chúng ta đã cài đặt xong lớp đối tượng là softmax regression và tương tự như vậy thì chúng ta sẽ tiến hành khởi tạo build và train mô hình thì khởi tạo thì chúng ta sẽ có là softmax"
        },
        {
          "index": 11,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 498,
          "end_time": 564,
          "text": "rồi như vậy là chúng ta đã cài đặt xong lớp đối tượng là softmax regression và tương tự như vậy thì chúng ta sẽ tiến hành khởi tạo build và train mô hình thì khởi tạo thì chúng ta sẽ có là softmax regression rồi khổng lồ tham số rồi chúng ta sẽ gọi cái hàm build chấm build lưu ý là đây chúng ta sẽ có hai tham số đầu vào là input dimension và output dimension do đó thì input dimension thì chúng ta sẽ có hai là do cái điểm trong không gian hai chiều output của mình thì ở trên đây số dữ liệu của mình đó là 4k là bằng 4 đúng không như vậy thì chúng ta sẽ truyền vào đây chính là k trong trường hợp này là bằng 4 rồi và chúng ta sẽ xem thử cái model này nó sẽ có cái cấu hình giống như mình bỏ vô chưa và input dimension là input của mình là cái vector"
        },
        {
          "index": 12,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 544,
          "end_time": 611,
          "text": "chúng ta sẽ truyền vào đây chính là k trong trường hợp này là bằng 4 rồi và chúng ta sẽ xem thử cái model này nó sẽ có cái cấu hình giống như mình bỏ vô chưa và input dimension là input của mình là cái vector 2 chiều và số tham số bằng 0 và output của mình nó sẽ là cái lớp đen với cái output của mình chính là 4 và số tham số của mình sẽ là 12 thì tại sao lại là 12 12 đó là bằng 2 cộng 1 tức là thêm cái phần 2 cộng này là thêm cái phần câu hỏi đồ vào của mình sẽ có input của mình và 1 là bias và ít mỗi sai như vậy tổng cộng của mình đã có 3 3 cái đầu vào đầu ra của mình thì trai trong trường hợp này can là bằng 4 như vậy là 3 nhân bố chứ là 12 tham số tổng số tham số là 12 và bây giờ mình sẽ tiến hành trên cái mô hình này thì"
        },
        {
          "index": 13,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 593,
          "end_time": 659,
          "text": "đã có 3 3 cái đầu vào đầu ra của mình thì trai trong trường hợp này can là bằng 4 như vậy là 3 nhân bố chứ là 12 tham số tổng số tham số là 12 và bây giờ mình sẽ tiến hành trên cái mô hình này thì thì xalt.trend với x và lưu ý ở đây là xtrend nhưng một y thì chúng ta sẽ phải lấy là y1 hot ở đây dữ liệu là x chúng ta sẽ lấy x và y1 hot x và y1 hot chúng ta cũng quan sát loss và thấy là loss ban đầu là khá là lớn sau đó thì ban đầu sẽ là 3 sau đó thì có xu hướng giảm dần là còn 2 rồi 1, 0.1 rồi"
        },
        {
          "index": 14,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 648,
          "end_time": 715,
          "text": "sau đó thì có xu hướng giảm dần là còn 2 rồi 1, 0.1 rồi rồi và do số lượng dữ liệu cũng nhiều và mô hình thì nó có nhiều tham số hơn một chút nên chúng ta sẽ thấy là cái mô hình của mình xtrend sẽ lâu hơn số lượng bass người ở đây mà nó rung r Kirchouн   ví dụ là người ở đây của tôi là có ngủ ở Rabodell rồi ở đây thì chúng ta sẽ thấy là cái loss của mình nó giảm xuống còn 0.05 và để trực quan hóa thì cái cách thức để mà trực quan hóa cái boolean này của mình"
        },
        {
          "index": 15,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 691,
          "end_time": 760,
          "text": "có ngủ ở Rabodell rồi ở đây thì chúng ta sẽ thấy là cái loss của mình nó giảm xuống còn 0.05 và để trực quan hóa thì cái cách thức để mà trực quan hóa cái boolean này của mình là chính là chúng ta sẽ lấy một cái grid, một cái lưới, ví dụ chúng ta sẽ có giá trị x1 là từ x1 min cho đến x1 max rồi x2 thì sẽ có là x2 min cho đến x2 max và chúng ta sẽ lấy grid, tức là chúng ta sẽ lấy chi lưới rồi cứ mỗi điểm trên cái lưới này thì chúng ta sẽ gọi cái hàm boolean của mình chúng ta sẽ gọi cái hàm boolean của mình để xem coi là nó sẽ được xếp vào điểm màu xanh lá, xanh dương, màu vàng hay là màu đỏ"
        },
        {
          "index": 16,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 745,
          "end_time": 809,
          "text": "rồi cứ mỗi điểm trên cái lưới này thì chúng ta sẽ gọi cái hàm boolean của mình chúng ta sẽ gọi cái hàm boolean của mình để xem coi là nó sẽ được xếp vào điểm màu xanh lá, xanh dương, màu vàng hay là màu đỏ thế thì ở đây để có thể trực quan hóa được thì ở đây chúng ta sẽ để là soft red, soft black direction rồi và chúng ta sẽ gọi cái hàm boolean này là x2 min cho đến x2 max rồi cái hàm predict này, chúng ta sẽ gọi cái hàm predict này để đưa ra cái y nè trong đó cái x test của mình là lấy các cái giá trị từ x và xy trong đó xx và xy thì sẽ lấy trên một cái matrix, tức là chúng ta sẽ lấy trên một cái lưới với x sẽ lấy từ trừ 8 cho đến 17 tức là cái này nó phải đúng ra là x1 rồi, tầm này sẽ là x2 nè rồi x2                       x1, x2 rồi thì chúng ta sẽ có"
        },
        {
          "index": 17,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 799,
          "end_time": 863,
          "text": "rồi, tầm này sẽ là x2 nè rồi x2                       x1, x2 rồi thì chúng ta sẽ có xx1 và xx2 rồi xx1 và xx2 rồi chúng ta sẽ truyền các cái giá trị x1, x3 rồi len của xx1, x1 và x2 xx1, xx2, x3, x5, x6, x7, x8, x9, x10, x12, x13, x14, x15, x16, x19, x20, x22 con đặt số 1 trong cái x tháng. thì chúng ta thấy là, 1 cái lưới, các cái ô, ở đây nó sẽ chạy từ trừ 8 cho đến 17 và x2 sẽ cũng chạy từ trừ 8 cho đến 17 và chúng ta sẽ có một lưới các cái điểm với mỗi điểm này chúng ta sẽ chạy cái hàm shock migration predict"
        },
        {
          "index": 18,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 842,
          "end_time": 914,
          "text": "thì chúng ta thấy là, 1 cái lưới, các cái ô, ở đây nó sẽ chạy từ trừ 8 cho đến 17 và x2 sẽ cũng chạy từ trừ 8 cho đến 17 và chúng ta sẽ có một lưới các cái điểm với mỗi điểm này chúng ta sẽ chạy cái hàm shock migration predict là predict rồi sau đó sẽ ra được cái i và từ cái i này thì chúng ta sẽ biết được là cái nhãn tương ứng của nó là gì chúng ta sẽ gọi qua cái hàm là np.argumensmax tại vì cái output của mình nó sẽ ra là một cái vector nó sẽ ra một cái vector 4 chiều và mình sẽ lấy cái thành phần max và tương ứng cái thành phần max thì mình sẽ có được cái chỉ số để gọi vô cái hàm color ở đây thì chúng ta đã xét áp sẵn là các giá trị các ký hiệu về màu sắc và ký hiệu rồi thì chúng ta thấy là cái softback direction nó cũng đã phân ra các cái tập màu xanh dương xanh lá màu vàng và màu đỏ thành các cái vùng khá là phù hợp như vậy thì trong cái bài softback direction này thì chúng ta đã tiến hành"
        },
        {
          "index": 19,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 899,
          "end_time": 960,
          "text": "softback direction nó cũng đã phân ra các cái tập màu xanh dương xanh lá màu vàng và màu đỏ thành các cái vùng khá là phù hợp như vậy thì trong cái bài softback direction này thì chúng ta đã tiến hành cài đặt cái mô hình sử dụng thư viện này thì cái lớp build cái phương thức build của softback direction nó cũng tương tự như cái lớp build cái phương thức build của linear direction và logistic direction nó cũng chỉ có một cái input đầu vào và một cái output đầu ra là kết quả của phép biến đổi fully connected điểm khác ở đây đó chính là chúng ta phải sử dụng hàm activation và cái output beam của mình bình thường là một thì ở đây cái show loss của mình nó lớn hơn hai tại giờ ta chúng ta sẽ phải có cái output beam ở đây và loss thì chúng ta cũng sẽ sử dụng là categorical gross entropy rồi cái điểm thú vị khác đó chính là"
        },
        {
          "index": 20,
          "video_id": "Chương 2_G71D3dacAds",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 4b： Cài đặt mô hình softmax regression",
          "video_url": "https://youtu.be/G71D3dacAds",
          "start_time": 950,
          "end_time": 979,
          "text": "và loss thì chúng ta cũng sẽ sử dụng là categorical gross entropy rồi cái điểm thú vị khác đó chính là trong cái phần trực quan hóa thì chúng ta sẽ phải lấy grid và ứng với từng cái điểm trong cái grid trong cái lưới này thì chúng ta sẽ gọi cái hàm predict rồi từ cái giá trị output i này thì chúng ta sẽ suy ra được cái nhãn và cái màu sắc và cái ký hiệu từng để vẽ lên và chúng ta sẽ gọi cái hàm predict rồi từ cái giá trị output i này thì chúng ta sẽ suy ra được cái nhãn và cái màu sắc và cái ký hiệu từ để vẽ lên"
        }
      ]
    },
    {
      "video_id": "Chương 2_aXB_C9IAyMg",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giải thích khái niệm cơ bản về **Neural Network** như là mô hình học sâu để xử lý các bài toán *phi tuyến* mà các mô hình tuyến tính trước đó (linear regression, logistic regression, softmax regression) không giải quyết tốt được. [1][2]  \n- Các khái niệm sẽ được đề cập: lớp ẩn (hidden layers), các hàm kích hoạt phi tuyến (sigmoid, tanh, ReLU), kiến trúc theo lớp (layered architecture), hàm softmax ở lớp đầu ra để chuyển về không gian xác suất, và hàm lỗi (cross-entropy). [4][5][8][12]  \n- Ngữ cảnh: so sánh nhanh với các mô hình đơn giản đã học (linear / logistic / softmax) và nêu lý do phải dùng mạng nhiều lớp cho dữ liệu phi tuyến. [1][2][15]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Tại sao cần Neural Network — dữ liệu phi tuyến\n- Với các mô hình tuyến tính (linear regression, logistic, softmax regression) điều kiện là các tập dữ liệu có thể phân tách bởi một đường thẳng (hoặc siêu phẳng). Nếu dữ liệu *phi tuyến* (ví dụ tập tam giác và tròn không thể tách bởi một đường thẳng) thì cần mô hình phức tạp hơn. [1][2][3]  \n- Trong thực tế, các trường hợp phi tuyến rất đa dạng, không nhất thiết là hình tròn; do đó cần tăng khả năng biểu diễn của mô hình bằng cách thêm lớp và hàm kích hoạt phi tuyến. [3][4]\n\n### 2.2. Kiến trúc cơ bản của một Neural Network\n- Mạng gồm: lớp input, một hoặc nhiều *lớp ẩn* (hidden layers) và lớp đầu ra (output layer). Ở mỗi lớp có phép biến đổi tuyến tính (nhân ma trận/nhân vô hướng với tham số θ) rồi tiếp theo là phép biến đổi phi tuyến (activation). [4][9]  \n  - Ví dụ: tại lớp đầu tiên: z(1) = θ1^T x  → a(1) = σ(z(1)) (σ có thể là sigmoid, tanh, ReLU, v.v.). [4][5]  \n  - Dãy các lớp được ghép lại thành hàm hợp nhiều lần: đầu vào x đi qua θ1 → activation → θ2 → activation → ... → θL → (output). [9][10]\n\n- Lưu ý về hàm activation: có thể dùng sigmoid, tanh, ReLU (và biến thể như leaky ReLU); *phải* có hàm kích hoạt phi tuyến giữa các lớp để tránh việc toàn bộ tổ hợp vẫn là một hàm tuyến tính (nếu chỉ có các phép biến đổi tuyến tính liên tiếp thì không tạo được phi tuyến tính). [5][6]\n\n### 2.3. Lớp ẩn, số node, số lớp — siêu tham số\n- Số lượng lớp ẩn và số node (neurons) mỗi lớp là *không biết trước* và phải chọn dựa trên tính chất dữ liệu và kinh nghiệm thiết kế. Dữ liệu phức tạp/phi tuyến nhiều → có thể tăng số lớp và số node; dữ liệu đơn giản/ít → giảm bớt. [6][7]  \n- Các giá trị này là *siêu tham số* (hyperparameters) và thường được tìm bằng cách search (ví dụ grid/random search) hoặc dựa trên kinh nghiệm. [7]\n\n### 2.4. Lớp đầu ra và softmax (đối với phân lớp nhiều lớp)\n- Ở lớp cuối L, thay vì dùng activation như lớp ẩn, với bài toán phân lớp nhiều lớp ta sử dụng hàm **softmax** để chuyển các giá trị đầu ra về *không gian xác suất* (mỗi ŷ_k ∈ [0,1] và tổng các ŷ_k = 1), từ đó có thể diễn giải xác suất thuộc từng lớp. [8][10]  \n- Việc dùng softmax giúp ta biết \"khả năng\" thuộc mỗi lớp (ví dụ phần trăm). [8]\n\n### 2.5. Công thức tổng quát (biểu diễn hàm dự đoán)\n- Mạng có thể được biểu diễn là một hàm hợp nhiều lớp:  \n  ŷ = softmax( θ_L · σ( θ_{L-1} · σ( ... σ( θ_1 · x ) ... ) ) )  \n  trong đó σ là hàm kích hoạt (sigmoid/tanh/ReLU), θ_i là tham số ở lớp i. [9][10][14]  \n- Càng nhiều lớp thì hàm hợp càng phức tạp và có khả năng biểu diễn các quan hệ phi tuyến phức tạp trong dữ liệu. [11]\n\n### 2.6. Hàm lỗi: Cross-Entropy cho softmax\n- Với phân lớp nhiều lớp dùng softmax, hàm lỗi cho một mẫu thường là cross-entropy: per-sample loss = − Σ_k y_k log(ŷ_k) (với ŷ là output softmax và y là one-hot label). Tổng (hoặc trung bình) trên toàn bộ n mẫu để được loss cuối cùng: J = (1/n) Σ cross_entropy(y^(i), ŷ^(i)). [11][12][13]  \n- Video nhắc cách tính per-sample bằng nhân từng phần tử tương ứng (y_k * ŷ_k) rồi cộng lại, sau đó trung bình trên tất cả mẫu — đây là dạng tương tự với softmax + cross-entropy. [11][12]\n\n### 2.7. Biểu diễn đồ thị và mối liên hệ với mô hình đơn giản\n- Các mô hình đơn giản như linear regression chỉ có một node tuyến tính (một lớp), logistic thêm sigmoid để ép về [0,1], softmax cho multi-class tạo ra vector ŷ. Neural Network là tổng quát hóa khi có nhiều node và nhiều layer hơn. [14][15][16][17]  \n- Thiết kế số layer và số node dựa trên tính chất bài toán: hồi quy (regression) thường dùng mô hình tuyến tính; phân lớp nhị phân dùng logistic (với sigmoid); phân lớp đa lớp dùng softmax; còn bài toán phi tuyến phức tạp cần NN nhiều lớp. [15][16][17]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa phi tuyến trong bài giảng: hai tập điểm hình tam giác và hình tròn không thể tách bởi một đường thẳng, nên cần mạng nhiều lớp để phân chia không gian. [2][3]  \n- Thay đổi kiến trúc để phù hợp dữ liệu: nếu dữ liệu càng phức tạp/phi tuyến nhiều → tăng số lớp ẩn và số node; nếu dữ liệu đơn giản hoặc mẫu ít → giảm layer/node để tránh overfitting. [7][17]  \n- Ứng dụng thực tế / trường hợp sử dụng:  \n  - Bài toán hồi quy: linear regression (một node tuyến tính). [15]  \n  - Phân lớp nhị phân: logistic với sigmoid. [15][16]  \n  - Phân lớp đa lớp: softmax ở đầu ra, dùng cross-entropy làm loss. [16][12]  \n- Liên hệ với các kiến trúc sâu hơn sẽ học sau: mạng convolutional (CNN) và mạng hồi tiếp (RNN) là các mở rộng/đặc thù của ý tưởng mạng nhiều lớp — trong các bài sau sẽ dùng các activation như ReLU / leaky ReLU phổ biến hơn. [5][14]\n\n- Lưu ý về bài tập/quizz: cuối buổi có phần quiz trắc nghiệm để kiểm tra lại kiến thức đã học. [18]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt: Neural Network là mở rộng của các mô hình tuyến tính bằng cách ghép nhiều phép biến đổi tuyến tính và các hàm kích hoạt *phi tuyến* (sigmoid/tanh/ReLU) theo nhiều lớp, và dùng softmax ở lớp cuối cho bài toán phân lớp nhiều lớp. Hàm lỗi phổ biến là cross-entropy trung bình trên các mẫu. Số lớp và số node là siêu tham số cần điều chỉnh theo dữ liệu. [1][4][5][8][12][14]  \n- Tầm quan trọng: Mạng nhiều lớp cho phép mô hình hóa các quan hệ phi tuyến phức tạp trong dữ liệu — điều cần thiết cho nhiều bài toán thực tế mà các mô hình tuyến tính không xử lý được. [2][3][11]  \n- Liên hệ với các bài giảng khác: Đây là nền tảng trước khi chuyển sang các kiến trúc deep hơn (CNN, RNN) và các lựa chọn activation phổ biến như ReLU sẽ được nhắc lại trong các bài sau. [5][14]  \n- Hướng tiếp theo: thực hành điều chỉnh siêu tham số, hiểu sâu hơn về backpropagation và tối ưu hóa (chưa trình bày ở phần này — sẽ được đề cập trong bài tiếp theo). [7][18]\n\n(Đã tóm tắt toàn bộ nội dung video theo từng đoạn/chunk được cung cấp.)",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 2_aXB_C9IAyMg",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5a： Mạng neural Network (Neural Network)",
          "video_url": "https://youtu.be/aXB_C9IAyMg",
          "start_time": 1,
          "end_time": 60,
          "text": "với mô hình neural network thì đây chính là mô hình học sâu đầu tiên mà chúng ta sẽ học trong cái khóa học này thì các cái mô hình linear regression logistic regression và softbox regression trước đây thì nó được sinh ra để giải quyết các cái bài toán tiến tính tức là ví dụ đối với cái mô hình linear regression thì cái dự kiện của Y của mình nó sẽ phụ thuộc một cách tiến tính với lại cái giá trị x tổ bào nó sẽ đồng bí hoặc là nghịch miếng đối với mô hình logistic regression nó là bài toán phân lớp và các cái tập điểm ở đây của mình thì nó hoàn toàn có thể phân tách được bởi một cái đường thẳng đối với mô hình softbox regression cho cái trường học phân lớp nhiều lớp thì ở đây chúng ta cũng tương tự như vậy ok đó là chúng ta sẽ"
        },
        {
          "index": 2,
          "video_id": "Chương 2_aXB_C9IAyMg",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5a： Mạng neural Network (Neural Network)",
          "video_url": "https://youtu.be/aXB_C9IAyMg",
          "start_time": 48,
          "end_time": 111,
          "text": "đối với mô hình softbox regression cho cái trường học phân lớp nhiều lớp thì ở đây chúng ta cũng tương tự như vậy ok đó là chúng ta sẽ có thể tách ra bởi các cái đoạn thẳng như thế này tách ra và sử dụng các cái đoạn thẳng thế thì đối với những cái trường hợp mà phi tuyến hoặc là non-linear thì chúng ta sẽ phải sử dụng cái mô hình phức tạp hơn và có cái số lớp biến đổi sâu hơn đó chính là neural network thì thế nào gọi là một cái dữ liệu phi tuyến thì ở đây đối với cái trường hợp mà tuyến tính thì chúng ta có thể chia tách được bởi một cái đường thẳng ví dụ hai cái tập tam giác và tròn có thể chia tách được bởi một cái đường thẳng còn trong cái ví dụ phi tuyến như ở đây thì không có cách nào chúng ta có thể dùng được một cái đường thẳng"
        },
        {
          "index": 3,
          "video_id": "Chương 2_aXB_C9IAyMg",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5a： Mạng neural Network (Neural Network)",
          "video_url": "https://youtu.be/aXB_C9IAyMg",
          "start_time": 99,
          "end_time": 160,
          "text": "có thể chia tách được bởi một cái đường thẳng còn trong cái ví dụ phi tuyến như ở đây thì không có cách nào chúng ta có thể dùng được một cái đường thẳng để chia ra làm hai ví dụ ở đây không có cách nào và chia cái hai cái tập tròn  và tam giác này ra làm hai hết thì ở đây nó sẽ gọi là phi tuyến và những cái trong cái trường hợp thực tế thì cái trường hợp phi tuyến này là nó muôn hình vạn trạng nó không nhất thiết phải là cái dạng hình tròn như thế này ở đây chúng ta lấy cái dạng hình tròn để cho nó đơn giản và dễ hình dung vậy thì đối với cái việc phân loại dữ liệu phi tuyến thì chúng ta cần phải hiệu chỉnh lại cái mạng softbox theo cái hướng như thế nào để mà có thể giải quyết được các cái bài toán phi tuyến này và cái cách để chúng ta hiệu chỉnh hiệu chỉnh cái mạng softbox đó là chúng ta sẽ tăng số lớp biến đổi lên hay còn gọi là lớp ẩn và thêm các cái hàm kích hoạt phi tuyến"
        },
        {
          "index": 4,
          "video_id": "Chương 2_aXB_C9IAyMg",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5a： Mạng neural Network (Neural Network)",
          "video_url": "https://youtu.be/aXB_C9IAyMg",
          "start_time": 149,
          "end_time": 210,
          "text": "đó là chúng ta sẽ tăng số lớp biến đổi lên hay còn gọi là lớp ẩn và thêm các cái hàm kích hoạt phi tuyến thế thì ở đây thế nào gọi là các cái lớp khái niệm về lớp ẩn và thế nào là các cái phép biến đổi phi tuyến thì tại cái lớp biến đổi đầu tiên tại lớp đầu tiên đó là cái lớp input rồi chúng ta sẽ thực hiện nhân tích vô hướng với lại cái bộ tham số theta1 thì đây chính là một cái lớp một cái lớp biến đổi hay còn gọi là lớp ẩn rồi sau khi chúng ta thực hiện cái phép tích vô hướng xong chúng ta sẽ đồng thời thực hiện ngay cái phép biến đổi là sigmoid ở đây là một cái hàm biến đổi phi tuyến và lưu ý đó là hàm phi tuyến này thì có thể làm sigmoid nhưng nó cũng có thể là hàm tanh nó cũng có thể làm relo"
        },
        {
          "index": 5,
          "video_id": "Chương 2_aXB_C9IAyMg",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5a： Mạng neural Network (Neural Network)",
          "video_url": "https://youtu.be/aXB_C9IAyMg",
          "start_time": 199,
          "end_time": 260,
          "text": "và lưu ý đó là hàm phi tuyến này thì có thể làm sigmoid nhưng nó cũng có thể là hàm tanh nó cũng có thể làm relo sau này thì đối với cái môn deep learning với cái mạng CNN thì chúng ta sẽ sử dụng relo, leaky relo, vv thì mình sao nó phải làm một cái hàm phi tuyến tại vì nếu như chúng ta tiến hành các phép biến đổi tiếp theo và không có cái lớp biến đổi phi tuyến này thì nó sẽ dẫn đến cái việc là phép biến đổi tuyến tính ngay sau đó là một cái phép biến đổi tuyến tính thì nó sẽ tạo ra một cái tổ hợp tuyến tính mà tổ hợp tuyến tính thì không thể giải quyết được các cái bài toán phi tuyến đó là lý do nó phải chèn vào ở giữa các cái hàm kích hoạt các cái hàm kích hoạt phi tuyến thì sigmoid nó gọi là hàm kích hoạt và sigmoid này nó phải làm một cái hàm phi tuyến và nó có thể làm sigmoid, hàm tanh, hàm relo"
        },
        {
          "index": 6,
          "video_id": "Chương 2_aXB_C9IAyMg",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5a： Mạng neural Network (Neural Network)",
          "video_url": "https://youtu.be/aXB_C9IAyMg",
          "start_time": 251,
          "end_time": 310,
          "text": "phi tuyến thì sigmoid nó gọi là hàm kích hoạt và sigmoid này nó phải làm một cái hàm phi tuyến và nó có thể làm sigmoid, hàm tanh, hàm relo miễn là một cái hàm phi tuyến Và đây là cái lớp ẩn thứ nhất sau đó nó qua đây nó sẽ là một cái lớp ẩn thứ hai và ở đây nó sẽ có các cái node ở đây nó sẽ có các cái node và cái số node này chúng ta cũng có thể tùy biến gia giảm nó không nhất thiết là bằng m nó có thể lớn hơn m hoặc nhỏ hơn m câu hỏi đặt ra đó là bao nhiêu node và bao nhiêu lớp ẩn thì trả lời luôn đó là mình sẽ không biết trước cây số lập hậu này là bao nhiêu m y ntn M. và bao nhiêu lớp ẩn thì trả lời luôn đó là mình sẽ không biết trước cây số lập hậu này là bao nhiêu m y ntn M. số nốt và số lớp ẩn là bao nhiêu mình không biết trước cái số lớp ẩn và cũng như là số nốt tối ưu nhưng mà mình sẽ phải dựa trên một số kinh nghiệm liên quan đến cái việc thiết kế kỹ trúc mạng ví dụ như nếu cái dữ liệu của mình nó phức tạp và nó rất là phi tuyến rồi dữ liệu của mình nó rất là nhiều"
        },
        {
          "index": 7,
          "video_id": "Chương 2_aXB_C9IAyMg",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5a： Mạng neural Network (Neural Network)",
          "video_url": "https://youtu.be/aXB_C9IAyMg",
          "start_time": 299,
          "end_time": 361,
          "text": "nhưng mà mình sẽ phải dựa trên một số kinh nghiệm liên quan đến cái việc thiết kế kỹ trúc mạng ví dụ như nếu cái dữ liệu của mình nó phức tạp và nó rất là phi tuyến rồi dữ liệu của mình nó rất là nhiều thì chúng ta có thể tăng cái số lớp ẩn lên và tăng cái số nốt lên tuy nhiên trong trường hợp dữ liệu của mình nó đơn giản hơn và cái dữ liệu của mình ít thì chúng ta có thể sẽ thu nhỏ thu hẹp cái số lớp ẩn và thu hẹp cái số nốt lại thì đó là dựa trên cái kinh nghiệm và mình cũng sẽ có một số kỹ thuật liên quan đến cái việc là tìm các cái siêu tham số các cái số nốt và các cái số lớp ẩn nó cũng chính là các cái siêu tham số mình có thể là dùng phương pháp research để tìm ra được các cái siêu tham số này rồi sau khi biến đổi qua lớp thứ nhất lớp thứ hai và đến cái lớp thứ L thì cái lớp L này là cái lớp cuối cùng thì chúng ta chú ý là với lớp L thì ngay sau khi thực hiện cái phép biến đổi tiến tính Sigma ở đây thì chúng ta sẽ thực hiện tiếp theo đó là cái hàm chốc mắt"
        },
        {
          "index": 8,
          "video_id": "Chương 2_aXB_C9IAyMg",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5a： Mạng neural Network (Neural Network)",
          "video_url": "https://youtu.be/aXB_C9IAyMg",
          "start_time": 349,
          "end_time": 410,
          "text": "thì chúng ta chú ý là với lớp L thì ngay sau khi thực hiện cái phép biến đổi tiến tính Sigma ở đây thì chúng ta sẽ thực hiện tiếp theo đó là cái hàm chốc mắt thay vì cái hàm kích hoạt ở đây chúng ta sẽ sử dụng hàm chốc mắt tại sao chúng ta lại phải dùng cái hàm chốc mắt này để chúng ta đưa tất cả các cái giá trị y ngã này về cái không gian xác xúc rồi chúng ta sẽ đưa về không gian xác xúc này  sát xuất, không gian sát xuất nghĩa là sao tất cả các cái giá trị Y này nó sẽ thuộc cái giá trị là từ 0 cho đến 1 và tổng tất cả các cái YK này nó sẽ là bằng 1 để khi chúng ta đưa về cái không gian sát xuất thì chúng ta sẽ cảm nhận được là cái khả năng nó thuộc về lớp số 1 là bao nhiêu phần trăm, khả năng thuộc về lớp số 2 là bao nhiêu phần trăm thì đó chính là cái kiến trúc của mạng Neural Network và cái công thức"
        },
        {
          "index": 9,
          "video_id": "Chương 2_aXB_C9IAyMg",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5a： Mạng neural Network (Neural Network)",
          "video_url": "https://youtu.be/aXB_C9IAyMg",
          "start_time": 398,
          "end_time": 461,
          "text": "cái khả năng nó thuộc về lớp số 1 là bao nhiêu phần trăm, khả năng thuộc về lớp số 2 là bao nhiêu phần trăm thì đó chính là cái kiến trúc của mạng Neural Network và cái công thức cho cái việc thiết kế cái hàm dự đoán của mình nhìn ở đây thì chúng ta sẽ thấy khá là phức tạp nhưng mà thật ra nếu để ý kỹ thì nó cũng có cái logic để cho chúng ta có thể nhớ một cách rất là nhanh đầu tiên, cái lớp biến đổi đầu tiên đó là chúng ta sẽ từ cái X này X của mình chính là cái dự kiện đầu bào qua cái theta 1, nhân tích vô hướng theta 1 rồi sau đó chúng ta sẽ thực hiện cái phép sigmoid và phép sigmoid này sẽ được thực hiện lần lượt trên từng cái phần tử đầu ra do đó chúng ta sẽ gọi là 11y thì đây chính là cái layer số 1 cái tầng số 1 sau đó thì chúng ta lại qua tiếp nhân với lại cái sigmoid chúng ta sẽ nhân với lại cái sigmoid thứ 2"
        },
        {
          "index": 10,
          "video_id": "Chương 2_aXB_C9IAyMg",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5a： Mạng neural Network (Neural Network)",
          "video_url": "https://youtu.be/aXB_C9IAyMg",
          "start_time": 447,
          "end_time": 509,
          "text": "thì đây chính là cái layer số 1 cái tầng số 1 sau đó thì chúng ta lại qua tiếp nhân với lại cái sigmoid chúng ta sẽ nhân với lại cái sigmoid thứ 2 chúng ta sẽ nhân với lại cái theta 2 rồi sau đó nó sẽ qua cái sigmoid thì đầu ra của nàng này nó chính là cái theta 2, nó sẽ qua cái sigmoid thứ 2    tầng số 2 cái layer số 2 rồi cứ như vậy, chấm chấm chấm cho đến cái layer thứ trừ 1 và layer thứ l thì đối với cái layer thứ l thì chúng ta lưu ý đó là ngay sau đó chúng ta sẽ không thực hiện cái sigmoid mà chúng ta sẽ phải thực hiện cái hàm softmax tại vì trong trường hợp mà phân lớp nhiều lớp thì chúng ta sẽ sử dụng cái hàm softmax này để đưa nó về cái không gian xác xúc như vậy thì công thức này thì nó sẽ rất là dài, nó bao gồm là một cái hàm hợp của rất nhiều hàm hàm nhân với lại theta 1, sigmoid, rồi nhân với theta 2, sigmoid"
        },
        {
          "index": 11,
          "video_id": "Chương 2_aXB_C9IAyMg",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5a： Mạng neural Network (Neural Network)",
          "video_url": "https://youtu.be/aXB_C9IAyMg",
          "start_time": 499,
          "end_time": 561,
          "text": "như vậy thì công thức này thì nó sẽ rất là dài, nó bao gồm là một cái hàm hợp của rất nhiều hàm hàm nhân với lại theta 1, sigmoid, rồi nhân với theta 2, sigmoid vân vân, cho đến theta thứ l rồi softmax thì đây là một cái hàm hợp rất là phức tạp và càng cái lớp, số lớp ở ứng của mình càng lớn thì cái hàm này nó sẽ càng biến nổi nhiều và sang cái bước số 2 sang cái bước số 2 đó là thiết kế cái hàm nổi thì chúng ta dùng công thức hoàn toàn hoàn toàn tương tự với lại cái softmax nếu như cái trường hợp mà nhiều mẫu và không vector hóa đúng không thì cái yk này nè cái y tức y nè nó sẽ là một cái vector và chúng ta sẽ có cái chỉ số k chạy từ 1 cho đến k lớn rồi cái y ngã của mình đây chính là y ngã nè đây chính là y nè rồi và cũng sẽ có k phần tử và chúng ta sẽ duyệt qua chúng ta sẽ duyệt qua từng phần tử rồi lấy"
        },
        {
          "index": 12,
          "video_id": "Chương 2_aXB_C9IAyMg",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5a： Mạng neural Network (Neural Network)",
          "video_url": "https://youtu.be/aXB_C9IAyMg",
          "start_time": 548,
          "end_time": 610,
          "text": "rồi cái y ngã của mình đây chính là y ngã nè đây chính là y nè rồi và cũng sẽ có k phần tử và chúng ta sẽ duyệt qua chúng ta sẽ duyệt qua từng phần tử rồi lấy y thứ 1 nhân với y ngã 1 và y thứ 2 nhân với y ngã 2, y thứ 3 và khi nhân xong rồi cộng lại thì chúng ta sẽ ra được một cái loss cho một mẫu và cái loss cho một mẫu này chúng ta sẽ đi tính trung bình cộng cho tất cả n mẫu này thì chúng ta sẽ được cái công thức cho cái cross entropy, thì công thức này nó hoàn toàn tương tự với lại cái softmax. và viết dưới dạng là nhiều mẫu nhưng mà ở dạng vector hóa thì chúng ta có thể viết gọn lại như thế này hàm loss của mình nó sẽ là bằng trung bình cộng của cross entropy của softmax chúng ta lưu ý là ở đây cái công thức này là công thức của softmax nhưng mà công thức này đúng ra nó phải là cái công thức ở bên tay trái vì công thức này nó quá lớn"
        },
        {
          "index": 13,
          "video_id": "Chương 2_aXB_C9IAyMg",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5a： Mạng neural Network (Neural Network)",
          "video_url": "https://youtu.be/aXB_C9IAyMg",
          "start_time": 599,
          "end_time": 662,
          "text": "chúng ta lưu ý là ở đây cái công thức này là công thức của softmax nhưng mà công thức này đúng ra nó phải là cái công thức ở bên tay trái vì công thức này nó quá lớn nên ở đây chúng ta có thể viết lại công thức ở đây là mình dùng nhầm của softmax công thức này nếu đúng nó phải là y ngã trong đó y ngã y ngã nó chính là bằng cái công thức này nếu mà đưa cái công thức đó qua đây thì nó rất là dài do đó mình viết gọn lại là y ngã của rồi tính softmax của y ngã và sự là nguyên cái này nó sẽ là cái softmax luôn nguyên cái này là softmax luôn như vậy là y ngã và y cross entropy của y ngã và y rồi công thức này mình sẽ viết lại là trung bình cộng của cross entropy của y ngã và y trong đó y ngã thì nó sẽ là bằng softmax mình chép công thức ở slide trước qua softmax của theta thứ l"
        },
        {
          "index": 14,
          "video_id": "Chương 2_aXB_C9IAyMg",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5a： Mạng neural Network (Neural Network)",
          "video_url": "https://youtu.be/aXB_C9IAyMg",
          "start_time": 649,
          "end_time": 710,
          "text": "của cross entropy của y ngã và y trong đó y ngã thì nó sẽ là bằng softmax mình chép công thức ở slide trước qua softmax của theta thứ l rồi nhân với lại sigmoid của ... rồi của sigmoid của theta thứ 2 rồi sigmoid của theta thứ 2  và thư mục rồi nhân với x rồi thì đây là cái công thức cho cái mạng neural network và hàm độ lỗi của neural network vậy chúng ta sẽ tổng kết lại dựa trên cái cách biểu diễn dạng đồ thị của các kiến trúc mạng đồ thị của các kiến trúc mạng ở đây nói là kiến trúc cho nó sang thôi chứ còn nó mới chỉ là những cái nốt đầu tiên đơn giản trước khi chúng ta qua cái môi nọc sâu như là CNN, RNN"
        },
        {
          "index": 15,
          "video_id": "Chương 2_aXB_C9IAyMg",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5a： Mạng neural Network (Neural Network)",
          "video_url": "https://youtu.be/aXB_C9IAyMg",
          "start_time": 699,
          "end_time": 759,
          "text": "đồ thị của các kiến trúc mạng ở đây nói là kiến trúc cho nó sang thôi chứ còn nó mới chỉ là những cái nốt đầu tiên đơn giản trước khi chúng ta qua cái môi nọc sâu như là CNN, RNN thì đối với cái mạng đầu tiên đó là linear regression nó chỉ bao gồm duy nhất một nốt và cái nốt này sẽ là cái nốt tuyến tính để tổng hợp thông tin có trọng số từ các cái thông tin đầu bạc và cái này thì thường dùng cho giải quyết các cái bài toán hồi quy giải quyết các cái bài toán hồi quy và tuyến tính tức là các cái bài toán hồi quy và tuyến tính  và cái giá trị y ngã này nó phụ thuộc một cách tuyến tính với lại cái dự kiện đầu bào x trong trường hợp mà cái y ngã này nó sẽ là cái bài toán phân lớp tức là nó sẽ nhận 2 giá trị là 0,1 thì ngoài cái nốt nốt sigma ở đây tổng hợp thông tin đây chúng ta sẽ có thêm một cái hàm sigma để ép cái miền giá trị từ trừ vô cùng"
        },
        {
          "index": 16,
          "video_id": "Chương 2_aXB_C9IAyMg",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5a： Mạng neural Network (Neural Network)",
          "video_url": "https://youtu.be/aXB_C9IAyMg",
          "start_time": 748,
          "end_time": 798,
          "text": "thì ngoài cái nốt nốt sigma ở đây tổng hợp thông tin đây chúng ta sẽ có thêm một cái hàm sigma để ép cái miền giá trị từ trừ vô cùng cộng vô cùng về cái miền giá trị từ 0 cho đến 1 và tương ứng sẽ tạo ra cái nhãn y mình mong muốn dự đoán đối với cái mô hình đối với cái bài toán mà chúng ta phân lớp mà nhiều hơn 2 lớp cụ thể đây là k là lớn hơn 2 thì chúng ta sẽ sử dụng cái mô hình shock max sau khi chúng ta thực hiện cái theta chuyển vị nhân với x x là cái dự kiện đầu bào này thì chúng ta sẽ qua cái hàm shock max chúng ta sẽ có cái hàm shock max"
        },
        {
          "index": 17,
          "video_id": "Chương 2_aXB_C9IAyMg",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5a： Mạng neural Network (Neural Network)",
          "video_url": "https://youtu.be/aXB_C9IAyMg",
          "start_time": 810,
          "end_time": 862,
          "text": "shock max của theta chuyển vị nhân với x thì đây chính là cái giá trị y dự đoán y này là của bên đây nha rồi thì cái y này của mình nó sẽ là một cái vector bao gồm cả cái thành phần và khi giải quyết các cái bài toán mà mang tính chất phi tuyến thì chúng ta sẽ phải sử dụng cái mạng neural network bao gồm nhiều layer hơn và với mỗi layer thì chúng ta sẽ có nhiều cái node hơn và cái số node này thì nó sẽ gia giảm tăng hay giảm là tùy vô cái tính chất của cái dữ liệu dữ liệu mà càng phức tạp càng phi tuyến thì chúng ta sẽ sử dụng nhiều layer hơn và dữ liệu mà càng đơn giản thì chúng ta sẽ càng sử dụng ít layer hơn và số node của mình nó ít hơn thì đó là dựa trên cái kinh nghiệm thiết kế rồi thì như vậy là cái nội dung của bài neural network cũng như là"
        },
        {
          "index": 18,
          "video_id": "Chương 2_aXB_C9IAyMg",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5a： Mạng neural Network (Neural Network)",
          "video_url": "https://youtu.be/aXB_C9IAyMg",
          "start_time": 849,
          "end_time": 883,
          "text": "và dữ liệu mà càng đơn giản thì chúng ta sẽ càng sử dụng ít layer hơn và số node của mình nó ít hơn thì đó là dựa trên cái kinh nghiệm thiết kế rồi thì như vậy là cái nội dung của bài neural network cũng như là mô hình máy học tổng hóa linear direction, logistic direction, shock max direction chúng ta đã học qua và như vậy thì nếu như có những cái câu hỏi nào thì chúng ta có thể đặt ra cái câu hỏi cũng như là cuối buổi thì chúng ta sẽ có cái bài tập quiz cái bài tập trắc nghiệm để kiểm tra lại kiến thức"
        }
      ]
    },
    {
      "video_id": "Chương 2_DGNdZGdwihs",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: cài đặt một mạng Neural Network đơn giản để giải quyết bài toán phân lớp phi tuyến (ví dụ: các tập điểm hình tam giác và hình tròn không thể phân tách bởi một đường thẳng), minh họa bằng một mạng có một hidden layer để phân tách các điểm trong/và ngoài vòng tròn. [1] [2]  \n- Các khái niệm sẽ được đề cập: phân loại phi tuyến, kiến trúc mạng (input → hidden → output), activation sigmoid cho output nhị phân, loss là *binary cross entropy*, cách khởi tạo dữ liệu (sử dụng thư viện scikit-learn), và các chi tiết cài đặt (build model, layer dense fully-connected, optimizer với momentum, hàm gateway để truy xuất layer, quá trình train và quan sát loss). [1] [2] [3]\n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 Bài toán và dữ liệu (Problem setup & data generation)\n- Bài toán: phân lớp phi tuyến (ví dụ điểm thuộc vòng tròn bên trong vs. bên ngoài) — không thể dùng một đường thẳng để phân tách, cần mô hình phi tuyến. [1]  \n- Tạo dữ liệu: dùng hàm của scikit-learn (được nhắc là \"makesicle\" trong video) để sinh các điểm nằm trong và nằm ngoài vòng tròn, các điểm trong/ngoài được tô màu khác nhau và gán nhãn 1/0 tương ứng; X gồm hai chiều (x1, x2) và y là nhãn 0/1 (ép về kiểu số thực). [3] [4]\n\n### 2.2 Kiến trúc mạng (Network architecture)\n- Mạng minh họa có: lớp Input → một Hidden Layer (dense fully-connected) → lớp Output (dense). [2] [6]  \n- Vì bài toán là phân lớp nhị phân nên output là một node duy nhất và activation cuối dùng hàm sigmoid để đưa giá trị về miền [0,1]. [2] [8]  \n- Hidden layer được cấu hình là fully-connected (dense), có bias và activation sigmoid; trong ví dụ số nốt (neurons) ở hidden layer được chọn là 8. [6] [7] [8]  \n- Tổng quan tham số mô hình: ở ví dụ cụ thể model.summary cho thấy tổng số 33 tham số, gồm một hidden layer 8 neurons và một output node. [12]\n\n### 2.3 Hàm loss và bài toán nhãn nhị phân (Loss for binary classification)\n- Do bài toán có hai phân lớp nên không dùng softmax ở output mà dùng sigmoid + độ đo *binary cross entropy* (binary cross entropy được sử dụng làm loss). [2] [3] [10]\n\n### 2.4 Cấu trúc code và các phương thức chính (Implementation details)\n- Phần cốt lõi có một framework chung tương tự các mô hình linear/logistic/softback direction, và cần viết lại (override) phương thức *gateway* để có thể truy xuất tham số/đầu ra của từng layer bằng cách truyền chỉ số layer. [3] [5] [11]  \n- Trong phương thức build: khai báo input shape (số chiều input = 2) → tạo hidden dense (activation sigmoid, use_bias = True) → tạo output dense (1 node, activation sigmoid, use_bias = True). Sau đó đóng gói các layer vào một biến model và lưu vào cell.model. [5] [6] [8] [9]  \n- Optimizer và hyperparameters: video đề cập setting momentum (mặc định 0 nhưng theo kinh nghiệm nên để 0.9); compile model với optimizer (opt) và loss = binary cross entropy. [9] [10]\n\n### 2.5 Huấn luyện (Training)\n- Tham số epoch: có tham số n_epoch (num_epoch) để truyền vào khi train. [10] [11]  \n- Gọi fit: self.model.fit với dữ liệu x_train, y_train và số epoch; mặc định số mẫu (n input) đặt là 1000 nhưng có thể giảm vì dùng momentum giúp cập nhật nhanh hơn. [13] [14]  \n- Quan sát lịch sử huấn luyện (history): ban đầu loss khá cao (~0.7), sau đó giảm dần (ví dụ giảm xuống khoảng 0.28 trong một bước), và giá trị loss liên tục giảm trong quá trình train. Giá trị history được lưu để vẽ/quan sát đường loss. [14] [15] [16]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa cụ thể trong video: phân lớp các điểm sinh quanh một vòng tròn (inner vs outer), dùng dữ liệu tạo bởi scikit-learn (\"makesicle\"), gán nhãn 0 cho điểm màu đỏ (outside) và 1 cho điểm màu xanh lá (inside). Mạng được dựng với 2-dim input, 1 hidden layer (8 neurons), và 1-node sigmoid output; train với binary cross entropy và optimizer có momentum. [3] [4] [6] [7] [9]  \n- Ứng dụng thực tế: bài toán dạng này minh họa các tình huống phân lớp phi tuyến trong thực tế (các tập không thể tách tuyến tính) — cần ít nhất một hidden layer để học ranh giới phi tuyến. (Nội dung mô tả yêu cầu sử dụng hidden layer để giải bài toán phi tuyến trong video). [1] [2]\n\n- Trường hợp sử dụng/ghi chú thao tác: số epoch, kích thước mẫu có thể điều chỉnh; momentum được đề xuất là 0.9 theo kinh nghiệm để giúp quá trình tối ưu nhanh và ổn định hơn. [13] [9] [10]\n\n---\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính: để giải bài toán phân lớp phi tuyến (vòng tròn trong/ngoài), một mạng Neural Network với một hidden layer (dense fully-connected) và output sigmoid là đủ để học ranh giới phi tuyến; dữ liệu sinh bằng scikit-learn, loss dùng binary cross entropy, optimizer có thể sử dụng momentum ~0.9; quá trình build → compile → fit thực hiện theo luồng đã trình bày và loss giảm dần khi training. [1] [2] [3] [6] [9] [10] [14] [15]  \n- Tầm quan trọng: bài giảng minh họa quy trình từ tạo dữ liệu tới cài đặt mạng và huấn luyện cho bài toán phi tuyến cơ bản, cung cấp tiền đề cho các mạng phức tạp hơn (nhiều hidden layer) nếu cần. [1] [2]  \n- Liên hệ với bài giảng khác: video nhắc khung cài đặt tương tự với các mô hình linear, logistic và \"softback direction\", cho thấy sự nhất quán trong thiết kế framework khi mở rộng từ mô hình tuyến tính sang mạng nhiều layer. [3] [5]\n\n---\n\nGhi chú: Các trích dẫn [1]... [16] tương ứng với các đoạn (chunks) trong video, có timestamp như trong nguồn.",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 2_DGNdZGdwihs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_1： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/DGNdZGdwihs",
          "start_time": 0,
          "end_time": 64,
          "text": "Trong phần này thì chúng ta sẽ tiến hành cài đặt một cái mạng Neural Network Thì cái đặc tính của cái mạng Neural Network đó là nó sẽ giúp cho chúng ta giải quyết được các cái bài toán non-linear Và trong trường hợp này thì chúng ta sẽ lấy cái tình huống đơn giản nhất của non-linear đó chính là các cái tập điểm hình tam giác và hình tròn Thì hai cái tập điểm này không thể nào chia tách ra được bởi một cái đường thẳng Do đó thì cái đường đúng nó sẽ phải là một cái đường tròn như thế này Thì nó mới có thể phân ra làm hai phần riêng biệt được Thế thì đây là một cái bài toán phi tuyến Và để giải quyết bài toán này thì chúng ta cũng không cần thiết phải sử dụng một cái mạng Neural Network quá phức tạp Thì ở đây nó chỉ cần có một lớp ẩn thôi Ở đây là một Hidden Layer Cái mạng Neural Network đúng của chúng ta thì nó có thể có một, hai hoặc là rất nhiều cái Hidden Layer"
        },
        {
          "index": 2,
          "video_id": "Chương 2_DGNdZGdwihs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_1： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/DGNdZGdwihs",
          "start_time": 49,
          "end_time": 113,
          "text": "Thì ở đây nó chỉ cần có một lớp ẩn thôi Ở đây là một Hidden Layer Cái mạng Neural Network đúng của chúng ta thì nó có thể có một, hai hoặc là rất nhiều cái Hidden Layer Nhưng mà trong trường hợp này thì chúng ta chỉ cần minh họa với một Hidden Layer Cái thứ hai đó là cái tập điểm này của mình là chỉ có hai thành phần Do đó thì ở đây chúng ta sẽ có duy nhất một cái node output cuối cùng Thì ở đây là chúng ta sẽ có một cái lớp input Và ở đây là chúng ta sẽ có một cái lớp output cuối cùng thì ở đây là chúng ta sẽ có một cái lớp input Một cái Hidden Layer và một cái output Và cái Output này Thì do là cái giá trị của mình nó chỉ có một phân lớp À xíu gọi nó có hai phân lớp Nên ở đây chúng ta không có sử dụng hàm Summask Mà chúng ta sẽ sử dụng một cái hàm sigmoid Tại vì sigmoid nó sẽ đưa cái miền giá trị của mình về cái đoạn từ 0 đến 1 Và lúc này thì cái giá trị Y và Ybalance này thì mình phải giữ lại ở trên này"
        },
        {
          "index": 3,
          "video_id": "Chương 2_DGNdZGdwihs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_1： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/DGNdZGdwihs",
          "start_time": 97,
          "end_time": 161,
          "text": "Nên ở đây chúng ta không có sử dụng hàm Summask Mà chúng ta sẽ sử dụng một cái hàm sigmoid Tại vì sigmoid nó sẽ đưa cái miền giá trị của mình về cái đoạn từ 0 đến 1 Và lúc này thì cái giá trị Y và Ybalance này thì mình phải giữ lại ở trên này  mình sẽ sử dụng cái độ đo là binary cross entropy thì đây là một cái biến thể đơn giản của mạng Neural Network tiếp theo thì chúng ta sẽ tiến hành cài đặt cho cái ví dụ này rồi thì cũng tương tự chúng ta sẽ có cái đoạn code để khởi tạo cho các cái tập điểm nằm trong và nằm bên ngoài vòng tròn thì ở đây chúng ta có một thư viện là scikit-learn nó sẽ có cái hàm gọi là hàm makesicle và cái hàm makesicle này thì nó sẽ giúp cho chúng ta tạo ra các cái điểm nằm trong và nằm ngoài vòng tròn các cái điểm nằm trong thì chúng ta sẽ được đánh dấu bằng hộ đỏ và các cái điểm nằm"
        },
        {
          "index": 4,
          "video_id": "Chương 2_DGNdZGdwihs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_1： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/DGNdZGdwihs",
          "start_time": 148,
          "end_time": 212,
          "text": "tạo ra các cái điểm nằm trong và nằm ngoài vòng tròn các cái điểm nằm trong thì chúng ta sẽ được đánh dấu bằng hộ đỏ và các cái điểm nằm à xin lỗi là các cái điểm nằm trong thì được đến dấu bằng các các điểm màu xanh lá và các cái điểm nào ngoài thì được character giống Look bằng các điểm màu đỏ màu đỏ và những cái điểm nào màu đỏ thì được sẽ gắn nhãn là bằng 0 và những cái điểm nào mà màu xanh lá thì sẽ được gắn nhãn là bằng 1 và tất cả thì đều được ép về kiểu số thật rồi thì x của mình tạo độ x của mình nó chính là cái tập dữ liệu tạo độ theo trục x1 và x2 tức là bao gồm 2 chiều y thì nó sẽ là cái nhãn hoặc là những giá trị không hoặc là những giá trị là 1 rồi bây giờ về cái phần cài đặt thuật toán thì cũng tương tự cho các cái mô hình linear logistic và softback direction thì"
        },
        {
          "index": 5,
          "video_id": "Chương 2_DGNdZGdwihs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_1： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/DGNdZGdwihs",
          "start_time": 196,
          "end_time": 265,
          "text": "nó sẽ là cái nhãn hoặc là những giá trị không hoặc là những giá trị là 1 rồi bây giờ về cái phần cài đặt thuật toán thì cũng tương tự cho các cái mô hình linear logistic và softback direction thì chúng ta sẽ có một cái bộ khung và ở đây thì chúng ta sẽ có một cái hàm nội ước đó là hàm gateway thì riêng cái hàm gateway này thì chúng ta sẽ phải viết lại so với hệ linear direction tại vì trong cái mạng neural network thì chúng ta sẽ có nhiều layer và như vậy thì nếu chúng ta muốn quan sát cái layer cái tham số của layer nào thì chúng ta phải truyền thêm cái chỉ số của cái layer đó vào như vậy thì chúng ta sẽ có thêm một cái phương thức này nữa thì viết lại cái phương thức này rồi bây giờ đối với build thì ở đây chúng ta sẽ có input và output dimension ừ ừ đó thì chúng ta cũng tương tự sẽ cài đặt là input với shape là bằng input"
        },
        {
          "index": 6,
          "video_id": "Chương 2_DGNdZGdwihs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_1： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/DGNdZGdwihs",
          "start_time": 246,
          "end_time": 314,
          "text": "thức này rồi bây giờ đối với build thì ở đây chúng ta sẽ có input và output dimension ừ ừ đó thì chúng ta cũng tương tự sẽ cài đặt là input với shape là bằng input rồi tiếp theo đó là chúng ta sẽ có cái lớp hidden layer chúng ta sẽ có một cái lớp hidden layer như vậy ở đây sẽ để là hidden rồi lớp hidden layer này thì nó sẽ được được thực hiện bởi một cái phép biến đổi là fully connected tại vì từ cái lớp input sang cái lớp hidden này thì nó được kết nối đầy đủ và chúng ta lưu ý là activation của mình thì chúng ta sẽ sử dụng hàm sigmoid và đồng thời là chúng ta có sử dụng bias thì ở đây sẽ là layer sẽ là dense rồi output của mình thì nó sẽ di slash các lần sau đây thì như ở đây sẽ là layer sẽ là dense rồi output của mình thì nó sẽ di함 bất k Researchers also talking about��가M Douglas."
        },
        {
          "index": 7,
          "video_id": "Chương 2_DGNdZGdwihs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_1： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/DGNdZGdwihs",
          "start_time": 292,
          "end_time": 363,
          "text": "này thì nó được kết nối đầy đủ và chúng ta lưu ý là activation của mình thì chúng ta sẽ sử dụng hàm sigmoid và đồng thời là chúng ta có sử dụng bias thì ở đây sẽ là layer sẽ là dense rồi output của mình thì nó sẽ di slash các lần sau đây thì như ở đây sẽ là layer sẽ là dense rồi output của mình thì nó sẽ di함 bất k Researchers also talking about��가M Douglas. có nhiều nốt thì giả sử như ở đây chúng ta có 8 nốt thôi số nốt ở giữa ở đây chúng ta có 8 nốt rồi, activation thì chúng ta sẽ để là sigmoid rồi, use by thì chúng ta sẽ để là true và chúng ta sẽ phải truyền cái lớp input cho nó đó chính là input ở đây rồi, chúng ta sẽ có cái output là hidden và với output là hidden, chúng ta lại một lần nữa một lần nữa thì chúng ta sẽ đưa qua cái lớp biến đổi là fully connected tại vì bản chất ở đây, tất cả các cái nốt đầu vào"
        },
        {
          "index": 8,
          "video_id": "Chương 2_DGNdZGdwihs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_1： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/DGNdZGdwihs",
          "start_time": 347,
          "end_time": 409,
          "text": "và với output là hidden, chúng ta lại một lần nữa một lần nữa thì chúng ta sẽ đưa qua cái lớp biến đổi là fully connected tại vì bản chất ở đây, tất cả các cái nốt đầu vào và cái nốt đầu ra thì nó kết nối đầy đủ với nhau và đó thì ở đây nó cũng là một cái dense và cái dense này thì cái output của mình nó chỉ có duy nhất một nốt nó chỉ có duy nhất một nốt tại sao một nốt? tại vì ở đây chúng ta phân lớp những phong rồi ở đây sẽ có là output là bằng dense trong đó chỉ có một nốt activation thì chúng ta sẽ để là sigmoid rồi, sử dụng bias bằng true và input của nó chính là cái hidden ở phía trước rồi, bây giờ chúng ta sẽ đóng gói"
        },
        {
          "index": 9,
          "video_id": "Chương 2_DGNdZGdwihs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_1： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/DGNdZGdwihs",
          "start_time": 399,
          "end_time": 466,
          "text": "bằng true và input của nó chính là cái hidden ở phía trước rồi, bây giờ chúng ta sẽ đóng gói cả cái này vào trong cái biến gọi là model và chúng ta sẽ trả về cho cell.model ở đây thì chúng ta sẽ không cần phải return cái gì ra bên ngoài rồi, tương tự như vậy ở đây chúng ta sẽ có uptm model này, chúng ta sẽ có uptm model này, chúng ta sẽ không cần phải return cái gì ra bên ngoài �� ở đây thì chúng bằng 0.9, thông dự mặc định chúng ta sẽ để đây là 0 nhưng mà theo kinh nghiệm thì momentum nên để bằng 0.9"
        },
        {
          "index": 10,
          "video_id": "Chương 2_DGNdZGdwihs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_1： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/DGNdZGdwihs",
          "start_time": 430,
          "end_time": 512,
          "text": "ở đây thì chúng bằng 0.9, thông dự mặc định chúng ta sẽ để đây là 0 nhưng mà theo kinh nghiệm thì momentum nên để bằng 0.9 và bây giờ thì chúng ta sẽ compile nó vào trong cái mô đồ optimizer thì để là bằng opt rồi loss thì chúng ta sẽ để là pf.keras.loss.binary classification, ta sẽ gọi là binary cross entropy rồi ở đây chúng ta sẽ có thêm một tham số này chúng ta sẽ có thêm một tham số nữa đó là cái số epoch"
        },
        {
          "index": 11,
          "video_id": "Chương 2_DGNdZGdwihs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_1： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/DGNdZGdwihs",
          "start_time": 490,
          "end_time": 565,
          "text": "pf.keras.loss.binary classification, ta sẽ gọi là binary cross entropy rồi ở đây chúng ta sẽ có thêm một tham số này chúng ta sẽ có thêm một tham số nữa đó là cái số epoch sẽ có thêm một số epoch gọi là n epoch sẽ là bằng num epoch rồi đối với cái hàm gateway thì chúng ta sẽ phải truyền vào cái layer số mấy layer số mấy rồi đó thì ở đây chúng ta sẽ bị tên là cell.model.layer và chúng ta sẽ truyền vào cái chỉ số của cái layer rồi .netway rồi bây giờ chúng ta sẽ tiến hành chạy thử cái đoạn chương trình này may quá không có lỗi và để khởi tạo thì chúng ta sẽ tạo một cái đối tượng tên là neural network"
        },
        {
          "index": 12,
          "video_id": "Chương 2_DGNdZGdwihs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_1： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/DGNdZGdwihs",
          "start_time": 554,
          "end_time": 602,
          "text": "rồi bây giờ chúng ta sẽ tiến hành chạy thử cái đoạn chương trình này may quá không có lỗi và để khởi tạo thì chúng ta sẽ tạo một cái đối tượng tên là neural network rồi neural network.build thì chúng ta sẽ truyền vào là số input là 2 số output là 1 tại vì chúng ta không lất nghỉ phân rồi .summary thì ở đây mô hình này có tất cả là 33 tham số trong đó có một cái lớp hẳn và một cái lớp output total, lớp hẳn thì còn có 8 neural rồi bây giờ chúng ta sẽ tiến hành trend và chúng ta sẽ truyền vào các cái giá trị xtrend và ytrend ở đây"
        },
        {
          "index": 13,
          "video_id": "Chương 2_DGNdZGdwihs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_1： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/DGNdZGdwihs",
          "start_time": 598,
          "end_time": 661,
          "text": "và chúng ta sẽ truyền vào các cái giá trị xtrend và ytrend ở đây ở đây thì mặc định chúng ta để số input là 1000 tuy nhiên thì chúng ta có thể giảm cái số input này xuống tại vì chúng ta dùng momentum nên nên chúng ta có thể giảm cái số input này xuống tại vì chúng ta dùng momentum nên số thao tác này, bước cập nhật của mình nó rất là nhanh ... rồi nó không hiểu kỹ, bóc ... ... ... ... mình sẽ phải gọi vào cái hàm, ok combine nó sẽ không hiểu"
        },
        {
          "index": 14,
          "video_id": "Chương 2_DGNdZGdwihs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_1： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/DGNdZGdwihs",
          "start_time": 657,
          "end_time": 709,
          "text": "mình sẽ phải gọi vào cái hàm, ok combine nó sẽ không hiểu rồi quên mất cái này nó sẽ phải gọi vào hàm fit thì self.model.fit và chúng ta sẽ fit những cái trend, btrend và số lượng epoch rồi chúng ta sẽ trend lại ban đầu thì chúng ta thấy là loss rất là cao là 0.7 sau đó thì loss đã giảm xuống còn dưới 0.7 là 0.7 dưới 0.1 nó sẽ ra là 0.28"
        },
        {
          "index": 15,
          "video_id": "Chương 2_DGNdZGdwihs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_1： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/DGNdZGdwihs",
          "start_time": 699,
          "end_time": 757,
          "text": "sau đó thì loss đã giảm xuống còn dưới 0.7 là 0.7 dưới 0.1 nó sẽ ra là 0.28 rồi bây giờ thì chúng ta sẽ vẽ cái history ok thôi cái quá trình trend này thì nó thực hiện rất là nhanh vậy đó thì chúng ta sẽ chạy lại cái này ở đây chúng ta sẽ để là history để chút nữa chúng ta sẽ quan sát cái giá trị loss nó đã giảm như thế nào ok ở đây thì chúng ta sẽ trả về cái loss của cái hàm trend này rồi thì chúng ta thấy là cái giá trị của cái hàm loss nó liên tục giảm xuống nó liên tục giảm xuống"
        },
        {
          "index": 16,
          "video_id": "Chương 2_DGNdZGdwihs",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_1： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/DGNdZGdwihs",
          "start_time": 749,
          "end_time": 757,
          "text": "thì chúng ta thấy là cái giá trị của cái hàm loss nó liên tục giảm xuống nó liên tục giảm xuống"
        }
      ]
    },
    {
      "video_id": "Chương 2_XBS1JuTrxVI",
      "summary": "## 1. Giới thiệu (Introduction)\n- **Mục tiêu chính của bài giảng**: Hướng dẫn cách *trực quan hóa các tham số* (weights) của một neural network đã cài đặt — cụ thể là lấy trọng số từ các layer, đánh giá “độ tin cậy” của từng neuron theo trị tuyệt đối của weight và vẽ các đường thẳng tương ứng để hiểu ảnh hưởng của từng neuron lên output. [1][2][3]  \n- **Các khái niệm sẽ được đề cập**: lấy trọng số của layer (neuronetwork.getweight), cấu trúc vector trọng số của từng node (bias, w1, w2), cách chuyển công thức trọng số sang dạng y = a x + b để vẽ đường phân tách, lựa chọn các neuron có “độ tin cậy” cao theo trị tuyệt đối của trọng số, sử dụng vòng for để vẽ dữ liệu và các đường thẳng, và nhắc tới việc *sử dụng thư viện Keras*. [1][2][3][4][5][6][7][8][9][12][13]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 Lấy và quan sát trọng số của layer (theta2)\n- Lấy tham số theta2 bằng gọi hàm neuronetwork.getweight với tham số layer = 2. [1]  \n- Theta2 có 8 giá trị, tương ứng trọng số của 8 node trong layer đó — đây là các giá trị dùng để đánh giá vai trò (tin cậy) của từng node khi tạo output. [1][2]\n\n### 2.2 Độ “tin cậy” của neuron = trị tuyệt đối của weight\n- Ý tưởng đánh giá: dùng trị tuyệt đối của trọng số để biểu diễn “độ tin cậy” của node, không dùng dấu đại số. [2][3]  \n- Ví dụ trong video nêu những node có độ tin cậy rất cao với giá trị lớn về trị tuyệt đối (ví dụ: -17, -14, -11, -8). [2][3]\n\n### 2.3 Chiến lược trực quan hóa: vẽ đường thẳng tương ứng neuron có độ tin cậy cao\n- Với các neuron có độ tin cậy cao, sẽ vẽ *đường thẳng* do các trọng số tạo ra để trực quan hóa hướng đóng góp của node đó. [3]  \n- Tham khảo ý tưởng chuyển công thức sang dạng y = a x + b (đã học trong bài Logistic Direction) để vẽ đường. [3]\n\n### 2.4 Lấy trọng số cho layer 1 (theta1) và ánh xạ các thành phần\n- Lấy theta1 bằng neuronet.getweight (ví dụ truyền layer = 1, tham số 1 trong video). [5]  \n- Trong vector trọng số cho một node ở layer 1, các thành phần được ánh xạ như sau:  \n  - param0: trọng số nối tới bias (bias weight)  \n  - param1: trọng số nối tới x1 (w1)  \n  - param2: trọng số nối tới x2 (w2)  \n  (giải thích này xuất từ việc mô tả cách ánh xạ param0/1/2 trong video). [5]\n\n### 2.5 Chuyển sang dạng y = a x + b — công thức và cách tính\n- Chuyển phương trình của một neuron về dạng y = a x + b để vẽ. [4][5]  \n- Công thức:  \n  - a = - param0 / param1  (ở video ký hiệu là A = - param0 / param1) [5][7]  \n  - b = - bias / param1    (bias là thành phần bias; param1 là trọng số tương ứng với x1) [7]  \n- Trong thực thi sẽ in các giá trị bias và param ra trước để kiểm tra trước khi vẽ. [7]\n\n### 2.6 Cấu trúc dữ liệu trọng số và cách trích xuất theo node\n- Bias là một bộ 8 giá trị (tương ứng 8 node). [8]  \n- Param (trọng số kết nối x1,x2) có 2 thành phần (w1, w2) cho mỗi node. [8]  \n- Để lấy trọng số cho node thứ idx, cần chỉ định chỉ số node khi truy xuất, ví dụ param[idx][0], bias[idx]… (cách index này được nêu trong video). [8]\n\n### 2.7 Vẽ đường thẳng trên khoảng x ∈ [-1, 1]\n- Chọn hai điểm x (ví dụ x = -1 và x = 1) trên trục x để tính y theo y = a * x + b rồi dùng plt.plot để vẽ đoạn thẳng tương ứng. [8][9]  \n- Quy trình vẽ tổng quát: đầu tiên vẽ các điểm dữ liệu (scatter), sau đó với vòng for duyệt qua các node (chỉ chọn những node có độ tin cậy cao) và vẽ các đường thẳng tương ứng. [4]\n\n### 2.8 Thực hiện bằng code (vòng for và plotting)\n- Viết một vòng for: phần đầu vẽ các điểm data; phần sau lọc các neuron có độ tin cậy cao (video liệt kê các chỉ số ví dụ như 0, 1, 2, 3, 4, 5, 6, 7 trong quá trình minh họa) và vẽ các đường thẳng cho các neuron này. [4]  \n- Gắn hàm tính a và b từ param vào để tính y cho hai điểm x = -1 và x = 1, sau đó plt.plot các giá trị thu được. [8][9]\n\n### 2.9 Ghi chú ngắn về công cụ\n- Video có nhắc tới *sử dụng thư viện Keras* (một lần hoặc lặp lại) khi xây dựng/cài đặt mạng neural. [12][13]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n- Ví dụ minh họa trong video: lấy theta2 (8 giá trị) để quan sát những node có trọng số lớn về trị tuyệt đối (ví dụ -17, -14, -11, -8), chọn các node đó để trực quan hóa đường thẳng tương ứng nhằm hiểu vai trò của từng neuron. [1][2][3]  \n- Minh họa chi tiết cách lấy theta1 bằng neuronet.getweight, trích bias và các param w1,w2, tính a,b rồi vẽ đường trên khoảng x ∈ [-1,1] bằng plt.plot — đây là ví dụ thực thi từ video. [5][6][7][8][9]  \n- Ứng dụng thực tế: phương pháp này giúp *giải thích* (interpret) các node trong mạng nhỏ 2-input bằng cách nhìn trực quan các đường thẳng mà mỗi neuron biểu diễn — thuận tiện cho phân tích mô hình và debug (ý tưởng được trình bày xuyên suốt video). [2][3][4]\n\n## 4. Kết luận (Conclusion)\n- Tóm tắt các ý chính:  \n  - Lấy trọng số layer bằng neuronetwork.getweight để quan sát theta (ví dụ theta2 có 8 giá trị) và dùng trị tuyệt đối để đánh giá “độ tin cậy” của từng node. [1][2]  \n  - Với các node có độ tin cậy cao, chuyển trọng số sang dạng y = a x + b (a = -param0/param1, b = -bias/param1) để vẽ đường thẳng diễn tả ảnh hưởng của node đó. [5][7]  \n  - Thực hiện bằng một vòng for: vẽ dữ liệu, chọn node theo chỉ số, tính a,b và dùng plt.plot với x trong [-1,1] để trực quan hóa. [4][8][9]  \n  - Video cũng nhắc đến việc sử dụng thư viện Keras trong bối cảnh cài đặt mạng. [12][13]  \n- Tầm quan trọng: Phương pháp trực quan hóa trọng số như trên giúp hiểu rõ cách từng neuron đóng góp vào quyết định, hỗ trợ phân tích, kiểm tra và giải thích mô hình neural network đơn giản. [2][3][4]  \n- Liên hệ với các bài giảng khác: video tham chiếu lại khái niệm *Logistic Direction* (dùng để đưa công thức về dạng y = ax + b) — liên kết này là cơ sở lý thuyết cho bước trực quan hóa đường thẳng. [3]\n\nGhi chú: Các bước và công thức trên đều được trình bày và minh họa trong video (xem các đoạn thời gian tương ứng để xem code và biểu diễn trực quan): [1], [2], [3], [4], [5], [6], [7], [8], [9], [12], [13].",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 2_XBS1JuTrxVI",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_2： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/XBS1JuTrxVI",
          "start_time": 0,
          "end_time": 60,
          "text": "rồi bây giờ để trực quan hóa thì chúng ta sẽ phải lót ra các tham số cho cái mô hình này ở đây chúng ta thấy là chúng ta sẽ có hai cái là theta1 và theta2 trong đó cái thành phần theta2 là cái mà chúng ta sẽ quan sát đầu tiên xem coi cái giá trị của nó là như thế nào thì để lấy giá trị tham số đầu tiên theta2 thì chúng ta sẽ lấy là neuronetwork.getweight và chúng ta sẽ truyền vô layer là layer số 2 rồi rồi như vậy thì chúng ta sẽ thấy là các cái giá trị của cái theta2 này nó sẽ có 8 giá trị tất cả 8 giá trị này nó tương ứng sẽ là cái trọng số"
        },
        {
          "index": 2,
          "video_id": "Chương 2_XBS1JuTrxVI",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_2： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/XBS1JuTrxVI",
          "start_time": 51,
          "end_time": 111,
          "text": "cái theta2 này nó sẽ có 8 giá trị tất cả 8 giá trị này nó tương ứng sẽ là cái trọng số của cái mô hình nó sẽ tương ứng là cái trọng số của các cái node ở đây nó tương ứng là cái trọng số của các cái node trong cái mô hình này và cái trọng số này á thì nó sẽ thể hiện là cái vai trò khi đưa ra cái output cuối cùng thì tôi sẽ tin cậy vào cái node nào tôi sẽ tin cậy vào cái node nào thế thì trong số 8 cái node này thì đâu đó có những node có độ tin cậy thấp ví dụ như là 0.3, 0.5, 0.4 nhưng cũng có những node độ tin cậy rất là cao ví dụ như là trừ 17 lưu ý là cái độ tin cậy ở đây chúng ta sẽ thể hiện ở chỗ là cái trị việt đối chứ không phải là cái sự lớn bé về mặt đại số của nó"
        },
        {
          "index": 3,
          "video_id": "Chương 2_XBS1JuTrxVI",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_2： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/XBS1JuTrxVI",
          "start_time": 98,
          "end_time": 161,
          "text": "nhưng cũng có những node độ tin cậy rất là cao ví dụ như là trừ 17 lưu ý là cái độ tin cậy ở đây chúng ta sẽ thể hiện ở chỗ là cái trị việt đối chứ không phải là cái sự lớn bé về mặt đại số của nó như vậy là trừ 17, 14, 11, trừ 8 đó là những cái node có cái độ tin cậy rất là cao như vậy thì chúng ta sẽ có một cái ý tưởng đó là với những cái node mà có cái độ tin cậy cao thì chúng ta sẽ tìm cách chúng ta sẽ tìm cách trực quan hóa cái đường thẳng mà được tạo bởi các cái trọng số đường thẳng được tạo bởi các cái trọng số đến cái node mà có trọng số cao này thì cái cách mà chúng ta trực quan hóa đường thẳng thì chúng ta đã tìm hiểu ở trong cái bài về Logistic Direction rồi như vậy thì ở đây chúng ta sẽ viết một cái vòng for viết một cái vòng for cái đoạn đầu thì chủ yếu đó là chúng ta vẽ các cái điểm data lên thôi đoạn đầu thì chủ yếu là chúng ta vẽ các cái điểm data"
        },
        {
          "index": 4,
          "video_id": "Chương 2_XBS1JuTrxVI",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_2： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/XBS1JuTrxVI",
          "start_time": 149,
          "end_time": 211,
          "text": "như vậy thì ở đây chúng ta sẽ viết một cái vòng for viết một cái vòng for cái đoạn đầu thì chủ yếu đó là chúng ta vẽ các cái điểm data lên thôi đoạn đầu thì chủ yếu là chúng ta vẽ các cái điểm data rồi cái phần sau thì chúng ta sẽ đi lấy những cái đường thẳng có cái độ tin cậy cao ví dụ như ở đây là 0 nè 1 nè 0 1 nè rồi 2,3,4 bỏ nè 5,6,7 nè rồi chúng ta sẽ visualize các cái neuron có cái độ tin cậy cao và để vẽ đường thẳng với từng neuron thì chúng ta phải biến đổi cái công thức này là cái công thức của mình là từ nó về cái dạng y bằng x cộng b trong đó A chính là trừ param 0 chia cho param 1 rồi lưu ý là cái param này là cái trọng số cho các cái cạnh nối đến cái x1 và x2"
        },
        {
          "index": 5,
          "video_id": "Chương 2_XBS1JuTrxVI",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_2： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/XBS1JuTrxVI",
          "start_time": 200,
          "end_time": 261,
          "text": "trong đó A chính là trừ param 0 chia cho param 1 rồi lưu ý là cái param này là cái trọng số cho các cái cạnh nối đến cái x1 và x2 rồi param 0 đó chính là cái trọng số param 0 chính là cái trọng số nối đến đây nối đến cái thành phần bias param 1 là nối đến cái x1 là tương ứng với lại x1 và param 2 là cái trọng số tương ứng với lại x2 cho cái theta 1 vậy bây giờ chúng ta sẽ phải tính cái theta 1 trước là sẽ bằng neuronet.getweight neuronet.getweight và ở đây chúng ta sẽ truyền là 1 và ở đây chúng ta sẽ truyền là 1 rồi bây giờ chúng ta sẽ lấy cái thành phần đầu tiên"
        },
        {
          "index": 6,
          "video_id": "Chương 2_XBS1JuTrxVI",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_2： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/XBS1JuTrxVI",
          "start_time": 255,
          "end_time": 309,
          "text": "và ở đây chúng ta sẽ truyền là 1 và ở đây chúng ta sẽ truyền là 1 rồi bây giờ chúng ta sẽ lấy cái thành phần đầu tiên đó là thành phần bias thì nó chính là bằng theta 1 theta 1 thành phần param thành phần param thành phần param thì nó sẽ là bằng theta 1,0 theta 1,0 rồi bây giờ chúng ta sẽ có cái công thức này chúng ta sẽ thế vào để tính ra cái phương trình đường thẳng cho cái các cái node của mình rồi và a đúng không thì sẽ là bằng trừ param 0 chia cho param 1"
        },
        {
          "index": 7,
          "video_id": "Chương 2_XBS1JuTrxVI",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_2： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/XBS1JuTrxVI",
          "start_time": 298,
          "end_time": 360,
          "text": "thì sẽ là bằng trừ param 0 chia cho param 1 b thì sẽ là bằng trừ bias chia cho param 1 rồi và ở đây thì param ở đây chúng ta sẽ phải lấy là cái ok bây giờ để biết là chúng ta sẽ tính như thế nào chúng ta sẽ in nó ra trước ok bây giờ để biết là chúng ta sẽ tính như thế nào chúng ta sẽ in nó ra trước để coi nó như thế nào kìa bias bias bias param bias thì nó là cái bộ 8 các cái giá trị tương ứng với lại 8 node trong đó cái param thì nó sẽ có 2 thành phần là cho w1 và cho"
        },
        {
          "index": 8,
          "video_id": "Chương 2_XBS1JuTrxVI",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_2： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/XBS1JuTrxVI",
          "start_time": 350,
          "end_time": 410,
          "text": "thì nó là cái bộ 8 các cái giá trị tương ứng với lại 8 node trong đó cái param thì nó sẽ có 2 thành phần là cho w1 và cho w2 rồi như vậy thì ở đây mình muốn lấy ra cái thành phần nào thì mình sẽ phải truyền thêm cái chỉ số nữa tức là mình truyền vào chỉ số node thứ mấy thì ở đây sẽ là param ờ param và 0 node thứ idx vị trí là thứ idx rồi param 1 idx bias thứ idx và param 1 idx rồi bây giờ chúng ta sẽ vẽ nó lên lt.plot và 2 cái điểm của mình ở đây thì chúng ta sẽ lấy cái điểm này từ trừ 1 cho đến 1 chúng ta sẽ lấy cái điểm này từ trừ 1 cho đến 1 rồi tương ứng cái đoạn lấy cái điểm từ trừ 1 cho đến 1 và cái điểm theo cái trục x2"
        },
        {
          "index": 9,
          "video_id": "Chương 2_XBS1JuTrxVI",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_2： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/XBS1JuTrxVI",
          "start_time": 398,
          "end_time": 486,
          "text": "chúng ta sẽ lấy cái điểm này từ trừ 1 cho đến 1 rồi tương ứng cái đoạn lấy cái điểm từ trừ 1 cho đến 1 và cái điểm theo cái trục x2 hay là trục y chúng ta ký hiệu ở đây thì nó sẽ là công thức này là ax cộng b sẽ là a nhân với lại trừ 1 cộng b rồi rồi"
        },
        {
          "index": 10,
          "video_id": "Chương 2_XBS1JuTrxVI",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_2： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/XBS1JuTrxVI",
          "start_time": 426,
          "end_time": 516,
          "text": ""
        },
        {
          "index": 11,
          "video_id": "Chương 2_XBS1JuTrxVI",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_2： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/XBS1JuTrxVI",
          "start_time": 486,
          "end_time": 576,
          "text": ""
        },
        {
          "index": 12,
          "video_id": "Chương 2_XBS1JuTrxVI",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_2： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/XBS1JuTrxVI",
          "start_time": 546,
          "end_time": 609,
          "text": "sử dụng thư viện keras"
        },
        {
          "index": 13,
          "video_id": "Chương 2_XBS1JuTrxVI",
          "chapter": "Chương 2",
          "video_title": "[CS431 - Chương 2] Part 5b_2： Cài đặt mạng neural network",
          "video_url": "https://youtu.be/XBS1JuTrxVI",
          "start_time": 576,
          "end_time": 620,
          "text": "sử dụng thư viện keras rồi rồi rồi  rồi rồi"
        }
      ]
    },
    {
      "video_id": "Chương 3_q3oZyk3l8EU",
      "summary": "## 1. Giới thiệu (Introduction)\n- Mục tiêu chính của bài giảng  \n  - Giới thiệu kiến trúc mạng Convolutional Neural Network (CNN) và lý do cần thiết khi áp dụng mạng Neural Network cho bài toán phân loại ảnh. [1]\n- Các khái niệm sẽ được đề cập  \n  - Biểu diễn ảnh (grayscale và color / tensor 3 kênh). [2][3]  \n  - Vấn đề khi dùng mạng fully-connected trên ảnh lớn (số lượng tham số rất lớn, overfitting). [4][5][6][7]  \n  - Các cơ chế giảm tham số: local connectivity (kết nối cục bộ) và weight sharing (chia sẻ tham số) — dẫn tới phép biến đổi convolution / filter, ví dụ Sobel. [11][13][14][15][16]  \n  - Mạng CNN tự học các kernel bằng quá trình huấn luyện (backprop + \"radiant descent\" theo bài giảng). [17]\n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 Biểu diễn ảnh (Image representation)\n- Ảnh mức xám (grayscale): mỗi pixel là một giá trị số (thường 1 byte từ 0 đến 255); ảnh được biểu diễn dưới dạng ma trận 2 chiều (width × height). [2]  \n- Ảnh màu (RGB): gồm 3 kênh (Red, Green, Blue); mỗi kênh là một ma trận — ghép lại tạo thành một tensor với kích thước (height × width × depth). [3]\n\n### 2.2 Áp dụng mạng Neural Network \"tiêu chuẩn\" cho ảnh: bài toán về số tham số\n- Ví dụ tối thiểu từ bài giảng: ảnh kích thước 200 × 200 (height = 200, width = 200). [5]  \n- Thiết kế mạng đơn giản: một layer duy nhất với số node bằng số phần tử ảnh = 40.000 node. [5]  \n- Nếu kết nối fully-connected (mọi node đầu ra nối tới mọi pixel đầu vào): mỗi kết nối tương ứng một trọng số → tổng số tham số:  \n  - Công thức: 200 × 200 × 40.000 ≈ 1.6 × 10^9 (khoảng 1.6 tỷ tham số). [6][7]  \n- Hệ quả: số tham số quá lớn dẫn đến overfitting (mô hình học quá khớp tập huấn luyện, kém tổng quát trên test). [7]  \n- Mối liên hệ số tham số vs. dữ liệu: để xác định bộ trọng số cần nhiều mẫu (giống analog với hệ phương trình: cần ít nhất số phương trình tương ứng để tìm nghiệm duy nhất). Tương ứng với 1.6 tỷ tham số thì cần xấp xỉ 1.6 tỷ mẫu — con số phi thực tế (so sánh với dân số Trung Quốc). [8][9][10]\n\n### 2.3 Giảm số tham số — Local connectivity (kết nối cục bộ)\n- Thay vì kết nối đầy đủ, mỗi node chỉ kết nối tới một vùng cục bộ (local receptive field), ví dụ 10 × 10 patch. [11]  \n- Với 40.000 node và receptive field 10 × 10, số tham số: 40.000 × 10 × 10 = 4.000.000 (4 triệu tham số). [11][12]  \n- Kết luận: từ ~1.6 tỷ giảm xuống ~4 triệu — giảm mạnh nhưng 4 triệu vẫn còn lớn (vẫn cần nhiều dữ liệu). [12]\n\n### 2.4 Giảm thêm bằng Weight sharing (chia sẻ tham số) — cơ sở của Convolution\n- Ý tưởng: cùng một bộ trọng số (kernel / filter) áp dụng cho nhiều vị trí khác nhau trên ảnh (trượt toàn ảnh). Điều này gọi là *weight sharing* và là đặc trưng chính của convolutional layer. [13][14]  \n- Khi dùng weight sharing, ta chỉ lưu một tập trọng số cho từng filter thay vì một bộ trọng số riêng cho mỗi vị trí — giảm rất lớn số tham số so với locally-connected không chia sẻ. [13][14]\n\n### 2.5 Phép biến đổi convolution / filter — bản chất toán học và vai trò\n- Phép biến đổi tại mỗi vị trí thực chất là nhân từng phần tử của kernel với các pixel tương ứng trong patch, sau đó cộng tổng (linear operation followed by possible nonlinearity). (Đề cập: phép biến đổi là nhân rồi cộng tổng hợp). [15]  \n- Kết quả khi trượt kernel khắp ảnh là một ảnh (feature map / output map) biểu diễn đặc trưng đã trích xuất ở mỗi vị trí. [14][15]  \n- Ví dụ cụ thể: Sobel filter (kernel) để phát hiện biên theo chiều dọc. Kernel được mô tả trong bài:  \n  - Hàng 1: [1, 2, 1]  \n  - Hàng 2: [0, 0, 0]  \n  - Hàng 3: [-1, -2, -1]  \n  Kernel này lấy tổng các pixel bên trái trừ tổng các pixel bên phải → cho tín hiệu biên dọc. [15][16]\n\n### 2.6 Học kernel từ dữ liệu (trainable filters)\n- Thay vì do con người thiết kế như Sobel, CNN hiện đại học các giá trị trọng số (kernel) tự động dựa trên dữ liệu lớn thông qua quá trình huấn luyện: backpropagation kết hợp (theo bài giảng) \"radiant descent\" (ý chỉ thuật toán tối ưu như gradient descent) để cập nhật trọng số. [17]  \n- Kết quả: các kernel học được sẽ chuyên hóa để trích xuất các đặc trưng hữu ích cho nhiệm vụ (ví dụ classification). [17]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n- Minh họa ảnh Lena: dùng để trình bày biểu diễn ma trận của ảnh mức xám. [2]  \n- Bài toán phân loại ảnh: input là ảnh, output là nhãn (ví dụ: xe cộ, nhà cửa, con người). Mạng phải trả nhãn đúng nếu nhận diện đúng đối tượng. [4]  \n- Kích thước ảnh phổ biến: ảnh hiện đại có thể lên đến Full HD (~800–1000 px mỗi chiều), do đó kích thước input lớn hơn nhiều so với ví dụ 200×200. [5]  \n- Ví dụ Sobel filter: cụ thể cho tác vụ phát hiện biên dọc; minh họa cách một kernel với trọng số đã cho trượt khắp ảnh và tạo feature map chỉ ra biên. [15][16]  \n- Ứng dụng thực tế: sử dụng CNN cho các bài toán thị giác máy tính như phân loại ảnh, trích xuất đặc trưng cục bộ, phát hiện biên/đối tượng; giảm overfitting và chi phí tính toán bằng local connectivity và weight sharing để làm cho học khả thi. [11][13][14]\n\n---\n\n## 4. Kết luận (Conclusion)\n- Tóm tắt các ý chính:  \n  - Dùng mạng fully-connected trực tiếp trên ảnh dẫn tới số tham số cực lớn (ví dụ 200×200×40.000 ≈ 1.6 tỷ) và gây overfitting; cần lượng dữ liệu tương ứng để ước lượng trọng số. [5][6][7][8][9][10]  \n  - Hai nguyên tắc then chốt để xử lý ảnh hiệu quả là *local connectivity* (kết nối cục bộ) và *weight sharing* (chia sẻ trọng số) — nền tảng của convolutional layers trong CNN — từ đó giảm hàng loạt số tham số (ví dụ giảm xuống ~4 triệu với receptive field 10×10 trước khi chia sẻ trọng số). [11][12][13]  \n  - Convolution (trượt kernel) là phép biến đổi tuyến tính trích xuất đặc trưng cục bộ (ví dụ Sobel cho biên dọc); các kernel này có thể do con người thiết kế hoặc được học tự động qua huấn luyện (backprop + tối ưu). [14][15][16][17]\n- Tầm quan trọng của nội dung: những khái niệm này giải thích vì sao CNN trở thành kiến trúc chủ đạo cho các bài toán thị giác — cho phép xử lý ảnh kích thước lớn một cách hiệu quả cả về tham số lẫn khả năng học. [1][11][13][14]  \n- Liên hệ với bài giảng trước: xây dựng trên kiến thức về mạng Neural Network (bài 2) — từ mạng NNs tổng quát chuyển tới kiến trúc chuyên biệt hơn cho ảnh là CNN. [1]\n\n---\n\n(Các trích dẫn [1] … [17] tương ứng với các đoạn trong video ở timestamps đã cung cấp; mỗi citation trên đây trỏ tới đoạn nội dung nêu tại timestamp tương ứng.)",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 3_q3oZyk3l8EU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 1： Giới thiệu mạng CNN",
          "video_url": "https://youtu.be/q3oZyk3l8EU",
          "start_time": 13,
          "end_time": 66,
          "text": "Trong bài số 3 thì chúng ta sẽ bắt đầu vào một cái kiến trúc mạng rất là nổi tiếng trong lĩnh vực học sâu đó chính là mạng Corrosional Neural Network hay còn gọi là mạng CNN thì ở phần đầu tiên chúng ta sẽ giới thiệu qua về cái bài toán phân loại ảnh với cái mạng Neural Network tức là trong bài 2 chúng ta đã học và học đến cái bài về mạng học sâu đầu tiên đó là mạng Neural Network tuy nhiên khi chúng ta áp dụng cái mạng này đối với một cái loại dữ liệu ảnh và cho một cái bài toán nó tương đối là phức tạp thì điều gì sẽ xảy ra? Đầu tiên đó là chúng ta sẽ giới thiệu qua cái bài toán phân loại ảnh và ảnh ở đây thì nó sẽ có hai dạng loại đầu tiên đó là cảnh mức xám thì ảnh mức xám này thì mỗi một cái pixel này nó sẽ biểu diễn bởi một cái giá trị màu"
        },
        {
          "index": 2,
          "video_id": "Chương 3_q3oZyk3l8EU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 1： Giới thiệu mạng CNN",
          "video_url": "https://youtu.be/q3oZyk3l8EU",
          "start_time": 48,
          "end_time": 109,
          "text": "thì điều gì sẽ xảy ra? Đầu tiên đó là chúng ta sẽ giới thiệu qua cái bài toán phân loại ảnh và ảnh ở đây thì nó sẽ có hai dạng loại đầu tiên đó là cảnh mức xám thì ảnh mức xám này thì mỗi một cái pixel này nó sẽ biểu diễn bởi một cái giá trị màu và cái giá trị này thì thông thường nó sẽ là biểu diễn bởi một con số 1 byte từ 0 cho đến 255 thì ví dụ như với cái ảnh Lena ở bên tay trái thì cái dạng biểu diễn ở đây là mang tính chữ chất minh họa thôi thì nó sẽ mô tả bởi một cái ma trận trong đó từng cái phần tử của ma trận nó sẽ nhận các cái giá trị từ 0 cho đến 255 và ở đây chúng ta sẽ có các cái thông tin về bề ngang và bề cao tương ứng là hai chiều không gian tấm ảnh Đối với cái loại ảnh thứ hai đó là ảnh màu và ảnh màu thì thông thường sẽ được biểu diễn bởi ba kênh màu là red, green và blue tương ứng là đỏ, xanh lá và xanh dương"
        },
        {
          "index": 3,
          "video_id": "Chương 3_q3oZyk3l8EU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 1： Giới thiệu mạng CNN",
          "video_url": "https://youtu.be/q3oZyk3l8EU",
          "start_time": 99,
          "end_time": 158,
          "text": "Đối với cái loại ảnh thứ hai đó là ảnh màu và ảnh màu thì thông thường sẽ được biểu diễn bởi ba kênh màu là red, green và blue tương ứng là đỏ, xanh lá và xanh dương màu ra cái tấm ảnh màu này chúng ta sẽ có ba cái kênh là red, green và blue và tương ứng từng cái kênh này chúng ta sẽ có các cái ma trận đây là ma trận biểu diễn cho kênh red, đây là ma trận biểu diễn cho kênh green, kênh màu xanh lá và đây sẽ là ma trận biểu diễn cho kênh blue tức là màu xanh dương và ba kênh màu này nó sẽ tương ứng với một cái thông số nó gọi là độ sâu và toàn bộ ba cái ma trận này khi chúng ta ghép lại với nhau thì nó sẽ được gọi là một cái tensor và bây giờ chúng ta sẽ tiến hành sử dụng cái mạng Neural Network để đi giải quyết các bài toán đó là bài tán phân loại ảnh"
        },
        {
          "index": 4,
          "video_id": "Chương 3_q3oZyk3l8EU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 1： Giới thiệu mạng CNN",
          "video_url": "https://youtu.be/q3oZyk3l8EU",
          "start_time": 149,
          "end_time": 214,
          "text": "và bây giờ chúng ta sẽ tiến hành sử dụng cái mạng Neural Network để đi giải quyết các bài toán đó là bài tán phân loại ảnh thì đầu vào của mình sẽ là một cái tấm ảnh và đầu ra mình sẽ có các cái nhãn tương ứng để cho biết cái loại đối tượng ở bên trong cái tấm ảnh này là gì? thì cái loại đối tượng ở bên trong cái tấm ảnh này là gì?  Những tấm ảnh nổi tượng này nó sẽ có thể là cái nhãn xe cộ, nhà cửa và con người. Thế thì nếu như cái mạng Neural Network này mà nhận diện đúng thì nó sẽ phải trả ra cái nhãn đó là con người. Và điều gì sẽ xảy ra nếu như chúng ta sẽ thiết kế một cái mạng Neural Network với một cái kích thước gọi là tối thiểu. Cái tối thiểu này nó thể hiện ở cái việc là cái tấm ảnh đầu vào của mình. Thông thường cái ảnh đầu vào của mình kích thước nó rất là lớn. Với những cái chuẩn ảnh hiện đại bây giờ chúng ta thấy là ảnh Full HD có thể lên đến trên 800 cho đến 1000 pixel cho một cái chiều ngang hoặc là chiều dọc. Nhưng mà ở đây chúng ta đang xét một cái tấm ảnh tối thiểu có kích thước đó là 200 x 200."
        },
        {
          "index": 5,
          "video_id": "Chương 3_q3oZyk3l8EU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 1： Giới thiệu mạng CNN",
          "video_url": "https://youtu.be/q3oZyk3l8EU",
          "start_time": 199,
          "end_time": 260,
          "text": "Với những cái chuẩn ảnh hiện đại bây giờ chúng ta thấy là ảnh Full HD có thể lên đến trên 800 cho đến 1000 pixel cho một cái chiều ngang hoặc là chiều dọc. Nhưng mà ở đây chúng ta đang xét một cái tấm ảnh tối thiểu có kích thước đó là 200 x 200. Bề ngang là 200 và bề cao là 200. Và cái mạng này nó chỉ bao gồm duy nhất là một layer. Và với cái mạng này thì cái số node của cái mạng Neural Network này chúng ta sẽ cho đúng bằng cái số phần tử của cái ảnh đầu vào. Thì 200 x 200 nó tương ứng chính là 40.000. Như vậy là cái layer duy nhất này nó sẽ có chứa 40.000 node. Và điều gì sẽ xảy ra với cái kiến trúc mạng tối thiểu này thì chúng ta sẽ xem xét. Tổng số mạng.  Tổng số trọng số. Tổng số trọng số của một cái mạng tối thiểu này ha."
        },
        {
          "index": 6,
          "video_id": "Chương 3_q3oZyk3l8EU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 1： Giới thiệu mạng CNN",
          "video_url": "https://youtu.be/q3oZyk3l8EU",
          "start_time": 249,
          "end_time": 311,
          "text": "Và điều gì sẽ xảy ra với cái kiến trúc mạng tối thiểu này thì chúng ta sẽ xem xét. Tổng số mạng.  Tổng số trọng số. Tổng số trọng số của một cái mạng tối thiểu này ha. Thì mỗi trọng số nó tương ứng là một cái cạnh nối từ cái điểm ảnh đầu vào đến một cái node đầu ra. Thì ở đây chúng ta sẽ có cái khái niệm gọi là fully connected. Tức là kết nối đầy đủ. Mỗi một cái node đầu ra sẽ được kết nối đầy đủ với tất cả các cái điểm ảnh đầu vào. Thì số tham số trong trường hợp này sẽ là bao nhiêu? Thì do là kết nối đầy đủ nên chúng ta sẽ có cái số lượng tham số của cái tầng này đó chính là 200 x 200 tức là cái số điểm ảnh đầu vào. Và 40.000 đó chính là cái số node đầu ra. Như vậy 200 x 200 x 40.000 thì chúng ta có thể dùng máy tính để tính. Nó sẽ ra là khoảng 1.6 tỷ tham số."
        },
        {
          "index": 7,
          "video_id": "Chương 3_q3oZyk3l8EU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 1： Giới thiệu mạng CNN",
          "video_url": "https://youtu.be/q3oZyk3l8EU",
          "start_time": 298,
          "end_time": 357,
          "text": "Và 40.000 đó chính là cái số node đầu ra. Như vậy 200 x 200 x 40.000 thì chúng ta có thể dùng máy tính để tính. Nó sẽ ra là khoảng 1.6 tỷ tham số. 1.6 tỷ tham số. Và với 1.6 tỷ tham số này thì chúng ta có kết luận là gì? Nó quá nhiều. Quá nhiều tham số. Thế thì khi cái số lượng tham số mà quá nhiều thì điều gì sẽ xảy ra? Khi mà số tham số của mình nhiều thì chúng ta sẽ bị cái hiện trượng nó gọi là overfitting. Nó sẽ bị hiện trượng overfitting. Overfitting nghĩa là sao? Khi cái mô hình của mình nó học nó sẽ cố gắng bắt trước trên những cái mẫu dữ liệu. Đó. Đang có. Nhưng mà không có tổng quát. Khi áp dụng lên trên những cái tập dữ liệu test thì nó độ chính xác cực kỳ thấp. Thì overfitting đó là tốt. Trên tập trend."
        },
        {
          "index": 8,
          "video_id": "Chương 3_q3oZyk3l8EU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 1： Giới thiệu mạng CNN",
          "video_url": "https://youtu.be/q3oZyk3l8EU",
          "start_time": 347,
          "end_time": 413,
          "text": "Khi áp dụng lên trên những cái tập dữ liệu test thì nó độ chính xác cực kỳ thấp. Thì overfitting đó là tốt. Trên tập trend. Nhưng rất là tệ. Nó rất là tệ. Trên cái tập test. Thì cái điều này có thể minh họa. Nó có thể lấy một cái ví dụ giống như là trong cái giải hệ phương trình. Hồi xưa mình học á. Nếu mà cái hệ phương trình của mình nó có 3 ẩn x, y, z. Đúng không? Thì chúng ta cần có bao nhiêu phương trình để có thể giải được. Cái phương trình cần bao nhiêu phương trình mật 1 để có thể giải được 3 cái ẩn này. Thì rõ ràng là nếu như chúng ta chỉ có 2 hệ phương trình. 3 mình cho đại một cái hệ phương trình. 3x cộng cho 4y cộng cho 6z trừ 5 bằng 0. Rồi 7x cộng cho trừ cho 6y. Cộng cho. 3z cộng 1 hoặc 0. Thì nếu như chỉ có 2 cái mẫu dữ liệu này. Với chỉ 2 mẫu dữ liệu này thì nó sẽ có vô số cái nghiệm xyz."
        },
        {
          "index": 9,
          "video_id": "Chương 3_q3oZyk3l8EU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 1： Giới thiệu mạng CNN",
          "video_url": "https://youtu.be/q3oZyk3l8EU",
          "start_time": 399,
          "end_time": 459,
          "text": "Rồi 7x cộng cho trừ cho 6y. Cộng cho. 3z cộng 1 hoặc 0. Thì nếu như chỉ có 2 cái mẫu dữ liệu này. Với chỉ 2 mẫu dữ liệu này thì nó sẽ có vô số cái nghiệm xyz. Và cái sát xuất để chúng ta tìm ra được một cái nghiệm. Tìm ra được một cái nghiệm của cái kiến trúc mạng này. Là sát xuất của nó sẽ là bằng 1 phần vô cùng. Tại vì ở đây chúng ta có vô số nghiệm. Tức là sát xuất là bằng 0. Do đó muốn mà tìm ra được cái nghiệm xyz. Tức là tìm ra cái bộ trọng số. Đúng cho cái kiến trúc mạng này. Thì chúng ta sẽ phải cần thêm ít nhất 1 phương trình nữa. 1 phương trình nữa. Thì cứ mỗi 1 phương trình. Thì tương ứng nó sẽ là 1 cái mẫu dữ liệu. 1 cái mẫu dữ liệu. Như vậy thì ở bên đây. Chúng ta có 1.6 tỷ tham số. Tức là chúng ta sẽ cần đâu đó khoảng 1.6 tỷ mẫu dữ liệu."
        },
        {
          "index": 10,
          "video_id": "Chương 3_q3oZyk3l8EU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 1： Giới thiệu mạng CNN",
          "video_url": "https://youtu.be/q3oZyk3l8EU",
          "start_time": 451,
          "end_time": 510,
          "text": "Như vậy thì ở bên đây. Chúng ta có 1.6 tỷ tham số. Tức là chúng ta sẽ cần đâu đó khoảng 1.6 tỷ mẫu dữ liệu. Cái mức độ nó tương đối là như vậy. Và các bạn tưởng tượng. Cái con số 1.6 tỷ này nó tương đương là dân số của Trung Quốc. Tức là với mỗi người Trung Quốc. Chúng ta sẽ phải yêu cầu họ đi tạo cho chúng ta 1 cái mẫu dữ liệu. Như vậy đây là 1 cái con số vô cùng kinh khủng. Như vậy thì với 1 cái việc áp dụng mạng Neural Network. Cho cái loại dữ liệu ảnh. Với cái kiến trúc rất là tối thiểu. Thì chúng ta sẽ bị ngay cái vấn đề đó là quá nhiều tham số. Và gây ra cái hiện tượng overfitting. Vậy thì bây giờ làm sao để có thể giảm được. Cái số lượng tham số này. Thì chúng ta sẽ có 1 cái cơ chế đầu tiên. Đó là thay vì chúng ta fully connected. Thì chúng ta sẽ chuyển sang là locally connected. Nghĩa là sao? Mỗi 1 cái node của cái mạng Neural. Thay vì chúng ta kết nối với tất cả."
        },
        {
          "index": 11,
          "video_id": "Chương 3_q3oZyk3l8EU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 1： Giới thiệu mạng CNN",
          "video_url": "https://youtu.be/q3oZyk3l8EU",
          "start_time": 500,
          "end_time": 559,
          "text": "Thì chúng ta sẽ chuyển sang là locally connected. Nghĩa là sao? Mỗi 1 cái node của cái mạng Neural. Thay vì chúng ta kết nối với tất cả. Các cái điểm ảnh của cái ảnh đầu vào. Thì bây giờ nó sẽ kết nối với 1 cái vùng cục bộ. Và cái vùng cục bộ này. Nó sẽ có cái kích thước. Mình lấy ví dụ như là 10. Nhân 10. Tức là bề ngang là 10. Và bề cao là 10. Còn những cái điểm ảnh khác. Nó sẽ không kết nối. Nó sẽ không có kết nối tới. Thì cái điểm ảnh. Cái Neural này. Nó sẽ tổng hợp thông tin. Trên 1 cái vùng cục bộ như thế này thôi. Vậy thì trong trường hợp này. Khi mỗi 1 cái node. Sẽ được kết nối với 1 cái vùng. Có kích thước là 10 x 10. Vậy thì hỏi. Tổng số tham số. Trong trường hợp này. Sẽ là bao nhiêu. Và. Đáp số đó chính là. Chúng ta có 40.000 node. Đúng không? Chúng ta có 40.000 node. Và mỗi node. Thì kết nối với 1 cái vùng 10 x 10. Như vậy."
        },
        {
          "index": 12,
          "video_id": "Chương 3_q3oZyk3l8EU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 1： Giới thiệu mạng CNN",
          "video_url": "https://youtu.be/q3oZyk3l8EU",
          "start_time": 550,
          "end_time": 611,
          "text": "Đáp số đó chính là. Chúng ta có 40.000 node. Đúng không? Chúng ta có 40.000 node. Và mỗi node. Thì kết nối với 1 cái vùng 10 x 10. Như vậy. Tổng số tham số của mình. Nó sẽ là. Nhân vô cái con số này. Nó sẽ ra. Là. 4 triệu tham số. 4 triệu tham số. Vậy thì. Như vậy. Từ 1,6 tỷ. Nó đã giảm xuống còn 4 triệu. Tức là chúng ta cảm nhận được là. Sự sụt giảm rất là đáng kể. Nhưng mà 4 triệu tham số này. Thì liệu là nhiều hay ít. Thì chúng ta cũng hiểu. Là 4 triệu tham số. Thì chúng ta sẽ cần đâu đó. Sắp xỉ khoảng. 4 triệu mẫu đi. Thì. Cái 4 triệu này. Nó tương đương quy mô. Của 1 dân số. Của 1 thành phố. Ví dụ dân số thành phố Hồ Chí Minh. Có thể là khoảng 4 triệu. Thì chúng ta ra ngoài đường. Chúng ta sẽ cứ mỗi người đi ngang qua. Chúng ta sẽ nhờ họ. Là 1 mẫu dữ liệu. Thì rõ ràng là 4 triệu. Nó vẫn còn là 1 con số. Rất là lớn. Nhưng tuy nhiên. Nó cũng đã giảm 1 cách đáng kể. So với lại cái phiên bản là fully connected rồi."
        },
        {
          "index": 13,
          "video_id": "Chương 3_q3oZyk3l8EU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 1： Giới thiệu mạng CNN",
          "video_url": "https://youtu.be/q3oZyk3l8EU",
          "start_time": 599,
          "end_time": 660,
          "text": "Chúng ta sẽ cứ mỗi người đi ngang qua. Chúng ta sẽ nhờ họ. Là 1 mẫu dữ liệu. Thì rõ ràng là 4 triệu. Nó vẫn còn là 1 con số. Rất là lớn. Nhưng tuy nhiên. Nó cũng đã giảm 1 cách đáng kể. So với lại cái phiên bản là fully connected rồi. Vậy thì. Bây giờ. Làm thế nào. Để có thể giảm thêm được. Cái số lượng tham số này. 4 triệu còn còn nhiều. Đúng không? Chúng ta làm sao có thể giảm được. Thì cái cơ chế đó chính là. Chia sẻ. Cái tham số. Giữa các nốt. Nghĩa là sao. Cái nốt này. Và nốt này. Nó được biểu diễn bởi 2 cái màu đen. Màu đỏ. Nốt này biểu diễn bởi màu xanh lá. Xanh dương. Thì nó đang sử dụng. Các cái bộ trọng số khác nhau. Và bây giờ. Mình sẽ tạo 1 cái cơ chế. Đó là dùng chung. Mình chia sẻ cái bộ trọng số này. Nghĩa là. Cái bộ trọng số. Dùng cho cái nốt này. Cũng chính là bộ trọng số. Dùng cho cái nốt này. Cũng chính là bộ trọng số. Dùng cho cái nốt này. Đó gọi là. Way sharing locally connected. Gọi là way sharing locally connected. Thì tham số được chia sẻ."
        },
        {
          "index": 14,
          "video_id": "Chương 3_q3oZyk3l8EU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 1： Giới thiệu mạng CNN",
          "video_url": "https://youtu.be/q3oZyk3l8EU",
          "start_time": 648,
          "end_time": 710,
          "text": "Cũng chính là bộ trọng số. Dùng cho cái nốt này. Đó gọi là. Way sharing locally connected. Gọi là way sharing locally connected. Thì tham số được chia sẻ. Trên toàn bộ. Các cái vùng. Của ảnh cần biến đổi. Nghĩa là sao. Trên cái vị trí này. Nó sẽ dùng cái bộ tham số. Giống như tại đây. Dùng. Với cùng 1 cái bộ tham số. Trên cái vùng tại đây. Tức là. Nó sẽ. Có 1 cái bộ tham số. Duyệt qua. Hết. Toàn bộ cái tấm hình. Đó. Và. Cứ mỗi 1 cái lần. Mà chúng ta sẽ dừng ở đây. Chúng ta sẽ trút trích thông tin. Và tạo ra. Giá trị cho cái nốt này. Và như vậy thì nhìn cái hình này. Chúng ta sẽ. Sẽ. Liên tưởng đến cái việc đó là. Khi chúng ta thực hiện cái phép. Tổng hợp thông tin sau. Thì nó sẽ tạo ra 1 cái tấm ảnh. Đúng không? Nó sẽ tạo ra 1 cái tấm ảnh. Khi chúng ta trượt. Chúng ta trượt. 1 cái bộ tham số. Lên trên. Toàn bộ các cái vị trí ảnh. Thì ở trên đây. Chúng ta cũng sẽ. Trượt. Và điền các cái giá trị. Lên trên cái vùng."
        },
        {
          "index": 15,
          "video_id": "Chương 3_q3oZyk3l8EU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 1： Giới thiệu mạng CNN",
          "video_url": "https://youtu.be/q3oZyk3l8EU",
          "start_time": 699,
          "end_time": 760,
          "text": "Khi chúng ta trượt. Chúng ta trượt. 1 cái bộ tham số. Lên trên. Toàn bộ các cái vị trí ảnh. Thì ở trên đây. Chúng ta cũng sẽ. Trượt. Và điền các cái giá trị. Lên trên cái vùng. Ảnh output này. Thì đây chính là. Cái phép biến đổi. Conversion. 1 trong những cái phép biến đổi. Và. Cái phép biến đổi. Conversion này. Bản chất. Nó chính là 1 cái phép biến đổi. Thiên tính. Thó chỉ là cái thao tác. Nhân. Sau đó rồi. Cộng tổng hợp lại thôi. Và. Ý nghĩa. Của cái phép biến đổi. Conversion này. Đó chính là. Nó rút trích. Đặc trưng. Ngày nảnh. Ở đây. Chúng ta lấy ví dụ. Là 1 cái bộ lọc. Tên là. Sobel. Tên của 1 cái nhà khoa học. Nghĩ ra cái bộ lọc này. Thì cái. Cái cái trọng số. Cho cái filter. Cái cái trọng số. Ở đây. Đó là. 1. 2. 1. 0. 0. 0. Trừ 1. Trừ 2. Trừ 1. Thì ý nghĩa. Của cái. Tham số này. Đó chính là. Nó sẽ lấy. Tổng. Các cái pixel. Ở bên tay trái."
        },
        {
          "index": 16,
          "video_id": "Chương 3_q3oZyk3l8EU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 1： Giới thiệu mạng CNN",
          "video_url": "https://youtu.be/q3oZyk3l8EU",
          "start_time": 750,
          "end_time": 810,
          "text": "1. 0. 0. 0. Trừ 1. Trừ 2. Trừ 1. Thì ý nghĩa. Của cái. Tham số này. Đó chính là. Nó sẽ lấy. Tổng. Các cái pixel. Ở bên tay trái. Trừ cho tổng. Tất cả các pixel. Bên tay phải. Và khi chúng ta đem. Cái. Cái filter này. Chúng ta trượt. Chúng ta trượt. Trên toàn bộ. Cái tấm hình này. Đầu. Cái giá trị kết quả. Sau khi thực hiện. Điền lên đây. Và chúng ta quan sát. Cái kết quả. Thì chúng ta thấy là. Cái filter này. Cái đặc trưng này. Nó có cái tính chất gì. Đặc trưng này. Nó có tính chất đó là. Nó. Thể hiện được. Những cái biên cạnh. Những cái biên cạnh. Theo chiều dọc. Những cái biên. Theo chiều dọc. Và. Cái nhà khoa học. Họ nghĩ ra. Cái trọng số. Cho cái. Filter này. Đúng không."
        },
        {
          "index": 17,
          "video_id": "Chương 3_q3oZyk3l8EU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 1： Giới thiệu mạng CNN",
          "video_url": "https://youtu.be/q3oZyk3l8EU",
          "start_time": 802,
          "end_time": 839,
          "text": "Và. Cái nhà khoa học. Họ nghĩ ra. Cái trọng số. Cho cái. Filter này. Đúng không. Tuy nhiên thì. Mạng CNN sau này. Nó sẽ tự. Nó học. Và nó sẽ tự. Điền. Các cái giá trị. Trọng số. Cho các cái. Kernel này. Dựa trên. Tập hợp. Rất là nhiều. Cái dữ liệu. Và được huấn luyện. Như vậy. Cái trọng số này. Thay vì được. Gán nhãn. Bởi kinh nghiệm. Của các nhà khoa học. Thì. Các cái trọng số này. Nó sẽ được. Tự động. Điền. Bằng cách. Đó là. Huấn luyện. Với thực toán. Radiant descent. Thực toán. Back propagation. Nó dựa trên. Cái ý tưởng. Của radiant descent."
        }
      ]
    },
    {
      "video_id": "Chương 3_SKcHedTJIL0",
      "summary": "## 1. Giới thiệu (Introduction)\n- Mục tiêu chính của bài giảng: Trình bày các **thành phần chính của một mạng Convolutional Neural Network (CNN)** — gồm các phép biến đổi convolution, activation, pooling và fully-connected — và cách phối hợp chúng trong kiến trúc chuẩn của CNN. [1][2][3]  \n- Các khái niệm sẽ được đề cập: *convolution layer*, *activation layer* (ví dụ ReLU), *pooling layer*, *fully-connected layer*, cùng các tham số liên quan như số lượng filter K, độ sâu D, stride và padding; đồng thời trình bày ví dụ số học minh họa phép convolution và cách stride/padding ảnh hưởng tới kích thước đầu ra. [2][6][11][15][16][17]  \n- Liên hệ với bài trước: Tầng convolution được giới thiệu phần 1, ở đây tiếp tục nêu công thức và ví dụ cụ thể. [6]\n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 Tổng quan kiến trúc CNN — các thành phần và cách phối hợp\n- Bốn phép biến đổi/tầng chính xuất hiện xuyên suốt trong CNN là: **convolution**, **activation (hàm kích hoạt)**, **pooling**, và **fully-connected**. [2]  \n- Thông thường các tầng *convolution* và *activation* đi theo cặp (convolution → activation), thường dùng ReLU cho activation; các cặp này được lặp nhiều lần, và thỉnh thoảng xen kẽ với một tầng *pooling*. [2][3]  \n- Mục tiêu của tầng *pooling* là **giảm kích thước** của feature (giảm độ phân giải) để giảm số lượng tham số cho tầng *fully-connected* cuối cùng. [3]  \n- Sau nhiều lần nhóm các cặp convolution+activation (+ pooling), chuỗi này kết thúc bằng tầng **fully-connected** thực hiện phân lớp (output là phân bố xác suất cho các lớp, ví dụ class \"car\" có xác suất cao nhất). Tầng fully-connected thực ra là một mạng neural truyền thống dùng để phân loại các đặc trưng đã trích xuất. [3][4]\n\n- Ví dụ tham số cấu trúc: người ta hay mô tả một kiến trúc theo hai tham số k và n:\n  - k = số lần convolution+activation lồng trong mỗi block (ví dụ k = 2 nghĩa là thực hiện 2 lần convolution→ReLU trong một block).  \n  - n = số block (ví dụ n = 3 nghĩa là có 3 block như vậy), sau đó đến fully-connected. [5]\n\n### 2.2 Tầng Convolution — khái niệm và tính chất\n- Bản chất phép convolution: có một filter (kernel) trượt lên toàn ảnh (hoặc toàn tensor đầu vào) và tại mỗi vị trí thực hiện nhân-tích theo phần tử rồi tổng lại để sinh ra một giá trị đặc trưng tại vị trí đó — kết quả cho một *feature* (về sau khi có nhiều filter sẽ tạo thành *feature map* / tensor). [6][8][10]  \n- Nếu ảnh là grayscale thì filter có 2 chiều không gian; nếu ảnh có 3 kênh màu (RGB) thì filter có thêm chiều sâu (depth) bằng đúng **độ sâu của input** (ví dụ D = 3). Tức là: filter.depth = input.depth = D. [7][8][11]  \n- Khi dùng nhiều filter (số lượng filter = K), mỗi filter tạo ra một feature; xếp chồng các feature này theo chiều depth tạo thành một khối 3D (tensor) đầu ra. Do đó **độ sâu của tensor output = K** (số filter). [9][11]  \n- Kết quả không gian (chiều rộng/chiều cao) của feature map có thể giảm so với input do kích thước filter (ví dụ input 28 → output 24 trong minh họa vì filter trượt làm mất biên). [10]\n\n(Formulas được video nêu/nhấn mạnh trực tiếp)\n- filter.depth = D (D là độ sâu của input). [11]  \n- output.depth = K (K là số filter). [11]\n\n### 2.3 Ví dụ số học cho phép convolution (minh họa chi tiết)\n- Ví dụ trong video: input 5x5, filter 3x3 — khi áp filter lên một vị trí, nhân từng phần tử và tổng lại (các giá trị minh họa trên slide được nhân và cộng) — tại một vị trí tác giả tính tổng các tích thành một giá trị cụ thể (ví dụ trong đoạn tính có phép cộng 75 + 80 + 80 + 0 = 35, dẫn tới một giá trị đầu ra tại vị trí đó được nêu là 235 trong animation minh họa). [12][13]  \n- Ý nghĩa của một filter cụ thể: có filter mà về mặt số học tương đương \"giữ\" các vị trí có hệ số 1 và \"loại\" các vị trí hệ số 0 — ví dụ filter đó cộng các giá trị theo một cột (ví dụ 3+4+1 = 8 cho một vị trí), hay các tổ hợp khác khi trượt qua ảnh. Qua đó filter có thể thực hiện các phép tách thành phần (ví dụ như lọc theo cột giữa). [14]  \n- Khi input 5x5 áp filter 3x3 với stride = 1 thì output là 3x3 (kết quả không gian giảm từ 5→3). [15]\n\n### 2.4 Stride và ảnh hưởng của stride\n- **Stride** là bước nhảy khi trượt filter. Thông thường stride = 1 (di chuyển 1 đơn vị). Nếu đặt stride = 2 thì filter nhảy 2 đơn vị mỗi lần, làm giảm độ phân giải của feature map (ví dụ kết quả được lấy theo quy tắc \"giả lập copy & bỏ qua hàng/cột\" trong slide). [15][16][17]  \n- Ví dụ minh họa với stride = 2: các giá trị thu được trên output trong ví dụ là 1, 2, 2, 5 — nghĩa là output được sao chép/giảm từ feature map gốc nhưng bỏ qua hàng/cột, dẫn tới giảm khoảng một nửa độ phân giải. [16][17]\n\n### 2.5 Padding — giữ kích thước không đổi\n- Nếu không dùng padding, kích thước không gian của output sẽ giảm; ví dụ input 3x3 với filter 3x3 cho output 1x1. Để **giữ nguyên kích thước đầu vào**, ta có thể chèn các giá trị (ví dụ số 0) vào biên ảnh (padding) để biến input 3x3 thành 5x5, khi đó áp filter 3x3 sẽ cho output 3x3 — tức input/output cùng kích thước. [17][18]  \n- Có nhiều chiến lược padding (zero-padding, copy giá trị biên, padding không đều theo các chiều, v.v.). Giảng viên nhận xét rằng chiến lược padding cụ thể thường **không ảnh hưởng lớn** tới kết quả phân loại cuối cùng vì object trong ảnh thường nằm ở phần nội tại, phần biên ít đóng góp. Do vậy dùng padding kiểu nào cũng thường ổn. [19][20]\n\n### 2.6 Tầng Activation — cần thiết và lựa chọn hàm\n- Tầng activation là một phép biến đổi **phi tuyến** đặt ngay sau convolution. Nếu nối nhiều phép convolution tuyến tính mà không có activation phi tuyến, tổ hợp vẫn là tuyến tính và sẽ không mô hình hóa được các quan hệ phi tuyến phức tạp trong dữ liệu. Vì vậy cần activation phi tuyến giữa các convolution. [21]  \n- Trước đây người ta hay dùng sigmoid; gần đây khi dữ liệu lớn và mạng sâu hơn, chuyển sang **ReLU** giúp huấn luyện nhanh hơn vì giảm hiện tượng *vanishing gradient* (đạo hàm nhỏ dần gây làm chậm cập nhật). ReLU làm giảm hiện tượng này và do đó giúp huấn luyện hiệu quả hơn. [22][23]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n- Ví dụ minh họa trượt filter 3x3 trên ảnh 5x5, tính tích từng phần tử và tổng lại để ra giá trị feature tại từng vị trí (các bước số học cụ thể được trình bày và minh họa bằng animation). [12][13][14]  \n- Ví dụ filter giống Sobel: một filter có thể phát hiện cạnh theo chiều dọc/ngang — mỗi filter sinh ra một đặc trưng (feature) tương ứng; nhiều filter cho nhiều đặc trưng, xếp chồng thành feature map/tensor. [9]  \n- Ứng dụng thực tế: sau các block trích xuất đặc trưng (convolution+activation±pooling), tầng fully-connected dùng để phân lớp — ví dụ output là phân bố xác suất các lớp, trong đó class \"car\" có xác suất cao nhất nếu ảnh là chiếc xe. Pooling giúp giảm kích thước feature để giảm số lượng tham số trong tầng fully-connected, từ đó tiết kiệm chi phí tính toán và tránh overparameterization. [3][4]  \n- Trường hợp sử dụng stride/padding: khi muốn giảm độ phân giải (giảm phép tính, tăng invariance) dùng stride>1 hoặc pooling; khi muốn giữ kích thước đầu vào dùng padding phù hợp. [16][17][18]\n\n---\n\n## 4. Kết luận (Conclusion)\n- Tóm tắt các ý chính: Kiến trúc CNN gồm các tầng convolution (trượt filter tạo feature), activation (phi tuyến, thường ReLU), pooling (giảm kích thước feature) và fully-connected (phân lớp). Convolution yêu cầu filter.depth = input.depth (D), số lượng filter K quyết định độ sâu của output (output.depth = K). Stride và padding là hai tham số quan trọng ảnh hưởng kích thước feature map. Các ví dụ số học và minh họa (filter 3x3 trên ảnh 5x5, stride/padding) giúp hiểu rõ cơ chế. [1][2][3][6][11][15][16][17]  \n- Tầm quan trọng: Hiểu rõ từng thành phần này là nền tảng để thiết kế và phân tích kiến trúc CNN, tối ưu hóa cấu trúc (số filter, stride, padding, activation) và hiểu cách các thành phần tác động tới kích thước/đặc trưng đầu ra cũng như hiệu quả huấn luyện (ví dụ ReLU giảm vanishing gradient). [3][22][23]  \n- Liên hệ với các bài giảng khác: Tầng convolution đã được giới thiệu ở phần 1 (tham khảo phần 1) và ở đây tiếp tục mở rộng bằng công thức/ ví dụ cụ thể; các chủ đề như nguyên nhân sâu xa của *vanishing gradient* được nhắc tới như hướng tìm hiểu thêm. [6][22]\n\n---\n\nGhi chú: Tóm tắt trên sử dụng trực tiếp các nội dung và ví dụ được trình bày trong video (các chunk tương ứng đã được dẫn nguồn). Các ví dụ số học, minh họa stride/padding và nhận xét về việc chọn chiến lược padding đều được lấy từ nội dung giảng. [12][13][16][17][19]",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 0,
          "end_time": 60,
          "text": "về cấu tạo của một cái mạng convolutional neural network thì nó sẽ có những cái thành phần chính sau đây nếu như chúng ta tra cứu trên mạng internet chúng ta thấy là khi mà người ta vẽ một cái kiến trúc mạng CNN thì nó hay sử dụng cái dạng là hình khối ảnh đầu vào, ví dụ ở đây là ảnh một chiếc xe rồi nó sẽ biến đổi thành một cái khối thì cái khối này nó gọi là đặc trưng và nó được thực hiện bởi cái phép convolution đây là phép convolution và ngay sau phép convolution nó sẽ thực hiện cái phép preload rồi khi tạo ra cái feature này xong chúng ta sẽ thực hiện cái phép pooling để mà giảm cái kích thước của tấm hình này lại giảm kích thước của cái đặc trưng này lại và cứ như vậy, tuy nhiên thì khi chúng ta mới bắt đầu tìm hiểu cái kiến trúc này thì chúng ta sẽ hơi bị rối do đó thì ở đây chúng ta sẽ phân loại ra 4 cái phép biến đổi chính mà cái mạng CNN được sử dụng xuyên suốt trong"
        },
        {
          "index": 2,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 49,
          "end_time": 108,
          "text": "cái kiến trúc này thì chúng ta sẽ hơi bị rối do đó thì ở đây chúng ta sẽ phân loại ra 4 cái phép biến đổi chính mà cái mạng CNN được sử dụng xuyên suốt trong toàn bộ cái kiến trúc này 4 cái phép biến đổi đó chính là phép convolution phép activation tức là cái tương ứng là cái hàm kích hoạt tầng kích hoạt, rồi cái tầng pooling và cái tầng fully connected thì đây chính là 4 cái phép 4 cái tầng biến đổi chính và chúng ta sẽ phối hợp như thế nào, thông thường tất cả các cái tầng convolution và activation tầng kích hoạt nó sẽ đi chung với nhau thành 1 cặp tức là ngay sau convolution nó sẽ là cái tầng activation và tầng activation này thì thường người ta sử dụng cái hàm đó là hàm relu và như vậy nó sẽ đi theo cặp với nhau và cái cặp biến đổi convolution, activation này nó sẽ được thực hiện ca lần thì lâu lâu nó sẽ chọt vào 1 cái tầng pooling"
        },
        {
          "index": 3,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 98,
          "end_time": 161,
          "text": "và như vậy nó sẽ đi theo cặp với nhau và cái cặp biến đổi convolution, activation này nó sẽ được thực hiện ca lần thì lâu lâu nó sẽ chọt vào 1 cái tầng pooling chọt vào cái tầng pooling, mục tiêu của cái tầng pooling này nó để giảm cái kích thước của cái feature giảm kích thước của cái feature và khi giảm cái kích thước của cái feature thì sau này ở cái tầng fully-credited nó sẽ giảm cái số lượng tham số cái việc giảm cái số lượng tham số này thì nó sẽ có tác dụng gì thì chúng ta sẽ bàn luận sau và phối hợp các cái cặp convolution, activation và pooling này thì chúng ta sẽ lập n lần và cứ thực hiện đi thực hiện lại thì hết cái giai đoạn này thì nó sẽ gọi là grouptrip đặc trưng nó sẽ gọi là grouptrip đặc trưng và khi kết thúc cái giai đoạn grouptrip đặc trưng này nó sẽ đến cái tầng gọi là tầng fully-connected thì ở đây sẽ là tầng thực hiện cái công việc đó là phân lớp"
        },
        {
          "index": 4,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 148,
          "end_time": 210,
          "text": "nó sẽ gọi là grouptrip đặc trưng và khi kết thúc cái giai đoạn grouptrip đặc trưng này nó sẽ đến cái tầng gọi là tầng fully-connected thì ở đây sẽ là tầng thực hiện cái công việc đó là phân lớp phân lớp đặc trưng và các cái đặc trưng mà đã được thực hiện bởi 3 cái tầng trước thì qua đến cái tầng fully-connected này tầng này thật ra nó chính là cái mạng neural network đó chính là cái mạng neural network của mình rồi và cái tầng neural network này nhiệm vụ của nó sẽ là đi phân loại cái đặc trưng thì ở đây chúng ta sẽ có một cái sơ đồ ha đó là một cái ảnh một chiếc xe nó thực hiện phép convolution rồi lại relu và convolution relu sau đó lại pooling rồi sau đó convolution relu convolution relu rồi lại pooling thì ở đây tương ứng với lại cái tham số k và n ở đây á thì k của mình trong trường hợp này nó chính là 2 nghĩa là sao"
        },
        {
          "index": 5,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 200,
          "end_time": 259,
          "text": "convolution relu rồi lại pooling thì ở đây tương ứng với lại cái tham số k và n ở đây á thì k của mình trong trường hợp này nó chính là 2 nghĩa là sao chúng ta thực hiện 2 lần cái trập biến đổi convolution relu, convolution relu n này là bằng 3 có nghĩa là sao nguyên một cái bộ này chúng ta sẽ thực hiện 3 lần convolution relu convolution relu và pooling đây là một bộ rồi một bộ nữa và một bộ nữa như vậy là n trong trường hợp này là bằng 3 và khi thực hiện xong thì nó sẽ đến cái tầng fully connected để thực hiện cái phân bố lớp và cái output của mình đầu ra kỳ vọng nó sẽ ra một cái phân bố sát xuất trong đó cái phần car tức là chiếc xe nó cho cái phân bố cao nhất thì đây chính là một cái kiến trúc mạng CNN phổ dụng thế thì bây giờ tiếp theo chúng ta sẽ đến với cái cái"
        },
        {
          "index": 6,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 249,
          "end_time": 308,
          "text": "nó cho cái phân bố cao nhất thì đây chính là một cái kiến trúc mạng CNN phổ dụng thế thì bây giờ tiếp theo chúng ta sẽ đến với cái cái công thức biến đổi của từng cái lớp biến đổi này đầu tiên đó chính là tầng convolution tầng convolution như đã giới thiệu trong phần 1 thì bản chất của cái phép biến đổi convolution tức là chúng ta sẽ có một cái filter một cái filter và chúng ta sẽ trượt lên trên toàn bộ cái tấm ảnh này và ảnh đây lưu ý là ảnh sám ảnh ở đây là ảnh sám và khi chúng ta biến đổi xong thì chúng ta sẽ tạo ra một cái feature ở đây không phải là feature map mà là feature nha tức là chúng ta mới có một cái đặc trưng thôi rồi kết quả của cái phép biến đổi x với cái phép biến đổi convolution trên cái filter w thì nó sẽ tạo ra một cái feature thì đây là phép biến đổi convolution"
        },
        {
          "index": 7,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 298,
          "end_time": 362,
          "text": "rồi kết quả của cái phép biến đổi x với cái phép biến đổi convolution trên cái filter w thì nó sẽ tạo ra một cái feature thì đây là phép biến đổi convolution và điều gì xảy ra nếu như chúng ta thực hiện cái phép biến đổi convolution nhưng mà trên cái ảnh 3 cái màu là red green blue như vậy ở đây một cách tổng quát đó có thể là có cái độ sâu độ sâu trong trường hợp này nó sẽ là bằng 3 do là có 3 cái màu thì ở đây chúng ta sẽ sử dụng một cái cái filter chúng ta sẽ sử dụng một cái filter nó sẽ có cái độ sâu tương ứng với lại cái độ sâu của cái input thì đây chính là cái dữ liệu đầu vào còn đây là cái filter và cái filter này nó sẽ có độ sâu đúng bằng với lại cái độ sâu của cái input và khi chúng ta tưởng tượng cái filter này nó giống như là một cái cục Rubik chúng ta cũng sẽ trượt chúng ta sẽ trượt lên trên toàn bộ cái dữ liệu đầu vào này"
        },
        {
          "index": 8,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 347,
          "end_time": 409,
          "text": "và cái filter này nó sẽ có độ sâu đúng bằng với lại cái độ sâu của cái input và khi chúng ta tưởng tượng cái filter này nó giống như là một cái cục Rubik chúng ta cũng sẽ trượt chúng ta sẽ trượt lên trên toàn bộ cái dữ liệu đầu vào này cái feature app đầu vào này thì chúng ta sẽ tại một cái vị trí này chúng ta sẽ tính ra được một giá trị dịch chuyển tiếp thì chúng ta sẽ lại tính một giá trị tiếp theo dịch chuyển tiếp chúng ta sẽ dịch chuyển đến một cái vị trí mới chúng ta sẽ tính ra một cái giá trị cứ như vậy chúng ta sẽ tạo ra một cái feature như vậy đối với cái phép convolution nhưng mà trên cái dữ liệu đầu vào thay vì ảnh sám mà là ảnh red-blue thì cái độ sâu của philter của mình nó phải đúng bằng cái độ sâu của cái ảnh đầu vào và như vậy thì kết quả ở đây chúng ta sẽ có là cái kết quả cho một cái đặc trưng tức là một cái filter một cái filter thì chúng ta sẽ ra một cái đặc trưng giống như hồi nãy trong cái slide minh họa cho cái lọc sobel thì"
        },
        {
          "index": 9,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 400,
          "end_time": 461,
          "text": "một cái filter thì chúng ta sẽ ra một cái đặc trưng giống như hồi nãy trong cái slide minh họa cho cái lọc sobel thì cái đặc trưng của philter nó chính là nhận ra là theo dõi phần tương tương cho philter doesn't show up  của lọc show bell tương ứng là nó sẽ tìm ra cái cạnh theo cái chiều thẳng đứng theo cái chiều dọc vậy thì với mỗi filter chúng ta sẽ có một đặc trưng và nhiều filter nhiều filter chúng ta sẽ tạo ra nhiều đặc trưng thì ở đây với cạnh đầu vào nhân với lại filter màu vàng chúng ta sẽ tạo ra đặc trưng vàng với cái filter màu xanh chúng ta sẽ tạo ra đặc trưng xanh và cứ như vậy ở đây chúng ta có 4 cái filter thì tương ứng chúng ta sẽ có 4 cái đặc trưng và chúng ta sẽ trồng lớp 4 cái đặc trưng này lại với nhau thì nó sẽ tạo ra thành một cái tensor output sẽ tạo ra một cái tensor output và trồng cái cái filter này thì chúng ta sẽ tạo ra một cái khối 3D khối 3D này nó được gọi chính là tensor"
        },
        {
          "index": 10,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 452,
          "end_time": 509,
          "text": "sẽ tạo ra một cái tensor output và trồng cái cái filter này thì chúng ta sẽ tạo ra một cái khối 3D khối 3D này nó được gọi chính là tensor và tên của nó nó gọi là feature map trong cái slide trước trong slide trước thì cái này nó gọi là feature còn tập hợp các cái feature thì người ta sẽ gọi nó là feature map thì nếu như cái ảnh đầu vào của mình kích thước là 28 thì ảnh đầu ra kích thước nó sẽ còn 24 là tại vì sao tại vì khi chúng ta app khi chúng ta app cái filter chúng ta trượt lên đây thì không thể nào khi mà chúng ta app lên cái pin của cái tấm ảnh rồi chúng ta trượt đến đây và chúng ta sẽ chạm đến cái pin này và nó sẽ không lố ra bên ngoài nó sẽ không lố ra bên ngoài do đó nó sẽ bị thất thoát sẽ bị mất đi giảm từ 28 xuống còn 24"
        },
        {
          "index": 11,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 500,
          "end_time": 562,
          "text": "và nó sẽ không lố ra bên ngoài nó sẽ không lố ra bên ngoài do đó nó sẽ bị thất thoát sẽ bị mất đi giảm từ 28 xuống còn 24 thì đó là lý do tại sao nó giảm xuống và ở đây thì chúng ta chỉ cần nhớ đến một cái công thức liên quan đến với cái việc kích thước của cái filter nếu như cái ảnh đầu vào ảnh đầu vào trong trường hợp này có cái độ sâu là D là D trong trường hợp này bằng 3 thì cái filter của mình nó sẽ có cái độ sâu đúng bằng D luôn đúng bằng D luôn tức là bằng 3 và ở đây nếu như cái số lượng filter của mình số lượng filter của mình là K tổng quát là K trong trường hợp này K là bằng 4 thì cái độ sâu của cái tensor output của mình nó cũng chính là bằng K có bao nhiêu filter thì ở đây nó sẽ có bấy nhiêu cái độ sâu thì ở đây nó sẽ có bấy nhiêu cái độ sâu thì đây là một cái của quy luật chúng ta ráng nhớ cái công thức của nó"
        },
        {
          "index": 12,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 550,
          "end_time": 610,
          "text": "nó cũng chính là bằng K có bao nhiêu filter thì ở đây nó sẽ có bấy nhiêu cái độ sâu thì ở đây nó sẽ có bấy nhiêu cái độ sâu thì đây là một cái của quy luật chúng ta ráng nhớ cái công thức của nó và ở đây chúng ta sẽ có cái ví dụ tính toán số học ở đây chúng ta sẽ có cái phép biến đổi là convolution với cái dữ liệu ảnh đầu vào là ảnh 5x5 với cái giá trị như trên khi chúng ta app cái filter 3x3 lên đây thì chúng ta sẽ lần lượt lấy các cái giá trị này nhân với lại cái phần tự ở đây thì 0 nhân với trừ 1 nó sẽ ra là 0 và cứ như vậy 0 nhân với 25 nó sẽ ra là 0 và cứ nhân vô thì chúng ta sẽ có được cái kết quả như thế này và chúng ta sẽ lưu ý là phải thực hiện thêm một cái thao tác nữa là tổng tất cả các cái phần tự trên cái đây trên cái mask này đúng không thì 75 cộng 80 cộng 80 cộng 0 thì nó sẽ ra là 35 như vậy thì tại cái vị trí này"
        },
        {
          "index": 13,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 599,
          "end_time": 660,
          "text": "tổng tất cả các cái phần tự trên cái đây trên cái mask này đúng không thì 75 cộng 80 cộng 80 cộng 0 thì nó sẽ ra là 35 như vậy thì tại cái vị trí này tại cái vị trí này khi nhân với lại cái filter 3x3 này thì nó sẽ tạo ra một cái giá trị đó là 235 và chúng ta sẽ lần lượt trượt từ trái sang phải thì tương ứng ở đây chúng ta sẽ điền các cái giá trị ở đây và chúng ta sẽ có một cái animation để minh họa cho cái phép trượt này ảnh đầu vào sẽ là ảnh 5x5 và filter của mình là 3x3 thì chúng ta sẽ cho cái filter này trượt lên trên cái vị trí đầu tiên và chúng ta sẽ thấy rằng là cái ý nghĩa của cái filter này nó chính là những cái con số 0 này khi nhân với các cái giá trị trên cái điểm ảnh gốc thì nó sẽ trượt tiêu 0 này sẽ là trượt tiêu chỉ còn lại các cái số 1 này"
        },
        {
          "index": 14,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 648,
          "end_time": 711,
          "text": "thấy rằng là cái ý nghĩa của cái filter này nó chính là những cái con số 0 này khi nhân với các cái giá trị trên cái điểm ảnh gốc thì nó sẽ trượt tiêu 0 này sẽ là trượt tiêu chỉ còn lại các cái số 1 này như vậy ý nghĩa của cái filter này đó chính là tổng tất cả các cái giá trị màu của cả ảnh đầu vào theo cái trục dọc này thì ở đây chúng ta thấy là 0 nè cộng 0 nè, cộng 1 nè tương ứng nó sẽ là 1 như vậy thì các bạn sẽ thử tự điền vào các cái giá trị này xem nó là bao nhiêu khi trượt qua đây đúng không thì nó tương ứng sẽ là 3 cộng 4 cộng 1 cái ý nghĩa của filter này nó là cộng các cái thành phần ở trên cột ở giữa 3 cộng 4 cộng 1 nó sẽ ra là 8 0 cộng 2 cộng 0 nó sẽ ra là 2 rồi trượt xuống dưới 0 cộng 1 cộng 0 nó ra 1 cứ như vậy nó sẽ lấp đầy và lưu ý là ở đây ảnh đầu vào của mình là 3 x 3"
        },
        {
          "index": 15,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 700,
          "end_time": 761,
          "text": "rồi trượt xuống dưới 0 cộng 1 cộng 0 nó ra 1 cứ như vậy nó sẽ lấp đầy và lưu ý là ở đây ảnh đầu vào của mình là 3 x 3 nhưng mà khi chúng ta tính với cái filter này xong thì nó chỉ còn lại cái feature là kích thước là 3 x 3 thôi đầu vào là 5 x 5 thì output của mình chỉ là 3 x 3 thôi rồi ở đây chúng ta sẽ có thêm 1 cái tham số nữa đó là stride tức là cái độ dài của cái bước trượt filter thì ở đây nếu bình thường chúng ta sẽ trượt 1 đơn vị đúng không thì ở đây chúng ta sẽ có cái stride trong trường hợp này chúng ta sẽ cho stride là bằng bằng 2 tức là chúng ta sẽ nhảy khóc rồi ở trong trường hợp này ví dụ mà chúng ta đã làm trong slide trước thì stride là bằng 1 nhưng mà bây giờ chúng ta sẽ làm thử với stride bằng 2 với stride bằng 2 thì các cái giá trị ở đây là bao nhiêu thì cái mẹo để chúng ta có thể tính nhanh đó đó chính là chúng ta sẽ lấy các cái giá trị này"
        },
        {
          "index": 16,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 749,
          "end_time": 810,
          "text": "thì stride là bằng 1 nhưng mà bây giờ chúng ta sẽ làm thử với stride bằng 2 với stride bằng 2 thì các cái giá trị ở đây là bao nhiêu thì cái mẹo để chúng ta có thể tính nhanh đó đó chính là chúng ta sẽ lấy các cái giá trị này chúng ta điền xuống và nhảy khóc bỏ qua cái này rồi chúng ta lấy giá trị này chúng ta điền xuống nhảy khóc bỏ qua và điền xuống nhảy khóc bỏ cái này điền xuống như vậy khi chúng ta trượt rồi như vậy thì khi chúng ta trượt đó thì cái giá trị khi mà với cái bước nhảy stride bằng 2 nó sẽ là 1,2,2,5 và giá trị này hiểu 1 cách nôn na đó là nó copy từ cái Feature Map ở phía dưới cập xuống nhưng mà nó bỏ qua hàng và cột này tức là nó đang làm giảm nó đang làm giảm cái độ phân giải của Feature Map"
        },
        {
          "index": 17,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 798,
          "end_time": 861,
          "text": "cập xuống nhưng mà nó bỏ qua hàng và cột này tức là nó đang làm giảm nó đang làm giảm cái độ phân giải của Feature Map của Feature Map của đặc trưng của đặc trưng với cái bước nhảy là stride bằng 2 với cái bước nhảy là stride bằng 2 và chúng ta sẽ giảm khoảng 1 nửa rồi tiếp theo đó chính là Padding thì hồi nãy chúng ta đã nói rồi với 1 cái ảnh 5x5 sau khi nhân với lại cái Feature 3x3 thì nó sẽ giảm xuống là còn 3x3 thì nó sẽ giảm xuống là còn 3x3 nhưng mà chúng ta mong muốn là giữ nguyên cái thông tin của cái đặc trưng đầu vào giữ nguyên cái thông tin của đặc trưng đầu vào thì thay vì là giảm xuống còn 3x3 chúng ta mong muốn là không nó vẫn giữ nguyên cái kích thước gốc đầu vào là 5x5 thì ở đây chúng ta lấy 1 cái ví dụ nhỏ hơn chúng ta lấy 1 cái ví dụ nhỏ hơn để dễ tính ảnh đầu vào nếu như kích thước là 3x3 thì khi chúng ta lấy cái Feature 3x3 chúng ta trồng lên đây chúng ta thực hiện cái phép tính tổng ở đây đúng không"
        },
        {
          "index": 18,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 850,
          "end_time": 910,
          "text": "thì ở đây chúng ta lấy 1 cái ví dụ nhỏ hơn chúng ta lấy 1 cái ví dụ nhỏ hơn để dễ tính ảnh đầu vào nếu như kích thước là 3x3 thì khi chúng ta lấy cái Feature 3x3 chúng ta trồng lên đây chúng ta thực hiện cái phép tính tổng ở đây đúng không rồi tổng theo cái cột ở giữa nè thì 4 cộng 1 cộng 4 nó sẽ ra là 9 và kết thúc cái quá trình nhân Convolution và cái kích thước của mình nó giảm xuống chỉ còn là 1x1 thì mình không muốn cái điều này mình muốn là giữ nguyên cái kích thước đầu vào mình muốn giữ nguyên cái kích thước đầu vào thì nếu như ở đây ảnh của mình là 3x3 mình sẽ chèn thêm các cái giá trị ở bên ngoài vào thì nó sẽ tạo ra thành 1 cái ảnh là 5x5 với các cái giá trị Padding ở đây và lấy cái ảnh 5x5 này nhân với filter 3x3 thì nó sẽ tạo ra cái Feature Map là 3x3 như vậy là 3x3 đầu vào và đầu ra của mình nó vẫn giữ nguyên là 3x3 thì cái giá trị ở đây đó chính là cái giá trị Padding cái giá trị Padding"
        },
        {
          "index": 19,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 898,
          "end_time": 961,
          "text": "như vậy là 3x3 đầu vào và đầu ra của mình nó vẫn giữ nguyên là 3x3 thì cái giá trị ở đây đó chính là cái giá trị Padding cái giá trị Padding và mình có thể tạo ra  rất nhiều cái chiến thuật thì trong trường hợp này chúng ta đang chèn thêm cái số 0 vào viền của cái tấm ảnh nó sẽ có 1 số cái chiến thuật khác thì thật ra theo quan điểm của mình cái chiến thuật các bạn chọn Padding kiểu nào thì nó cũng không ảnh hưởng nhiều lắm đến cái kết quả cuối cùng tại vì sao? tại vì cái tấm ảnh của các bạn đó là 1 cái tấm ảnh rất là lớn và cái object của các bạn trong cái tấm hình này cũng là những cái object rất là lớn và cái việc đưa ra cái object của các bạn là cái quyết định phân loại cái nội dung của tấm ảnh này nó sẽ dựa trên cái phần ruột của tấm ảnh chứ còn cái phần biên của tấm ảnh này thì nó sẽ không đóng góp nhiều trong cái việc là đưa ra cái thông tin để mình phân loại cái dữ liệu bên trong này do đó cái phần ngoại biên này"
        },
        {
          "index": 20,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 949,
          "end_time": 1011,
          "text": "chứ còn cái phần biên của tấm ảnh này thì nó sẽ không đóng góp nhiều trong cái việc là đưa ra cái thông tin để mình phân loại cái dữ liệu bên trong này do đó cái phần ngoại biên này nó không quá quan trọng nó sẽ không quan trọng do đó các bạn dùng chiến thuật Padding nào cũng được 0 Padding chèn số 0 hoặc là Padding theo mỗi chiều nó sẽ có cái thước khác nhau ví dụ như chèn bên chiều phía trên là 2 nhưng mà bên chiều dọc thì nó lại chỉ chèn có 1 ở đây là chèn theo kiểu đó là lấy cái giá trị gần nhất để copy ra ví dụ như đây số 1 đúng không thì nó sẽ copy ra đây số 9 thì nó sẽ copy ra đây thì các cái chiến thuật chèn này thì theo mình đó là nó cũng không ảnh hưởng nhiều đến cái kết quả nhận diện cuối cùng và đến cái tầm nhìn của tấm ảnh này tầm tiếp theo đó chính là tầm Activation tầm Activation này thì đây là 1 cái tầm biến đổi phi tuyến"
        },
        {
          "index": 21,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 998,
          "end_time": 1061,
          "text": "nó cũng không ảnh hưởng nhiều đến cái kết quả nhận diện cuối cùng và đến cái tầm nhìn của tấm ảnh này tầm tiếp theo đó chính là tầm Activation tầm Activation này thì đây là 1 cái tầm biến đổi phi tuyến thì như chúng ta đã từng nhận xét trước đó cái phép Convolution này đó là cái phép biến đổi tuyến tính nếu như chúng ta thực hiện cái phép Convolution nối tiếp với 1 cái phép Convolution mà không có cái phép tuyến tính ở giữa thì không có 1 cái phép phi tuyến ở giữa thì đâu đó nó sẽ tạo ra thành 1 cái tổ hợp 1 cái tổ hợp tuyến tính mà thôi 1 cái tổ hợp tuyến tính mà thôi tức là tuyến tính rồi lại biến đổi tuyến tính thì nó sẽ tạo ra 1 cái tổ hợp tuyến tính mà cái tổ hợp tuyến tính thì nó sẽ không giải được nó sẽ không giải quyết được các cái bài toán phi tuyến các cái bài toán phi tuyến do đó thì chúng ta sẽ phải ngay sau cái phép tuyến đổi Convolution chúng ta phải có 1 cái tầm Activation phi tuyến thì trước đây người ta sử dụng cái hàm"
        },
        {
          "index": 22,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 1049,
          "end_time": 1110,
          "text": "các cái bài toán phi tuyến do đó thì chúng ta sẽ phải ngay sau cái phép tuyến đổi Convolution chúng ta phải có 1 cái tầm Activation phi tuyến thì trước đây người ta sử dụng cái hàm là hàm sigmoid nhưng mà gần đây thì khi cái khối lượng dữ liệu lớn khi cái kiến trúc mạng nó càng sâu hơn thì người ta nhận thấy rằng là đổi từ sigmoid sang relu thì sẽ giúp cho cái việc huấn luyện sẽ nhanh hơn cái việc huấn luyện sẽ nhanh hơn cái việc huấn luyện sẽ nhanh hơn và cái việc này đó là do chúng ta làm giảm cái hiện đường vanishing và cái việc này đó là do chúng ta làm giảm cái hiện đường vanishing radian thì đây sẽ là 1 cái chủ đề thêm để cho các bạn tìm hiểu về sao nhưng đại khí đó là với cái việc sử dụng tầm Activation là relu thì nó đã giúp cho mình giảm cái hiện tượng vanishing radian tức là tiêu biến cái đạo hàm đạo hàm của mình mà trong quá trình cập nhật mà nó càng lúc càng nhỏ"
        },
        {
          "index": 23,
          "video_id": "Chương 3_SKcHedTJIL0",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_1： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/SKcHedTJIL0",
          "start_time": 1097,
          "end_time": 1123,
          "text": "với cái việc sử dụng tầm Activation là relu thì nó đã giúp cho mình giảm cái hiện tượng vanishing radian tức là tiêu biến cái đạo hàm đạo hàm của mình mà trong quá trình cập nhật mà nó càng lúc càng nhỏ thì dẫn đến cái bước cập nhật của mình nó sẽ càng chậm thì Activation mà dùng hàm relu thì cái đạo hàm của mình nó sẽ bình tĩnh vì không có bị cái hiện tượng này và không bị cái hiện tượng này thì nó sẽ hỗn luyện và nhanh hơn"
        }
      ]
    },
    {
      "video_id": "Chương 3_A3iFEk5jllM",
      "summary": "## 1. Giới thiệu (Introduction)\n- Mục tiêu chính của bài giảng:\n  - Giải thích các **thành phần chính của mạng CNN** (convolution, activation — ReLU, pooling, flattening, fully connected) và vai trò của từng tầng trong pipeline trích xuất đặc trưng và phân lớp. [11][12][13][14]  \n- Các khái niệm sẽ được đề cập:\n  - Hàm activation ReLU và các biến thể, vị trí đặt activation trong CNN. [1][2]  \n  - Phép pooling (max/average), stride, tính *phi tham số* của pooling. [4][5][6]  \n  - Flattening trước fully connected và cấu trúc cuối cùng gồm các lớp FC + Softmax. [8][9][10][14][15]  \n  - Hàm loss (cross-entropy với softmax), tối ưu bằng gradient descent / backpropagation, và các tham số (theta) của CNN. [16][17][18]  \n  - Ứng dụng thực tế của CNN trong computer vision: classification, object detection, instance segmentation. [18][19][20]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 Activation — ReLU\n- Định nghĩa và công thức:\n  - Hàm ReLU: f(z) = max(0, z). Tức là mọi giá trị z < 0 được triệt tiêu về 0; các giá trị z ≥ 0 được giữ nguyên. [1]  \n- Vị trí trong kiến trúc:\n  - ReLU thường đặt ngay sau tầng convolution vì convolution là phép biến đổi tuyến tính nên cần một phép biến đổi **phi tuyến** sau đó. [1][2]  \n- Biến thể và hiệu ứng lên huấn luyện:\n  - Có thể thay thế ReLU bằng sigmoid, tanh, leaky ReLU, GELU... Tuy nhiên trong thực tế gần đây ReLU được dùng rộng rãi vì giúp mạng hội tụ nhanh hơn; thay sigmoid thường làm quá trình huấn luyện chậm hơn. [2]\n\n### 2.2 Ví dụ tính ReLU trên tensor\n- Bài tập minh họa:\n  - Cho input là tensor kích thước 3 x 3 x 2, áp dụng ReLU sẽ biến các phần tử âm thành 0, phần tử dương giữ nguyên — kết quả là một kernel/feature map tương ứng với việc loại bỏ các giá trị âm. (Ví dụ được mô tả trong bài tập trên video). [3]\n\n### 2.3 Pooling (Max pooling & Average pooling)\n- Khái niệm chung:\n  - Pooling là tầng *phi tham số* (không có tham số để huấn luyện); nhiệm vụ chính là giảm kích thước không gian (height × width) của feature map. [4]  \n- Cách hoạt động (ví dụ cụ thể):\n  - Với ảnh 4×4, filter 2×2 và stride = 2, kích thước giảm từ 4×4 → 2×2. [4]  \n  - Ví dụ một cửa sổ 2×2 chứa các giá trị [2,0,1,1]:\n    - max pooling → 2; average pooling → (2+0+1+1)/4 = 1. [5]  \n  - Các cửa sổ tiếp theo tương tự cho kết quả max = 4, avg = 2; max = 3, avg = 2; max = 5, avg = 3 (như minh họa trong video). [6]  \n- Quy tắc áp dụng:\n  - Pooling được áp dụng độc lập trên từng kênh (channel) của feature map; nếu đầu vào có độ sâu D thì output có cùng độ sâu D, chỉ khác về chiều ngang/chiều cao tùy stride. [7]\n\n### 2.4 Flattening và Fully Connected (FC)\n- Tại sao phải flatten:\n  - Các tầng convolution/reLU/pooling giữ đầu ra dạng tensor 3D (H×W×D). Tuy nhiên lớp fully connected (mạng neuron tiêu chuẩn) nhận đầu vào dạng vector 1D, nên cần bước *flatten* để biến tensor 3D → vector 1D. [8][9]  \n- Ví dụ flatten:\n  - Cho tensor kích thước 2×2×2 với các giá trị (mỗi \"lá cắt\" 2×2 có các phần tử như 0,1,0,1...), flatten sẽ nối các phần tử thành một vector 1D theo thứ tự (ví dụ: 0,1,0,1,...) để làm đầu vào cho FC. [9][10]  \n- FC và lớp đầu ra:\n  - Các lớp fully connected có thể xếp chồng nhiều lớp FC; lớp cuối thường là softmax để sinh phân phối xác suất cho các lớp (ví dụ: 3 node đầu ra cho 3 lớp nhà/người/cây). [15][10][11]  \n- Vai trò:\n  - FC ở phần cuối hoạt như một *máy phân lớp* nhận đặc trưng đã rút trích (từ convolution + ReLU + pooling) để phân lớp. [11][15]\n\n### 2.5 Toàn bộ pipeline và tham số của CNN\n- Kiến trúc điển hình:\n  - Input (thường ảnh màu depth = 3) → convolution (d filter) → ReLU → pooling → lặp lại (conv+ReLU+pool) → flatten → FC → (softmax). Qua mỗi conv với d filter sẽ tạo feature map có depth = d. Nếu pooling stride = 2 thì spatial dims giảm một nửa, còn depth được giữ nguyên. [12][13][14]  \n- Tham số (θ) của mạng:\n  - Những tham số cần tối ưu là trọng số của các tầng convolution và fully connected. Pooling không có tham số. Ví dụ trong minh họa: Theta1 (conv1), Theta2 (conv2), Theta3/Theta4 (các FC) là các tham số cần học. [18]  \n- Hàm loss và tối ưu:\n  - Hàm loss dùng là *cross-entropy* kết hợp softmax để đo sai khác giữa y_pred và y_true; tối ưu bằng gradient descent / backpropagation để tìm θ làm cho loss nhỏ nhất. (Backprop đã được framework triển khai để tối ưu θ). [16][17]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa trong video:\n  - Tính ReLU trên một tensor 3×3×2 để thấy các giá trị âm bị triệt tiêu (→0). [3]  \n  - Ví dụ pooling trên ảnh 4×4 với filter 2×2, stride 2: với cửa sổ [2,0,1,1] thì max pooling → 2, average → 1; các cửa sổ tiếp theo cho các kết quả max/avg tương ứng (4/2, 3/2, 5/3) như minh họa. [4][5][6]  \n  - Ví dụ flatten một tensor 2×2×2 thành vector 1D và dùng FC để phân lớp (ví dụ bài toán 3 lớp: nhà, người, cây). [9][10][11]\n\n- Ứng dụng thực tế:\n  - Phân loại ảnh (image classification). [18]  \n  - Phân lớp đối tượng (object classification/recognition). [18]  \n  - Định vị đối tượng / Phát hiện đối tượng (Object Detection): xác định vị trí (bounding box) của các đối tượng trong ảnh, có thể nhiều đối tượng cùng lúc. [18][19]  \n  - Instance Segmentation: cấp độ cao nhất của định vị/ phân vùng — khoanh vùng chính xác đến cấp pixel (trái ngược với bounding box của object detection). Tất cả các bài toán localize/detect/segment đều thường sử dụng kiến trúc CNN. [19][20]\n\n## 4. Kết luận (Conclusion)\n- Tóm tắt ý chính:\n  - Một mạng CNN tiêu chuẩn bao gồm chuỗi các tầng: convolution → activation (thường ReLU) → pooling (không có tham số) → (lặp lại) → flatten → fully connected → softmax. Các tham số θ nằm ở các tầng convolution và fully connected; tối ưu qua backprop/gradient descent dùng hàm loss cross-entropy. [1][4][7][8][15][16][18]  \n- Tầm quan trọng:\n  - Kiến trúc CNN là nền tảng cho hầu hết các ứng dụng nhìn máy (computer vision) hiện nay: classification, detection, segmentation do khả năng trích xuất đặc trưng cục bộ và chia sẻ trọng số. [11][18][19][20]  \n- Liên hệ với các bài giảng khác:\n  - Nội dung này là phần cụ thể hoá *feature extraction* (giai đoạn đầu) và *classification* (giai đoạn sau) trong thiết kế f_theta(x). Thiết kế f_theta(x) gồm 2 bước: (1) xây dựng kiến trúc (feature extractor + classifier) và (2) chọn hàm loss + tối ưu (cross-entropy + gradient descent / backprop). [15][16][17]\n\nNếu bạn muốn, tôi có thể:\n- Liệt kê lại từng chunk (timestamp) kèm tóm tắt ngắn nội dung mỗi chunk để dễ nhảy tới phần cụ thể.  \n- Hoặc mở rộng phần toán học (ví dụ công thức softmax và cross-entropy) nếu bạn cần công thức chi tiết để tính gradient.",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 0,
          "end_time": 60,
          "text": "thì đối với cái tầng activation thì chúng ta sử dụng hàm relu và cái công thức của cái hàm relu nó sẽ là bằng relu của hàm của z, z là đầu vào sẽ là bằng max của 0 và z thì hiểu một cách nôn đa đó là những cái dữ liệu z mà bé hơn 0 thì nó sẽ triệt tiêu đi, nó sẽ đưa về con số đó là 0 còn những cái dữ liệu z những cái giá trị đầu vào của mình là những cái giá trị lớn hơn 0 thì nó sẽ giữ nguyên nếu z mà lớn hơn 0 thì nó sẽ giữ nguyên hay hiểu một cách nôn đa relu này nó sẽ lọc những cái thông tin không cần thiết và chỉ trừa những cái thông tin quan trọng mà thôi rồi và cái tầng chúng ta có một cái lưu ý đó là trong cái mạng CNN thì cái tầng relu là thường phải ngay sau phải thường ngay theo sau tầng convolution tại vì cái thằng này là"
        },
        {
          "index": 2,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 49,
          "end_time": 111,
          "text": "và cái tầng chúng ta có một cái lưu ý đó là trong cái mạng CNN thì cái tầng relu là thường phải ngay sau phải thường ngay theo sau tầng convolution tại vì cái thằng này là convolution đó là tuyến tính mà ngay sau tuyến tính thì chúng ta phải có một cái phép biến nổi phi tuyến ngoài ra thì relu chúng ta có thể thay cho các cái hàm khác là hàm sigmoid, hàm tanh hàm blicky relu, global nhưng mà như chúng ta nói cái biến thể của cái mạng CNN mà trong những thời gian gần đây thì người ta rất hay sử dụng relu là vì nó giúp cho cái mạng mình hỗn luyện nhanh thì trong cái phần bài tập chúng ta sẽ có cái phần thử nghiệm của nó là thử nghiệm thay vì sử dụng relu chúng ta sẽ dùng sigmoid thì khi mà chúng ta đưa go với hàm sigmoid nó sẽ hỗn luyện rất là chậm nhưng mà nếu như chúng ta sử dụng cái hàm relu thì tốc độ hỗn luyện nó sẽ rất là nhanh rồi thì ở đây chúng ta sẽ có một cái bài tập để tính nháp trên cái phép biến nổi trên cái tầng activation này giả sử như chúng ta có một cái input"
        },
        {
          "index": 3,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 100,
          "end_time": 154,
          "text": "rồi thì ở đây chúng ta sẽ có một cái bài tập để tính nháp trên cái phép biến nổi trên cái tầng activation này giả sử như chúng ta có một cái input là một cái tensor 3 x 3 x 2 3 x 3 x 2 thì ở đây chúng ta sẽ có 2 lá cắt thì ở đây mỗi cái ma trận này nó tương ứng là một cái lá cắt thì chúng ta sẽ có các giá trị này và nếu như chúng ta nhân xin lỗi chúng ta thực hiện với cái tầng activation và hàm relu thì cái output của mình nó sẽ ra cái kernel như thế nào thì các bạn sẽ tính toán thử ha số 0 nó sẽ biến thành số 0 trừ 1 nó sẽ biến thành số 0 và cái output của mình nó sẽ ra cái kernel như thế nào   0 sẽ biến thành số 0 cứ như vậy số này là số dương đúng không nó sẽ diễn quy rồi"
        },
        {
          "index": 4,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 150,
          "end_time": 211,
          "text": "cứ như vậy số này là số dương đúng không nó sẽ diễn quy rồi thì đây là những cái số cái chữ màu đỏ đó chính là cái kết quả sau khi chúng ta thực hiện với lại cái phép biến nổi rectify linear unit relu tầng thứ 3 trong cái kiến trúc mạng cdn chính là cái tầng pooling thì cái pooling này là phi tham số phi tham số nghĩa là sao tức là chúng ta sẽ không có cái tham số để huấn luyện không có cái tham số huấn luyện nhiệm vụ của cái tầng pooling này nó chỉ đơn giản là để giảm cái kích thước của cái feature map của mình ví dụ trong trường hợp này chúng ta có một cái ảnh 4x4 khi áp dụng với cái filter 2x2 và với cái bức nhảy là 2"
        },
        {
          "index": 5,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 198,
          "end_time": 260,
          "text": "của cái feature map của mình ví dụ trong trường hợp này chúng ta có một cái ảnh 4x4 khi áp dụng với cái filter 2x2 và với cái bức nhảy là 2 thì đâu đó chúng ta sẽ thấy là ảnh 4x4 nó sẽ giảm xuống còn một cái ảnh kích thước là 2x2 và cái cách thức chúng ta sẽ thực hiện với hai cái phép biến nổi max pooling và average pooling max pooling là gì khi chúng ta khi chúng ta đưa cái filter này lên trên đây thì chúng ta sẽ lấy ra được 4 giá trị là 2011 và chúng ta sẽ thực hiện cái phép biến nổi là max thì 2011 giá trị lớn nhất là 2011 và cái phép biến nổi nhất của mình đó chính là 2 chúng ta sẽ điền 2 vào đây và 2011 mà cộng trung bình thì nó sẽ ra là 1 do đó thì giá trị lớn nhất max pooling thì tại đây nó sẽ ra là 2 nhưng mà average pooling thì ở đây nó sẽ ra là 1 rồi chúng ta sẽ trượt với"
        },
        {
          "index": 6,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 250,
          "end_time": 310,
          "text": "do đó thì giá trị lớn nhất max pooling thì tại đây nó sẽ ra là 2 nhưng mà average pooling thì ở đây nó sẽ ra là 1 rồi chúng ta sẽ trượt với cái bức nhảy strike là bằng 2 như vậy chúng ta bỏ qua cái ô này chúng ta bỏ qua ô này và đến đây thì chúng ta sẽ điền tiếp các giá trị max của nó sẽ là 4 và trung bình của nó sẽ là 2 rồi lại tiếp tục nhảy cóc vào đây max của nó sẽ là 3 trung bình sẽ là 2 rồi max sẽ là 5 và trung bình sẽ là 3 thì đây chính là cái phép biến nổi pooling và strike thì thường có kích thước bằng với lại cái kích thước của cái filter ví dụ như ở đây filter là 2 nhân 2 thì strike của mình nó sẽ là bằng 2 và các cái filter này thì được áp dụng độc lập áp dụng độc lập ví dụ như cái feature map đầu vào của mình nó sẽ có cái độ sâu là D"
        },
        {
          "index": 7,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 300,
          "end_time": 360,
          "text": "áp dụng độc lập ví dụ như cái feature map đầu vào của mình nó sẽ có cái độ sâu là D nó sẽ lấy cái kernel này nó sẽ áp dụng độc lập trên từng cái lá cắt feature này trên từng cái lá cắt feature này và sau đó nó sẽ tạo ra với cái phép pooling này nó sẽ tạo ra một cái feature map kích thước có cái độ sâu đúng bằng D luôn ví dụ ở đây là D thì ở đây đúng bằng D tại vì cứ một cái lá cắt bên đây nó sẽ tạo ra một lá cắt bên đây một cái lá cắt bên đây nó sẽ tạo ra một lá cắt bên đây còn kích thước của bề ngang bề cao thì có thể thay đổi nha do strike bằng 2 thì kích thước này nó có thể giảm xuống vào một nửa thôi rồi và cuối cùng đó chính là tầng fully connected thì trước khi thực hiện tầng fully connected này nó sẽ có một cái bước nó là flattening tại sao lại như vậy"
        },
        {
          "index": 8,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 349,
          "end_time": 408,
          "text": "rồi và cuối cùng đó chính là tầng fully connected thì trước khi thực hiện tầng fully connected này nó sẽ có một cái bước nó là flattening tại sao lại như vậy tại vì sao cái phép biến đổi convolution đúng không nó biến một cái tensor nó sẽ biến thành một cái tensor rồi cái phép relu cái hàm kích hoạt relu thì nó cũng sẽ biến đổi một cái tensor thành một cái tensor tensor thành một cái tensor  rồi cái phép biến đổi rồi cái phép biến đổi pooling pooling thì nó cũng sẽ biến đổi một cái tensor biến thành một cái tensor tuy nhiên cái tensor này thường nó sẽ có kích thước nhỏ hơn"
        },
        {
          "index": 9,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 399,
          "end_time": 460,
          "text": "thì nó cũng sẽ biến đổi một cái tensor biến thành một cái tensor tuy nhiên cái tensor này thường nó sẽ có kích thước nhỏ hơn thì suy cho cùng công sơn relu và pooling một chuỗi phối hợp các cái phép biến đổi này nó sẽ biến tensor thành một cái tensor rồi mà tensor thì nó không phải là cái dạng chuẩn đầu vào để mà cho chúng ta thực hiện với cái phép fully connected đây chính là cái mạng neuron đây chính là cái mạng neuron network mình như vậy thì chúng ta sẽ phải có một cái bước để chuyển đổi cái tensor này biến nó thành một cái vector để làm đầu vào cho cái mạng fully connected ở đây rồi thì ở đây chúng ta giả sử có một cái tensor kích thước là 2 x 2 x 2 và ở đây thì chúng ta sẽ cắt cái thằng này ra đúng không mỗi lá cắt chúng ta tạo ra ở đây"
        },
        {
          "index": 10,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 448,
          "end_time": 510,
          "text": "kích thước là 2 x 2 x 2 và ở đây thì chúng ta sẽ cắt cái thằng này ra đúng không mỗi lá cắt chúng ta tạo ra ở đây thì 0 1 0 1 mỗi cái lá cắt này chúng ta sẽ có các cái giá trị đầu vào như thế này thì flatten bản chất nó là dũi nó dũi một cái tensor 3D để tạo thành một cái tensor 1D tức là một cái vector thì số 0 chép qua đây, số 1 chép qua đây, rồi số 0 chép qua đây, số 1 chép qua đây và số 0 đó thì nó sẽ tạo thành vector và với cái vector này thì nó sẽ thực hiện cái phép biến đổi fully connected để tạo ra từ một cái vector tạo ra thành một cái vector khác thì trong trường hợp ví dụ như bài này chúng ta nhận dạng 3 lớp đó là nhà cửa nè, người nè cây nè đúng không thì ở đây nó sẽ có"
        },
        {
          "index": 11,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 498,
          "end_time": 558,
          "text": "để tạo ra từ một cái vector tạo ra thành một cái vector khác thì trong trường hợp ví dụ như bài này chúng ta nhận dạng 3 lớp đó là nhà cửa nè, người nè cây nè đúng không thì ở đây nó sẽ có 3 cái node đầu ra thì ở đây chúng ta sẽ có cái bộ tham số theta để phân loại cái đặc trưng đã rút trích được từ cái bước là convolution, reLU và fully connected, đây là cái đặc trưng và chúng ta sẽ đi qua cái fully connected này như là một cái máy phân lớp để phân lớp và tạo ra một cái neuron output thì đây chính là các cái thành phần để tạo ra một cái mạng cnn như vậy tổng kết thì mạng cnn nó sẽ kế thừa từ cái mạng neuron network và cái đầu tiên của nó đó là nó không có sử dụng cái phép biến đổi fully connected nó không có sử dụng cái phép biến đổi fully connected"
        },
        {
          "index": 12,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 549,
          "end_time": 608,
          "text": "từ cái mạng neuron network và cái đầu tiên của nó đó là nó không có sử dụng cái phép biến đổi fully connected nó không có sử dụng cái phép biến đổi fully connected nó sẽ không còn sử dụng cái cơ chế fully connected nó sẽ không còn sử dụng cái cơ chế fully connected nữa mà nó sẽ dùng cơ chế là chia sẻ trọng số và kết nối cục bộ thì bản chất của nó này đó chính là cái phép convolution rồi đồng thời cnn sẽ bao gồm các cái tầng biến đổi đó là tầng convolution activation pooling và kết nối đầy đủ thì sau đây mình sẽ vẽ một cái mạng cnn mà nó có cái sự kết nối giữa các cái tầng này và đương nhiên cái mạng cnn này thì chúng ta sẽ vẽ ở mức độ là đơn giản thôi đầu vào của mình nó sẽ có một cái tấm ảnh và thường ảnh này là ảnh màu và thường ảnh này là ảnh màu thì depth ở đây nó sẽ là bằng 3 qua cái phép biến đổi qua cái phép biến đổi"
        },
        {
          "index": 13,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 600,
          "end_time": 661,
          "text": "và thường ảnh này là ảnh màu và thường ảnh này là ảnh màu thì depth ở đây nó sẽ là bằng 3 qua cái phép biến đổi qua cái phép biến đổi  với d cái filter d cái filter thì chúng ta sẽ tạo ra một cái feature map có kích thước là d rồi sau đó chúng ta nếu mà chúng ta kết hợp cả cái convolution này cộng với lại relu luôn ha cộng với relu thì nó sẽ tạo ra một cái feature map như thế này rồi sau đó chúng ta thực hiện cái phép pooling thì nó sẽ tạo ra một cái feature map có cái bề ngang và bề cao nhỏ hơn một nửa nếu như stride là bằng 2 ha nó sẽ nhỏ hơn một nửa và cái độ sâu của mình nó cũng diễn nguyên nó là bằng d tại vì cái phép pooling này nó sẽ thực hiện độc lập trên từng cái kênh độc lập trên từng kênh"
        },
        {
          "index": 14,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 648,
          "end_time": 711,
          "text": "và cái độ sâu của mình nó cũng diễn nguyên nó là bằng d tại vì cái phép pooling này nó sẽ thực hiện độc lập trên từng cái kênh độc lập trên từng kênh do đó ở đây có d cái lá cắt thì qua bên đây nó sẽ là có d cái lá cắt rồi sau đó nó lại tiếp tục convolution kết hợp với lại relu nó sẽ tạo ra một cái tensor và cái số dựng tensor này nó có thể thay đổi do là cái số lượng filter của mình thay đổi nó sẽ ra là d phải đi rồi sau đó nó lại pooling rồi sau đó nó sẽ thực hiện cái này là phép pooling rồi sau đó nó sẽ thực hiện cái phép flatten để tạo ra thành một cái vector cái vector này chúng ta sẽ thực hiện cái phép biến đổi fully connected"
        },
        {
          "index": 15,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 700,
          "end_time": 760,
          "text": "để tạo ra thành một cái vector cái vector này chúng ta sẽ thực hiện cái phép biến đổi fully connected cái phép biến đổi fully connected fully connected phía tắc ha và lưu ý là cái phép fully connected này nó có thể kết hợp nhiều cái phép fully connected với nhau nhiều cái phép fully connected với nhau ví dụ như đây là một lớp nè chúng ta sẽ tạo ra thêm một lớp nữa nè rồi là fc nè rồi cái lớp cuối thì chúng ta sẽ qua cái hàng sortmax để tạo nó ra thành một cái vector nó thỏa mãn một cái phân bố sát xuất nó thỏa mãn một cái phân bố sát xuất rồi thì đây chính là cái bước số một rồi thì đây chính là cái bước số một  toàn bộ nãy giờ mình nói đó chính là cái bước số một trong cái việc thiết kế f theta x thế thì cái câu hỏi đó là cái bước số hai đúng không, cái bước số hai là hàm loss của mình"
        },
        {
          "index": 16,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 750,
          "end_time": 809,
          "text": "trong cái việc thiết kế f theta x thế thì cái câu hỏi đó là cái bước số hai đúng không, cái bước số hai là hàm loss của mình trong trường hợp này là như thế nào thì ở đây chúng ta có cái giá trị là y ngã là giá trị dự đoán và mình sẽ có cái thêm bước số hai cái giá trị y là giá trị thực tế và để 2 cái thằng này gần sắp xỉn với nhau thì chúng ta sẽ sử dụng 1 cái hàm loss theta và hàm loss này chúng ta sẽ sử dụng luôn đó chính là công thức growth entropy y chang như cái bài softmax y chang như cái bài softmax thì đây là toàn bộ cái mạng cnn khi chúng ta đã biến đổi thì cái giai đoạn đầu đó là feature extraction group trích đặc trưng rồi cái giai đoạn sau thì nó tương ứng đó là đi phân lớp"
        },
        {
          "index": 17,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 799,
          "end_time": 860,
          "text": "group trích đặc trưng rồi cái giai đoạn sau thì nó tương ứng đó là đi phân lớp các cái đặc trưng và nó sử dụng cái mạng neural network rồi khi chúng ta đã có cái loss này rồi chúng ta sẽ có cái loss này rồi thì chúng ta sẽ sử dụng cái thuật toán gradient descent và chúng ta sẽ có cái loss này rồi với cái tên gọi khác cho cái mạng cnn này đó là thuật toán back propagation và lưu ý đó là cái back propagation này thì đâu đó trong cái deep learning framework nó đã giúp cho chúng ta đi tối ưu tìm cái theta để cho cái hàm loss này là nhỏ nhất rồi ở đây chúng ta sẽ có 1 cái câu hỏi theta của mình nó là cái gì theta của mình là cái gì  đó thì theta của mình là cái gì    Theta của mình trong trường hợp này nó chính là những cái trọng số nè."
        },
        {
          "index": 18,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 848,
          "end_time": 911,
          "text": "tìm cái theta để cho cái hàm loss này là nhỏ nhất rồi ở đây chúng ta sẽ có 1 cái câu hỏi theta của mình nó là cái gì theta của mình là cái gì  đó thì theta của mình là cái gì    Theta của mình trong trường hợp này nó chính là những cái trọng số nè. Đó là những cái trọng số nè. Ví dụ đây là Theta 1 nè. Đến đây Pooling là không có tham số. Đến đây là Convolution. Chúng ta sẽ có là Theta 2 nè. Rồi Pooling không có tham số. Đến đây là FC đúng không? Chúng ta sẽ có là Theta 3 nè. Đến đây chúng ta sẽ có Theta 4 nè. Thì toàn bộ Theta 1, Theta 2 cho đến Theta 4 chính là những cái tham số của cái mạng CNN của mình. Và cái mạng CNN này nó có ứng dụng cực kỳ nhiều trong cái bài toán của lĩnh vực thị giác máy tính. Nó có ứng dụng trong bài toán là phân loại, phân lớp. Phân lớp đối tượng. Nó có tác dụng trong cái bài là định vị đối tượng. Tức là khi chúng ta đã phân lớp là biết trong cái tấm hình này nó đã có cái đối tượng tên là con mèo rồi."
        },
        {
          "index": 19,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 901,
          "end_time": 962,
          "text": "Phân lớp đối tượng. Nó có tác dụng trong cái bài là định vị đối tượng. Tức là khi chúng ta đã phân lớp là biết trong cái tấm hình này nó đã có cái đối tượng tên là con mèo rồi. Thì chúng ta sẽ cho biết là cái vị trí của con mèo này nó nằm ở đâu. Và chúng ta sẽ có thể dùng cái mạng CNN này để ứng dụng cho cái bài toán là Object Detection. Tức là phát hiện xem trong tấm hình này có những cái loại đối tượng gì. Đây là khu vực có hình con chó. Đây là khu vực có hình con vịt. Đây là khu vực có hình con mèo. Nó sẽ chỉ ra được cái vị trí. Và ở trong trường hợp Object Detection thì nó sẽ là nhiều Object. Có thể phát hiện cùng lúc nhiều Object. Và ở cấp độ cao nhất của cái việc định vị đối tượng á. Đó chính là Instant Segmentation. Tức là chúng ta sẽ khoanh vùng chính xác đến cái cấp độ là Pixel. Đối với Object Detection thì chúng ta phân vùng chính xác đến cái cấp độ là Bounding Box. Còn Instant Segmentation thì nó sẽ chính xác đến cái cấp độ là Pixel."
        },
        {
          "index": 20,
          "video_id": "Chương 3_A3iFEk5jllM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 2_2： Một số thành phần của mạng CNN",
          "video_url": "https://youtu.be/A3iFEk5jllM",
          "start_time": 949,
          "end_time": 977,
          "text": "Tức là chúng ta sẽ khoanh vùng chính xác đến cái cấp độ là Pixel. Đối với Object Detection thì chúng ta phân vùng chính xác đến cái cấp độ là Bounding Box. Còn Instant Segmentation thì nó sẽ chính xác đến cái cấp độ là Pixel. Và mạng CNN của mình cho đến bây giờ tất cả các cái mô hình Localize Object, Định vị Object. Rồi phát hiện đối tượng. Rồi phân đoạn ngữ nghĩa đối tượng. Thì đều sử dụng cái kiến trúc mạng CNN. Ơ."
        }
      ]
    },
    {
      "video_id": "Chương 3_KeNRQw9j_ps",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: cài đặt một mạng CNN (LeNet-like) và huấn luyện trên tập dữ liệu ảnh đầu vào kích thước 28×28. [1][2][3]  \n- Tập dữ liệu được đề cập: giảng viên nói \"Enix\" khi giới thiệu, và trong phần code sử dụng phương thức load từ Keras với tên 'mnix' (tương ứng với tập dữ liệu 28×28) để lấy về các biến x_train, y_train, x_test, y_test. [1][3][4]  \n- Các khái niệm sẽ được đề cập: kiến trúc mạng LeNet (hai lớp convolution + pooling + các lớp fully connected), kích thước filter (3×3), số filter ở từng lớp (6 và 16), phép pooling (max pooling 2×2), flatten → dense, chuẩn hóa dữ liệu, one-hot encoding, hàm mất mát và optimizer (categorical cross-entropy + Adam). [1][2][12][15][17][19][20]\n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 Kiến trúc mạng (LeNet-like) — tổng quan\n- Mạng gồm hai lớp convolution (sử dụng filter 3×3) và hai bước pooling giữa các convolution, kết thúc bằng các lớp fully connected để ra vector kích thước lần lượt 120, 84 và 10 (10 là số lớp output). [1][2][3]  \n- Cụ thể: convolution1 sử dụng 6 filter; convolution2 đầu ra là 16 filter (độ sâu đầu vào cho conv2 là 6 do output của conv1). [1][2]\n\n### 2.2 Tiền xử lý dữ liệu\n- Dữ liệu được load bằng keras.datasets.mnix.load_data() → trả về x_train, y_train, x_test, y_test. [3][4]  \n- Kích thước tensor đầu vào (ví dụ): x_train có shape 60000 × 28 × 28 (60.000 mẫu, mỗi ảnh 28×28). [4]  \n- Quan sát mẫu bằng matplotlib.pyplot.imshow với một index (ví dụ index=123) để hiển thị ảnh và in label tương ứng. [5][6]  \n- Chuẩn hóa ảnh: thay vì giá trị 0–255, đưa về miền 0–1 để giúp huấn luyện nhanh hơn. [6]  \n- Chuyển nhãn sang dạng one-hot encoding (ví dụ 10 lớp → vector one-hot độ dài 10). [6]\n\n### 2.3 Cài đặt model — cấu hình và tham số\n- Tham số hoá các giá trị cấu hình cho model: số filter conv1 = 6, conv2 = 16; số neuron của FC1 = 120, FC2 = 84; số lớp output cố định = 10. [8][9]  \n- Hàm kích hoạt mặc định được dùng cho các layer convolution và các FC giữa là sigmoid (theo thiết lập giảng viên, trừ output). [9][10]  \n- Output layer sử dụng activation = softmax cho bài toán phân lớp đa lớp (multiclass classification). [19]\n\n### 2.4 Cài đặt chi tiết các layer bằng Keras-like API\n- Input layer: khai báo shape = input_dimension (28×28). [10][11]  \n- Conv2D:  \n  - kernel_size = (3,3), filters = n (ví dụ 6 cho conv1). [11][12]  \n  - stride để mặc định là 1. [12]  \n  - padding = 'same' để giữ kích thước chiều ngang/dọc sau convolution (ví dụ 28→28, 14→14). [12]  \n  - bias = True và activation = function (ở đây là sigmoid theo mặc định). [13]  \n  - Kết quả đặt tên là C1 (hoặc tương tự) để nối tiếp pipeline. [13][14]\n- MaxPooling2D:  \n  - pool_size = (2,2) (mặc định), giảm kích thước mỗi chiều còn một nửa; padding = 'same'. [14][15]  \n  - Input cho pooling lần hai là C3 → output S4 theo sơ đồ. [15][16]\n- Conv2D thứ hai: copy cấu trúc conv1 nhưng thay filter count = ncon2 (16), kernel vẫn 3×3; framework tự tính chiều sâu input nên không cần khai báo 3×3×6 tường minh. [16]\n- Flatten trước khi qua Dense: gọi Flatten(S4) → đưa lên FC layer. [17]\n- Dense (fully connected):  \n  - FC đầu tiên units = FC1 (120), activation = function (sigmoid theo mặc định), use_bias = True; input là kết quả flatten. [17][18]  \n  - Tiếp tục Dense (FC5 → FC6) với units tương ứng (tập hợp FC1 = 120, FC2 = 84 trong sơ đồ). [18]  \n  - Output Dense: units = 10, activation = softmax (phân lớp đa lớp). [19]\n- Đóng gói model: model = Model(input, output). [19]\n\n### 2.5 Cấu hình huấn luyện (train)\n- Optimizer sử dụng: Adam (được đề xuất do hiệu quả trong tìm cực tiểu của loss). [20]  \n- Loss function: categorical cross-entropy (phù hợp bài toán phân lớp đa lớp với one-hot labels). [20]  \n- Metric đánh giá: accuracy. [20]  \n- Hàm/trợ giúp lấy trọng số các layer: model.layer[i].get_weights() (được giảng viên đề cập như cách truy xuất weights). [20]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa load và hiển thị mẫu: load dataset bằng keras.datasets.mnix.load_data(), lấy x_train, y_train; hiển thị ảnh tại index = 123 bằng plt.imshow(x_train[123]) và in label y_train[123] (ví dụ là số 7). [3][4][5][6]  \n- Thay index (ví dụ 10000) sẽ hiển thị ảnh khác và in label tương ứng (ví dụ 3) — minh họa cách kiểm tra dữ liệu thủ công trước khi huấn luyện. [6]  \n- Ứng dụng thực tế: bài giảng rõ ràng hướng tới bài toán nhận dạng chữ số (10 lớp: 0–9) với mạng LeNet giản lược, dùng như bài demo/benchmark trong thị giác máy tính cho ảnh kích thước nhỏ 28×28. [1][2][3][19]\n\n---\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt: Bài giảng hướng dẫn cài đặt một mạng CNN kiểu LeNet với: conv(3×3,6) → pool(2×2) → conv(3×3,16) → pool(2×2) → flatten → dense(120) → dense(84) → dense(10, softmax). Dữ liệu được load từ Keras, chuẩn hóa về [0,1], nhãn chuyển sang one-hot; huấn luyện với optimizer Adam và loss categorical cross-entropy. [1][2][3][4][6][12][15][17][19][20]  \n- Tầm quan trọng: minh họa đầy đủ pipeline từ load dữ liệu → tiền xử lý → xây dựng kiến trúc → compile model → huấn luyện; là ví dụ tiêu chuẩn để học cách triển khai CNN nhỏ cho bài toán phân lớp ảnh nhỏ. [3][6][19][20]  \n- Liên hệ với các bài giảng khác: nội dung là phần triển khai thực hành (cài đặt/huấn luyện) cho kiến thức về CNN đã được giới thiệu trước đó (LeNet, convolution, pooling, fully connected) — giảng viên nhắc tới thiết kế mạng LeNet (từ 1998) làm nền tảng. [1][2]\n\nGhi chú: tóm tắt trên hoàn toàn dựa vào nội dung các đoạn trích trong video (như đã cung cấp) và trích dẫn tương ứng sau mỗi thông tin. [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21]",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 0,
          "end_time": 61,
          "text": "Trong phần tiếp theo thì chúng ta sẽ cài đặt một cái mạng Corrational Network và tập dữ liệu mà chúng ta sẽ sử dụng ở đây chính là tập dữ liệu Enix thì đây là một trong những tập dữ liệu rất là kinh điển khi làm trong lĩnh vực về thị giác máy tính ảnh đầu vào của cái tập dữ liệu này sẽ có kích thước là 28 x 28 đúng bằng kích thước ở đây và cái kiến trúc mạng CNN ở đây thì chúng ta sẽ sử dụng đó là kiến trúc mạng Linux được có từ nguồn năm 1998 và kiến trúc mạng này thực sự mà nó không có sâu nó chỉ bao gồm 2 cái lớp convolution và 2 lớp convolution này thì có sử dụng các cái filter có kích thước là 3 x 3 và đối với cái lớp convolution đầu tiên thì chỉ có 6 cái phép convolution 6 cái filter đối với lớp convolution thứ 2 thì sẽ có cái kích thước đó là 3 x 3 x 6 tại vì cái đầu vào của cái lớp convolution này chính là cái cái lớp convolution này  feature map này mà feature map này có cái depth là bằng 6 vậy đó đây sẽ là 6"
        },
        {
          "index": 2,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 45,
          "end_time": 113,
          "text": "đối với lớp convolution thứ 2 thì sẽ có cái kích thước đó là 3 x 3 x 6 tại vì cái đầu vào của cái lớp convolution này chính là cái cái lớp convolution này  feature map này mà feature map này có cái depth là bằng 6 vậy đó đây sẽ là 6 tuy nhiên thì trong quá trình mà chúng ta cài đặt thì chúng ta cũng không cần phải chỉ ra tường minh là cái số input của mình là bao nhiêu tự cái chương trình nó sẽ tự cái deep learning framework nó sẽ tính cho mình cái con số này chúng ta chỉ cần cho biết cái kích thước bề ngang bề cao của cái filter là được và đồng thời chúng ta cũng cho cái deep learning framework biết số filter đầu ra mong muốn là trong cái phép convolution thứ 2 chính là 16 các cái phép biến đổi subsampling ở đây thực chất nó chính là cái phép biến đổi max pooling đó chính là cái phép biến đổi max pooling rồi và phần cuối của mạng cnn này đó chính là các cái lớp biến đổi fully connected để tạo ra các cái vector có kích thước là 120 84 và 10 trong đó 10 thì tương ứng với lại cái số lớp đầu ra của mình"
        },
        {
          "index": 3,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 97,
          "end_time": 162,
          "text": "rồi và phần cuối của mạng cnn này đó chính là các cái lớp biến đổi fully connected để tạo ra các cái vector có kích thước là 120 84 và 10 trong đó 10 thì tương ứng với lại cái số lớp đầu ra của mình đó chính là các cái con số từ 0 cho đến 9 bây giờ chúng ta sẽ tự ch grips hội địa điểm và đây là cái phấn quantum tAy ES索game này pull em thanh nạp Đoạn đầu tiên thì chúng ta sẽ khởi tạo các cái data set của mình thì trong keras nó đã có phương thức giúp cho mình load data set rất là dễ dàng đó là keras.dataset và chúng ta sẽ import tập dữ liệu là mnix sau đó chúng ta chỉ việc gọi là mnix.load data thì tự động nó sẽ lấy từ trên mạng internet về giải nén"
        },
        {
          "index": 4,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 149,
          "end_time": 208,
          "text": "load data set rất là dễ dàng đó là keras.dataset và chúng ta sẽ import tập dữ liệu là mnix sau đó chúng ta chỉ việc gọi là mnix.load data thì tự động nó sẽ lấy từ trên mạng internet về giải nén và đưa vào các cặp biến là xtreme, etreme và xtest, etest thì ở đây chúng ta sẽ quan sát thử kích thước của các biến này xtreme.set thì có kích thước là 60.000 x 28.000 x 28.000 thì 60.000 này tương ứng là tổng số mẫu còn 28.000 x 28.000 đó chính là cái kích thước bề ngang và bề cao của cái hạng triệu số big time etreme.set thì nó sẽ có kích thước là 60.000 thì ứng với từng cái xtreme nó sẽ có một cái giá trị label cái nhãn output của etreme thì ở đây chúng ta sẽ thử quan sát một số cái mẫu dữ liệu để quan sát thì chúng ta sẽ sử dụng thư viện đó là map.lib map.lib.pyplot"
        },
        {
          "index": 5,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 198,
          "end_time": 260,
          "text": "thì ở đây chúng ta sẽ thử quan sát một số cái mẫu dữ liệu để quan sát thì chúng ta sẽ sử dụng thư viện đó là map.lib map.lib.pyplot s.plt rồi plt. chúng ta sẽ sử dụng cái hàm đó là imso hàm imso với cái biến là xtreme và xtreme này thì nó có các cái phần trực hợp cái chiều đó là 60.000 x 28.000 x 28.000 thì ở chiều đầu tiên chúng ta sẽ lấy ra tại một cái vị trí nào đó đó là index và trong trường hợp này thì chúng ta sẽ cho index là bằng 123 cái con số bất kỳ trong khoảng từ 1 từ 0 cho đến 60.000 rồi thành phần vài loại thì sẽ là 2.2. tức là chúng ta sẽ lấy toàn bộ cái nội dung của tấm ảnh ra để chúng ta hiển thị rồi sau đó chúng ta sẽ thực hiện thì thấy là cái ảnh này mình đoán đoán nó hình như là số 7 thì muốn biết chính xác đó là nhãn bao nhiêu thì chúng ta sẽ in ra là"
        },
        {
          "index": 6,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 250,
          "end_time": 310,
          "text": "rồi sau đó chúng ta sẽ thực hiện thì thấy là cái ảnh này mình đoán đoán nó hình như là số 7 thì muốn biết chính xác đó là nhãn bao nhiêu thì chúng ta sẽ in ra là nhãn của dữ liệu rồi ở đây chúng ta sẽ lấy là i trend và chúng ta cũng sẽ truyền vào cái chỉ số là i index rồi đúng như dự đoán thì cái nhãn này chính là nhãn của dữ liệu này chính là số 7 và chúng ta có thể thay đổi các cái chỉ số này ví dụ như là 10.000 rồi đó thì đây là tương ứng của dữ liệu này là số 7.000  và tương ứng nhãn của nó sẽ là số 3 rồi bước tiếp theo đó là chúng ta sẽ tiền xử lý chúng ta sẽ chuẩn hóa cái dữ liệu x trend và x test của mình bằng cách đó là thay vì đưa cái miền giá trị từ 0 đến 255 thì chúng ta sẽ đưa về cái miền giá trị là từ 0 cho đến 1 để giúp cho cái quá trình huấn luyện nó được nhanh hơn và đồng thời là cái giá trị i của mình cũng sẽ được"
        },
        {
          "index": 7,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 299,
          "end_time": 365,
          "text": "từ 0 đến 255 thì chúng ta sẽ đưa về cái miền giá trị là từ 0 cho đến 1 để giúp cho cái quá trình huấn luyện nó được nhanh hơn và đồng thời là cái giá trị i của mình cũng sẽ được chuyển đổi từ cái dạng nhãn và các chỉ số từ 0 cho đến 9 chúng ta sẽ đưa nó về cái dạng one hot encoding cái dạng one hot encoding thì one hot encoding nó nghĩa là gì là ví dụ số 0 thì chúng ta sẽ đưa một cái vector trong đó có duy nhất một cái phần tử bật lên là 1 và tất cả các cái phần tử còn lại sẽ để là số 0 và tương tự như vậy cho số 2 đi thì nó sẽ bật lên là 0 ở đây là 0 ở đây là 0 và nó sẽ bật lên ở đây và tất cả các cái phần tử còn lại sẽ để là số 0 thì đây là cái dạng one hot encoding rồi bước tiếp theo đó là chúng ta sẽ tiến hành cài đặt cái thực toán huấn luyện hay cụ thể đó là cài đặt cái mô hình thì cái mạng cnn ở đây chúng ta sẽ có các cái phương thức như là build rồi trend rồi constructor v.v"
        },
        {
          "index": 8,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 349,
          "end_time": 412,
          "text": "rồi bước tiếp theo đó là chúng ta sẽ tiến hành cài đặt cái thực toán huấn luyện hay cụ thể đó là cài đặt cái mô hình thì cái mạng cnn ở đây chúng ta sẽ có các cái phương thức như là build rồi trend rồi constructor v.v load gateway v.v thì ở đây có cái phương thức gateway là chúng ta sẽ chưa cài đặt chúng ta sẽ cài đặt và đưa lên trên xong hành cùng với lại cái hầm trend kẹo chúng ta quên và sau đó chúng ta sẽ đưa lên trend ngang với lại phương thức là build không lượt tiếp nữa chúng ta sẽ quên thì cái quá trình trend của mạng CNN rất là lâu nếu mà chúng ta quên thực hiện cái gì đấy và chúng ta thực hiện lại thì nó sẽ tốn thời gian rất là nhiều thì ở đây chúng ta sẽ phải cho cái model nó biết đó là input dimension rồi đồng thời là các cái cấu hình ví dụ như số lượng filter nè là 6 nè số lượng filter là 16 nè rồi số các cái output của các lớp fully connected là 120 84"
        },
        {
          "index": 9,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 398,
          "end_time": 462,
          "text": "rồi đồng thời là các cái cấu hình ví dụ như số lượng filter nè là 6 nè số lượng filter là 16 nè rồi số các cái output của các lớp fully connected là 120 84 thì chúng ta sẽ phải tham số hóa 4 cái bộ số này riêng cái con số cuối cùng đó là 10 đó chính là số lượng cái nhãn mà mình cần nhận diện rồi thì nó sẽ cố định là 10 tại vì mình ý trước tập dữ liệu này là là có 10 mẫu 10 loại 10 nhãn 10 class và đồng thời thì chúng ta cũng sẽ tham số hóa cái hàm kích hoạt activation function rồi activation function rồi chúng ta sẽ có convolution số 1 convolution số 2 rồi NFC 1 NFC 2 và mặc nhiên thì hàm activation chúng ta sẽ để là sigmoid chúng ta sẽ để là sigmoid rồi convolution thì mặc nhiên chúng ta sẽ để là sigmoid"
        },
        {
          "index": 10,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 452,
          "end_time": 509,
          "text": "và mặc nhiên thì hàm activation chúng ta sẽ để là sigmoid chúng ta sẽ để là sigmoid rồi convolution thì mặc nhiên chúng ta sẽ để là sigmoid  giống như trong cái thiết kế ở đây convolution số 2 thì mặc nhiên chúng ta sẽ để là 16 FC 1 thì chúng ta sẽ để là 120 và FC 2 chúng ta sẽ để là 84 rồi sau đó thì chúng ta sẽ tiến hành cài đặt cái các cái thành phần của cái mạng này bằng cách đó là chúng ta sẽ tiến hành lần lượt qua các cái lớp đồng hồ này  bằng cách đó là chúng ta sẽ tiến hành lần lượt qua các cái lớp đồng hồ này lớp đầu tiên chính là cái lớp input input rồi input thì chúng ta sẽ cho biết cái shape cái shape của nó sẽ là bằng input input dimension"
        },
        {
          "index": 11,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 500,
          "end_time": 560,
          "text": "rồi input thì chúng ta sẽ cho biết cái shape cái shape của nó sẽ là bằng input input dimension rồi và chúng ta sẽ trả ra cái biến tên là input cái biến tên là input rồi tương tự như vậy thì chúng ta sẽ tiến hành tiến hành thực hiện thực hiện cái phép biến đổi convolution thì ở đây là convolution chúng ta sẽ sử dụng convolution 2D và nó sẽ có các cái tham số đầu tiên là số lượng filter thì chúng ta sẽ để là số lượng convolution số 1 cái tham số thứ hai là kernel size thì như hồi nãy chúng ta đề cập đó là kích thước của kernel size này chính là kích thước của này nó sẽ là 3x3 3x3 3x3"
        },
        {
          "index": 12,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 552,
          "end_time": 609,
          "text": "kích thước của này nó sẽ là 3x3 3x3 3x3 rồi stride thì ở đây chúng ta sẽ để mặc định là 1 chúng ta sẽ không để cái stride ở đây rồi padding thì chúng ta sẽ để là send tại vì trong cái shadow này chúng ta thấy trong shadow này chúng ta thấy là ảnh đầu vào và ảnh đầu ra có kích thước giống nhau, ảnh đầu vào là 28, 28 thì ảnh đầu ra là 28, 28 ảnh đầu vào là 14, 14 thì ảnh đầu ra cũng sẽ là 14, 14 thì qua cái phép biến đổi convolution thì chúng ta thấy là cái kích thước bề ngang và bề cao là không thay đổi khi thực hiện cái phép convolution giờ đó chúng ta sẽ để padding là mặc send rồi, đây thì chắc là mình sẽ phải"
        },
        {
          "index": 13,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 597,
          "end_time": 661,
          "text": "cái kích thước bề ngang và bề cao là không thay đổi khi thực hiện cái phép convolution giờ đó chúng ta sẽ để padding là mặc send rồi, đây thì chắc là mình sẽ phải điền cái bias bias là bằng true bias là bằng true rồi thì cơ bản nó là, nó đã đầy đủ những cái à nó còn thiếu 1 cái nữa đó là cái activation activation activation này sẽ để trước bias bias sẽ là bằng function function rồi rồi như vậy thì chúng ta đã cài đặt cho cái đối tượng tên là convolution 2D và chúng ta sẽ phải truyền vào"
        },
        {
          "index": 14,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 654,
          "end_time": 711,
          "text": "rồi như vậy thì chúng ta đã cài đặt cho cái đối tượng tên là convolution 2D và chúng ta sẽ phải truyền vào và cho nó là cái input và trả ra nó sẽ ra là cái biến tên là c1 giống như trong cái shadow ở đây giống như trong cái shadow ở đây rồi tiếp theo thì chúng ta sẽ thử chạy ha ok nó sẽ báo lỗi à 3 x 3 ok nó không hiểu 3 x 3 là gì 3.3 rồi hết lỗi rồi bây giờ chúng ta sẽ thực hiện cái phép pooling pooling thì tương ứng đó chính là cái max pooling 2D ở đây ha và chúng ta sẽ có cái tham số là pool size thì thì bằng bản nhiên thì nó sẽ sử dụng 2 x 2 do đó thì một cách tự minh chúng ta sẽ để ở đây là 2 x 2 thì với cái pooling bằng 2 x 2 như thế này thì cái kích thước của mình"
        },
        {
          "index": 15,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 699,
          "end_time": 769,
          "text": "2 x 2 do đó thì một cách tự minh chúng ta sẽ để ở đây là 2 x 2 thì với cái pooling bằng 2 x 2 như thế này thì cái kích thước của mình nó sẽ được giảm xuống một nửa size ở đây thì chúng ta sẽ để size là bằng 1 x 2 để sau khi thực hiện cái pooling này thì cái kích thước của nó sẽ giảm xuống một nửa rồi ngoài ra thì có pattern thì chúng ta sẽ để là same rồi và chúng ta sẽ truyền cái đầu vào cho nó chính là C1 và đầu ra sẽ là S2 giống như trong cái phiên bản trong cái kế hoạch ở đây rồi đối với cái phép biến đổi compression tiếp theo chúng ta sẽ copy số nhưng mà khi copy thì cần phải lưu ý là sửa lại thay vì ở đây để là input thì nó sẽ để là S, S2 rồi và đầu ra sẽ là C, C3 và số compression ở đây số filter ở đây nó sẽ là"
        },
        {
          "index": 16,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 749,
          "end_time": 806,
          "text": "ta sẽ copy số nhưng mà khi copy thì cần phải lưu ý là sửa lại thay vì ở đây để là input thì nó sẽ để là S, S2 rồi và đầu ra sẽ là C, C3 và số compression ở đây số filter ở đây nó sẽ là ncon2 kích thước không thay đổi lưu ý là hồi nãy kích thước là 3x3 và nó sẽ tự biết cái input s2 kích thước là 3x3 và nó sẽ tự viết cái input s2 kích thước là cyp Ncon2, nếu chúng ta xong rồi thì  cái depth là bao nhiêu thì nó sẽ chọn cái filter cho nó phù hợp cho đó chúng ta không cần phải tường minh để chỉ ra là kích thước 3x3x3 rồi activation thì chúng ta cũng tái sử dụng lại tiếp theo nó sẽ chuyển sang cái phép là pooling rồi thì đầu vào chúng ta sẽ có là C3 và nó sẽ tạo ra là S4 và cái cấu hình thì cũng tương tự"
        },
        {
          "index": 17,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 798,
          "end_time": 856,
          "text": "rồi thì đầu vào chúng ta sẽ có là C3 và nó sẽ tạo ra là S4 và cái cấu hình thì cũng tương tự rồi bây giờ chúng ta sẽ tiếp tục cài đặt cho cái phép biến đổi fully connected thì để thực hiện được cái fully connected này chúng ta sẽ phải có một cái bước là flatten thì chúng ta sẽ gọi cái đối tượng flatten ở đây và truyền vào cái S4 để trả ra là FC4 ở đây thì nó sẽ đặt tên là FC4 đi ha rồi tại vì thực sự mà nó phép thì chúng ta sẽ có một cái bước là flatten nó không có biến đổi gì hết tiếp theo thì chúng ta sẽ thực hiện cái phép fully connected nó chính là dense rồi và tham số đầu tiên là số lượng unit tức là số lượng output neuron sẽ trả ra thì chúng ta sẽ lấy cái tham số FC1 này đưa vào rồi"
        },
        {
          "index": 18,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 848,
          "end_time": 910,
          "text": "tức là số lượng output neuron sẽ trả ra thì chúng ta sẽ lấy cái tham số FC1 này đưa vào rồi activation thì chúng ta sẽ để là function rồi chúng ta sẽ đặt chữ là fc3 rồi use bias bằng true và chúng ta sẽ truyền vào cái biến đó là FC4 thì trong cái sơ đồ ở đây nó để là C5 nhưng mà để đúng với nãy cái cái tên của nó đó là fully connected thì chúng ta sẽ đặt tên lại đó là FC5 tương tự như vậy cho cái miếng đổi tiếp theo chúng ta sẽ để đầu vào là FC5 đầu ra sẽ là FC6 và số neuron đầu ra sẽ là FC2 rồi cuối cùng đó chính là output"
        },
        {
          "index": 19,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 900,
          "end_time": 963,
          "text": "đầu ra sẽ là FC6 và số neuron đầu ra sẽ là FC2 rồi cuối cùng đó chính là output thì FC6 sẽ được chuyển vào đây và đầu ra sẽ là output và số neuron của mình sẽ là 10 tại vì mình biết trước cái đầu ra của mình sẽ là 10 class riêng cái hạm activation function thì chúng ta sẽ phải để là softmax tại vì đây là phân lớp đa lớp chứ không phải là phân lớp nhịp văn nếu mà phân lớp nhịp văn thì chúng ta sẽ sử dụng sigmoid rồi cuối cùng thì chúng ta sẽ đóng gói toàn bộ cái input và output trong cái miếng tên là model cell.model sẽ là bằng model rồi input và output rồi rồi như vậy thì chúng ta đã cài xong cho cái phần build cái mô hình đối với cái hàm trend thì chúng ta sẽ sử dụng optimizer là ADAM thì đây là một trong những cái optimizer rất là hiệu quả"
        },
        {
          "index": 20,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 950,
          "end_time": 1002,
          "text": "rồi rồi như vậy thì chúng ta đã cài xong cho cái phần build cái mô hình đối với cái hàm trend thì chúng ta sẽ sử dụng optimizer là ADAM thì đây là một trong những cái optimizer rất là hiệu quả nó giúp cho chúng ta track ra được cái điểm cực tiểu của bộ hàm loss thì chúng ta sẽ sử dụng chúng ta sẽ sử dụng là cross entropy categorical cross entropy tức là chúng ta thực hiện phân lớp nhiều lớp rồi độ đo thì chúng ta sẽ sử dụng độ đo để đánh giá là accuracy về weight thì chúng ta sẽ trả về cell.model.layer và chúng ta sẽ truyền vào cái chỉ số của cái layer mà mình muốn trả về xong rồi gọi hàm get weight rồi như vậy thì chúng ta đã cài xong cái mạng cnn"
        },
        {
          "index": 21,
          "video_id": "Chương 3_KeNRQw9j_ps",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_1： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/KeNRQw9j_ps",
          "start_time": 998,
          "end_time": 1002,
          "text": "rồi như vậy thì chúng ta đã cài xong cái mạng cnn"
        }
      ]
    },
    {
      "video_id": "Chương 3_TNrJYPuDADM",
      "summary": "## 1. Giới thiệu (Introduction)\n- Mục tiêu chính của bài giảng: Hướng dẫn **cài đặt và thử nghiệm một mô hình CNN**, bao gồm việc định nghĩa kiến trúc, chạy training, trực quan hóa filter và so sánh các biến thể (activation, pooling) để hiểu vai trò các module trong mạng CNN. [1][3][12][13]  \n- Các khái niệm sẽ được đề cập: cấu hình kiến trúc (input dim, số filter, fully-connected layers), inisialisasi/kiểm tra cấu trúc bằng summary, quá trình training và lưu history để vẽ loss, trực quan hóa filter (weights), hàm predict và tiền xử lý input, cũng như thực nghiệm biến thể (thay sigmoid → ReLU, bỏ pooling). [1][2][3][4][6][8][12][13][15]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 Định nghĩa kiến trúc CNN (cài đặt tham số và build)\n- Input dimension: ảnh đầu vào đặt là 28×28×1 (tensor 3 chiều: height, width, channel). [1]  \n- Activation mặc định ban đầu: *sigmoid*. [1]  \n- Cấu hình các lớp convolution và fully-connected: Conv1: 6 filter; Conv2: 16 filter; FC output: 1; FC1: 120; FC2: 84; activation cho FC là *sigmoid*. [1]  \n- Khi gọi hàm build cho CNN, các tham số trên được copy vào để tránh sai sót và mô hình được khởi tạo theo cấu hình này. [1]\n\n[Citation về summary kiến trúc và giảm kích thước feature map]  \n- Kiểm tra bằng summary: mạng thực hiện các phép correlation (convolution) đầu tiên với 6 filter, sau đó lớp thứ hai với 16 filter; kích thước không gian giảm từ 28 → 14 → 7 như thiết kế (do các bước pooling/stride tương ứng). [2]\n\n### 2.2 Kích thước mô hình và số tham số\n- Tổng số tham số ước tính ~ 100.000 tham số. [2][3]\n\n### 2.3 Training: cách gọi, lưu history và trực quan hóa loss\n- Gọi hàm train: truyền vào x_train và y_train (ở định dạng tương ứng). Lưu ý: y_train (y trend) cần ở dạng phù hợp (được ghi là \"phải ở dạng là 100\" trong video — ý chỉ định dạng one-hot hoặc shape tương ứng của label được yêu cầu bởi hàm train). [3]  \n- Để vẽ đồ thị loss theo epoch cần gán kết quả training cho biến *history* (ví dụ history = cnn.train(...)) để sau đó trực quan hóa. [3]  \n- Ví dụ loss giảm theo epoch: bắt đầu ~0.18 (epoch đầu), giảm về ~0.13, ~0.10, đến khoảng epoch 25–26 ~0.01, và kỳ vọng đến epoch 30 giảm còn ~0.007; accuracy trên tập train đạt ~99.85%. [4][5]\n\n### 2.4 Trực quan hóa trọng số (filters)\n- Lấy weights của layer convolution đầu tiên bằng phương thức lấy layer (ví dụ cnn.getway layer số 1, vì layer 0 là input). [4]  \n- Weights (w0) của Conv1 có kích thước 3×3×1×6: 3×3 là kích thước kernel, 1 là số channel đầu vào, 6 là số filter đầu ra. [6]  \n- Cách duyệt để hiển thị filter: lặp i từ 0 đến 5 (tương ứng 6 filter) và lấy từng kernel để trực quan. [6][7]  \n- Ý nghĩa trực quan: mỗi filter biểu diễn một đặc trưng (ví dụ sự chênh lệch vùng phải/dưới so với trái/trên, hay độ khác biệt giữa hàng ở giữa và hàng trên/dưới). Mỗi filter thể hiện một đặc trưng khác nhau của ảnh. [7]\n\n### 2.5 Hàm predict và tiền xử lý input để đánh giá mẫu đơn lẻ\n- Trước khi predict cần đảm bảo x_state (mẫu input) được load, chuẩn hóa (chuẩn hóa/normalize) và reshape về đúng kích thước 28×28 (và sau đó reshape lại thứ tự batch/channel như mô hình yêu cầu, ví dụ (1,28,28) hoặc (1,1,28,28) tuỳ implement). Video nhắc cần \"reset\" về dạng 28×28 rồi đưa vào model để predict. [8][9]  \n- Lưu ý về thứ tự chiều: mô hình yêu cầu một ordering (ví dụ batch trước), nên phải đặt kích thước batch lên trước (1,28,...) khi truyền vào predict. [10]  \n- Output của model là vector one-hot → cần dùng np.argmax để chuyển sang nhãn dự đoán (ví dụ np.argmax → ra 4 cho một mẫu). [11]  \n- Thực nghiệm nhiều mẫu (ví dụ idx = 100, idx = 300, v.v.) cho thấy độ chính xác dự đoán rất cao trên tập thử nghiệm nội bộ. [11][12]\n\n### 2.6 Application study — Thử nghiệm các biến thể cấu hình\n- Mục tiêu: hiểu vai trò từng module bằng cách thay đổi cấu hình và so sánh kết quả (loss/accuracy). [12]  \n- Biến thể 1: thay hàm activation từ *sigmoid* sang *ReLU* (ghi là \"relo\" trong video) bằng cách sửa tham số khi gọi build (không cần viết lại toàn bộ code). Sau đó train lại và lưu vào history thứ 2 (history2) để vẽ so sánh. [13][14]  \n- Khi vẽ loss so sánh, thêm đường từ history2 (trend_loss_v2) để so sánh hai biến thể trên cùng biểu đồ. [14][15]  \n- Biến thể 3: loại bỏ toàn bộ lớp pooling (kết nối trực tiếp c1 → c3, c3 nhận input trực tiếp từ c1, v.v.), lưu thành phiên bản cnsv3. Cần đảm bảo ghép nối các biến (gối đầu) đúng để mạng vẫn hợp lệ. [15][16]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n- Ví dụ minh họa việc build và kiểm tra kiến trúc: sau khi build, dùng summary để kiểm tra các layer, kích thước tensor qua từng lớp (28→14→7), và tổng số tham số ~100k. [2][3]  \n- Ví dụ training và kết quả trực quan: loss giảm từ ~0.18 → ~0.007 sau ~30 epoch; accuracy tập train ~99.85% — minh họa mạng đã học tốt trên dữ liệu huấn luyện. (Đồ thị loss được lưu trong biến history và có thể vẽ; khi so sánh biến thể, thêm history2 để vẽ đường thứ hai). [4][5][14][15]  \n- Ví dụ trực quan filter: lấy w0 (3×3×1×6), duyệt 6 filter để hiển thị, nhận ra mỗi filter nắm bắt đặc trưng biên/độ chênh lệch vùng ảnh. [6][7]  \n- Ví dụ predict mẫu đơn: tiền xử lý (reshape, normalize), gọi cnn.predict(x_state) → nhận vector one-hot → np.argmax để lấy nhãn dự đoán; so sánh với nhãn thực tế cho các idx khác nhau (ví dụ idx=100, idx=300) để đánh giá. [8][9][10][11][12]  \n- Ứng dụng thực tế / trường hợp sử dụng: bài giảng tập trung vào việc hiểu cấu trúc và thí nghiệm biến thể nhằm đưa ra quyết định thiết kế mạng (chọn activation, có/không pooling). Những thí nghiệm này giúp chọn cấu hình tốt hơn cho bài toán phân loại ảnh nhỏ (28×28). [12][13][15]\n\n## 4. Kết luận (Conclusion)\n- Tóm tắt các ý chính: Đã trình bày cách cài đặt một CNN cơ bản (input 28×28×1, Conv: 6→16, FC:120→84→1), kiểm tra kiến trúc bằng summary (kích thước giảm 28→14→7), huấn luyện và lưu history để vẽ loss, trực quan hóa filters (w0 shape 3×3×1×6) và thực hiện predict mẫu đơn; thực hiện study các biến thể như đổi activation (sigmoid → ReLU) và loại bỏ pooling để so sánh hiệu quả. [1][2][3][4][6][7][8][13][15][16]  \n- Tầm quan trọng: Việc xây dựng, kiểm tra, trực quan hóa và so sánh biến thể là bước cần thiết để hiểu chức năng từng module trong CNN và chọn cấu hình phù hợp cho bài toán thực tế. Việc lưu history cho phép so sánh trực quan hiệu quả training giữa các biến thể. [3][4][14][15]  \n- Liên hệ với các bài giảng khác: video nhắc đến việc đây là phần cài đặt trong Chương 3; mục tiêu là áp dụng các khái niệm đã học về convolution/pooling/activation vào mô hình thực tế và thực hiện experiment để hiểu vai trò từng thành phần. (Tham khảo các phần trước/sau trong Chương 3 để hiểu nguồn gốc thiết kế kiến trúc và các biến thể được thí nghiệm). [1][12]\n\nGhi chú: toàn bộ nội dung tóm tắt đã dựa hoàn toàn trên các đoạn trích từ video: [1] [00:00–01:01], [2] [00:49–01:50], [3] [01:37–02:40], [4] [02:33–03:31], [5] [03:19–04:20], [6] [04:09–05:10], [7] [04:58–06:00], [8] [05:48–06:48], [9] [06:36–07:39], [10] [07:29–08:29], [11] [08:19–09:18], [12] [09:09–10:11], [13] [10:00–11:02], [14] [10:49–11:52], [15] [11:40–12:41], [16] [12:28–12:43].",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 3_TNrJYPuDADM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_2： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/TNrJYPuDADM",
          "start_time": 0,
          "end_time": 61,
          "text": "bước tiếp theo chúng ta sẽ tạo các mô hình rồi cnn.build và ở đây chúng ta sẽ copy xuống các tham số để tránh bị xơ xót đầu tiên input dimension thì ảnh này của mình nếu thông thường chúng ta sẽ để là 28 tuy nhiên cái mô hình cnn chỉ có thể thực hiện được khi nó phải làm một cái tensor 3 chiều do đó ở đây thì chúng ta sẽ để là 28.28.1 và activation thì chúng ta sẽ để là sigmoid rồi cnn số 1 chúng ta sẽ để là 6 cnn số 2 thì chúng ta sẽ để là 16 và fc ở đây chúng ta sẽ để là 1 fclook số 1 chúng ta sẽ để là 120 fc số 2 thì chúng ta sẽ để là 84 và hàm activation ở đây thì chúng ta sẽ để là hàm sigmoid"
        },
        {
          "index": 2,
          "video_id": "Chương 3_TNrJYPuDADM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_2： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/TNrJYPuDADM",
          "start_time": 49,
          "end_time": 110,
          "text": "và fc ở đây chúng ta sẽ để là 1 fclook số 1 chúng ta sẽ để là 120 fc số 2 thì chúng ta sẽ để là 84 và hàm activation ở đây thì chúng ta sẽ để là hàm sigmoid rồi bây giờ chúng ta sẽ chạy thử và chương trình thì chạy được rồi bây giờ chúng ta sẽ xem coi là cái mạng cnn này chấm summary xem có thể thực hiện được hay không để xem cái kích thước, cái kiến trúc của cái mạng cnn này thì chúng ta có thể thấy là trong cái mạng cnn này nó thả mảng được và chúng ta sẽ chạy thử đúng như cái kiến trúc mà chúng ta mong muốn là bao gồm thực hiện cái phép correlation số 1 với 6 filter thực hiện correlation số 2 với 16 filter rồi và cái kích thước của các cái tensor thì cũng giảm dần đó là từ 28 xuống 14 xuống 7 giống như trong thiết kế ở đây và số neurone của mình sẽ là xin lỗi số tham số của mình nó sẽ là 100.000 tham số"
        },
        {
          "index": 3,
          "video_id": "Chương 3_TNrJYPuDADM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_2： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/TNrJYPuDADM",
          "start_time": 97,
          "end_time": 160,
          "text": "đó là từ 28 xuống 14 xuống 7 giống như trong thiết kế ở đây và số neurone của mình sẽ là xin lỗi số tham số của mình nó sẽ là 100.000 tham số 100.000 tham số 100.000 tham số chúng ta sẽ tiến hành trend chúng ta sẽ truyền vào 2 tham số đó là x trend và y trend tuy nhiên y trend nó phải ở dạng là 100 rồi thì cái việc trend này đâu đó nó có thể tốn ở đây chúng ta quên mất một cái việc đó là sau này để mà có thể vẽ được cái hàm loss, vẽ được cái giá trị loss theo cái số epoch chúng ta sẽ phải gán vào một cái biến đó là history rồi sau đó thì ở đây chúng ta mới có thể thực hiện được cái việc trực quan hóa này rồi để trực quan hóa cho cái mô hình thì chúng ta sẽ phải lấy ra các cái filter"
        },
        {
          "index": 4,
          "video_id": "Chương 3_TNrJYPuDADM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_2： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/TNrJYPuDADM",
          "start_time": 153,
          "end_time": 211,
          "text": "rồi để trực quan hóa cho cái mô hình thì chúng ta sẽ phải lấy ra các cái filter thì ở đây chúng ta sẽ lấy ra filter ở cái node đầu tiên đó chính là cnn.getway ở đây chúng ta sẽ để cái layer số 1 tại vì layer số 0 chính là cái input rồi layer số 1 chính là cái function rồi chúng ta sẽ cùng quan sát nhưng mà đương nhiên là phải chờ cái mô hình này nó húi lại xong thì chúng ta mới có thể thấy được cái hàm loss này nó chạy như thế nào ở đây thì chúng ta quan sát thấy là cái loss của mình nó đã giảm từ 0.18 trong cái epoch đầu tiên giảm xuống còn 0.13 giảm xuống còn 0.10 và đến cái epoch thứ 25, 26 thì giảm xuống còn 0.01 và hy vọng là đến cái epoch số 30"
        },
        {
          "index": 5,
          "video_id": "Chương 3_TNrJYPuDADM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_2： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/TNrJYPuDADM",
          "start_time": 199,
          "end_time": 260,
          "text": "giảm xuống còn 0.10 và đến cái epoch thứ 25, 26 thì giảm xuống còn 0.01 và hy vọng là đến cái epoch số 30 thì cái loss của mình nó đã giảm xuống còn 0.007 và accuracy cho cái tập dữ liệu trend nó đã lên đến 99.85% rồi và chúng ta quan sát thấy cái loss giảm rất là tốt chúng ta quan sát cái trend loss này giảm rất là tốt rồi bây giờ chúng ta sẽ xem cái mytrend w này có cái giá trị là bao nhiêu thì ở đây chúng ta sẽ thấy là cái w này nó sẽ là một cái array xin lỗi nó là một cái list bao gồm hai phần tử thì cái phần tử đầu tiên chính là cái số trọng số cái số filter của cái phép biến đổi compression đầu tiên và cái thành phần thứ hai nó chính là cái bias tại vì chúng ta có sử dụng bias w0 chính là cái trọng số của mình"
        },
        {
          "index": 6,
          "video_id": "Chương 3_TNrJYPuDADM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_2： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/TNrJYPuDADM",
          "start_time": 249,
          "end_time": 310,
          "text": "cái số filter của cái phép biến đổi compression đầu tiên và cái thành phần thứ hai nó chính là cái bias tại vì chúng ta có sử dụng bias w0 chính là cái trọng số của mình rồi để xem coi cái trọng số này có kích thước bao nhiêu thì chúng ta lại chấm xét thì trong đó 3 3 1 6 thì 3 3 chính là cái kích thước của cái channel và 1 chính là cái input dimension của input của đầu vào của mình nó chỉ có một channel thôi nó sẽ là 1 và output của mình sẽ là 6 6 cái filter rồi rồi chúng ta sẽ xem là cái output của mình sẽ là 6 rồi thì để trực quan chúng ta sẽ có số filter là 6 rồi chúng ta sẽ duyệt qua i từ 0 cho đến 5 để truyền vô đây rồi đây là w0 w0 chấm xếp chính là 3 3 1 6 thì chúng ta sẽ lấy cái chỉ số i chạy ở đây trước rồi sau đó lấy chỉ số z chạy ở đây thì ở đây một cách tổng quát trong cái lớp compression số 2 thì cái số 1 này nó sẽ qua"
        },
        {
          "index": 7,
          "video_id": "Chương 3_TNrJYPuDADM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_2： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/TNrJYPuDADM",
          "start_time": 298,
          "end_time": 360,
          "text": "w0 chấm xếp chính là 3 3 1 6 thì chúng ta sẽ lấy cái chỉ số i chạy ở đây trước rồi sau đó lấy chỉ số z chạy ở đây thì ở đây một cách tổng quát trong cái lớp compression số 2 thì cái số 1 này nó sẽ qua nó chuyển thành là số 16 do đó thì ở đây chúng ta sẽ để là i là chạy cho một cái rank rank này thì ở đây chúng ta để là 1 nhưng mà sắp tới có thể để là 16 rồi đây chính là 6 cái filter ở cái lớp đầu tiên thì chúng ta có thể hiểu cái ý nghĩa của cái filter này đó chính là chúng ta lấy cái size số cái sự chênh lệch của cái vùng phía bên phải phía dưới so với lại cái vùng ở phía trái bên trên ý nghĩa của filter này đó là lấy cái sự chênh lệch giữa cái hàng ở giữa này là 1 và 2 giữa sau với lại 2 cái hàng ở phía trên và phía dưới thì mỗi một cái filter này nó sẽ thể hiện một cái đặc trưng nào đó rồi tiếp theo là chúng ta sẽ tiến hành thử nghiệm với một số cái biến thể khác nhau"
        },
        {
          "index": 8,
          "video_id": "Chương 3_TNrJYPuDADM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_2： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/TNrJYPuDADM",
          "start_time": 348,
          "end_time": 408,
          "text": "thì mỗi một cái filter này nó sẽ thể hiện một cái đặc trưng nào đó rồi tiếp theo là chúng ta sẽ tiến hành thử nghiệm với một số cái biến thể khác nhau nhưng mà trước khi qua thử nghiệm một số biến thể khác nhau thì chúng ta sẽ thử cái hàm predict hơn cái hàm predict thì cln.predict rồi thì chúng ta sẽ truyền vào cái x state và mẫu dữ liệu thứ ví dụ như là mẫu dữ liệu thứ 300 rồi ok ở đây thì hàm predict chúng ta sẽ xem lại cái hàm predict của mình truyền vào cell.model.x state ok bây giờ chúng ta sẽ xem tiếp cái x state của mình đã được load rồi và đã được chủng hóa rồi đúng không rồi"
        },
        {
          "index": 9,
          "video_id": "Chương 3_TNrJYPuDADM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_2： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/TNrJYPuDADM",
          "start_time": 396,
          "end_time": 459,
          "text": "ok bây giờ chúng ta sẽ xem tiếp cái x state của mình đã được load rồi và đã được chủng hóa rồi đúng không rồi ok bây giờ chúng ta sẽ thử truyền vào như thế này rồi chúng ta sẽ xem cái x state của mình thôi rồi à x state của mình đó là cái mảng kích thước là 28 x 28 do đó thì chúng ta phải reset chúng ta phải reset nó về cái dạng là 28 x 28 rồi sau đó chúng ta mới đưa vào để cho cái mô hình của mình có thể predict được cln.predict rồi ok"
        },
        {
          "index": 10,
          "video_id": "Chương 3_TNrJYPuDADM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_2： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/TNrJYPuDADM",
          "start_time": 449,
          "end_time": 509,
          "text": "rồi sau đó chúng ta mới đưa vào để cho cái mô hình của mình có thể predict được cln.predict rồi ok ... rồi ha ở đây cái số này sẽ phải để lên trước đúng không kìa thì cái này nó sẽ phải để lên trước là 1,28 ok được rồi tức là nó sẽ phải để cái chỉ số của cái thứ tự ... lên trước nó hơi ngược nó hơi ngược rồi bây giờ chúng ta sẽ thử xem cái nhãn này nó sẽ ra cái giá trị là bao nhiêu tại vì ở đây nó chỉ trả ra 1 cái vector 1 hot chúng ta sẽ phải có thêm 1 cái hàm nữa đó là argument max là np.argument max"
        },
        {
          "index": 11,
          "video_id": "Chương 3_TNrJYPuDADM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_2： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/TNrJYPuDADM",
          "start_time": 499,
          "end_time": 558,
          "text": "tại vì ở đây nó chỉ trả ra 1 cái vector 1 hot chúng ta sẽ phải có thêm 1 cái hàm nữa đó là argument max là np.argument max rồi nó sẽ là 4 và bây giờ chúng ta sẽ xem coi cái mẫu thứ 300 này x y text của mình thứ 300 nó ra bằng bao nhiêu nó là 4 rồi bây giờ chúng ta sẽ thử những cái mẫu khác chúng ta sẽ thử những cái mẫu khác ở đây chúng ta sẽ để là predict predict predict predict predict nhãn dự đoán là predict rồi còn ở đây sẽ là nhãn thực tế"
        },
        {
          "index": 12,
          "video_id": "Chương 3_TNrJYPuDADM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_2： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/TNrJYPuDADM",
          "start_time": 549,
          "end_time": 611,
          "text": "predict rồi còn ở đây sẽ là nhãn thực tế và ở đây cái chỉ số này chúng ta sẽ tham số hóa nó là idx là bằng 100 ví dụ vậy và chúng ta sẽ để đây là idx rồi đó thì đại đa số chúng ta thấy là cái độ chính xác rất là cao chúng ta thử rất nhiều những cái nhãn khác nhau ha thì nó đều ra là dự đoán và thực tế khác với nhau bây giờ trong cái mạng cnn thì chúng ta thấy nó có rất nhiều những cái mô đun khác nhau và tại thời điểm hiện tại thì chúng ta sẽ chưa hiểu rõ cái vai trò của từng mô đun này do đó thì chúng ta sẽ làm một cái thí nghiệm nó gọi là application study với các cái biến thể khác nhau bằng cách đó là chúng ta sẽ lần lượt thay đổi một số cái cấu hình"
        },
        {
          "index": 13,
          "video_id": "Chương 3_TNrJYPuDADM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_2： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/TNrJYPuDADM",
          "start_time": 600,
          "end_time": 662,
          "text": "do đó thì chúng ta sẽ làm một cái thí nghiệm nó gọi là application study với các cái biến thể khác nhau bằng cách đó là chúng ta sẽ lần lượt thay đổi một số cái cấu hình của cái chương trình của mình chúng ta sẽ thay đổi một số cái cấu hình thì cái phiên bản cái biến thể đầu tiên đó là chúng ta sẽ bỏ đi cái thay cái hàm sigmoid bằng relo chúng ta sẽ thay cái sigmoid bằng relo như vậy thì chúng ta sẽ copy cái code ở đây đem xuống rồi мощninkenci 1 chúng ta sẽ thay hàm này thay cái sigmoid bằng relo như vậy thì bản chất là cái biến thể này chống ta không cần phải kè đặt lại biến thể này chúng ta không cần phải kè đặt lại mà chúng ta chỉ sửa cái time số của mình thôi chúng ta chỉ sửa cái time số khi gọi hàm build và ở đây là relo"
        },
        {
          "index": 14,
          "video_id": "Chương 3_TNrJYPuDADM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_2： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/TNrJYPuDADM",
          "start_time": 649,
          "end_time": 712,
          "text": "mà chúng ta chỉ sửa cái time số của mình thôi chúng ta chỉ sửa cái time số khi gọi hàm build và ở đây là relo đây là relo rồi sau đó thì chúng ta sẽ tiến hành là cnn.trend và is trend rồi is trend oh và lưu ý ở đây chúng ta sẽ để cái history là history số 2 rồi bây giờ chúng ta sẽ tiến hành build cái này và tranh thủ trong thời gian chờ đợi thì chúng ta sẽ thử viết code trước cho cái phần là vẽ cái giá trị loss chúng ta sẽ thêm một cái đường nữa đó là history số 2 rồi và ở đây sẽ là trend loss v1"
        },
        {
          "index": 15,
          "video_id": "Chương 3_TNrJYPuDADM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_2： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/TNrJYPuDADM",
          "start_time": 700,
          "end_time": 761,
          "text": "vẽ cái giá trị loss chúng ta sẽ thêm một cái đường nữa đó là history số 2 rồi và ở đây sẽ là trend loss v1 đây sẽ là trend loss v2 trong đó v2 đó là dùng value dùng value rồi tương tự như vậy bây giờ chúng ta sẽ chờ đợi chúng ta sẽ viết trước cái code cho các cái biến thể tiếp theo biến thể về bỏ hết các cái lớp pooling thì chúng ta làm cũng rất là nhanh pooling đúng không thì chúng ta sẽ mở nè xóa đi xóa đi và lưu ý đó là phải để gối đầu các cái biến ví dụ như ở đây c1 thì sẽ được truyền trực tiếp sang đây rồi c3 thì sẽ truyền trực tiếp sang đây như vậy là chúng ta đã xong cái biến thể số 3"
        },
        {
          "index": 16,
          "video_id": "Chương 3_TNrJYPuDADM",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_2： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/TNrJYPuDADM",
          "start_time": 748,
          "end_time": 763,
          "text": "phải để gối đầu các cái biến ví dụ như ở đây c1 thì sẽ được truyền trực tiếp sang đây rồi c3 thì sẽ truyền trực tiếp sang đây như vậy là chúng ta đã xong cái biến thể số 3 chúng ta sẽ để là cnsv3"
        }
      ]
    },
    {
      "video_id": "Chương 3_rVpEwMijtvQ",
      "summary": "## 1. Giới thiệu (Introduction)\n- Mục tiêu chính của bài giảng: thực hiện và so sánh nhiều biến thể của mạng CNN (thay đổi activation, bỏ pooling, bỏ convolution, thay đổi compression) để quan sát ảnh hưởng lên loss/accuracy và tốc độ hội tụ trong quá trình huấn luyện. [3][5][8][9]\n- Các khái niệm sẽ được đề cập: activation functions (sigmoid vs ReLU), pooling (presence vs without pooling), lớp Convolution (bỏ hoàn toàn), compression / downsampling (giảm kích thước đầu vào), và so sánh các lịch sử huấn luyện (history V1, V2, V3, V4). [1][3][5][7][8]\n- Phương pháp: chạy từng biến thể, lưu history (history 1..4), vẽ đồ thị loss/accuracy để so sánh trực quan. [3][4][7]\n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Thay sigmoid bằng ReLU — tốc độ hội tụ nhanh hơn\n- Khi thay activation sigmoid bằng ReLU (phiên bản V2), loss giảm nhanh hơn ở các epoch ban đầu; ví dụ tại epoch số 5 đường của V2 (ReLU) có loss thấp hơn so với phiên bản dùng sigmoid. [1][2]\n- Ý nghĩa: ReLU giúp hội tụ nhanh hơn ở giai đoạn sớm, mặc dù ở thời gian rất dài cả hai activation có thể tiệm cận nhau về accuracy, nhưng với giới hạn số epoch thực tế ReLU thường hiệu quả hơn. [2][3]\n\n### 2.2. Bỏ lớp pooling (without pooling) — ảnh hưởng tiêu cực lên việc giảm loss\n- Khi loại bỏ hoàn toàn các lớp pooling (phiên bản V3: without pooling), quan sát thấy loss gần như không giảm (đứng yên), nghĩa là pooling đóng vai trò quan trọng trong việc giúp loss giảm trong quá trình huấn luyện. [3][5][6]\n- Kết luận tạm thời: thiếu pooling làm cho việc huấn luyện khó khăn hơn và ảnh hưởng mạnh tới khả năng giảm loss. [5][6]\n\n### 2.3. Bỏ hết các lớp Convolution — thí nghiệm kiểm chứng vai trò convolution\n- Phiên bản V4 được thiết kế để bỏ hết các lớp Convolution (làm một thí nghiệm để kiểm tra vai trò của convolution). [5]\n- Khi chạy V4 (không có convolution, chỉ giảm kích thước đầu vào liên tiếp 2 lần — \"input S2\"), loss vẫn giảm nhưng rất chậm; điều này cho thấy convolution và/hoặc cấu trúc mạng có vai trò quan trọng trong tăng tốc học và trích xuất đặc trưng. [6][7][8]\n\n### 2.4. Vai trò của compression / downsampling (giảm kích thước)\n- Compression (giảm kích thước/downsamping trong kiến trúc) giúp tăng tốc độ huấn luyện: các biến thể có compression giảm loss nhanh hơn so với biến thể không có compression. [8]\n- Minh họa bằng đồ thị: đường của phiên bản không có compression (V4, màu đỏ) nằm phía trên (loss cao hơn) so với các phiên bản có compression; phiên bản tốt nhất trong thử nghiệm là V2 (thay sigmoid bằng ReLU, có both pooling và compression) nằm thấp nhất trên đồ thị. [8][9]\n\n### 2.5. Thực thi và so sánh nhiều history\n- Trong bài giảng, các lịch sử huấn luyện được lưu thành history 1, 2, 3, 4 và vẽ chung trên một biểu đồ để so sánh tác động của từng thay đổi (activation, pooling, convolution, compression). [3][4][7]\n- Giải thích trực tiếp từ quá trình: khôi phục (restore) cài đặt trước (ví dụ khôi phục sigmoid), chạy, và rồi vẽ các history để quan sát khác biệt. [3][4]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa từ video:\n  - So sánh trực tiếp giữa bản dùng sigmoid và bản dùng ReLU: tại epoch 5 ReLU cho loss thấp hơn (hội tụ nhanh ở giai đoạn đầu). [1][2]\n  - Thử nghiệm bỏ pooling (V3): loss gần như không giảm — minh chứng pooling quan trọng cho việc giảm loss. [3][5][6]\n  - Thử nghiệm bỏ toàn bộ convolution (V4): loss giảm chậm hơn so với kiến trúc hoàn chỉnh; cho thấy convolution và compression hỗ trợ hiệu quả huấn luyện. [5][7][8]\n  - Vẽ cùng lúc history 1..4 để so sánh trực quan: V2 (ReLU + pooling + compression) là phiên bản tốt nhất (đường thấp nhất), V4 (không compression / không convolution) là tệ hơn (đường nằm cao). [8][9]\n\n- Ứng dụng thực tế / trường hợp sử dụng:\n  - Thiết kế kiến trúc CNN: chọn activation (ReLU) để đạt hội tụ nhanh hơn trong thực hành khi số epoch giới hạn. [1][2]\n  - Quyết định có dùng pooling hay không: pooling thường cần thiết để giúp loss giảm ổn định trong các mạng CNN chuẩn. [5][6]\n  - Khi cân nhắc bỏ convolution hoặc giảm compression, cần lưu ý mất khả năng trích xuất đặc trưng và làm chậm quá trình huấn luyện. [5][8]\n  - Bài tutorial này phục vụ như một thí nghiệm hướng dẫn để hiểu vai trò từng thành phần (activation, pooling, convolution, compression) trong CNN. [9]\n\n---\n\n## 4. Kết luận (Conclusion)\n- Tóm tắt các ý chính:\n  - ReLU cải thiện tốc độ hội tụ ban đầu so với sigmoid (ví dụ rõ tại epoch 5). [1][2]\n  - Pooling có vai trò quan trọng: khi bỏ pooling, loss khó giảm (không giảm đáng kể). [3][5][6]\n  - Bỏ convolution làm giảm khả năng huấn luyện hiệu quả; loss vẫn giảm nhưng chậm. [5][6][8]\n  - Compression/downsampling giúp huấn luyện nhanh hơn; kiến trúc hoàn thiện (ReLU + pooling + compression) là hiệu quả nhất trong loạt thử nghiệm. [8][9]\n- Tầm quan trọng: bài tập/tutorial này giúp hiểu rõ vai trò từng phép biến đổi trong CNN và cách các thành phần ảnh hưởng trực tiếp tới tốc độ hội tụ và độ chính xác trong thực hành huấn luyện. [9]\n- Liên hệ với các bài giảng khác: (video trình bày như một tutorial thực hành trong chương này để minh họa vai trò của các thành phần mạng CNN). [9]\n\n---\n\nGhi chú: các nhận xét, kết luận và minh họa đều được rút trực tiếp từ quá trình thí nghiệm và đồ thị loss/accuracy được trình bày trong video. [1][2][3][4][5][6][7][8][9]",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 3_rVpEwMijtvQ",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_3： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/rVpEwMijtvQ",
          "start_time": 0,
          "end_time": 63,
          "text": "bây giờ chúng ta sẽ cài cho cái biên thể cuối cùng ok ở đây khi bỏ cái relu thì chúng ta thấy là nó đã chạy xong rồi ha nó đã chạy xong và chúng ta sẽ quan sát thử ok chúng ta sẽ vẽ thì nhìn vào cái sơ đồ này ok ở đây chúng ta sẽ phải gom nó lại gom 2 cái legend này lại rồi vẽ lại rồi chúng ta sẽ thấy là cái relu phi mạng số 2 nó giảm rất là nhanh đúng không nó giảm rất là nhanh nó nằm bên dưới cái đường màu xanh thì điều đó có nghĩa là gì điều đó đó là ví dụ tại cái epoch số 5 thì cái epoch số 5 là cái đường màu xanh thì cái phương pháp V2 tức là khi sử dụng relu nó cho cái loss thấp hơn so với cái phiên bản số 1 tức là dùng sigmoid"
        },
        {
          "index": 2,
          "video_id": "Chương 3_rVpEwMijtvQ",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_3： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/rVpEwMijtvQ",
          "start_time": 49,
          "end_time": 111,
          "text": "thì điều đó có nghĩa là gì điều đó đó là ví dụ tại cái epoch số 5 thì cái epoch số 5 là cái đường màu xanh thì cái phương pháp V2 tức là khi sử dụng relu nó cho cái loss thấp hơn so với cái phiên bản số 1 tức là dùng sigmoid tức là nó đã giúp cho mình hội tụ nhanh hơn nhưng mà đương nhiên khi mà cái số epoch càng lớn thì cả 2 thằng nó cũng sẽ tiện trọng về nhưng mà nó sẽ tốn thời gian hơn thì tập Enix là một cái tập rất là tuyến tính rất là dễ rất là đơn giản nó sẽ không thể nào thể hiện được cái sự khuất đại cái cái tốc độ mà trend của relu nó nhanh hơn so với sigmoid như thế nào khi mà chúng ta trend với tập dữ liệu lớn như là E-mainnet thì chúng ta sẽ thấy rõ là relu nó hiệu quả hơn rất là nhiều nó sẽ giảm xuống chúng ta sẽ thấy là cái sự sụp giảm về loss của nó rất là nhanh thì đó chính là cái ý nghĩa của cái biến thể đầu tiên đó là bỏ cái sigmoid và thay thế nó bằng relu thì tốc độ hội tụ của nó sẽ nhanh hơn"
        },
        {
          "index": 3,
          "video_id": "Chương 3_rVpEwMijtvQ",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_3： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/rVpEwMijtvQ",
          "start_time": 97,
          "end_time": 155,
          "text": "chúng ta sẽ thấy là cái sự sụp giảm về loss của nó rất là nhanh thì đó chính là cái ý nghĩa của cái biến thể đầu tiên đó là bỏ cái sigmoid và thay thế nó bằng relu thì tốc độ hội tụ của nó sẽ nhanh hơn còn về đường chính xác theo thời gian dài đâu đó nó vẫn sẽ sắp xỉ với sigmoid nhưng mà với cái thời gian mà mình có thể chờ đợi được để mà có thể huyện thì việc dùng sigmoid nó sẽ chậm hơn rất là nhiều rồi tiếp theo đó là chúng ta sẽ bỏ hết các lớp pooling rồi chúng ta đã cài đặt rồi và bây giờ chúng ta sẽ sử dụng chúng ta sẽ sử dụng nó rồi ở đây chúng ta sẽ để là CLP  nền v3 và history ở đây sẽ là history số 3 rồi ở đây chúng ta sẽ khôi phục ngược trở lại chúng ta sẽ khôi phục ngược trở lại là sigmoid rồi chạy"
        },
        {
          "index": 4,
          "video_id": "Chương 3_rVpEwMijtvQ",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_3： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/rVpEwMijtvQ",
          "start_time": 147,
          "end_time": 217,
          "text": "rồi ở đây chúng ta sẽ khôi phục ngược trở lại chúng ta sẽ khôi phục ngược trở lại là sigmoid rồi chạy rồi bây giờ chúng ta sẽ vẽ cái hàm loss khi có đồng thời cả 3 cái history 123 rồi v3 thì ở đây sẽ là without pooling without pooling rồi thì chúng ta có thể tool gọi lại chút xíu đi s spelledaterm Rồi kê, V3,ów19 niedali grupo Hearts và V3 đang할акс rồi tranh thủ trong khi chờ đợi thì chúng ta sẽ cài luôn cái phiên bản thứ tư cái phiên bản này đó chính là chúng ta bỏ hết các lớp Convolution một điều rất là thú vị đó là chúng ta đặt cái sự"
        },
        {
          "index": 5,
          "video_id": "Chương 3_rVpEwMijtvQ",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_3： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/rVpEwMijtvQ",
          "start_time": 200,
          "end_time": 260,
          "text": "đi s spelledaterm Rồi kê, V3,ów19 niedali grupo Hearts và V3 đang할акс rồi tranh thủ trong khi chờ đợi thì chúng ta sẽ cài luôn cái phiên bản thứ tư cái phiên bản này đó chính là chúng ta bỏ hết các lớp Convolution một điều rất là thú vị đó là chúng ta đặt cái sự nguy ngờ rằng là cái mẹ Convolution thì cái vai trò của Convolution rõ ràng rất là lớn nhưng bây giờ chúng ta sẽ làm một thí nghiệm đó là bỏ hết cái Convolution thì xem điều gì sẽ xảy ra thì đó chính là cái ý nghĩa của cái phiên bản số 4 rồi bây giờ may quá cái phiên bản số 3 nó đã chạy xong và chúng ta sẽ xem thử rồi chúng ta thấy là nếu như không có cái pooling nếu như không có pooling thì cái loss của mình gần như không giảm nó cứ diễn nguyên loss gần như không giảm nó cứ diễn nguyên thì rõ ràng là cái vai trò này nó sẽ dễ dàng hơn và nó sẽ có thể tạo ra những cái cái mặt khác của"
        },
        {
          "index": 6,
          "video_id": "Chương 3_rVpEwMijtvQ",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_3： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/rVpEwMijtvQ",
          "start_time": 249,
          "end_time": 306,
          "text": "cái loss của mình gần như không giảm nó cứ diễn nguyên loss gần như không giảm nó cứ diễn nguyên thì rõ ràng là cái vai trò này nó sẽ dễ dàng hơn và nó sẽ có thể tạo ra những cái cái mặt khác của cũng giúp Luminous về th 있었u Creating một máy truyền đó Nếu không có pooling mình gần như đi never nghe nó không giúp cho mình giảm xuống Bây giờ chúng ta sẽ quay an với phiên bản tiếp theo đó là không có cái lớp Convolution. ở đây chúng ta phải sử dụng cái biến thể đầu tiên thì nó không là sẽ nhận lẫn. Khi︰ Không có Convolution chúng ta sẽ mmc khling chút chúng ta sẽ bỏ đi lớp này bỏ đi lớp này rồi, và ở đây chúng ta sẽ truyền vào là input S2 sẽ truyền vào đây tức là chúng ta sẽ giảm kích thước liên tiếp 2 lần rồi, ok ở đây sẽ là CNN V4"
        },
        {
          "index": 7,
          "video_id": "Chương 3_rVpEwMijtvQ",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_3： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/rVpEwMijtvQ",
          "start_time": 300,
          "end_time": 346,
          "text": "giảm kích thước liên tiếp 2 lần rồi, ok ở đây sẽ là CNN V4 rồi, bây giờ chúng ta sẽ gọi cái hàm này khởi tạo để là V4 history là 4 và run và tương tự như vậy chúng ta sẽ vẽ cái shadow ở đây rồi chúng ta sẽ có history là 4 train loss ở đây sẽ là V4 without compression and"
        },
        {
          "index": 8,
          "video_id": "Chương 3_rVpEwMijtvQ",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_3： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/rVpEwMijtvQ",
          "start_time": 366,
          "end_time": 410,
          "text": "rồi, chúng ta thấy là cái loss của mình cũng có giảm, tuy nhiên cái tốc độ giảm của nó khá là chậm tốc độ giảm khá chậm, thì điều này cũng minh chứng cho cái việc đó là cái compression của mình nó đã giúp cho cái việc huấn luyện nó nhanh hơn, mặc dù accuracy thì nó cũng có xu hướng là nó càng lúc càng tăng, đúng không? nó có xu hướng càng tăng, nhưng với cùng cái số epoch thì không có compression, tốc độ nó sẽ chậm hơn rất là nhiều rồi, cái đường màu đỏ là V4 thì chúng ta thấy là nó nằm ở phía trên, nếu không có compression, nó sẽ nằm phía trên như vậy, cái phiên bản mà hoàn thiện nhất của chúng ta, chính là cái phiên bản màu kem ở đây là đường nằm ở dưới cùng"
        },
        {
          "index": 9,
          "video_id": "Chương 3_rVpEwMijtvQ",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 3_3： Cài đặt mạng CNN",
          "video_url": "https://youtu.be/rVpEwMijtvQ",
          "start_time": 398,
          "end_time": 430,
          "text": "thì chúng ta thấy là nó nằm ở phía trên, nếu không có compression, nó sẽ nằm phía trên như vậy, cái phiên bản mà hoàn thiện nhất của chúng ta, chính là cái phiên bản màu kem ở đây là đường nằm ở dưới cùng là tương ứng phiên bản số 2 là thay cái sigmoid bằng Renu trong đó vẫn phải giữ vừa có boolean và vừa có compression như vậy thì đây chính là cái cái bài tập cái tutorial để giúp cho chúng ta hiểu được cái vai trò của từng cái phép biến đổi ở bên trong cái mạng CNN"
        }
      ]
    },
    {
      "video_id": "Chương 3_7YLMIKqygPU",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giải thích và hướng dẫn cách **trực quan hóa một mạng CNN** để hiểu sâu hơn các phép biến đổi (convolution, pooling, normalization, ...), cả trên ảnh tĩnh lẫn trên chuỗi video frames. [1][4][5]  \n- Các khái niệm sẽ được đề cập: hiển thị các *feature map* (cắt theo chiều sâu), trực quan hóa *filter* (trọng số sau khi huấn luyện), quan sát thay đổi activation khi đưa video vào, và dùng thủ thuật thống kê ảnh kích hoạt mạnh để nhận dạng *concept* ở các lớp sâu hơn (ví dụ: face detector). [1][2][3][5][14][15]  \n- Bối cảnh: bài này tiếp nối sau phần đã giới thiệu các thành phần và cài đặt cơ bản của CNN, giờ tập trung vào trực quan hóa để hiểu hoạt động nội tại của mạng. [1]\n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Hai cách chính để trực quan hóa CNN\n- Hiển thị tất cả các *feature map* sinh ra trong suốt quá trình (sau các lớp convolution, pooling, ...): với ảnh đầu vào có độ sâu 3 (3 kênh màu) ta có thể cắt theo từng \"lá\" ở chiều sâu và hiển thị từng lá lên màn hình để quan sát trực quan. [1][2]  \n- Trực quan hóa *filter* (kernel) — tức là các trọng số mà mạng học được sau huấn luyện — để xem các bộ lọc trông như thế nào sau khi tự động cập nhật giá trị. [3]\n\n### 2.2. Công cụ minh họa: Deep Visualization Toolbox\n- Có một công cụ (Deep Visualization Toolbox) mà tác giả trình bày, cho phép truyền vào ảnh tĩnh hoặc một đoạn video để quan sát sự thay đổi của feature map theo thời gian. [3][4][5]  \n- Giao diện điển hình: phía trên bên trái hiển thị ảnh đầu vào (ảnh màu hoặc frame video); các lớp trên mạng (Convolution, Pooling, Normalize, FC, Softmax...) được liệt kê để quan sát; người dùng có thể focus vào lớp cụ thể để xem feature maps tương ứng. [4][5]\n\n### 2.3. Quan sát lớp Convolution đầu tiên (Conv1)\n- Cách đếm số feature map/độ sâu: ví dụ trên slide giao diện, hàng ngang có 10 ô và chiều dọc có 9 ô => khối hiển thị là 10 x 9 = 90 lá cắt, cộng thêm phần phía cuối (3,6...) tổng thành 96 feature maps cho lớp Conv1 trong ví dụ. Đây chính là độ sâu D = 96 của feature map lớp 1. [6]  \n- Khi phóng to từng feature map đầu tiên, có thể thấy một số lá hiển thị bóng dáng của đối tượng gốc (ví dụ con ngựa vàng), tức các filter đầu thường học các đặc trưng thấp cấp liên quan đến cạnh và texture. [6][7]\n\n### 2.4. Ý nghĩa cụ thể của một số feature map (edge detectors)\n- Một số feature map phản ứng mạnh (sáng) với cạnh theo chiều dọc; khi nhấp vào feature map, cửa sổ phóng to cho thấy rằng feature map đó tạo ra các biên cạnh dọc. [8]  \n- Có cặp feature map tương tự nhau (cùng phát hiện cạnh dọc) nhưng khác nhau về *độ nhạy theo hướng sáng/tối*: một filter phản hồi khi chuyển từ vùng sáng sang tối (light→dark), filter kia phản hồi khi chuyển từ tối sang sáng (dark→light). Do đó hai feature map cùng \"cạnh dọc\" nhưng mang ý nghĩa ngược nhau về polarity. [9][10][11][12]\n\n### 2.5. Liên hệ filter ↔ feature map\n- Khi trực quan hóa filter tương ứng với các feature map nói trên, ta thấy pattern trong kernel: một bên có giá trị cao (sáng), bên kia giá trị thấp (tối), khớp với việc lọc cạnh dọc và với polarity (sáng→tối hoặc tối→sáng). Điều này chứng tỏ filter là nguyên nhân sinh ra tính chất edge/polarity của feature map. [11][12]\n\n### 2.6. Trực quan hóa các lớp sâu hơn: pooling, normalization, và conv3/conv5\n- Sau các lớp pooling/normalization và khi lên các lớp convolution sâu hơn (ví dụ conv3, conv5), số lượng feature map tăng lên nhưng kích thước không gian (spatial size) giảm đi; đồng thời các feature map sâu hơn trở nên khó liên tưởng trực tiếp tới đối tượng gốc (mất bóng dáng tổng quan). [13]  \n- Ở các lớp sâu cuối (ví dụ conv5), một vài feature map có phản ứng rất mạnh với một khái niệm cụ thể; ta có thể dùng thủ thuật thống kê các ảnh làm cho feature map đó sáng mạnh nhất để suy ra *concept* mà feature map đại diện (ví dụ: khuôn mặt). [14][15]\n\n### 2.7. Phát hiện concept và theo dõi động trong video\n- Ví dụ trong demo: một feature map ở conv5 sáng mạnh cho nhiều ảnh có điểm chung là đều chứa *gương mặt*; khi người trong video di chuyển sang phải, vùng kích hoạt tương ứng trên feature map cũng dịch sang phải, cho thấy khả năng *cục bộ hóa* (localization) của feature map đó đối với concept \"face\". Khi một người mặc áo đen bước vào, lại thấy một vùng kích hoạt mới xuất hiện — xác nhận rằng feature map thực sự phản hồi sự xuất hiện của một face. [14][15][16]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa trong video:\n  - Ảnh con ngựa vàng: nhiều feature map ở conv1 vẫn giữ bóng dáng đối tượng, giúp ta thấy các filter cạnh/textures học được ở lớp đầu. [6][7]  \n  - Demo video frame: một số feature map sáng mạnh khi người trình diễn di chuyển, và có hai feature map cạnh dọc khác polarity (light→dark vs dark→light) được điểm danh và phóng to để quan sát chi tiết. [7][8][9][10][11][12]  \n  - Conv5 face detector: thu thập các ảnh kích hoạt mạnh của một feature map để nhận diện concept là *gương mặt*, rồi ánh xạ vùng kích hoạt này lên ảnh gốc để thấy vùng mặt được phát hiện; khi mặt dịch chuyển trên video, vùng kích hoạt dịch chuyển theo. [14][15][16]\n\n- Ứng dụng thực tế của trực quan hóa:\n  - Hiểu vai trò từng filter/feature map (debugging, giải thích mô hình). [2][3]  \n  - Phát hiện các feature khái niệm (concept detectors) ở các lớp sâu — dùng cho giải thích, weak localization, hoặc trích xuất biểu diễn ý nghĩa. [14][15][16]  \n  - Quan sát phản ứng theo thời gian khi dùng input là video để đánh giá robustness/temporal behavior của các feature. [5][7]\n\n- Trường hợp sử dụng:\n  - Nghiên cứu hiểu biết nội tại của mạng (interpretability). [3][14]  \n  - Thiết kế/điều chỉnh kiến trúc khi phát hiện filter không mong muốn hoặc khi cần tăng khả năng nhận diện các cấu trúc cụ thể. [1][3]\n\n---\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Có hai cách trực quan chính: hiển thị tất cả feature maps (slicing theo chiều sâu) và trực quan hóa filter (trọng số) sau huấn luyện. [1][2][3]  \n  - Deep Visualization Toolbox là một công cụ tiện dụng để quan sát cả trên ảnh tĩnh và chuỗi video; giao diện cho phép xem layer, feature maps, và phóng to từng feature map để phân tích chi tiết. [4][5]  \n  - Ở các lớp đầu, filter/feature map thường biểu diễn cạnh và texture (vẫn thấy bóng dáng đối tượng); ở các lớp sâu hơn, feature map có thể biểu diễn concept phức tạp (ví dụ: mặt người) và cho khả năng cục bộ hóa khi input là video. [6][7][13][14][15][16]  \n  - Việc trực quan này giúp hiểu tại sao một feature map phản ứng như vậy (qua việc trực quan filter tương ứng và thống kê ảnh kích hoạt mạnh). [11][12][14]\n\n- Tầm quan trọng:\n  - Trực quan hóa là công cụ thiết yếu để *hiểu*, *giải thích*, và *debug* mô hình CNN; giúp nối kết kiến trúc, trọng số, và hành vi thực tế trên dữ liệu. [1][3][14]  \n\n- Liên hệ với các bài giảng khác:\n  - Bài này tiếp nối phần đã giới thiệu cấu thành và cài đặt một mạng CNN (đã triển khai kiến trúc cơ bản) — bây giờ dùng trực quan hóa để hiểu hoạt động nội tại của kiến trúc đó. [1]\n\n(Trích dẫn theo các đoạn trong video: [1]...[16], mỗi số tương ứng timestamp đã cung cấp trong đề bài.)",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 3_7YLMIKqygPU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_1： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/7YLMIKqygPU",
          "start_time": 13,
          "end_time": 63,
          "text": "Trong những phần trước thì chúng ta đã tìm hiểu qua các thành phần cấu tạo của một mạng CNN Rồi sau đó chúng ta đã tiến hành cài đặt mạng CNN này với một kiến trúc rất là đơn giản đó là kiến trúc Linux Và để mà hiểu rõ hơn cái mạng CNN này thì không cách nào khác đó là chúng ta sẽ phải trực quan hóa cái mạng CNN Thế thì có rất nhiều cái cách thức để chúng ta có thể trực quan hóa được cái mạng CNN Cách đầu tiên đó là chúng ta sẽ hiển thị tất cả các cái feature map mà thực hiện được trong suốt cái quá trình mà mạng CNN Thực hiện các phép biến đổi như là Conversion, Pulling, Rally Thì đầu vào chúng ta thấy là có một cái ảnh Ờ Ờ Độ sâu là 3 tức là tương ứng 3 kênh màu Thì cái này là chúng ta trực quan hóa và con người nhìn vô là có thể hiểu một cách dễ dàng"
        },
        {
          "index": 2,
          "video_id": "Chương 3_7YLMIKqygPU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_1： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/7YLMIKqygPU",
          "start_time": 48,
          "end_time": 110,
          "text": "Thực hiện các phép biến đổi như là Conversion, Pulling, Rally Thì đầu vào chúng ta thấy là có một cái ảnh Ờ Ờ Độ sâu là 3 tức là tương ứng 3 kênh màu Thì cái này là chúng ta trực quan hóa và con người nhìn vô là có thể hiểu một cách dễ dàng Sau đó thì chúng ta sẽ tiến hành cái phép biến đổi là Conversion Thì ở cái phép biến đổi Conversion ở cái lớp đầu tiên thì nó sẽ tạo ra một cái mạng Xin lỗi nó sẽ tạo ra một cái feature map Và cái feature map này có cái độ sâu là D Thì chúng ta sẽ trực quan hóa bằng cách đó là Cắt ra các cái lá cắt ở D Cái độ sâu là D Cái độ sâu này Và ứng với mỗi cái lá cắt chúng ta sẽ hiển thị Trên màn hình Để xem coi là cái gì nằm ở bên trong cái lá cắt này Thì từ trong ra bên ngoài Đúng không? Thì chúng ta sẽ có D cái lá cắt Và có bao nhiêu cái lá cắt thì chúng ta sẽ hiển thị lên trên hết Cái màn hình Đó Từ trong Ngoài Có bao nhiêu cái lá cắt thì chúng ta sẽ hiển thị lên hết"
        },
        {
          "index": 3,
          "video_id": "Chương 3_7YLMIKqygPU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_1： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/7YLMIKqygPU",
          "start_time": 99,
          "end_time": 161,
          "text": "Thì chúng ta sẽ có D cái lá cắt Và có bao nhiêu cái lá cắt thì chúng ta sẽ hiển thị lên trên hết Cái màn hình Đó Từ trong Ngoài Có bao nhiêu cái lá cắt thì chúng ta sẽ hiển thị lên hết Thì đây là cái cách trực quan đầu tiên Các bạn có thể hiển thị lên hết   Và Cách trực quan thứ 2 đó là Chúng ta khi mà mô hình mạng CNN nó huấn luyện xong Thì nó sẽ có các cái filter Và filter này Là các cái trọng số Mà mạng CNN Nó đã huấn luyện Và tự động Bắt điền các cái giá trị ở bên trong cái filter này Và chúng ta sẽ trực quan hóa cái filter này Để xem coi Sau khi huấn luyện xong thì các cái filter này nó nhìn như thế nào Thì đây là 2 cái cách chính Để giúp cho chúng ta Có thể trực quan hóa cái filter này Trực quan hóa một cái mạng CNN Rồi Thì ở đây có các cái nhà khoa học Họ đã tạo ra một cái công cụ Đó là Deep Visualization Toolbox Thì chúng ta có thể gõ với cái từ khóa là Deep Visualization Toolbox như ở trên"
        },
        {
          "index": 4,
          "video_id": "Chương 3_7YLMIKqygPU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_1： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/7YLMIKqygPU",
          "start_time": 149,
          "end_time": 211,
          "text": "Rồi Thì ở đây có các cái nhà khoa học Họ đã tạo ra một cái công cụ Đó là Deep Visualization Toolbox Thì chúng ta có thể gõ với cái từ khóa là Deep Visualization Toolbox như ở trên Và nó sẽ ra cái video đầu tiên Với cái video đầu tiên này Thì chúng ta sẽ cùng Quan sát xem là Các tác giả họ đã tiến hành trực quan hóa như thế nào Thì đây là tên của Cái công trình nghiên cứu Của các tác giả họ đã tiến hành trực quan hóa như thế nào Các tác giả làm về Deep Visualization Toolbox Thì chúng ta sẽ xem qua cái giao diện Chúng ta sẽ cùng xem qua Cái giao diện của cái hướng dụng này Đầu tiên ở phía trên Bên tay trái Đó chính là cái tấm ảnh đầu vào Cái mạng CNN Đây chính là cái ảnh màu Và các tác giả đã thiết kế cái chương trình Để cho phép là chúng ta có thể truyền vào cái ảnh Tỉnh Hoặc là chúng ta có thể Truyền vào cái ảnh tỉnh Truyền vào một cái đoạn video Thì khi mà chúng ta đưa vào cái video Thì chúng ta sẽ quan sát xem"
        },
        {
          "index": 5,
          "video_id": "Chương 3_7YLMIKqygPU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_1： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/7YLMIKqygPU",
          "start_time": 197,
          "end_time": 260,
          "text": "Và các tác giả đã thiết kế cái chương trình Để cho phép là chúng ta có thể truyền vào cái ảnh Tỉnh Hoặc là chúng ta có thể Truyền vào cái ảnh tỉnh Truyền vào một cái đoạn video Thì khi mà chúng ta đưa vào cái video Thì chúng ta sẽ quan sát xem Cái Feature Mark Nó sẽ biến đổi như thế nào Trong suốt quá trình mà chúng ta chuyển động Ở bên trong cái video Rồi ở phía trên thì chúng ta sẽ thấy là Có các cái lớp biến đổi Ví dụ như là Conversion Pooling Normalize, Conversion số 2 Conversion số 3 Conversion số 5 Rồi các cái lớp biến đổi là FC Và lớp cuối cùng là Softmark Để tạo ra cái cái form Bốn sát xuất Thì ở đây người ta đang focus vào Cái lớp Conversion số 1 Và nhìn vào cái hình này Thì các bạn có thể đoán ra xem Cái Feature Mark Sau khi thực hiện cái lớp Conversion số 1 Là có độ sâu là bao nhiêu Các bạn thử tính toán xem Rồi Thì chúng ta để ý ha Là xét trên hàng nè"
        },
        {
          "index": 6,
          "video_id": "Chương 3_7YLMIKqygPU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_1： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/7YLMIKqygPU",
          "start_time": 247,
          "end_time": 309,
          "text": "Sau khi thực hiện cái lớp Conversion số 1 Là có độ sâu là bao nhiêu Các bạn thử tính toán xem Rồi Thì chúng ta để ý ha Là xét trên hàng nè Thì chúng ta có 3, 6, 9, 10 Như vậy là ở trên hàng này Có 10 10 cái ô Tương ứng là 10 cái lá cắt Còn theo chiều dọc Thì chúng ta có 3, 6, 9 Như vậy là nguyên cái khối này Là có 10 x 9 Tức là 90 cái lá cắt Cộng thêm 3, 6 Tức là 6 cái Feature Mark Ở phía cuối nữa Như vậy là tổng chúng ta sẽ có là 96 Như vậy là cái lớp Feature Mark đầu tiên Cái lớp Feature Mark đầu tiên Đó chính là có 96 cái lá cắt Rồi Và chúng ta có 6 cái Feature Mark đầu tiên Chúng ta sẽ nhìn vô cái hình thù của Cái lá cắt này nó như thế nào ha Thì ở đây chúng ta sẽ nhìn vô Đâu đó chúng ta có thể đoán Hình dáng của Của cái đối tượng Trên cảng góc Ví dụ ở đây chúng ta thấy có ngựa vàng"
        },
        {
          "index": 7,
          "video_id": "Chương 3_7YLMIKqygPU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_1： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/7YLMIKqygPU",
          "start_time": 298,
          "end_time": 360,
          "text": "Cái lá cắt này nó như thế nào ha Thì ở đây chúng ta sẽ nhìn vô Đâu đó chúng ta có thể đoán Hình dáng của Của cái đối tượng Trên cảng góc Ví dụ ở đây chúng ta thấy có ngựa vàng Thì ở trên cái Feature Mark Các bạn cũng có thể thấy Đâu đó có cái bóng dáng của Cái đối tượng chính của mình Đó là cái con ngựa vàng Nhưng mà đương nhiên là Ở đây mình đoán trước khi Mình thấy được cái ảnh đầu vào thôi Và khi thay đổi Với các cái hình này Thì cái Feature Mark của mình sẽ thay đổi Rồi bây giờ Để một cái phần rất là quan trọng Đó chính là chúng ta sẽ Thử nghiệm đưa vào Một cái chuỗi Video Frame Ở bên phía trên bên trái ha Và chúng ta sẽ xem cái Feature Mark ở bên phải nó thay đổi như thế nào Thì chúng ta sẽ quan sát là Có một số cái Feature Mark Có cái độ Respond Hay là cái độ sát Nó rất là sáng So với lại những Feature Mark khác Ví dụ như chúng ta thấy ở trung tâm màn hình này"
        },
        {
          "index": 8,
          "video_id": "Chương 3_7YLMIKqygPU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_1： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/7YLMIKqygPU",
          "start_time": 349,
          "end_time": 410,
          "text": "Có một số cái Feature Mark Có cái độ Respond Hay là cái độ sát Nó rất là sáng So với lại những Feature Mark khác Ví dụ như chúng ta thấy ở trung tâm màn hình này Có hai cái Feature Mark này Nó rất là sáng Còn các cái Feature Mark này Thì chúng ta nhìn thấy có bóng dáng của Cái người đang thực hiện cái Demo ở đây Nhưng mà cái độ sáng nó yếu hơn Vậy thì hai cái Feature Mark này Nó có cái ý nghĩa là gì Các bạn có thể đoán được hay không Về cái biên cạnh của phía dưới stirring Mình sẽ xem các bạn có thể tham gia thử với chúng tôi Vì vậy chúng ta sẽ Thêm một cái cửa sổ nữa ở đây Đó là cái cửa sổ này đó là phóng to Của cái Feature Mark mà chúng ta đang highlight ở đây Chúng ta nhấp vô chọn cái Feature Mark ở đây Và bên đây sẽ là phóng to ra Thì các bạn có thể đoán ra được Là cái tính chất của cái Feature Mark này Đó chính là tạo ra các cái biên cạnh theo chiều dọc"
        },
        {
          "index": 9,
          "video_id": "Chương 3_7YLMIKqygPU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_1： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/7YLMIKqygPU",
          "start_time": 398,
          "end_time": 460,
          "text": "Chúng ta nhấp vô chọn cái Feature Mark ở đây Và bên đây sẽ là phóng to ra Thì các bạn có thể đoán ra được Là cái tính chất của cái Feature Mark này Đó chính là tạo ra các cái biên cạnh theo chiều dọc Tạo ra các cái biên cạnh theo chiều dọc Tuy nhiên, các bạn có để ý là cái bên cạnh theo chiều dòng thì cái thằng bên phải, cái feature map bên phải nó cũng có cái hình thù tương tự như vậy và cũng tạo ra các cái bên cạnh theo chiều dòng tương tự như vậy Vậy thì cái sự khác nhau giữa hai cái feature map này đó là gì? Bây giờ chúng ta sẽ thử đưa vô một cái tòa giấy Đúng không? Thì các bạn sẽ thấy nè cái feature map ở bên tay trái là nó sẽ phát sáng lên cái đường biên theo chiều dòng Đúng như cái gì chúng ta dự đoán Nhưng tại sao cũng là đường biên theo chiều dòng nhưng mà cái feature map bên tay phải không phát sáng? Thì ở đây, hai cái feature map này đều là feature map để thể hiện cái biên cạnh theo chiều dòng nhưng mà nó sẽ có hai cái ý nghĩa khác nhau"
        },
        {
          "index": 10,
          "video_id": "Chương 3_7YLMIKqygPU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_1： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/7YLMIKqygPU",
          "start_time": 449,
          "end_time": 511,
          "text": "nhưng mà cái feature map bên tay phải không phát sáng? Thì ở đây, hai cái feature map này đều là feature map để thể hiện cái biên cạnh theo chiều dòng nhưng mà nó sẽ có hai cái ý nghĩa khác nhau Cái feature map bên tay trái là nó sẽ phát sáng, nó sẽ phản hồi Khi bên trái là cái vùng sáng và bên phải nó là vùng tối thì nó sẽ phản hồi Rồi khi đưa tòa giấy này qua Khi đưa cái tòa giấy này qua bên đây thì các bạn sẽ cùng xem, cùng theo dõi cái feature map bên tay phải thì chiếu ra bên đây chúng ta thấy là cái feature map bên tay phải cũng là biên cạnh theo chiều dòng và nó mới bắt đầu nó phát sáng Trong khi đó, cái feature map bên tay phải cũng là biên cạnh theo chiều dòng Bên tay trái nó đã tối, nó không còn phát sáng cái biên cạnh nữa Thì cái biên cạnh theo chiều dòng này nó có cái ý nghĩa đó là nó dịch chuyển từ vùng tối sang cái vùng sáng hơn Bên trái chúng ta thấy là cái gương mặt"
        },
        {
          "index": 11,
          "video_id": "Chương 3_7YLMIKqygPU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_1： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/7YLMIKqygPU",
          "start_time": 498,
          "end_time": 560,
          "text": "Bên tay trái nó đã tối, nó không còn phát sáng cái biên cạnh nữa Thì cái biên cạnh theo chiều dòng này nó có cái ý nghĩa đó là nó dịch chuyển từ vùng tối sang cái vùng sáng hơn Bên trái chúng ta thấy là cái gương mặt đó là có cái màu nó tối còn bên phải đó là vùng sáng thì nó trái ngược với lại cái feature map này ha feature map này là bên trái sáng bên phải tối thì nó sẽ respawn còn... Còn... feature map bên đây thì là bên trái tối, bên phải sáng thì nó mới respawn Thì đó chính là cái ý nghĩa của hai cái feature map này Dark to light Vậy, tương tự như hồi nãy đã đề cập thì hai cái feature map tương ứng ở hai cái vị trí này thì nó được tạo ra bởi các cái filter khác nhau và ở đây thì chúng ta sẽ cùng trực quan hóa cái filter của hai cái feature map này Đối với cái feature map bên tay phải thì chúng ta sẽ trực quan hóa cái filter của hai cái feature map này Bên trái sáng lên thì chúng ta thấy là những cái vùng nào mà có giá trị thấp"
        },
        {
          "index": 12,
          "video_id": "Chương 3_7YLMIKqygPU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_1： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/7YLMIKqygPU",
          "start_time": 549,
          "end_time": 611,
          "text": "cùng trực quan hóa cái filter của hai cái feature map này Đối với cái feature map bên tay phải thì chúng ta sẽ trực quan hóa cái filter của hai cái feature map này Bên trái sáng lên thì chúng ta thấy là những cái vùng nào mà có giá trị thấp nó sẽ là màu tối và vùng nào có giá trị cao thì nó sẽ là màu sáng thì chúng ta thấy là đúng như là cái hình minh họa của cái feature map thì đối với cái filter nó cũng có tính chất tương tự như vậy Filter chúng ta sẽ thấy là bên trái sẽ có vùng tối và bên phải nó sẽ có vùng sáng tương tự cái filter ở bên tay trái bên trái sẽ là vùng sáng và bên phải sẽ là vùng tối ổng thì ý nghĩa của hai cái filter này đó chính là lọc các cái miên cạnh theo chiều dọc nhưng mà cái filter này thì sẽ là chuyển từ tối sang sáng còn cái filter này thì chuyển từ sáng sang tối sau đó thì chúng ta"
        },
        {
          "index": 13,
          "video_id": "Chương 3_7YLMIKqygPU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_1： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/7YLMIKqygPU",
          "start_time": 599,
          "end_time": 661,
          "text": "thì chuyển từ sáng sang tối sau đó thì chúng ta sẽ tiến hành trực quan hóa các lớp pooling normalization Thì trong cái bài học này thì chúng ta sẽ tiến hành trực quan hóa các lớp pooling normalization chúng ta không đề cập đến normalization nhưng mà normalize chính là một cái phép để chủng hóa cái feature map của mình rồi, lớp comparison số 3 thì chúng ta thấy là đến cái lớp comparison số 3 thì cái số lượng feature map của mình nhiều hơn và đồng thời là cái kích thước của nó cũng sẽ nhỏ hơn nhưng mà đồng thời thì khi chúng ta nhìn vô các cái feature map này chúng ta sẽ khó mường tượng hơn cái đối tượng gốc ở đây chúng ta sẽ thấy các cái đốn sáng còn chúng ta sẽ không còn thấy cái bóng dáng của cái đối tượng gốc ở trên cái ảnh đầu vào nữa và đến cái lớp comparison cuối cùng, đó là lớp comparison số 5 thì chúng ta sẽ trực quan hóa"
        },
        {
          "index": 14,
          "video_id": "Chương 3_7YLMIKqygPU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_1： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/7YLMIKqygPU",
          "start_time": 653,
          "end_time": 708,
          "text": "và đến cái lớp comparison cuối cùng, đó là lớp comparison số 5 thì chúng ta sẽ trực quan hóa một vài cái feature map và ở đây chúng ta thấy là có một cái feature map nó rất là sáng và khi phóng to lên ở đây chúng ta thấy là nó sẽ có một số cái tính chất gì đấy thì để làm sao biết cái ý nghĩa của cái feature map này là gì thì người ta có một cái trick một cái mẹo đó là với cái feature map này người ta sẽ thống kê lại tốt chính cái tấm ảnh chính cái tấm ảnh mà làm cho cái feature map này nó sáng nhiều nhất nó sáng rực nhất thì các bạn đoán xem điểm chung của tất cả chính cái tấm ảnh này đó là gì? thì chúng ta sẽ biết được cái concept cái ý nghĩa của cái feature map này là gì?"
        },
        {
          "index": 15,
          "video_id": "Chương 3_7YLMIKqygPU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_1： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/7YLMIKqygPU",
          "start_time": 698,
          "end_time": 750,
          "text": "thì các bạn đoán xem điểm chung của tất cả chính cái tấm ảnh này đó là gì? thì chúng ta sẽ biết được cái concept cái ý nghĩa của cái feature map này là gì? thì chắc là các bạn có thể đoán ra được rồi đúng không? chính cái tấm ảnh này điểm chung của nó nó đều là có cái gương mặt và chúng ta cũng để ý đó là trên cái khu vực mà nó phát sáng ở đây đúng không? thì chúng ta ánh xạ lên trên cái ảnh góc chúng ta ánh xạ lên trên ảnh góc thì chúng ta cũng thấy đó là cái vùng gương mặt là nó phát sáng à cái vùng gương mặt là nó phát sáng và cái vùng gương mặt nó cũng tương ứng là cái vị trí mà mình phát sáng ở đây đó thì đó là chúng ta hình dung cái concept của cái lớp Conrugin số 5 tại cái feature map này ha cái ý nghĩa của cái concept này đó là thể hiện cái sự xuất hiện của gương mặt"
        },
        {
          "index": 16,
          "video_id": "Chương 3_7YLMIKqygPU",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_1： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/7YLMIKqygPU",
          "start_time": 747,
          "end_time": 798,
          "text": "thể hiện cái sự xuất hiện của gương mặt và khi cái người đàn ông này họ di chuyển về tay phải thì cái feature map tương ứng các bạn thấy là cái vùng đốn sáng của mình cũng di chuyển về tay phải thì điều này nó thể hiện cái chuyện gì thì chút nữa chúng ta sẽ cùng bình luận thôi thì chút nữa chúng ta sẽ cùng bình luận thôi sau đó mình góc một cái người đàn ông mà áo đen họ bước vào cái khung hình và chúng ta thấy lại có một cái sự xuất hiện của một cái đốn sáng nữa thì chúng ta thấy là ok như vậy cái giả thuyết của chúng ta là đã đúng đó là cái concept của chung của chính cái tấm hình này đó chính là có cái sự xuất hiện của gương mặt và khi có một cái gương mặt mới đi vào thì cái đốn sáng của cái gương mặt đó nó cũng phát sáng"
        }
      ]
    },
    {
      "video_id": "Chương 3_gmQTGRTHH2o",
      "summary": "## 1. Giới thiệu (Introduction)\n- Mục tiêu chính của bài giảng:\n  - Giải thích và trực quan hóa ý nghĩa của các feature map trong mạng CNN — cách các feature map phản ứng với sự biến đổi vị trí, tỉ lệ và nội dung ảnh, và cách tận dụng các feature map này cho các bài toán như Object Detection, Segmentation và OCR. [1][2][3][11]\n\n- Các khái niệm sẽ được đề cập:\n  - Hành vi không gian của feature map (mối liên hệ giữa vị trí vật thể trên ảnh và vị trí kích hoạt trên feature map). [1][2]  \n  - Tính *bất biến*/tương quan theo tỉ lệ (khi vật thể lớn/nhỏ thì vùng kích hoạt thay đổi theo tỉ lệ). [2]  \n  - Cách trích xuất vùng quan tâm từ feature map (threshold → blob → bounding box → nội suy về ảnh gốc) để giải quyết detection/segmentation. [3]  \n  - Ý nghĩa khái niệm (concept) của từng feature map thông qua ví dụ trực quan (mặt người, mặt mèo, nếp nhăn quần áo, chữ viết). [4][5][6][7][8][9][10]  \n  - Ứng dụng thực tế: Object Detection, Segmentation, OCR; và công cụ demo (Deep Visualization Toolbox) giúp hiểu feature map và filter. [3][11][12]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Mối quan hệ giữa vị trí vật thể và vị trí kích hoạt trên feature map\n- Khi một vật (ví dụ gương mặt) lùi/tiến hoặc dịch chuyển trên ảnh, điểm sáng (activation) tương ứng trên feature map cũng dịch chuyển theo cùng hướng; kích thước của điểm sáng thay đổi tương ứng với tỉ lệ của vật trên ảnh. Điều này cho thấy feature map phản ánh quan hệ không gian giữa ảnh đầu vào và bản đồ đặc trưng. [1]  \n- Cụ thể: gương mặt bên trái → đốm sáng bên trái trên feature map; gương mặt trên → đốm sáng trên; gương mặt nhỏ hơn → đốm sáng nhỏ hơn. [1][2]\n\n### 2.2. Tính bất biến/tương quan theo tỉ lệ (scale behavior)\n- Feature map thể hiện *tương quan theo tỉ lệ*: khi kích thước vật trong ảnh thay đổi (ví dụ gương mặt nhỏ hơn 1/2), kích thước vùng kích hoạt trên feature map tương ứng cũng giảm (xấp xỉ 1/2). Đây là cơ sở để dùng feature map cho những bài toán cần giữ mối liên hệ tỉ lệ. [2]\n\n### 2.3. Tận dụng feature map cho Object Detection và Segmentation\n- Phương pháp đơn giản từ bài giảng: threshold feature map để lấy các vùng \"đốm sáng\" (blobs) có độ sáng lớn hơn ngưỡng → xác định tọa độ các blob → tạo bounding box (hình chữ nhật bao quanh) cho từng blob → nội suy (upsample) tọa độ bounding box về kích thước ảnh gốc → thu được các hộp phát hiện đối tượng. Đây là ý tưởng cơ bản ứng dụng mạng CNN cho Object Detection và thậm chí Segmentation (khi cần trích xuất vùng chuẩn hơn). [3]\n\n### 2.4. Ý nghĩa khái niệm (concept) của từng feature map: từ đặc trưng đến mức độ kích hoạt\n- Một feature map tương ứng với một *concept* — tức là một tập các đặc trưng hình học/đặc trưng hình thái mà neuron/filter quan tâm. Mức độ \"sáng\" (activation) thể hiện mức độ khớp giữa vùng ảnh và concept đó. [4][6][12]\n\n- Ví dụ minh họa về khác biệt concept và mức độ kích hoạt:\n  - Mặt người: khi đưa ảnh mặt người, feature map cho concept \"mặt người\" phát sáng rõ rệt (activation mạnh). [4]  \n  - Mặt mèo: khi đưa ảnh con mèo, cùng feature map \"mặt người\" chỉ phát sáng yếu hơn, vì mèo chia sẻ vài đặc trưng (mắt, mũi) nhưng thiếu/khác các đặc trưng khác (ví dụ miệng) — do đó activation chỉ \"hơi sáng\". Điều này cho thấy concept được mã hóa một cách chọn lọc theo các đặc trưng hình học xuất hiện trong ảnh. [4][5][6]\n\n### 2.5. Feature map phát hiện các pattern cụ thể (ví dụ nếp nhăn quần áo)\n- Có feature map phát hiện nếp nhăn/texture của quần áo: khi nếp nhăn xuất hiện thì vùng tương ứng trên feature map sáng; khi nếp nhăn bị làm phẳng (nhẵn áo) thì vùng sáng biến mất, mặc dù quần áo vẫn tồn tại — điều này chỉ ra rằng feature map cụ thể đó không đơn thuần là \"quần áo\" mà là \"nếp nhăn/chi tiết bề mặt\" của quần áo. [7][8]\n\n### 2.6. Feature map phát hiện chữ viết (text) — hướng tới OCR\n- Có feature map tối (không sáng) với hầu hết ảnh, nhưng khi cho vào ảnh có chữ viết (ví dụ bìa sách, bìa tập), khu vực tương ứng phát sáng. Thậm chí khi vùng chữ tách thành hai nhóm ký tự thì feature map tách ra làm hai đốm sáng riêng biệt. Điều này chứng tỏ feature map đó mã hóa concept \"có chữ viết / dòng chữ\". [9][10][11]  \n- Từ đó, ta có thể dùng feature map để phát hiện vùng chứa chữ viết (detect vị trí xuất hiện chữ) rồi trích xuất và đưa vào mô-đun nhận dạng ký tự (OCR). [11]\n\n### 2.7. Vai trò của trực quan hóa (Deep Visualization Toolbox)\n- Việc trực quan hóa feature map và filter (qua một công cụ như Deep Visualization Toolbox được demo) giúp hiểu rõ hơn về ý nghĩa của các feature map, giúp thống kê những ảnh làm cho một feature map sáng nhất, từ đó suy ra concept mà feature map đó đại diện. [11][12]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ cụ thể từ video:\n  - Di chuyển gương mặt lùi/tiến và sang trái/phải → đốm sáng dịch chuyển tương ứng trên feature map và thay đổi kích thước theo tỉ lệ của gương mặt trên ảnh. [1][2]  \n  - Đưa ảnh con mèo vào: cùng feature map \"mặt người\" chỉ phát sáng yếu vì chỉ có một số đặc trưng tương tự (mắt, mũi) chứ không đầy đủ (miệng không giống) → activation yếu. [4][5][6]  \n  - Đưa ảnh người có nếp nhăn áo → một feature map phát sáng tương ứng các nếp nhăn; khi nếp nhăn bị làm phẳng (chùi tay áo) thì đốm sáng mất đi; bên tay còn nếp nhăn vẫn sáng. Điều này minh họa rõ ràng feature map nhận diện texture/nếp nhăn thay vì \"quần áo\" chung chung. [7][8]  \n  - Đưa ảnh bìa sách/của sổ có chữ viết → một feature map bật sáng tương ứng vùng chữ; khi chữ tách ra thành hai vùng thì feature map tạo hai đốm sáng riêng → concept là chữ viết/dòng chữ. [9][10][11]\n\n- Ứng dụng thực tế / trường hợp sử dụng:\n  - Object Detection: dùng threshold và bounding box từ feature map để phát hiện vị trí đối tượng trên ảnh. [3]  \n  - Segmentation: dùng trực tiếp các vùng sáng trên feature map để hỗ trợ tách đối tượng (segmentation) khi cần vùng phân lớp. [3]  \n  - OCR (Optical Character Recognition): phát hiện vị trí chữ viết bằng feature map phát hiện chữ, sau đó trích xuất và nhận dạng ký tự bằng mô-đun OCR. [11]  \n  - Hiểu và cải thiện mô hình: trực quan hóa giúp xác định feature map nào học được concept mong muốn (ví dụ texture, chữ viết, hình tròn) để điều chỉnh dữ liệu/kiến trúc nếu cần. [11][12]\n\n## 4. Kết luận (Conclusion)\n- Tóm tắt các ý chính:\n  - Feature map trong CNN phản ánh mối liên hệ không gian (vị trí và tỉ lệ) giữa ảnh đầu vào và activation trên bản đồ đặc trưng: khi vật trong ảnh dịch chuyển/tỉ lệ thay đổi thì vị trí/kích thước activation cũng thay đổi tương ứng. [1][2]  \n  - Mỗi feature map thường đại diện cho một concept cụ thể (mặt người, nếp nhăn, chữ viết, v.v.), và mức độ sáng của feature map biểu diễn mức độ khớp giữa vùng ảnh và concept đó. [4][6][7][9]  \n  - Từ feature map có thể trực tiếp thực hiện các bước đơn giản để detect/segment đối tượng (threshold → blob → bounding box → nội suy), và ứng dụng cho OCR khi feature map phát hiện chữ viết. [3][11]\n\n- Tầm quan trọng của nội dung:\n  - Trực quan hóa feature map và filter là công cụ rất hữu ích để hiểu vì sao CNN hoạt động, đánh giá chất lượng các đặc trưng học được, và suy luận cách tận dụng các đặc trưng đó cho các bài toán thực tế trong thị giác máy tính. [11][12]\n\n- Liên hệ với các bài giảng khác:\n  - (Video nhắc rằng) hiểu rõ feature map sẽ giúp sử dụng mạng CNN hiệu quả hơn cho nhiều bài toán thị giác khi huấn luyện trên tập dữ liệu lớn; trực quan hóa là bước hỗ trợ trong chuỗi kiến thức về CNN và ứng dụng của chúng. [12]\n\nTài liệu tham khảo (các đoạn trích sử dụng trong bản tóm tắt):\n- Đoạn demo và giải thích trực quan (Deep Visualization Toolbox) dùng để minh họa các điểm nêu trên. [1][2][3][4][5][6][7][8][9][10][11][12]",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 3_gmQTGRTHH2o",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_2： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/gmQTGRTHH2o",
          "start_time": 9,
          "end_time": 66,
          "text": "Khi người đàn ông này lùi ra xa, chúng ta thấy đốm sáng này cũng lùi ra xa. Theo nghĩa đó là khi gương mặt này lùi ra xa thì rõ ràng kích thước của đốm sáng này sẽ nhỏ hơn kích thước của đốm sáng này. Và đồng thời đốm sáng này tiến về phía trên, góc phía trên của mặt khung hình. Thì đốm sáng này cũng tiến về phía trên của feature map. Chúng ta cũng để ý thêm là tỷ lệ của vùng gương mặt này, nó khoảng 1 nửa so với gương mặt này. Thì đốm sáng này cũng bằng 1 nửa. Như vậy thì điều này thể hiện tính chất gì? Nó thể hiện tính chất đó là feature map của mình sẽ mất biến về trình tự không gian. Nó sẽ mất biến về trình tự không gian nghĩa là Ví dụ như là, gương mặt bên tay trái, gương mặt bên tay phải thì đốm sáng tương ứng cũng nằm bên tay trái."
        },
        {
          "index": 2,
          "video_id": "Chương 3_gmQTGRTHH2o",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_2： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/gmQTGRTHH2o",
          "start_time": 48,
          "end_time": 110,
          "text": "Nó thể hiện tính chất đó là feature map của mình sẽ mất biến về trình tự không gian. Nó sẽ mất biến về trình tự không gian nghĩa là Ví dụ như là, gương mặt bên tay trái, gương mặt bên tay phải thì đốm sáng tương ứng cũng nằm bên tay trái. Cái gương mặt này nằm ở phía trên so với gương mặt này. Thì đốm sáng tương ứng của nó cũng nằm ở phía trên so với lại đốm sáng này. Và tỷ lệ nó sẽ bất biến về phép tỷ lệ. Bất biến đối với yếu tố về mặt tỷ lệ. Gương mặt này bằng khoảng 1 nửa so với gương mặt này. Thì đốm sáng này cũng sẽ bằng 1 nửa so với đốm sáng này. và dựa trên cái tính chất này chúng ta có thể dùng nó để cho cái bài toán đó là bài toán Object Detection hoặc là bài toán Segmentation bài toán phát hiện đối tượng hoặc là bài toán phân đoạn ngưỡng nghĩa đối tượng ví dụ đối với cái bài toán phát hiện ngư mặt thì chúng ta có cái Feature Map này rồi đúng không chúng ta sẽ dùng cái phương pháp đó là phân ngưỡng để lấy ra những cái khu vực đống sáng"
        },
        {
          "index": 3,
          "video_id": "Chương 3_gmQTGRTHH2o",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_2： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/gmQTGRTHH2o",
          "start_time": 98,
          "end_time": 157,
          "text": "hoặc là bài toán phân đoạn ngưỡng nghĩa đối tượng ví dụ đối với cái bài toán phát hiện ngư mặt thì chúng ta có cái Feature Map này rồi đúng không chúng ta sẽ dùng cái phương pháp đó là phân ngưỡng để lấy ra những cái khu vực đống sáng 2 cái đống sáng này mà sáng hơn 1 cái ngưỡng cho trước chúng ta sẽ có cái tạ độ của 2 cái đống sáng này sau đó chúng ta sẽ lấy ra cái Mounding Box cái Mounding Box tức là cái hình chữ nhật bao xung quanh 2 cái đống sáng này từ cái tạ độ của cái hình chữ nhật 2 cái đống sáng này chúng ta sẽ nội suy lên trên cái tạ độ nội suy cái tạ độ ở phía trên này ảnh góc và như vậy chúng ta sẽ có 2 cái Mounding Box 2 cái hình hộp chữ nhật bao xung quanh 2 cái ngư mặt này thì đó chính là cái ý tưởng của việc ứng dụng mạng CNN cho giải quyết bài toán Object Detection và thậm chí nó có thể giải quyết luôn cả bài toán Segmentation rồi bây giờ chúng ta sẽ mở rộng thêm cái thí nghiệm này"
        },
        {
          "index": 4,
          "video_id": "Chương 3_gmQTGRTHH2o",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_2： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/gmQTGRTHH2o",
          "start_time": 154,
          "end_time": 211,
          "text": "rồi bây giờ chúng ta sẽ mở rộng thêm cái thí nghiệm này bằng cách đó là cũng với cái Feature Map hồi nãy chúng ta sẽ đưa vô 1 cái ảnh con mèo thì với cái Feature Map mà thể hiện là gương mặt như hồi nãy đúng không chúng ta thấy cái cái Feature Map này nó có tính chất gì tính chất đầu tiên đó là nó vẫn phát sáng nhưng mà cái tính chất thứ 2 đó là nó phát sáng yếu hơn hẳn so với lại trước đây đúng không với 2 mặt người thì chúng ta thấy là nó phát sáng rất là rõ nhưng mà với con mèo với con mèo thì chúng ta thấy là nó có phát sáng nhưng mà nó không có rực rỡ nó không có sáng như cái mặt của con người như vậy thì cái ý nghĩa của cái concept này nó phải thêm 1 cái nữa đó là nó phải là gương mặt của con người nhưng mà câu hỏi đặt ra là tại sao gương mặt con mèo"
        },
        {
          "index": 5,
          "video_id": "Chương 3_gmQTGRTHH2o",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_2： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/gmQTGRTHH2o",
          "start_time": 198,
          "end_time": 262,
          "text": "nó không có sáng như cái mặt của con người như vậy thì cái ý nghĩa của cái concept này nó phải thêm 1 cái nữa đó là nó phải là gương mặt của con người nhưng mà câu hỏi đặt ra là tại sao gương mặt con mèo thì nó vẫn sáng nhưng mà nó hơi hơi sáng thì có thể giải thích đó là cái gương mặt của con mèo thì nó cũng sẽ có 1 số cái tính chất giống như con người ví dụ mặt con mèo nó cũng sẽ có mắt hoặc có lỗ mũi tuy nhiên cái phần mặt của con mèo, cái phần miệng của con mèo các bạn thấy là cái phần miệng của con mèo nó rất là bé nó chúm chím, nó rất là bé ở đây trong khi đó cái miệng của con người các bạn thấy rồi nó to hơn nhiều và nó dài hơn nhiều so với cái miệng của con mèo và chính vì chỉ có 2 trên 3 cái đặc điểm trên nên cái feature map ở đây nó chỉ hơi sáng tức là hàm ý nó chỉ có đâu đó nó có cái đặc trưng về về mắt và về mũi thôi còn cái phần về miệng nó hơi yếu nên đâm ra là cái feature map này nó sắc yếu thì cái ý nghĩa của cái feature map này đó là"
        },
        {
          "index": 6,
          "video_id": "Chương 3_gmQTGRTHH2o",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_2： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/gmQTGRTHH2o",
          "start_time": 248,
          "end_time": 311,
          "text": "cái feature map ở đây nó chỉ hơi sáng tức là hàm ý nó chỉ có đâu đó nó có cái đặc trưng về về mắt và về mũi thôi còn cái phần về miệng nó hơi yếu nên đâm ra là cái feature map này nó sắc yếu thì cái ý nghĩa của cái feature map này đó là nó sẽ dựa trên cái yếu tố về mặt hình học nó sẽ coi có cái dáng của cái ánh mắt hay không nó có cái dáng của cái lỗ mũi hay không nếu có thì nó sẽ tăng cái trọng số lên cho cái feature map đó nhưng mà cái phần miệng thì cái tỷ lệ của nó, cái dáng của nó nó ít xuất hiện nên cái tỷ lệ của cái feature map này nó cũng sẽ sáng ít hơn so với lại mặt của con người rồi và bây giờ chúng ta sẽ cùng... đi qua một số cái ví dụ khác ví dụ ở đây, chúng ta quan sát thấy có một cái feature map khá là sáng và nó tương ứng là hai cái đốm này thì các bạn đoán xem ý nghĩa của chính cái tấm ảnh của chính cái tấm ảnh mà làm cho cái feature map này nó sáng nhất"
        },
        {
          "index": 7,
          "video_id": "Chương 3_gmQTGRTHH2o",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_2： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/gmQTGRTHH2o",
          "start_time": 299,
          "end_time": 360,
          "text": "và nó tương ứng là hai cái đốm này thì các bạn đoán xem ý nghĩa của chính cái tấm ảnh của chính cái tấm ảnh mà làm cho cái feature map này nó sáng nhất ý nghĩa chính cái tấm ảnh này, đó là gì? các bạn sẽ đoán đó chính là có sự xuất hiện của quần áo đúng không ạ, có sự xuất hiện của quần áo nhưng mà cái ý nghĩa của cái concept này nó không phải là như vậy ý nghĩa của đó là cái nếp nhăn cái nếp nhăn của quần áo ví dụ vậy thì khi nào mà trên cái tấm hình nó có cái nếp nhăn thì nó sẽ phát sang đó thì ở đây cái nhân vật làm demo ở đây họ sẽ tiến hành một thiết nghiệp đó là anh này mới tìm cách đó là phủi cho cái tay áo bên tay phải đúng không, phủi cái áo làm cho nó nhẵn hơn và không còn cái nếp nhăn nữa"
        },
        {
          "index": 8,
          "video_id": "Chương 3_gmQTGRTHH2o",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_2： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/gmQTGRTHH2o",
          "start_time": 349,
          "end_time": 411,
          "text": "anh này mới tìm cách đó là phủi cho cái tay áo bên tay phải đúng không, phủi cái áo làm cho nó nhẵn hơn và không còn cái nếp nhăn nữa thì nếu như bạn nào nói là ý nghĩa của cái concept này mà là quần áo thì vô lý tại vì ở đây anh này vẫn đang mặc cái áo này mà đúng không, vẫn đang mặc cái áo này nhưng mà cái chỗ này nó không còn phát sáng nữa đúng không, vẫn đang mặc cái áo này nó không còn phát sáng nhiều nữa đó khi anh này ảnh làm cho cái áo nó nhẵn hơn đúng không thì tương ứng chỗ này nó sẽ đốm sáng nó biến mất nó nhạt dần đi đó còn bên tay trái thì các bạn thấy nè là cái đốm sáng của mình nó vẫn còn cái đốm sáng của mình nó vẫn còn rõ hơn là do nó còn nhiều cái nếp nhăn ở bên cái áo bên tay trái đó thì ý nghĩa của cái feature map này chính là cái nếp nhăn có do đó hạt phẩm không? của các nếp nhăn để đốm sáng cuối cùng, đây là một nếp nhăn thú vị nữa rồi, bây giờ chúng ta sẽ đến với phần cái feature map"
        },
        {
          "index": 9,
          "video_id": "Chương 3_gmQTGRTHH2o",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_2： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/gmQTGRTHH2o",
          "start_time": 399,
          "end_time": 459,
          "text": "chính là cái nếp nhăn có do đó hạt phẩm không? của các nếp nhăn để đốm sáng cuối cùng, đây là một nếp nhăn thú vị nữa rồi, bây giờ chúng ta sẽ đến với phần cái feature map đó chính là cái feature map này thì hiện tại trên cái khung hình này chúng ta thấy là nó tối thui tức là nó không có phát sáng đó, thì tốt chính cái tấm ảnh mà làm cho cái tấm, làm cho cái feature map này nó phát sáng đó chính là chính cái tấm ảnh này và các bạn cũng đoán xem ý nghĩa, điểm chung của chính cái tấm ảnh này đó là gì? các bạn sẽ đoán đó chính là điểm chung của chính tấm ảnh này đó là nó đều có hình tròn đúng không ạ? hoặc các bạn sẽ nói là nó sẽ có cái chữ Canon nằm bên trong hình tròn nó có cái chữ nằm bên trong hình tròn vậy thì chúng ta sẽ cùng giải đáp xem rồi, khi anh này ảnh đưa vào một cái bìa của một cái củ sách thì chúng ta thấy là cái khu vực này nó phát sáng"
        },
        {
          "index": 10,
          "video_id": "Chương 3_gmQTGRTHH2o",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_2： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/gmQTGRTHH2o",
          "start_time": 449,
          "end_time": 509,
          "text": "rồi, khi anh này ảnh đưa vào một cái bìa của một cái củ sách thì chúng ta thấy là cái khu vực này nó phát sáng và cái này nó không hề có cái hình tròn nào hết đúng không? nó không hề có cái hình tròn nào hết đúng không? như vậy cái ý nghĩa của cái concept của cái Feature Map này đó chính là phải có cái chữ phải có cái chữ viết và khi có cái chữ thì nó sẽ phát sáng do đó điểm chung của tất cả chính cái tấm ảnh này đó chính là concept về chữ viết rồi khi anh này ảnh đưa thêm một cái bìa của một cái cuốn tập hay là một cái sổ khác đúng không? thì chúng ta cũng thấy cái khu vực này nó lại tiếp tục phát sáng và nó tách ra làm hai cái đốn sáng riêng biệt và nó tách ra làm hai cái đốn sáng riêng biệt thì điều này một lần nữa khẳng định đó là cái concept của cái Feature Map này đó chính là có cái sự xuất hiện của của các cái dòng chữ chữ viết của các cái dòng chữ chữ viết"
        },
        {
          "index": 11,
          "video_id": "Chương 3_gmQTGRTHH2o",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_2： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/gmQTGRTHH2o",
          "start_time": 499,
          "end_time": 560,
          "text": "và nó tách ra làm hai cái đốn sáng riêng biệt thì điều này một lần nữa khẳng định đó là cái concept của cái Feature Map này đó chính là có cái sự xuất hiện của của các cái dòng chữ chữ viết của các cái dòng chữ chữ viết và nếu như dựa trên cái ý tưởng của cái Feature Map này và nếu như dựa trên cái ý tưởng của cái Feature Map này các bạn đoán xem chúng ta có thể ứng dụng cái Feature Map này để làm gì đó là dùng cho bài toán OCR Optical Character Recognition tức là chúng ta sẽ phát hiện xem vị trí khu vực nào có sự xuất hiện của chữ viết phát hiện xem vị trí khu vực nào có sự xuất hiện của chữ viết rồi khi chúng ta sẽ detect ra được cái vị trí có chữ viết rồi rồi khi chúng ta sẽ detect ra được cái vị trí có chữ viết rồi chúng ta sẽ trích xuất ra và dùng các cái thuật toán nhận diện để xem coi cái mặt chữ của nó là gì thì đó là toàn bộ cái nội dung của thì đó là toàn bộ cái nội dung của cái bài Deep Visualization Toolbox và hy vọng là qua cái và hy vọng là qua cái một cái demo ngắn gọn này trực quan hóa các cái phần các cái feature map cũng như là trực quan hóa các cái filter thì sẽ giúp cho các bạn hiểu rõ hơn thì sẽ giúp cho các bạn hiểu rõ hơn cái concept của các cái feature map ý nghĩa của nó là gì"
        },
        {
          "index": 12,
          "video_id": "Chương 3_gmQTGRTHH2o",
          "chapter": "Chương 3",
          "video_title": "[CS431 - Chương 3] Part 4_2： Trực quan hóa mạng CNN",
          "video_url": "https://youtu.be/gmQTGRTHH2o",
          "start_time": 549,
          "end_time": 587,
          "text": "trực quan hóa các cái phần các cái feature map cũng như là trực quan hóa các cái filter thì sẽ giúp cho các bạn hiểu rõ hơn thì sẽ giúp cho các bạn hiểu rõ hơn cái concept của các cái feature map ý nghĩa của nó là gì thông qua cái việc là thống kê tốt chính cái tấm hình mà làm cho cái feature map này nó sáng nhất thì hy vọng là nếu như các bạn có thể hiểu rõ được hơn cái mạng CNN này rồi thì sau này chúng ta có thể sử dụng được cái mạng CNN này cho các cái bài toán bên lĩnh vực thị giác máy tính rất là hiệu quả là vì cái bộ đặc trưng feature map này nó cực kỳ phong phú và nó cực kỳ tổng quát khi chúng ta huấn luyện trên một cái tập dữ liệu đủ lớn"
        }
      ]
    },
    {
      "video_id": "Chương 4_PyC3pl_r8jw",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng:\n  - Ôn tập kiến trúc mạng CNN cơ bản, sau đó giới thiệu một số kiến trúc CNN phổ biến hiện nay và vai trò của các kiến trúc này làm nền tảng cho các thuật toán thị giác máy tính; cuối cùng sẽ trình bày các mức (cấp) để sử dụng mô hình huấn luyện sẵn (pretrained model). [1]\n\n- Các khái niệm sẽ được đề cập:\n  - Đầu vào hình ảnh (RGB / grayscale), phép biến đổi Convolution, ReLU, Feature map, Pooling, Flatten → Fully Connected, Softmax, cấu trúc tầng Convolution (nhiều filter → feature map), khái quát về các kiến trúc lịch sử (LeNet, AlexNet) và dữ liệu chuẩn như ImageNet. [1][2][3][4][5][7][9][10][11]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 Đầu vào và pipeline tổng quát của CNN\n- Đầu vào của mạng CNN là ảnh (màu RGB hoặc ảnh xám); ví dụ ảnh màu có 3 kênh: Red, Green, Blue. [1][2]  \n- Pipeline điển hình: Input image → Convolution + ReLU → Feature map (tensor) → Pooling (giảm kích thước không gian) → lặp lại (Conv/Relu/Pool) → Flatten → Fully Connected → Softmax. [2][3][4]\n\n### 2.2 Convolution và Feature Map\n- Phép Convolution là một phép biến đổi tuyến tính (linear transform) dùng để trích xuất đặc trưng hình ảnh (ví dụ filter phát hiện biên dọc). Các filter ban đầu được khởi tạo ngẫu nhiên và sau đó được cập nhật trong quá trình huấn luyện để tối ưu hóa hiệu năng nhận diện. [5][6]  \n- Một tầng convolution thực chất là áp dụng nhiều phép convolution song song với nhiều filter khác nhau; mỗi filter tạo ra một feature, và tập hợp các feature này tạo thành feature map. [6][7]  \n- Yêu cầu về kích thước filter: chiều sâu của filter phải khớp chiều sâu của input (nếu input có độ sâu D thì filter có depth = D). Nếu có K filter thì độ sâu của output (feature map) bằng K. (Filter_depth = D; Output_depth = K.) [7][8]\n\n### 2.3 Activation (kích hoạt) và Softmax\n- Sau Convolution thường áp dụng hàm kích hoạt như ReLU (trong phần ôn tập đã nêu ReLU). [2]  \n- Ở phần cuối, vector đầu ra của lớp fully connected được đưa qua hàm Softmax để chuyển thành phân bố xác suất (mỗi phần tử nằm trong [0,1] và tổng các xác suất trên các lớp bằng 1). Ví dụ các lớp Car, Truck, Van, Bicycle có tổng xác suất = 1 sau Softmax. [4]\n\n### 2.4 Pooling — giảm kích thước không gian\n- Pooling (ví dụ stride S = 2) giảm kích thước theo chiều ngang và chiều cao đi 1/S: W_out = W / S, H_out = H / S (ví dụ S=2 ⇒ W_out = W/2, H_out = H/2). [2][3]  \n- Pooling được thực hiện độc lập trên từng kênh (từng feature) nên chiều sâu D được giữ nguyên. [3]  \n- Nếu W và H đều giảm 2 lần thì tổng số phần tử trong tensor giảm 4 lần (S^2 khi S=2). Nói chung, khi giảm theo cả 2 chiều với cùng stride S, số phần tử giảm S^2 lần. [13]  \n- Pooling có hai lợi ích chính: giảm overfitting (bằng cách giảm số tham số khi đưa vào fully connected) và tăng tốc tính toán (ít dữ liệu hơn để xử lý). [14][15]\n\n(Công thức tóm tắt)\n- W_out = W / S, H_out = H / S  (S = stride pool)  — dẫn tới tổng phần tử giảm S^2 lần khi pool trên cả hai chiều. [2][3][13]\n\n### 2.5 Flatten và Fully Connected\n- Sau các tầng convolution/pooling, tensor feature cuối cùng được flatten (duỗi ra) thành vector để đưa vào các lớp fully connected (mạng neural dạng vector). Lý do: lớp fully connected chỉ hoạt động trên vector. [3][4][13][14]  \n- Kích thước vector giảm khi kích thước feature map giảm (ví dụ pooling làm giảm kích thước vector xuống còn 1/4 khi W và H giảm 2 lần), dẫn tới giảm số lượng trọng số trong các lớp fully connected. Điều này giúp giảm overfitting và giảm tính toán. [13][14][15]\n\n### 2.6 Lược sử ngắn và các mốc quan trọng\n- Mạng CNN có lịch sử từ những năm 1990 (khoảng gần 30 năm), nhưng độ chính xác chưa thực sự bứt phá cho tới các cải tiến như AlexNet (năm 2012), làm bùng nổ Deep Learning trong thị giác máy tính. [10]  \n- ImageNet (Large Scale Visual Recognition Challenge) là một tập dữ liệu quy mô lớn: ~14 triệu ảnh và ~20,000 lớp, được sử dụng cho cuộc thi hàng năm từ ~2010; hai bài toán chính là classification và object detection. [9][10]  \n- LeNet là một kiến trúc sớm nổi bật, đem lại hai ý tưởng quan trọng: sử dụng convolution (kết nối cục bộ, locally connected) thay vì fully connected toàn phần, và chia sẻ trọng số (weight sharing) để giảm số tham số và giảm overfitting. LeNet cũng sử dụng average pooling (subsampling). [11][12][15][16]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa filter phát hiện cạnh dọc: một filter nhân với vùng ảnh sẽ tạo ra một feature đại diện cho miền cạnh theo chiều dọc. Các filter này khởi tạo ngẫu nhiên rồi được học trên dữ liệu thực để tối ưu hóa nhiệm vụ nhận diện. [5][6]  \n- Ứng dụng thực tế / trường hợp sử dụng được đề cập:\n  - Các kiến trúc CNN được dùng làm nền tảng cho nhiều bài toán thị giác máy tính, cụ thể trong các cuộc thi trên ImageNet (phân lớp ảnh, phát hiện đối tượng). [1][9][10]  \n  - Việc giảm kích thước qua pooling và flatten giúp triển khai các mạng có hiệu năng tốt hơn trên bài toán phân loại với số lớp lớn (ví dụ phân lớp nhiều loại phương tiện như Car, Truck, ...). [4][13][14]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Mạng CNN xử lý ảnh đầu vào bằng chuỗi Convolution → Activation → Pooling, tạo feature map rồi flatten cho fully connected; Softmax tạo phân bố xác suất cuối cùng. [2][3][4]  \n  - Convolution với nhiều filter tạo thành tầng convolution (filter_depth = input_depth D; output_depth = K nếu có K filter). [6][7]  \n  - Pooling giảm kích thước không gian (W,H), giữ nguyên chiều sâu D, giảm tổng phần tử S^2 lần (với stride S), qua đó giảm tham số của fully connected và giảm overfitting, tăng tốc tính toán. [2][3][13][14][15]  \n  - Lược sử: LeNet khởi xướng convolution, weight sharing và pooling; AlexNet (2012) là mốc làm bùng nổ Deep Learning; ImageNet là benchmark lớn đóng vai trò then chốt. [11][10][9]\n\n- Tầm quan trọng:\n  - Kiến thức về cấu trúc và các thành phần cơ bản của CNN là nền tảng để hiểu và sử dụng các kiến trúc CNN phổ biến hiện nay, cũng như để tận dụng mô hình huấn luyện sẵn (pretrained models) trong các ứng dụng thực tế. [1][8][9]\n\n- Liên hệ với các bài giảng khác:\n  - Bài giảng sẽ tiếp tục với phần giới thiệu các kiến trúc CNN phổ biến (sẽ khảo sát LeNet, AlexNet, và các kiến trúc tiếp theo) và cách sử dụng pretrained models ở các mức khác nhau — các nội dung này đã được nhắc tới như định hướng tiếp theo trong bài. [1][10][11][8]\n\n(Chú ý: mọi thông tin trong bản tóm tắt trên được trích từ các đoạn tương ứng của video.)",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 4_PyC3pl_r8jw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_1_ Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/PyC3pl_r8jw",
          "start_time": 15,
          "end_time": 59,
          "text": "Chuyển sang bài 4, các kiến trúc mạng CNN phổ biến Chúng ta sẽ ôn tập lại kiến trúc mạng CNN, sâu đó chúng ta sẽ đề cập đến một số kiến trúc mạng CNN phổ biến hiện nay. Và các kiến trúc mạng này làm ra nền tảng để cho các thuật toán cũng như các bài toán trong thị giác máy tính về sâu họ sử dụng để hoàn phát triển tiếp. Cuối cùng, chúng ta sẽ tìm hiểu các các cấp thức để sử dụng một cái mạng huấn luyện sẵn Pretrained Model như thế nào. Về ôn tập mạng CNN, chúng ta biết đầu vào của mạng CNN sẽ là tấm ảnh và ảnh này có thể là ảnh màu hoặc là ảnh xám (grayscale). Trong ví dụ này, ảnh này có 3 kênh màu, đó là Red, Green và Blue."
        },
        {
          "index": 2,
          "video_id": "Chương 4_PyC3pl_r8jw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_1_ Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/PyC3pl_r8jw",
          "start_time": 43,
          "end_time": 113,
          "text": "Về ôn tập mạng CNN, chúng ta biết đầu vào của mạng CNN sẽ là tấm ảnh và ảnh này có thể là ảnh màu hoặc là ảnh xám (grayscale). Trong ví dụ này, ảnh này có 3 kênh màu, đó là Red, Green và Blue. Qua phép biến đổi Convolution và Relu ngay sâu đó, chúng ta sẽ tạo ra một cái tensor. Và cái tensor này, nó còn gọi là Feature Map. Rồi, sâu đó chúng ta sẽ tiến hành thực hiện phép Pooling để giảm chiều của dữ liệu này. Về tấm ảnh này, Feature Map sẽ có kích thước bề ngang là W, bề cao là H và độ sâu là D. Khi thực hiện phép Pooling, mặc định S của mình cho là bằng 2, kích thước theo chiều ngang và chiều cao sẽ giảm một nửa. Tức là bề ngang của mình lúc này sẽ còn là W chia 2 và bề cao của mình trong trường hợp này sẽ giảm xuống còn là H chia 2."
        },
        {
          "index": 3,
          "video_id": "Chương 4_PyC3pl_r8jw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_1_ Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/PyC3pl_r8jw",
          "start_time": 98,
          "end_time": 162,
          "text": "Khi thực hiện phép Pooling, mặc định S của mình cho là bằng 2, kích thước theo chiều ngang và chiều cao sẽ giảm một nửa. Tức là bề ngang của mình lúc này sẽ còn là W chia 2 và bề cao của mình trong trường hợp này sẽ giảm xuống còn là H chia 2. Nếu S bằng 3 thì cái này sẽ giảm 3 lần. Và chúng ta sẽ thực hiện phép Pooling trên các kênh độc lập nhau, các Feature độc lập nhau. Do đó thì cái D này sẽ duy trì nguyên. Chúng ta chỉ giảm bề ngang và bề cao của Feature Map thôi. Rồi tương tự như vậy cũng thực hiện với các phép Convolution, Relu và Pooling. Thì đến cái bước cuối cùng thì chúng ta cũng sẽ ra được một cái Tensor đó là Feature. Và Feature này thì nó sẽ được duỗi ra để tạo ra thành một cái dạng là Vector. Tại vì các phép biến đổi Fully Connected phía sâu hay còn gọi là cái mạng Neural Network của mình, thì nó chỉ có thể thực hiện được trên Vector mà thôi."
        },
        {
          "index": 4,
          "video_id": "Chương 4_PyC3pl_r8jw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_1_ Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/PyC3pl_r8jw",
          "start_time": 145,
          "end_time": 213,
          "text": "Và Feature này thì nó sẽ được duỗi ra để tạo ra thành một cái dạng là Vector. Tại vì các phép biến đổi Fully Connected phía sâu hay còn gọi là cái mạng Neural Network của mình, thì nó chỉ có thể thực hiện được trên Vector mà thôi. Rồi, thì cái Vector này khi mà chúng ta đi qua cái lớp biến đổi, kết nối đầy đủ, cho đến cái lớp cuối cùng, thì chúng ta sẽ gặp cái lớp là Softmax. Mục tiêu của cái lớp Softmax này đó là chuyển đổi các Vector về cái dạng không phân bố xác suất. Tức là với mỗi cái phần tử trong cái Vector cuối cùng này, thì miền giá trị của nó là từ 0 cho đến 1. Và tổng tất cả các cái xác suất này, xác suất thuộc về lớp Car, Truck, Van, Bicycle, tổng của nó sẽ là bằng một. Thì đây là cái công dụng của cái hàm Softmax. Thì chúng ta ôn lại về cái kiến trúc mạng CNN. Trong cái mạng CNN thì có một cái phép biến đổi, nó có một cái phép biến đổi đó là phép Convolution hay còn gọi là Tích chập."
        },
        {
          "index": 5,
          "video_id": "Chương 4_PyC3pl_r8jw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_1_ Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/PyC3pl_r8jw",
          "start_time": 199,
          "end_time": 270,
          "text": "Thì đây là cái công dụng của cái hàm Softmax. Thì chúng ta ôn lại về cái kiến trúc mạng CNN. Trong cái mạng CNN thì có một cái phép biến đổi, nó có một cái phép biến đổi đó là phép Convolution hay còn gọi là Tích chập. Thì bản kết của cái phép Convolution này đó là một cái phép biến đổi, tiến tính. Và nhiệm vụ của nó là để đi rút trích đặc trưng hình ảnh. Ví dụ như chúng ta có một cái ảnh đồ vào ở đây, và chúng ta nhân với lại một cái filter. Rồi, filter này nó sẽ tạo ra một cái feature. Và cái feature này nó có cái ý nghĩa, nó có cái concept đó là các miền cạnh. Theo chiều dọc. Đó, thì các cái biên cạnh, các cái filter này thì giá trị của nó ban đầu là được gọi là thiết lập ngẫu nhiên. Nhưng mà sâu cái quá trình mà mạng CNN huấn luyện, thì các cái giá trị tham số của cái filter này sẽ được mạng huấn luyện."
        },
        {
          "index": 6,
          "video_id": "Chương 4_PyC3pl_r8jw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_1_ Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/PyC3pl_r8jw",
          "start_time": 248,
          "end_time": 319,
          "text": "Đó, thì các cái biên cạnh, các cái filter này thì giá trị của nó ban đầu là được gọi là thiết lập ngẫu nhiên. Nhưng mà sâu cái quá trình mà mạng CNN huấn luyện, thì các cái giá trị tham số của cái filter này sẽ được mạng huấn luyện. Trên cái dữ liệu thật, được mạng CNN huấn luyện trên dữ liệu thật và nó sẽ tự động cập nhật. Và các cái trọng số của cái filter này nó sẽ cập nhật như thế nào để cho cái kết quả của cái việc nhận diện cuối cùng của mình đạt được độ chính xác cao nhất. Rồi, vừa rồi thì là phép biến đổi Convolution. Bây giờ chúng ta sẽ qua cái khái niệm gọi là tầng Convolution. Tức là tầng Convolution bản kết đó là chúng ta sẽ thực hiện với rất nhiều cái phép biến đổi Convolution với rất nhiều các filter khác nhau. Thì ở đây chúng ta sẽ có một cái animation đó là với cái input đầu vào qua nhiều cái filter thì chúng ta sẽ có nhiều cái feature và mỗi cái này nó sẽ gọi là một cái feature."
        },
        {
          "index": 7,
          "video_id": "Chương 4_PyC3pl_r8jw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_1_ Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/PyC3pl_r8jw",
          "start_time": 299,
          "end_time": 362,
          "text": "Tức là tầng Convolution bản kết đó là chúng ta sẽ thực hiện với rất nhiều cái phép biến đổi Convolution với rất nhiều các filter khác nhau. Thì ở đây chúng ta sẽ có một cái animation đó là với cái input đầu vào qua nhiều cái filter thì chúng ta sẽ có nhiều cái feature và mỗi cái này nó sẽ gọi là một cái feature. Và tập hợp của các cái feature thì nó sẽ gọi là feature map. Nó gọi là feature map là tập hợp của các cái feature. Thì ở đây chúng ta sẽ có một cái công thức để nhớ về cái kích thước của các cái filter cũng như là kích thước của cái tensor output. Nếu như cái đầu vào của mình, cái độ sâu này là có độ sâu là D. Thì cái filter của mình nó sẽ phải có độ sâu tương ứng cũng là D luôn. Để chia để khi chúng ta lấy cái filter này chúng ta trượt, nó trượt thì nó phải vừa khớp với lại cái input của mình. Rồi, và ở đây chúng ta sẽ có K filter. Chúng ta sẽ có K filter."
        },
        {
          "index": 8,
          "video_id": "Chương 4_PyC3pl_r8jw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_1_ Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/PyC3pl_r8jw",
          "start_time": 348,
          "end_time": 411,
          "text": "Để chia để khi chúng ta lấy cái filter này chúng ta trượt, nó trượt thì nó phải vừa khớp với lại cái input của mình. Rồi, và ở đây chúng ta sẽ có K filter. Chúng ta sẽ có K filter. Và khi chúng ta thực hiện K filter này thì output của mình nó cũng sẽ có cái độ sâu tương ứng là bằng K. Thì đây là chúng ta sẽ nhớ những cái cách thức để mà setup cho cái độ sâu của cái filter và độ sâu của cái output đầu ra trong cái quá trình lập trình. Rồi, như vậy thì chúng ta đã ôn qua một số cái kiến thức về những cái kiến trúc phổ biến, xây dựng những cái cấu tạo chung của một cái mạng CNN. Bây giờ chúng ta sẽ đến với một số cái kiến trúc mạng CNN phổ biến. Rồi, trên đây là sơ đồ về cái kết quả về độ lỗi khi mà nhận diện hình ảnh. Thì ở đây là càng thấp, càng thấp là càng tốt."
        },
        {
          "index": 9,
          "video_id": "Chương 4_PyC3pl_r8jw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_1_ Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/PyC3pl_r8jw",
          "start_time": 400,
          "end_time": 462,
          "text": "Rồi, trên đây là sơ đồ về cái kết quả về độ lỗi khi mà nhận diện hình ảnh. Thì ở đây là càng thấp, càng thấp là càng tốt. Và ở đây sẽ là các cái cột mốc về mặc thời gian. Thì ở đây chúng ta sẽ nói đến đầu tiên đó là cái tập dữ liệu ImageNet. Đây là một trong những cái tập dữ liệu vô cùng lớn. ImageNet thì là viết tắt của chữ là Large Scale Visual Recognition Challenge, tức là ImageNet được sử dụng cho cái cuộc thi là Large Scale Visual Recognition Challenge. Và cái scale, cái kích thước của tập ImageNet này nó rất là rất là lớn. Nó bao gồm là 14 triệu ảnh và tổng số lớp mà nó phải nhận diện đó là 20.000 lớp. Và cái cuộc thi này thì được tổ chức hàng năm từ năm 2010 trở về sâu. Và hai cái bài toán chính mà nó thực hiện đó chính là bài toán phân lớp, phân loại và bài toán phát hiện đối tượng."
        },
        {
          "index": 10,
          "video_id": "Chương 4_PyC3pl_r8jw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_1_ Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/PyC3pl_r8jw",
          "start_time": 450,
          "end_time": 512,
          "text": "Và cái cuộc thi này thì được tổ chức hàng năm từ năm 2010 trở về sâu. Và hai cái bài toán chính mà nó thực hiện đó chính là bài toán phân lớp, phân loại và bài toán phát hiện đối tượng. Thì ở đây chúng ta sẽ cùng điểm qua một số cái mốc của cái mạng CNN. Đầu tiên đó là cái mốc vào những năm 1990. Tức là mạng CNN không phải có trong những năm 2010 trở lại đây. Mạng CNN nó có từ những năm 1990, tức là khoảng gần 30 năm rồi. Và với những cái phiên bản đời đầu thì cho cái độ chính xác cũng chưa có được đủ tốt. Mà ấy cho đến khi năm 2012 với một số những cái cải tiến của AlexNet, chúng ta sẽ thấy ra là có một cái sự bùng nổ của mạng học sâu Deep Learning. sâu đây chúng ta sẽ lần lượt tìm hiểu qua một số cái kiến trúc mạng phổ biến, nổi tiếng. Đầu tiên chúng ta cũng không nên quên nhắc lại về kiến trúc mạng LeNet."
        },
        {
          "index": 11,
          "video_id": "Chương 4_PyC3pl_r8jw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_1_ Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/PyC3pl_r8jw",
          "start_time": 499,
          "end_time": 560,
          "text": "sâu đây chúng ta sẽ lần lượt tìm hiểu qua một số cái kiến trúc mạng phổ biến, nổi tiếng. Đầu tiên chúng ta cũng không nên quên nhắc lại về kiến trúc mạng LeNet. Kiến trúc mạng LeNet thì một trong những cái phát triển lớn nhất của nó chính là cái lớp tích chập, tức là cái phép biến đổi convolution. Và convolution là cái sự cải tiến của cái phép biến đổi là fully connected. Tức là cái phép kết nối đầy đủ. Convolution thì nó sẽ cải tiến, nó không sử dụng cái fully connected nữa, mà nó sẽ sử dụng cái cơ chế đó là locally connected. Và đồng thời là nó sẽ chia sẻ trọng số. Nó sẽ chia sẻ trọng số. Để chia, mục tiêu của nó đó chính là để làm giảm cái số lượng tham số."
        },
        {
          "index": 12,
          "video_id": "Chương 4_PyC3pl_r8jw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_1_ Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/PyC3pl_r8jw",
          "start_time": 547,
          "end_time": 611,
          "text": "Và đồng thời là nó sẽ chia sẻ trọng số. Nó sẽ chia sẻ trọng số. Để chia, mục tiêu của nó đó chính là để làm giảm cái số lượng tham số. Làm giảm cái số lượng tham số của cái phép biến đổi của mình. Và khi giảm cái số lượng tham số thì nó sẽ dẫn đến là giúp cho chúng ta giảm được cái hiện tượng overfitting. Cái hiện tượng overfitting này có nghĩa là gì? Khi mô hình của mình huấn luyện trên cái tập dữ liệu, thì tập train, thì cái độ chính xác của mình rất là cao hoặc là độ lỗi rất là thấp. Nhưng khi chúng ta áp dụng trong thực tế, áp dụng trên tập dữ liệu test, thì độ chính xác giảm rất là đáng kể. Thì đó là cái hiện tượng overfitting. Một trong những điểm nhấn khác của cái mạng CNN đó chính là cái lớp pooling. Lấy cái lớp pooling và sử dụng average pooling. Thì nhiệm vụ của average pooling này, như hồi nãy chúng ta có đề cập,"
        },
        {
          "index": 13,
          "video_id": "Chương 4_PyC3pl_r8jw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_1_ Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/PyC3pl_r8jw",
          "start_time": 596,
          "end_time": 661,
          "text": "Một trong những điểm nhấn khác của cái mạng CNN đó chính là cái lớp pooling. Lấy cái lớp pooling và sử dụng average pooling. Thì nhiệm vụ của average pooling này, như hồi nãy chúng ta có đề cập, đó là để giảm cái kích thước của cái feature map của mình. Ví dụ input feature map của mình là như thế này. sâu khi thực hiện cái phép pooling sâu, thì nó sẽ giảm xuống còn khoảng một nửa. Và lưu ý, đó là giảm một nửa cho cái kích thước theo bề ngang và bề cao, nhưng mà tổng số lượng các cái phần tử trong cái tensor này sẽ giảm 4 lần. Tại vì bề ngang mà giảm 2 lần, bề cao mà giảm 2 lần, thì lúc đó là nhân lên, thì chúng ta sẽ ra là giảm đến 4 lần. Và khi phép pooling này thực hiện cho đến cái bước cuối cùng, thực hiện cho đến bước cuối cùng, chúng ta sẽ có cái bước gọi là flatten, để mà đưa vào cái mạng fully connected ở phía sâu. Rõ ràng là khi cái kích thước của mình giảm xuống,"
        },
        {
          "index": 14,
          "video_id": "Chương 4_PyC3pl_r8jw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_1_ Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/PyC3pl_r8jw",
          "start_time": 647,
          "end_time": 712,
          "text": "thực hiện cho đến bước cuối cùng, chúng ta sẽ có cái bước gọi là flatten, để mà đưa vào cái mạng fully connected ở phía sâu. Rõ ràng là khi cái kích thước của mình giảm xuống, khi cái kích thước của cái tensor, cái feature map giảm xuống, thì khi chúng ta flatten ra, thì cái kích thước của cái vector này cũng sẽ giảm xuống. Ví dụ như nếu đây màu đỏ, là nó tạo ra cái vector này, thì khi chúng ta dùng cái pooling, thì cái feature map này, nó sẽ giảm xuống còn 1 phần tư. Và khi giảm xuống 1 phần tư, thì các bạn sẽ thấy rồi cái phép kết nối đầy đủ này, thì cái số lượng trọng số của mình cũng sẽ giảm đi, đáng kể. Thì cái phép pooling này, nó sẽ có thêm một cái công dụng, ngoài cái việc đó là giảm cái kích thước của cái tensor, thì nó sẽ còn giảm cái số lượng tham số ở cái bước fully connected phía sâu. Và đồng thời, cái việc này nó sẽ có 2 công dụng."
        },
        {
          "index": 15,
          "video_id": "Chương 4_PyC3pl_r8jw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_1_ Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/PyC3pl_r8jw",
          "start_time": 697,
          "end_time": 751,
          "text": "ngoài cái việc đó là giảm cái kích thước của cái tensor, thì nó sẽ còn giảm cái số lượng tham số ở cái bước fully connected phía sâu. Và đồng thời, cái việc này nó sẽ có 2 công dụng. Công dụng đầu tiên, đó chính là giảm cái hiện tượng overfitting. Và công dụng thứ 2, đó là tăng cái tốc độ của cái quá trình và tính toán của mình lên. Rồi, nó sẽ có một cái thành phần nữa đó là activation. Thì chúng ta trong cái phiên bản LeNet đời đầu vào những năm 1998, thì chúng ta sử dụng những cái hàm activation kinh điển, đó là sigmoid và hàm tanh. Và đây là cái hình vẽ cho cái kiến trúc của LeNet thời điểm đó. Thì lưu ý là ở đây, thời điểm đó người ta dùng cái từ khóa, đó là subsampling, chúng ta hiểu đó chính là pooling. Đây chính là cái phép pooling của mình."
        },
        {
          "index": 16,
          "video_id": "Chương 4_PyC3pl_r8jw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_1_ Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/PyC3pl_r8jw",
          "start_time": 748,
          "end_time": 751,
          "text": "chúng ta hiểu đó chính là pooling. Đây chính là cái phép pooling của mình."
        }
      ]
    },
    {
      "video_id": "Chương 4_KoBIBuqGb9A",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Ôn tập kiến trúc mạng CNN tiêu biểu (AlexNet, VGG) và các cải tiến chính đã đưa các mô hình này thành chuẩn mực trong thị trường học sâu. [1][6]  \n- Các khái niệm sẽ được đề cập: vai trò của hàm kích hoạt (sigmoid vs ReLU), hiện tượng vanishing gradient, chiến lược tăng chiều sâu mạng, data augmentation, tối ưu hóa (gradient descent), thiết kế kernel (stacking 3x3 thay cho 5x5/7x7), receptive field và so sánh số tham số giữa các thiết kế. [1][2][3][4][5][10][14]  \n- Nguồn / phạm vi: tóm tắt dựa hoàn toàn trên nội dung các đoạn đã trích từ video. [1] \n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### A. AlexNet — bước nhảy lớn (vô địch cuộc thi trên tập dữ liệu)  \n- AlexNet đạt độ chính xác cao nhất, thắng các phương pháp dùng đặc trưng thủ công trên cuộc thi (tập dữ liệu trong video: “tập dự luật MNS”) và gây tiếng vang lớn trong cộng đồng. [1]  \n- Các cải tiến chính của AlexNet:\n  - Thay sigmoid bằng ReLU làm activation function, giúp giảm hiện tượng vanishing gradient và tăng tốc huấn luyện. [1][6]  \n  - Tăng độ sâu của mạng (nhiều lớp convolution và fully connected hơn trước) để học biểu diễn phong phú hơn. [6]  \n  - Sử dụng data augmentation để giảm overfitting (xử lý biến đổi ảnh như xoay, thêm nhiễu, thay đổi độ sáng, ...). [7][8]  \n  - Tối ưu phần cứng/triển khai (ví dụ chạy trên TPU) giúp tăng tốc huấn luyện (video nêu tốc độ nhanh hơn ~50 lần). [8]  \n- Tầm ảnh hưởng học thuật: bài AlexNet được trích dẫn rất nhiều (khoảng 128.000 lần theo video). [9]\n\n### B. Hiện tượng vanishing gradient và lý do chuyển sang ReLU\n- Cập nhật tham số theo gradient descent: bước cập nhật theta thường là theta := theta - alpha * (dL / dtheta). Video mô tả ý này khi nói về “theta trừ đi alpha nhân đạo hàm của hàm loss theo theta”. [2]  \n- Do chain rule, đạo hàm tổng hợp khi truyền ngược qua nhiều lớp là tích các đạo hàm thành phần; nếu các thành phần < 1 thì tích nhiều lần sẽ tiến về 0 → vanishing gradient, khiến tham số gần như không được cập nhật. [2][3]  \n- Sigmoid dễ bị bão hòa: với đầu vào x ở vùng bão hòa, đạo hàm gần 0 → góp phần gây vanishing gradient. [4]  \n- ReLU giảm vấn đề này vì với x > 0 đạo hàm gần như là một hằng 1, nên các thành phần đạo hàm không bị co về 0 khi nhân dồn qua lớp. Điều này giúp tăng tốc độ huấn luyện và giảm vanishing gradient. [5][6]\n\n(Formula minh họa: gradient descent update — theta <- theta - α * ∂L/∂theta — được video nhắc đến khi mô tả cơ chế cập nhật). [2]\n\n### C. Data augmentation & overfitting trong AlexNet\n- Khi tăng số tham số (do tăng chiều sâu), nguy cơ overfitting tăng; AlexNet dùng data augmentation để tạo nhiều mẫu huấn luyện từ mỗi ảnh: xoay, thêm nhiễu, thay đổi độ sáng, ... nhằm tăng dữ liệu hiệu quả và giảm overfitting. [7][8]\n\n### D. VGG — thay filter lớn bằng các 3x3 chồng nhau\n- Ý tưởng chính: thay các filter lớn (5x5, 7x7) bằng nhiều filter 3x3 liên tiếp. Điều này vừa giữ vùng ảnh hưởng (receptive field) tương đương, vừa giảm số tham số. [10][11][12][13]  \n- Receptive field: một điểm trên feature map cuối cùng phụ thuộc vào một vùng trên ảnh đầu vào; dùng hai convolution 3x3 liên tiếp có receptive field tương đương một convolution 5x5. (Video gọi vùng ảnh hưởng này là \"receptive field\".) [11][12][13]  \n- So sánh số tham số (theo video):  \n  - Một kernel 5x5 có 25 tham số (ghi tắt P = 25). [14]  \n  - Hai lớp 3x3 liên tiếp có tổng 2 × 3 × 3 = 18 tham số. [14]  \n  - Tỉ lệ: 18/25 ≈ 72% → tiết giảm ~30% số tham số so với dùng 5x5 một lớp. [14][15]  \n- VGG tăng chiều sâu (các phiên bản VGG11, VGG13, VGG16, VGG19), nhưng cải tiến lớn là thay filter lớn bằng chuỗi 3x3 và thiết kế chuẩn cho việc xếp lớp. [15]  \n- Cách tính “VGG16”: số 16 là số phép biến đổi convolution + fully connected; pooling không đếm là phép tạo đặc trưng nên không tính vào con số này. [16]  \n- Tầm ảnh hưởng: VGG được trích dẫn rất nhiều (khoảng 121.000 trích dẫn theo video). [16][17]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa hiệu quả AlexNet: chiến thắng trong cuộc thi trên tập dữ liệu (được nêu ở đầu bài giảng) và chứng minh mạng CNN tự học đặc trưng hiệu quả hơn đặc trưng thiết kế thủ công. [1]  \n- Ví dụ về data augmentation cụ thể mà video liệt kê: xoay ảnh, thêm nhiễu, thay đổi độ sáng — từ mỗi ảnh tạo nhiều biến thể cùng nhãn để tăng kích thước dữ liệu và giảm overfitting. [7][8]  \n- Minh họa receptive field cho VGG: hai lớp 3x3 liên tiếp có receptive field tương đương 5x5; do vậy stacking 3x3 giữ được khả năng tổng hợp thông tin ở cùng vùng ảnh đầu vào nhưng giảm tham số. [11][12][13]  \n- Ứng dụng thực tế / tác dụng:  \n  - Thiết kế activation (ReLU) và kiến trúc (dịch chuyển từ filter lớn sang filter nhỏ xếp chồng) giúp mô hình huấn luyện nhanh hơn, ít gặp vanishing, và có hiệu năng tốt trên bài toán phân loại ảnh lớn. [5][6][10][11]  \n  - Tăng tốc triển khai nhờ phần cứng (ví dụ triển khai trên TPU tăng tốc ~50× so với phiên bản trước) giúp huấn luyện các mô hình sâu trở nên khả thi. [8]\n\n---\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt ý chính:  \n  - AlexNet là bước ngoặt chứng minh hiệu quả của CNN tự học đặc trưng; các cải tiến quan trọng gồm chuyển ReLU thay sigmoid, tăng độ sâu, data augmentation và tối ưu phần cứng (TPU). [1][5][6][7][8][9]  \n  - VGG tiếp tục tối ưu thiết kế kernel bằng cách thay các kernel lớn bằng chuỗi các kernel 3x3, đạt receptive field tương đương nhưng với ít tham số hơn (~30% tiết giảm so với 5x5), và mở rộng chiều sâu (VGG11→VGG19). [10][11][14][15][16]  \n- Tầm quan trọng: Những ý tưởng (ReLU, data augmentation, stacking 3x3) trở thành thành phần cơ bản trong thiết kế mạng CNN hiện đại, vừa cải thiện hiệu năng, vừa giảm overfitting và yêu cầu tính toán. [5][7][10][14]  \n- Liên hệ với các bài giảng khác: video nêu rõ đây là phần ôn tập kiến trúc CNN trong Chương 4 — các khái niệm này sẽ là nền tảng cho các biến thể kiến trúc CNN tiếp theo (phát triển từ các ý tưởng của AlexNet và VGG). [6][15]\n\n---\n\nGhi chú: Tất cả nội dung trên được trích tóm lược trực tiếp từ các đoạn trong video theo các chunk đã cung cấp; mỗi citation [N] tham chiếu đến đoạn thời gian tương ứng trong video.",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 4_KoBIBuqGb9A",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_2： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/KoBIBuqGb9A",
          "start_time": 0,
          "end_time": 61,
          "text": "rồi đến năm 2012 AlexNet và gây ra một cái tiếng vang lớn trong cái cộng đồng nghiên cứu khi cái kiến trúc mạng AlexNet giành được độ chính xác cao nhất độ lỗi thấp nhất và độ chính xác cao nhất cho cái cuộc thi trên tập dự luật MNS và nó chiến thắng tất cả những cái phương pháp mà sử dụng các đặc trưng mà do các nhà khoa học họ thiết kế thiết kế bằng tay còn cái mạng CNN của AlexNet nó được thiết kế để cho tự động học các bộ nút trích đặc trưng thông qua các phép biến đổi convolution thì những cái cải tiến chính những cái cải tiến chính của AlexNet đó chính là thay cái sigmoid hàm activation sigmoid bằng relu thì cái này nó sẽ giúp cho chúng ta tránh được cái hiện tượng giảm được hiển thị và"
        },
        {
          "index": 2,
          "video_id": "Chương 4_KoBIBuqGb9A",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_2： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/KoBIBuqGb9A",
          "start_time": 40,
          "end_time": 110,
          "text": "đổi convolution thì những cái cải tiến chính những cái cải tiến chính của AlexNet đó chính là thay cái sigmoid hàm activation sigmoid bằng relu thì cái này nó sẽ giúp cho chúng ta tránh được cái hiện tượng giảm được hiển thị và ra nutrition crown kỷ entering ở trick thăng số theta là bằng Thê ta trừ cho anh Pha nhân cho Đạo hàm của hàm lo theo thêta đúng không Thì cái hàm này này càng này từng cái thành phần cái hàm này nó sẽ được phân rãi ra là thành các cái hàm thành phần thì nếu như chúng ta viết dưới dạng là chain rule tức là cái đạo hàm của hàm hợp thì nó sẽ là đạo hàm của hàm loss theo một cái hàm ví dụ như là hàm số 1 rồi đạo hàm của cái hàm số 1 theo cái hàm số 2 rồi văn văn cho đến cái hàm thứ n"
        },
        {
          "index": 3,
          "video_id": "Chương 4_KoBIBuqGb9A",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_2： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/KoBIBuqGb9A",
          "start_time": 99,
          "end_time": 160,
          "text": "ví dụ như là hàm số 1 rồi đạo hàm của cái hàm số 1 theo cái hàm số 2 rồi văn văn cho đến cái hàm thứ n theo cái biến theta thì một loạt các cái đạo hàm này từng cái đạo hàm thành phần này nếu như nó là những cái con số rất là nhỏ, ví dụ như con số bé hơn 1 thì khi chúng ta nhân các cái con số bé hơn 1 thì nó sẽ có xu hướng tiếng này không trong cái quá trình cập nhật cái tham số của mình mà mục tiêu của cái việc cập nhật các cái tham số này là để cho cái đạo hàm của mình càng lúc càng nhỏ mà, gradient descent gradient descent tức là cái đạo hàm càng lúc càng giảm thì khi đạo hàm càng giảm thì các cái thành phần này nè sẽ càng lúc càng giảm các cái thành phần này càng lúc càng giảm thì dẫn đến đó là các cái con số mà nhỏ mà nhân với nhau nó sẽ tiến về 0 và khi đạo hàm mà bằng 0 tức là cái bước nhảy theta này nó gần như nó không cập nhật nó gần như không cập nhật"
        },
        {
          "index": 4,
          "video_id": "Chương 4_KoBIBuqGb9A",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_2： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/KoBIBuqGb9A",
          "start_time": 148,
          "end_time": 209,
          "text": "các cái con số mà nhỏ mà nhân với nhau nó sẽ tiến về 0 và khi đạo hàm mà bằng 0 tức là cái bước nhảy theta này nó gần như nó không cập nhật nó gần như không cập nhật thì đó chính là cái hiện tượng vanishing gradient nó sẽ làm cho cái quá trình huấn luyện chậm rồi thì tại sao sigmoid nó lại khiến cho cái hiện tượng vanishing gradient nó diễn ra gọi là phổ biến còn ReLU thì nó sẽ giúp cho mình giảm cái hiện tượng này đó là vì chúng ta quan sát cái hàm sigmoid rồi với cái hàm sigmoid này thì chúng ta thấy nó rất dễ bị bão hòa bão hòa theo nghĩa là gì khi cái giá trị đầu vào x của mình đó sigmoid khi cái giá trị đầu vào x của mình nó chỉ mới đạt được những cái giá trị rất là bé thôi thì nó đã đạt được cái trạng thái đó là"
        },
        {
          "index": 5,
          "video_id": "Chương 4_KoBIBuqGb9A",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_2： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/KoBIBuqGb9A",
          "start_time": 197,
          "end_time": 260,
          "text": "khi cái giá trị đầu vào x của mình đó sigmoid khi cái giá trị đầu vào x của mình nó chỉ mới đạt được những cái giá trị rất là bé thôi thì nó đã đạt được cái trạng thái đó là đạo hàm cái độ dốc của đạo hàm nó gần như là đi ngang độ dốc đạo hàm gần như đi ngang tức là cái đạo hàm của mình rất là nhỏ, nó tiến về 0 mà đạo hàm tiến về 0 thì tức là khi chúng ta nhân những cái giá trị này vô thì nó sẽ trịt tiêu thế thì tại sao ReLU lại chống được cái việc này ReLU nó lại chống được cái việc này đó là vì cái hàm ReLU của mình nó sẽ có tính chất đó là với những cái giá trị x mà lớn đúng không, lớn 1-0 thì nó sẽ giữ nguyên cái giá trị hay nói cách khác đó là đạo hàm của mình trong trường hợp này, cái độ dốc của mình trong trường hợp này luôn luôn là một cái hàng số cố định và độ dốc của mình trong trường hợp này đó là bằng 1 thì cái việc đạo hàm bằng 1 này thì khiến cho các cái thành phần này đâu đó các cái giá trị của mình nó sẽ cố định là bằng 1, nó không có tiến về cái con số 0"
        },
        {
          "index": 6,
          "video_id": "Chương 4_KoBIBuqGb9A",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_2： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/KoBIBuqGb9A",
          "start_time": 249,
          "end_time": 311,
          "text": "đó là bằng 1 thì cái việc đạo hàm bằng 1 này thì khiến cho các cái thành phần này đâu đó các cái giá trị của mình nó sẽ cố định là bằng 1, nó không có tiến về cái con số 0 nó sẽ không tiến về con số 0 mà nó sẽ để các cái giá trị là bằng 1 mà các cái giá trị bằng 1 thì khi nhâm vô nó sẽ không có giảm bớt cái hiện tượng kéo cái giá trị của mình về 0 thì đó là lý giải một cách hơi ngắn gọn cho cái việc là tại sao dùng ReLU nó sẽ tiết kiệm cho mình hơn tốc độ tăng cái tốc độ hướng luyện của mình hơn và giảm cái hiện tượng vanishing và giảm cái hiện tượng vanishing và giảm cái hiện tượng vanishing rồi bây giờ chúng ta sẽ nói thêm các cái cải tiến tiếp theo của cái AlexNet đó chính là nó tăng cái độ sâu của cái kiến trúc mạng nó tăng cái độ sâu bình thường AlexNet nó chỉ có 2 cái phép Convolution và thêm 2 cái phép biến đổi fully connected tức là 4 hoặc là 5 lớp biến đổi gì đó thôi còn cái mạng AlexNet thì nó sẽ có nhiều hơn"
        },
        {
          "index": 7,
          "video_id": "Chương 4_KoBIBuqGb9A",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_2： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/KoBIBuqGb9A",
          "start_time": 299,
          "end_time": 358,
          "text": "bình thường AlexNet nó chỉ có 2 cái phép Convolution và thêm 2 cái phép biến đổi fully connected tức là 4 hoặc là 5 lớp biến đổi gì đó thôi còn cái mạng AlexNet thì nó sẽ có nhiều hơn cái số lượng phép biến đổi Convolution và fully connected nó nhiều hơn và khi cái mô hình học sâu này của mình nó tăng lên thì đồng nghĩa là cái số lượng tham số nó cũng tăng lên do đó để tránh cái hiện tượng overfitting AlexNet đã tăng cường dữ liệu AlexNet đã tăng cường dữ liệu tăng cường dữ liệu nhiều hơn nó dùng cái phương pháp nó gọi là data augmentation data augmentation bài cách đó là với mỗi ảnh bài cách đó là với mỗi ảnh với mỗi ảnh thì chúng ta sẽ thực hiện các cái phép là tỉ lệ chúng ta sẽ thực hiện các cái phép là tỉ lệ xoay rồi chúng ta thực hiện cái phép là thêm nhiễu rồi chúng ta thực hiện cái phép là thêm nhiễu rồi thay đổi cái độ sáng rồi thay đổi cái độ sáng"
        },
        {
          "index": 8,
          "video_id": "Chương 4_KoBIBuqGb9A",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_2： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/KoBIBuqGb9A",
          "start_time": 350,
          "end_time": 411,
          "text": "xoay rồi chúng ta thực hiện cái phép là thêm nhiễu rồi chúng ta thực hiện cái phép là thêm nhiễu rồi thay đổi cái độ sáng rồi thay đổi cái độ sáng đó thì với một cái ảnh chúng ta sẽ làm những cái phép biến đổi rất nhiều những cái phép biến đổi khác nhau để tạo ra những cái mẫu dữ liệu mới với cùng một cái nhãn giống như là cái ảnh góc đầu vào thì như vậy thì nó sẽ giúp cho mình tăng cái data lên và tăng cái data này lên thì nó sẽ giúp cho mình giảm cái hiện tượng overfitting thì nó sẽ giúp cho mình giảm cái hiện tượng overfitting được được rồi và một cái cải tiến cuối cùng so với lại những cái phiên bản trước đây thì không có sử dụng TPU thì AlexNet đã cài đặt cái thực toán của mình để cho có thể chạy được trên TPU và tốc độ của mình huấn luyện nó nhanh hơn gấp 50 lần thì đây chính là những cái cải tiến chính của mạng AlexNet và khi chúng ta google cái bài báo khi chúng ta google cái tên bài báo ở đây thì chúng ta thấy là cái AlexNet"
        },
        {
          "index": 9,
          "video_id": "Chương 4_KoBIBuqGb9A",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_2： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/KoBIBuqGb9A",
          "start_time": 399,
          "end_time": 463,
          "text": "thì đây chính là những cái cải tiến chính của mạng AlexNet và khi chúng ta google cái bài báo khi chúng ta google cái tên bài báo ở đây thì chúng ta thấy là cái AlexNet là cho cái số lượt citation là khoảng 128 ngàn 128 ngàn 128 ngàn trích dẫn tức là gì khi cái mỗi bài báo mà được xuất bản thì họ sẽ trích dẫn đến những cái bài báo mà họ tham khảo thì cái bài email net cái bài AlexNet này được cộng đồng tham khảo đến 128 ngàn lần được cộng đồng tham khảo đến 128 ngàn lần thì đây là một trong những cái con số vô cùng khủng khiếp thì đây là một trong những cái con số vô cùng khủng khiếp thì đây là một trong những cái con số vô cùng khủng khiếp rồi tiếp theo thì chúng ta sẽ tìm hiểu đến cái kiến trúc mạng của VGG rồi tiếp theo thì chúng ta sẽ tìm hiểu đến cái kiến trúc mạng của VGG đâu đó là cũng vào 2014 2015 2015 rồi thì các cái cải tiến rồi thì các cái cải tiến của AlexNet của VGG so với AlexNet cải tiến so với AlexNet rất là đơn giản đó chính là chúng ta thay các cái filter"
        },
        {
          "index": 10,
          "video_id": "Chương 4_KoBIBuqGb9A",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_2： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/KoBIBuqGb9A",
          "start_time": 450,
          "end_time": 510,
          "text": "rồi thì các cái cải tiến rồi thì các cái cải tiến của AlexNet của VGG so với AlexNet cải tiến so với AlexNet rất là đơn giản đó chính là chúng ta thay các cái filter kích thước 5577 bằng các cái filter hay còn gọi là các vụ lọc bằng các cái filter có cái kích thước là 3x3 bằng các cái filter có cái kích thước là 3x3 và thực hiện liên tiếp nhau thì cái việc này nó sẽ giúp cho chúng ta giải quyết vấn đề gì thì chúng ta sẽ thử làm trên một cái filter là 5x5 trước thì nếu như bình thường chúng ta sẽ có một cái ảnh đầu vào chúng ta sẽ có một cái ảnh đầu vào rồi sau đó chúng ta nhân convolution với một cái filter kích thước là 5x5 với một cái filter kích thước là 5x5 thì chúng ta sẽ tạo ra một cái feature mask thì chúng ta sẽ tạo ra một cái feature mask và một cái điểm một cái điểm đặc trưng ở trên cái feature mask output này nó được tạo bởi nó được tạo bởi một cái vùng"
        },
        {
          "index": 11,
          "video_id": "Chương 4_KoBIBuqGb9A",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_2： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/KoBIBuqGb9A",
          "start_time": 500,
          "end_time": 566,
          "text": "và một cái điểm một cái điểm đặc trưng ở trên cái feature mask output này nó được tạo bởi nó được tạo bởi một cái vùng nó được tạo bởi một cái vùng có kích thước 5x5 ở trên cái ảnh input trên cái ảnh input thì nó cái điểm này nó sẽ bị phụ thuộc bởi một cái vùng có kích thước là 5x5 thì đây là cái cách bình thường còn cái cải tiến của  VGG đó là thay vì sử dụng cái kernel 5x5 thì nhóm tác giả không sử dụng kernel 5x5 nữa hoặc là 7x7 nữa mà thay hết bằng kernel 3x3, cái filter 3x3 và thực hiện liên tiếp nhau ví dụ đây chúng ta có một cái ảnh chúng ta thực hiện convolution với một cái kernel kích thước là 3x3 với một cái kernel kích thước là 3x3 rồi sau đó chúng ta sẽ tạo ra một cái tấm ảnh rồi sau đó chúng ta sẽ tạo ra một cái tấm ảnh để tạo ra một cái tấm ảnh khác"
        },
        {
          "index": 12,
          "video_id": "Chương 4_KoBIBuqGb9A",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_2： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/KoBIBuqGb9A",
          "start_time": 549,
          "end_time": 610,
          "text": "với một cái kernel kích thước là 3x3 với một cái kernel kích thước là 3x3 rồi sau đó chúng ta sẽ tạo ra một cái tấm ảnh rồi sau đó chúng ta sẽ tạo ra một cái tấm ảnh để tạo ra một cái tấm ảnh khác để tạo ra một cái tấm ảnh khác   thì bây giờ chúng ta sẽ xem cái điểm đặc trưng ở trên cái feature map cuối cùng ở đây thì cái điểm đặc trưng này nó được tạo bởi một cái vùng có kích thước là 3x3 của cái feature map này và cái vùng 3x3 này thì nó được tạo ra bởi cái vùng 5x5 5x5  và cái vùng 3x3 này ví dụ như cái điểm này thì nó được tạo ra bởi cái vùng này cái điểm này thì được tạo ra bởi cái vùng này rồi cái điểm này thì được tạo ra bởi cái vùng này như vậy thì cái feature map"
        },
        {
          "index": 13,
          "video_id": "Chương 4_KoBIBuqGb9A",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_2： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/KoBIBuqGb9A",
          "start_time": 599,
          "end_time": 653,
          "text": "thì nó được tạo ra bởi cái vùng này cái điểm này thì được tạo ra bởi cái vùng này rồi cái điểm này thì được tạo ra bởi cái vùng này như vậy thì cái feature map ở cái phép biến đổi cuối cùng nó sẽ được tạo bởi các cái điểm ảnh các cái điểm trong cái feature map của lớp trung gian là feature map này và có vùng kích thước cái vùng ảnh hưởng trong đó sẽ là 3x3 và cái vùng 3x3 này thì sẽ được tạo bởi một cái vùng 5x5 ở trên cảnh đầu bào như vậy xét về bản chất về tổng hợp thông tin nếu sử dụng cái phép biến đổi 5x5 convolution 5x5 thì một cái feature map output nó sẽ bị ảnh hưởng bởi một cái vùng 5x5 input đầu bào và dùng hai cái phép convolution liên tiếp nhau thì nó cũng tương đương như vậy tức là một cái điểm ở đây nó sẽ được tổng hợp thông tin bởi một cái vùng 5x5 thì cái vùng này cái vùng ảnh hưởng này người ta gọi là reset tfield"
        },
        {
          "index": 14,
          "video_id": "Chương 4_KoBIBuqGb9A",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_2： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/KoBIBuqGb9A",
          "start_time": 649,
          "end_time": 710,
          "text": "thì cái vùng này cái vùng ảnh hưởng này người ta gọi là reset tfield như vậy thì cái cách làm của VGG nó có cái gì hơn so với cái cách làm bình thường này nó có cái cách làm có cái gì hay hơn thì ở đây chúng ta sẽ để ý cái số lượng tham số nếu như chúng ta dùng cái kernel 5x5 thì tổng số lượng tham số của mình sẽ là 25 tham số sau này mình sẽ viết tắt là bằng chữ P đi rồi còn nếu như thực hiện hai cái phép convolution liên tiếp nó sẽ là 2 x 3 x 3 tức là bằng 18 tham số vậy thì rõ ràng 18 nó sẽ bé hơn so với lại 25 và nếu chúng ta chia tỉ lệ thì 18 chia cho 25 thì đâu đó nó cỡ khoảng 70% tức là chúng ta đã tiết giảm được khoảng 30%"
        },
        {
          "index": 15,
          "video_id": "Chương 4_KoBIBuqGb9A",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_2： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/KoBIBuqGb9A",
          "start_time": 699,
          "end_time": 760,
          "text": "25 và nếu chúng ta chia tỉ lệ thì 18 chia cho 25 thì đâu đó nó cỡ khoảng 70% tức là chúng ta đã tiết giảm được khoảng 30% chúng ta đã tiết giảm được khoảng 30% cái số param thì đây chính là giải thích tại sao cái việc mà bỏ các cái filter kích thước là 5577 bằng 3x3 thì nó sẽ giúp cho mình giảm cái số tham số và giảm số tham số thì chúng ta biết rồi nó sẽ giúp cho mình giảm cái hiện tượng overfitting overfitting rồi đồng thời cái này thì không gọi là cải tiến nhưng mà VGG đã tăng cái độ sâu của mạng lên từ VGG 11 lên VGG 13 rồi lên VGG 16 và lên VGG 19 thì cái này nó không được tính là cải tiến và cải tiến lớn nhất của nó nó sẽ nằm ở cái chỗ này đó là"
        },
        {
          "index": 16,
          "video_id": "Chương 4_KoBIBuqGb9A",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_2： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/KoBIBuqGb9A",
          "start_time": 749,
          "end_time": 810,
          "text": "từ VGG 11 lên VGG 13 rồi lên VGG 16 và lên VGG 19 thì cái này nó không được tính là cải tiến và cải tiến lớn nhất của nó nó sẽ nằm ở cái chỗ này đó là nó sẽ giải thích các cái filter lớn bằng những cái filter 3x3 liên tiếp và trên đây thì chúng ta sẽ thấy là cái sơ đồ của một cái kiến trúc mạng là VGG 16 trong đó là VGG 16 thì 16 thể hiện là các cái phép biến đổi bao gồm convolution và fully connected ví dụ như đây là 1, 2, 3, 4 ở cái bức pooling này thì nó không có tạo ra đặc trưng mà nó chỉ là giảm chiều thôi nên nó không được tính 1, 2, 3, 4 5, 6, 7, 8 9, 10, 11, 12 13, 14, 15, 16 thì cái ý nghĩa của con số 16 là như vậy rồi và bây giờ nếu như chúng ta nhìn vô cái số lượng trích dẫn của cái kiến trúc mạng VGG thì thấy là VGG có số lượng trích dẫn là 121.000"
        },
        {
          "index": 17,
          "video_id": "Chương 4_KoBIBuqGb9A",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_2： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/KoBIBuqGb9A",
          "start_time": 799,
          "end_time": 817,
          "text": "nếu như chúng ta nhìn vô cái số lượng trích dẫn của cái kiến trúc mạng VGG thì thấy là VGG có số lượng trích dẫn là 121.000 121.000 trích dẫn thì đây cũng là một số lượng vô cùng khủng khiếp thì đây cũng là một số lượng vô cùng khủng khiếp"
        }
      ]
    },
    {
      "video_id": "Chương 4_tMKUb4k5nZw",
      "summary": "## 1. Giới thiệu (Introduction)\n- Mục tiêu chính của bài giảng: Ôn tập các kiến trúc CNN tiêu biểu (GoogLeNet/Inception, bottleneck 1x1 conv, so sánh AlexNet–VGG–Inception) và trình bày ý tưởng chính của ResNet (skip/residual connection) cùng tác dụng của nó đối với vấn đề vanishing gradient và khả năng mở rộng chiều sâu mạng. [1][8][12][14]  \n- Các khái niệm sẽ được đề cập: *Bottleneck* (1×1 convolution để giảm chiều sâu), *Inception module* (nhánh song song 1×1, 3×3, 5×5, max-pool + concat), so sánh độ sâu các kiến trúc (AlexNet, VGG, GoogLeNet), vấn đề khi tăng depth (sai lệch tăng lên), và *Residual/skip connection* với biểu thức H(x) = F(x) + x để chống vanishing gradient. [1][4][8][10][12][14]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 GoogLeNet — Bottleneck (1×1 conv) và Inception module\n- Ý tưởng chính của GoogLeNet:\n  - Dùng *bottleneck* (1×1 convolution) để *giảm chiều sâu* (số channels) của feature map trước khi áp dụng các phép biến đổi có kernel lớn (ví dụ 5×5), từ đó giảm mạnh số phép tính và số tham số. [1][4]  \n  - Kết hợp các nhánh song song (Inception module): thực hiện đồng thời convolution 1×1, 3×3, 5×5 và max-pooling (thường kèm 1×1 projection) rồi *concat* các feature map theo chiều depth để tổng hợp thông tin ở nhiều receptive field khác nhau. *Lưu ý*: kích thước không gian (height×width) của các feature map sau mỗi nhánh phải giống nhau để có thể concat. [1][8][9]\n  - (Ghi chú từ video) Bài GoogLeNet có lượng trích dẫn rất lớn (~58.000 trích dẫn theo bài giảng). [2]\n\n- Phân tích chi phí tính toán / tham số (ví dụ minh họa trong video):\n  - Thông thường, một convolution với kernel k×k, input depth D_in và output filters N_out trên một vị trí không gian tạo ra chi phí tỉ lệ ∝ k×k×D_in×N_out (và tổng chi phí nhân thêm kích thước không gian H×W). (Ví dụ cụ thể trong video). [2][3]\n  - Ví dụ cụ thể (theo bài giảng):\n    - Nếu không dùng bottleneck: áp dụng 5×5 conv trực tiếp trên input có depth = 480 để ra 48 filters → chi phí tỉ lệ về mặt kernel là 5×5×480×48 (và nhân thêm kích thước không gian 14×14) → chi phí rất lớn. [2][3]\n    - Nếu dùng bottleneck 1×1 trước: dùng 16 filters 1×1×480 (1×1 conv) để giảm depth trung gian xuống 16, sau đó thực hiện 5×5 conv có depth = 16 để ra 48 filters. Nhờ vậy các hạng tử nhân trong 5×5 conv thay vì nhân với 480 thì chỉ nhân với 16 → giảm đáng kể số phép tính. Tổng hợp lại, ví dụ trong bài giảng cho thấy tổng phép tính giảm từ khoảng ~100 triệu xuống ~5 triệu (giảm rất nhiều). [4][5][6]\n  - Kết luận: bottleneck 1×1 là thành phần then chốt giúp tiết kiệm phép tính và tham số trong Inception/GoogLeNet. [4][6][7]\n\n### 2.2 Inception module — cấu trúc và lý do\n- Thiết kế: thực hiện song song các phép biến đổi với kernel kích thước khác nhau (1×1, 3×3, 5×5) và nhánh max-pooling 3×3, sau đó *concat* các feature map theo chiều depth để tạo một feature map tổng hợp chứa nhiều loại thông tin (scale khác nhau). *Mục đích*: vì không biết kích thước kernel nào là tốt nhất, nên thực hiện nhiều loại rồi tổng hợp. [8][9]\n- Yêu cầu: các nhánh phải trả về cùng kích thước không gian (H×W) để concat. [9]\n\n### 2.3 So sánh tiến hóa: AlexNet → VGG → GoogLeNet\n- Xu thế lịch sử: AlexNet (khoảng 8 layer) → VGG (đến ~19 layer) → GoogLeNet (~22 layer). Thực nghiệm cho thấy càng về sau, các kiến trúc có chiều sâu tăng lên và độ chính xác cũng thường được cải thiện (qua các cuộc thi như ImageNet). [10][11]\n\n### 2.4 Vấn đề khi tăng Depth quá nhiều (trước ResNet)\n- Quan sát: khi tăng depth lên vượt một giới hạn (ví dụ lớn hơn ~20), việc tăng depth không còn giúp giảm lỗi huấn luyện mà đôi khi còn làm tệ đi — mạng sâu hơn có thể có lỗi huấn luyện cao hơn mạng nông hơn. Ở ví dụ trong bài giảng, mô hình 56 lớp có lỗi lớn hơn mô hình 20 lớp. Điều này cho thấy vấn đề không chỉ là overfitting mà là khó tối ưu hóa (optimization degradation) khi mạng quá sâu. [12][13]\n\n### 2.5 ResNet — Residual / Skip Connection (ý tưởng và công thức)\n- Kiến trúc chính: sử dụng *residual module* với *skip connection* (bypass) để truyền trực tiếp input x vào sau một vài lớp biến đổi, tức là học một hàm dư. Công thức biểu diễn:\n  - H(x) = F(x) + x  \n    (tức output của block là kết quả của hàm phụ F(x) cộng với input x) [14][15]\n- Tác dụng đối với gradient / vanishing gradient:\n  - Vanishing gradient: chuỗi đạo hàm theo chain rule khi lan truyền gradient qua nhiều lớp có thể trở nên rất nhỏ (nhân nhiều hệ số < 1), dẫn đến gradient tiến về 0. Điều này gây khó khăn khi tối ưu hóa mạng rất sâu. [15][16]\n  - Với residual: đạo hàm của H w.r.t x = (đạo hàm của F w.r.t x) + 1 (tồn tại tác tử cộng 1 do thành phần tuyến tính identity). Việc cộng thêm 1 làm cho các phần đạo hàm không bị suy giảm quá mức, tức là *kéo lớn hơn* các đạo hàm thành phần, giúp giảm hiện tượng vanishing gradient và giúp huấn luyện dễ dàng hơn. [16][17][18]\n- Hậu quả:\n  - Skip connection giúp huấn luyện mạng sâu hơn một cách ổn định, huấn luyện nhanh hơn do gradient đủ lớn (bước cập nhật hiệu quả hơn). [18]\n  - Với cải tiến đơn giản này, ResNet có ảnh hưởng rất lớn và trở thành backbone phổ biến trong bài toán thị giác máy tính (đến thời điểm 2024 vẫn được nhắc đến nhiều). [19]\n\n### 2.6 Kết quả và ảnh hưởng (số liệu từ video)\n- ResNet cho phép mạng rất sâu đạt lỗi huấn luyện thấp (ví dụ 110, 56, 44, 32, 20 layers đều có lỗi thấp trong thí nghiệm với residual module). [19][20]\n- Bài ResNet nhận lượng trích dẫn rất lớn (~214.000 theo bài giảng), phản ánh tầm ảnh hưởng mạnh mẽ so với các công trình trước (2014–2015). [20][21]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n- Ví dụ minh họa trong video:\n  - Tính toán cụ thể về chi phí convolution trên feature map 14×14: không dùng bottleneck phải thực hiện convolution 5×5×480×48 (nhân với 14×14) → rất nhiều phép tính; dùng bottleneck với 1×1 conv giảm depth xuống 16 rồi 5×5 conv trên depth 16 → tổng phép tính giảm từ ~100 triệu xuống ~5 triệu trong ví dụ của bài giảng. [2][3][4][5][6]\n  - Minh họa Inception: thực hiện nhánh 1×1, 3×3, 5×5, max-pool rồi concat để thu được feature map \"rộng\" (deep) cung cấp nhiều thông tin đa tỉ lệ. [8][9]\n- Ứng dụng thực tế:\n  - GoogLeNet/Inception dùng architectue này để đạt hiệu quả cao trên bài toán phân loại ảnh (ImageNet) và làm nền tảng cho các mô hình nén tham số/hiệu quả tính toán. [1][8]\n  - ResNet trở thành backbone phổ biến cho nhiều bài toán thị giác máy tính (classification, detection, segmentation, ...), nhờ khả năng huấn luyện mạng rất sâu ổn định. Video nhấn mạnh việc ResNet vẫn được nhắc đến rộng rãi làm backbone tính đến năm 2024. [19][20]\n- Trường hợp sử dụng:\n  - Khi cần giảm số tham số / FLOPs nhưng vẫn giữ receptive field lớn: áp dụng bottleneck 1×1 trước các conv lớn (Inception-style) để tiết kiệm chi phí. [4][5][6]\n  - Khi muốn xây dựng backbone rất sâu để học biểu diễn mạnh hơn: sử dụng residual connections để tránh khó tối ưu hóa do vanishing gradient. [14][18][19]\n\n## 4. Kết luận (Conclusion)\n- Tóm tắt các ý chính:\n  - GoogLeNet (Inception) giới thiệu hai ý tưởng chính: dùng bottleneck 1×1 conv để giảm chiều sâu trước conv lớn và dùng module Inception (nhánh song song + concat) để khai thác nhiều kích thước receptive field cùng lúc — giúp giảm tham số và thu thập đa tỉ lệ thông tin. [1][4][8][9]\n  - Tăng chiều sâu mạng không luôn luôn cải thiện hiệu suất do vấn đề tối ưu hóa (ví dụ mô hình sâu hơn đôi khi có lỗi cao hơn). ResNet giải quyết vấn đề này bằng *skip/residual connections* với H(x)=F(x)+x, giúp chống vanishing gradient và cho phép huấn luyện các mạng rất sâu hiệu quả. [12][13][14][16][17]\n- Tầm quan trọng:\n  - Cả hai hướng cải tiến (Inception giảm chi phí/đa tỉ lệ, ResNet giải bài toán huấn luyện mạng sâu) đều là bước tiến quan trọng làm thay đổi cách thiết kế backbone cho các bài toán thị giác máy tính, dẫn đến những kiến trúc có hiệu năng tốt hơn và khả năng ứng dụng rộng rãi. [4][8][18][19]\n- Liên hệ với các bài giảng khác:\n  - Video đã đặt GoogLeNet và ResNet vào chuỗi phát triển lịch sử của CNN: AlexNet (2012) → VGG → GoogLeNet (2014) → ResNet (2015), cho thấy xu hướng tăng chiều sâu và đồng thời các giải pháp kỹ thuật để khắc phục vấn đề phát sinh khi tăng depth (bottleneck, inception, residual). [10][11][12][20]\n\n---\n\nGhi chú: Tóm tắt trên dựa hoàn toàn và chỉ trích dẫn nội dung được nêu trong các đoạn video đã cung cấp; các citation [1]...[21] tương ứng với các đoạn thời gian đã cho trong nguồn.",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 0,
          "end_time": 59,
          "text": "Tiếp theo thì chúng ta sẽ tìm hiểu về cái kiến trúc mạng Google LayNet Thì cái chữ Lay này là tương ứng là chữ LayCool, danh LayCool Thì khu Google LayNet nó đã có những cái cải tiến sau Đầu tiên đó chính là nó sử dụng cái Botanix, cái lớp Botanix Conversion 1x1 để giảm chiều của đặc trưng Để giảm chiều của đặc trưng trước khi thực hiện cái phép biến đổi Trên cái filter có kích thước lớn hơn là 5x5 Lưu ý là trong cái Google LayNet thì họ phát triển song song với lại VGG Nên ở đây họ vẫn sử dụng những cái filter có kích thước lớn Rồi đồng thời cái cải tiến thứ 2 đó là họ sử dụng cái module Inception Là các cái nhánh song song, ở đây chúng ta thấy là có các cái nhánh song song Rồi sau đó nó sẽ tổng hợp thông tin lại Với các cái filter với kích thước khác nhau Thì chút nữa chúng ta sẽ bàn thêm Là 2 cái filter có kích thước khác nhau Thì 2 cái cải tiến này cụ thể đó là gì"
        },
        {
          "index": 2,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 49,
          "end_time": 110,
          "text": "Rồi sau đó nó sẽ tổng hợp thông tin lại Với các cái filter với kích thước khác nhau Thì chút nữa chúng ta sẽ bàn thêm Là 2 cái filter có kích thước khác nhau Thì 2 cái cải tiến này cụ thể đó là gì Thì cái kiến trúc mạng của Google LayNet Cho cái số lượng trích dẫn đó là 58.000 58.000 trích dẫn Cũng rất là lớn Rồi đầu tiên đó là Cái cải tiến đầu tiên đó là Botanix Thì bình thường nếu như chúng ta không sử dụng Botanix Tức là cách làm bình thường Đây là chính là cái cách làm bình thường Thì chúng ta sẽ Thực hiện cái phép Conversion Trên cái kernel này Trên cái kernel Trên cái kernel có kích thước là 5x5 Và số lượng filter ở đây sẽ là 48 filter Số lượng filter ở đây sẽ là 48 filter Thì khi nhân với cái kernel kích thước là 5x5 Đương nhiên chúng ta sẽ ngầm hiểu là nó sẽ có Cái 5x5 nhân cho 480 Tại vì cái quy ước đó là độ sâu của cái filter"
        },
        {
          "index": 3,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 100,
          "end_time": 159,
          "text": "Thì khi nhân với cái kernel kích thước là 5x5 Đương nhiên chúng ta sẽ ngầm hiểu là nó sẽ có Cái 5x5 nhân cho 480 Tại vì cái quy ước đó là độ sâu của cái filter Nó phải bằng với độ sâu của cái input của mình Thì nó sẽ tạo ra một cái Feature map này Có kích thước là 14x14 Đây là cái trục không gian Và cái độ sâu của mình Cái độ sâu của mình nó sẽ bằng Đúng cái số lượng cái filter của mình Độ sâu của mình ở đây là 48 Đúng bằng cái số lượng filter Thì nếu như không sử dụng Botanix Thì ở đây chúng ta sẽ xem Tổng số tham số của mình Nếu như chúng ta sử dụng Nếu như chúng ta sử dụng Đó chính là 14 x 14 14 x 400 80 x 5 x 5 x 400 À À xin lỗi x 48 Thì đây chính là Tổng số lượng phép tính Tổng số lượng phép tính"
        },
        {
          "index": 4,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 147,
          "end_time": 210,
          "text": "80 x 5 x 5 x 400 À À xin lỗi x 48 Thì đây chính là Tổng số lượng phép tính Tổng số lượng phép tính Rồi Và nếu như có sử dụng Nếu như có sử dụng cái Botanix Thì cái Botanix ở đây ý là gì Botanix đó là Khi cái input đầu vào của mình Đó thay vì chúng ta trực tiếp Biến đổi thành cái output có kích thước Đó là 14 x 14 x 48 Thì chúng ta sẽ qua một cái mức trung gian Và cái trung gian này nó sẽ sử dụng cái 1 x 1 convolution Nó sử dụng 1 x 1 convolution Và thay vì biến đổi trực tiếp sang 48 Thì chúng ta sẽ qua một cái trung gian đó là sử dụng 16 Cái filter 16 cái filter Có kích thước đó là 1 x 1 x 480 Rồi như vậy thì"
        },
        {
          "index": 5,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 200,
          "end_time": 260,
          "text": "16 cái filter Có kích thước đó là 1 x 1 x 480 Rồi như vậy thì Sau đó thì chúng ta sẽ Nhân với lại cái Filter có kích thước là 5 x 5 x 16 Thì bình thường ở bên đây Là 5 x 5 x 480 Thì nhờ cái Botanix này Thay vì chúng ta nhân với 480 Thì chúng ta chỉ việc nhân với 16 thôi Và đây chính là cái lý do Để giúp cho cái kiến trúc mạng của mình Nó giảm cái số lượng tham số Và đồng thời nó cũng sẽ giảm cái số lượng tham số Rất là nhiều Thì ở đây Đối với cái phép biến đổi Convolution 1 x 1 Và 16 cái phép biến đổi này Thì chúng ta sẽ có là 14 x 14 Nhân cho 480 Nhân cho 1 x 1 Nhân cho 16 Thì đây chính là cái số phép tính Đây số lượng phép tính Rồi tương tự như vậy thì 5 x 5 x 48"
        },
        {
          "index": 6,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 249,
          "end_time": 312,
          "text": "Nhân cho 480 Nhân cho 1 x 1 Nhân cho 16 Thì đây chính là cái số phép tính Đây số lượng phép tính Rồi tương tự như vậy thì 5 x 5 x 48 Thì chúng ta sẽ có như đây cái phép tính Và tổng lại thì nó chỉ có 5 triệu phép tính So với lại 100 triệu phép tính đó là 1 x 480 Thì chúng ta thấy là cái số lượng phép tính nó giảm đi rất là nhiều Rồi Và khi cái số phép tính này giảm Thì cái số tham số của mình đồng thời nó cũng sẽ giảm luôn Cái số tham số của mình trong trường hợp này nó cũng sẽ giảm luôn Thì bình thường ở bên đây Cái số tham số của mình đó là 5 x 5 x 480 À xin lỗi Nhân cho 48 Và À xin lỗi 5 x 5 x 480 Đây là kích thước của cái filter Sau đó nó sẽ giảm đi rất là nhiều Nó sẽ nhân với lại 48 cái filter nữa Nó sẽ nhân với 48 filter nữa Thì bên đây Chúng ta chỉ có là 1 x 1 Nhân cho 480 Đây là cái kích thước của cái filter của mình Và mình sẽ nhân với lại số lượng là 16"
        },
        {
          "index": 7,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 297,
          "end_time": 361,
          "text": "Nó sẽ nhân với 48 filter nữa Thì bên đây Chúng ta chỉ có là 1 x 1 Nhân cho 480 Đây là cái kích thước của cái filter của mình Và mình sẽ nhân với lại số lượng là 16 16 cái filter kích thước là 1 x 1 x 480 Rồi Và sau đó chúng ta sẽ thực hiện phép conclusion với Cái kernel Cái filter có kích thước là 5 x 5 x 480    Nhân với lại 16 Đây là cái kích thước của cái filter của mình Để thoải mãn là cái độ sâu giống với bên này Và nhân với lại 48 Nhân với lại 48 Thì ở đây sẽ ra là cái số lượng tham số Và chúng ta có thể tính toán là Tổng của 2 cái này Nó sẽ Nhỏ hơn rất là nhiều So với lại cái số lượng tham số ở đây Thì đây là cái lớp bottleneck Thì đây là cái lớp bottleneck Ngoài ra thì 1 cái cải tiến khác của Google Linux Ngoài ra thì 1 cái cải tiến khác của Google Linux Là cái lớp cái module gọi là inception Thì ý tưởng của cái module inception đó là gì"
        },
        {
          "index": 8,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 350,
          "end_time": 409,
          "text": "Thì đây là cái lớp bottleneck Thì đây là cái lớp bottleneck Ngoài ra thì 1 cái cải tiến khác của Google Linux Ngoài ra thì 1 cái cải tiến khác của Google Linux Là cái lớp cái module gọi là inception Thì ý tưởng của cái module inception đó là gì Thì ý tưởng của cái module inception đó là gì Chúng ta sẽ không biết được là Cái filter của mình kích thước 1 x 1 Hay là 3 x 3 Hay là 5 x 5 Hay là 7 x 7 v.v Chúng ta không biết được Cái kích thước của cái filter bao nhiêu là tốt nhất Do đó thì chúng ta cứ thực hiện hết Do đó thì chúng ta cứ thực hiện hết Thì ở đây chúng ta sẽ thực hiện với conclusion 1 x 1 Ở đây thực hiện conclusion 3 x 3 Ở đây thực hiện conclusion 5 x 5 Ở đây là max boolding 3 x 3 Và sau đó chúng ta sẽ thực hiện cái phép biến đổi đó là concat Và sau đó chúng ta sẽ thực hiện cái phép biến đổi đó là concat Tại vì sau mỗi cái phép biến đổi này Sau mỗi cái phép biến đổi này Chúng ta sẽ có 1 cái Feature map Và lưu ý đó là cái feature map này Có thể có các cái độ sâu khác nhau Nhưng cái kích thước bề ngang bề cao nó phải giống nhau"
        },
        {
          "index": 9,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 402,
          "end_time": 462,
          "text": "Và lưu ý đó là cái feature map này Có thể có các cái độ sâu khác nhau Nhưng cái kích thước bề ngang bề cao nó phải giống nhau Nhưng cái kích thước bề ngang bề cao nó phải giống nhau rồi và khi chúng ta còn các lại thì chúng ta sẽ tạo ra một cái feature map có kích thước rất là dài thì chết bạn này là cái này bạn này chính là cái này bạn này nó cái này và bạn này chính là cái này thì đây là cái minh họa cho cái việc là thực hiện cái phép con cát con cát tức là nối đặc trưng lại và cái đặc trưng tổng này người ta hi vọng là nó chứa đầy đủ thông tin để giúp cho chúng ta thực hiện cái quá trình biến đổi và nhận dịa rồi Vậy thì sau khi chúng ta đã khảo sát qua các kiến"
        },
        {
          "index": 10,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 449,
          "end_time": 513,
          "text": "và cái đặc trưng tổng này người ta hi vọng là nó chứa đầy đủ thông tin để giúp cho chúng ta thực hiện cái quá trình biến đổi và nhận dịa rồi Vậy thì sau khi chúng ta đã khảo sát qua các kiến trúc mạng lên nét Alex net và Google lên nét thì chúng ta đang quan sát có một cái điều gì đang xảy ra đầu tiên đó chính là Alex net Alex net nó chỉ có 8 layer và có độ dài như thế này VGG có đến 19 layer và xét về cái mối tương quan thì chúng ta thấy là nó dài hơn rất là nhiều so với lại cái 8 layer và Google lên nét đó cho cái số layer là 22 layer như vậy chúng ta thấy là càng về sau cái số layer của mình sẽ càng lúc càng tăng và cái kết quả của mình càng lúc độ chính xác càng tốt thực hiện qua cái việc đó là Alex net khi chiến thắng trong cái cuộc thi vào năm 2012 Viğ Thịuction thì năm"
        },
        {
          "index": 11,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 495,
          "end_time": 563,
          "text": "số layer của mình sẽ càng lúc càng tăng và cái kết quả của mình càng lúc độ chính xác càng tốt thực hiện qua cái việc đó là Alex net khi chiến thắng trong cái cuộc thi vào năm 2012 Viğ Thịuction thì năm 2014 vào Google lên nét financiering equipenEPY và Shah Когда Opening nét�a Sciences St Cecelia Whitman được Martin bạn thừa công hoàn carefully có thể có遠 ch nächsten và Công tác kКiên kịp only là antioxidant của mình sẽ bao vây lòng kia làm việc đấy còn chút nhé tới Let's Just Win cũng sẽ nói bây ngờ tất cả là Peter c ebenfalls thẻ được sáng wires cho aftermas meine phòng chăm sóc cho Personal thì mình sẽ thấy nó nhỏ chi chích như thế này. Vậy thì theo một cái cách nội suy bình thường thì chúng ta sẽ suy nghĩ rằng là à thôi chúng ta cứ việc tăng cái layer lên thì tự nhiên cái độ chính xác của mình nó sẽ tăng lên đúng không? Thì ở đây chúng ta sẽ có một cái biểu đồ để so sánh cái mối tương quan về cái kích thước của các cái kiến trúc mạng. Outlet net 8 layer nó chỉ có như đây, VGG 19 nó chỉ có kích thước này như đây và cái rest net chiến thắng trong cái cuộc thi năm 2015 nó có cái kích thước như đây. Rất là dài so với lại những kiến trúc trước đây. Thế thì khi cái kiến trúc mạng mà"
        },
        {
          "index": 12,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 549,
          "end_time": 609,
          "text": "nó chỉ có kích thước này như đây và cái rest net chiến thắng trong cái cuộc thi năm 2015 nó có cái kích thước như đây. Rất là dài so với lại những kiến trúc trước đây. Thế thì khi cái kiến trúc mạng mà càng dài, kiến trúc mạng nó càng có nhiều các lớp biến đổi thì nó sẽ có những cái vấn đề gì và cái rest net nó đã giải quyết cái vấn đề đó như thế nào thì chúng ta sẽ cùng tìm hiểu trong cái phần tiếp theo đó là kiến trúc mạng rest net. Cái vấn đề mà rest net họ phát hiện ra đó là khi tăng cái độ chính xác lên thì hình như có vẻ cái độ chính xác sẽ càng tăng. Đó là cái quan sát khi trên ba cái kiến trúc mạng là Alex net, VGG rồi Inception. Tuy nhiên khi mà họ tiến hành càng tăng nhiều hơn nữa khi số lượng layer mà lớn hơn 20 thì điều này nó không còn đúng nữa. Tăng cái độ sâu lên và nó không còn hiệu quả."
        },
        {
          "index": 13,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 580,
          "end_time": 665,
          "text": "Cái vấn đề mà rest net họ phát hiện ra đó là khi tăng cái độ chính xác lên thì hình như có vẻ cái độ chính xác sẽ càng tăng. Đó là cái quan sát khi trên ba cái kiến trúc mạng là Alex net, VGG rồi Inception. Tuy nhiên khi mà họ tiến hành càng tăng nhiều hơn nữa khi số lượng layer mà lớn hơn 20 thì điều này nó không còn đúng nữa. Tăng cái độ sâu lên và nó không còn hiệu quả. Thể hiện qua cái việc ở đây là hạm độ lỗi ha. Độ lỗi là càng thấp càng tốt thì các bạn thấy là 20 layer thì nó nằm ở dưới cùng tức là tốt nhất. Đây là tốt nhất. Còn cái 56 layer nhiều nhất thì nó lại nằm ở trên cùng tức là tệ nhất. Nó không còn đúng như cái mà mình mong đợi nữa. Tức là càng tăng số layer thì cái độ lỗi của mình nó càng càng giảm hay là độ lỗi của mình càng nhỏ độ chính xác càng cao. Thì cái cải tiến của Rested nó rất là đơn giản. Đó là nó vẫn sẽ tăng cái độ sâu, tăng cái số lớp và cái độ sâu của cái kiến trúc mạng lên. Thì cái này nó không được tính là cải tiến ha. Nhưng cái cải tiến lớn nhất của nó, cái vấn đề hay nhất của nó đó chính là cái skip connection. Một cái mạng bình thường, Feature Map x đồ vào nó sẽ thực hiện cái phép convolution, relu, convolution, relu để tạo ra cái hát."
        },
        {
          "index": 14,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 639,
          "end_time": 710,
          "text": "Thì cái cải tiến của Rested nó rất là đơn giản. Đó là nó vẫn sẽ tăng cái độ sâu, tăng cái số lớp và cái độ sâu của cái kiến trúc mạng lên. Thì cái này nó không được tính là cải tiến ha. Nhưng cái cải tiến lớn nhất của nó, cái vấn đề hay nhất của nó đó chính là cái skip connection. Một cái mạng bình thường, Feature Map x đồ vào nó sẽ thực hiện cái phép convolution, relu, convolution, relu để tạo ra cái hát. Đây là cái hàm biến đổi. Theo cái cách bình thường. Thì cái residual, cái ResNet đã có cái module gọi là residual ha. Là nó đã thực hiện cái phép cộng với lại chính cái đặc trưng x đồ vào. Nếu như chúng ta nhìn vô cái hàm ở đây thì chúng ta thấy công thức nó rất là đơn giản. X thực hiện convolution, convolution thì đây là cái cách làm bình thường. Và nó sẽ lấy cái dự kiện x đồ vào, cộng vào chính cái kết quả của 2 cái phép convolution vừa rồi. Thì h x sẽ làm mặt f x cộng x. Thì đây chính là cái cải tiến của nó nè. Bình thường, đây là bình thường nè. Và cải tiến của nó cực kỳ đơn giản đúng không?"
        },
        {
          "index": 15,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 699,
          "end_time": 761,
          "text": "Thì h x sẽ làm mặt f x cộng x. Thì đây chính là cái cải tiến của nó nè. Bình thường, đây là bình thường nè. Và cải tiến của nó cực kỳ đơn giản đúng không? Vậy thì chúng ta sẽ cùng tìm hiểu xem tại sao cái phép biến đổi này nó có thể cải thiện được cái mô hình. Thì chúng ta phải nhắc lại đến cái hiện tượng gọi là Vanishing. Cái hiện tượng Vanishing gradient này nó gây ra cái mô hình này.    Nó gây ra khi cái đạo hàm của hàm hợp đúng không? Là đạo hàm của hàm loss theo cái hàm 1 nè. Rồi đạo hàm của hàm 1 nè. Theo cái hàm thứ 2 nè. Rồi đạo hàm, đạo hàm thứ n. Theo cái biến theta nè. Thì trong quá trình cập nhật thì các cái đạo hàm này nó sẽ càng lúc nó sẽ càng bé. Đúng không? Đạo hàm này càng lúc nó sẽ càng bé. Và các cái giá trị bé mà nhân với nhau là mô hình.  Nhưng mà nhân với nhau thì nó sẽ dẫn đến cái thằng này nó sẽ tiến về 0."
        },
        {
          "index": 16,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 747,
          "end_time": 811,
          "text": "Thì trong quá trình cập nhật thì các cái đạo hàm này nó sẽ càng lúc nó sẽ càng bé. Đúng không? Đạo hàm này càng lúc nó sẽ càng bé. Và các cái giá trị bé mà nhân với nhau là mô hình.  Nhưng mà nhân với nhau thì nó sẽ dẫn đến cái thằng này nó sẽ tiến về 0. Như vậy thì để hãm lại cái việc tiến cái cái đạo hàm này. Cái chain rule này á. Cái cái cái đạo hàm hàm hợp này nó tiến về 0. Thì chúng ta sẽ tìm cách đó là gì? Tăng cái giá trị này lên. Tăng cái giá trị này lên. Tăng cái giá trị đạo hàm lên. Đúng không? Và cái cách tăng rất là dễ đó là chúng ta sẽ cộng thêm một cái đại lực x. Thì bây giờ chúng ta sẽ xem thử ha. Nếu như bình thường cái hàm Hx của mình. Đó là hàm Hx của mình. Đó là hàm Hx của mình. Thì khi chúng ta tính đạo hàm đúng không h phải x thì nó sẽ là đạo hàm của cái phép biến đổi con lưu sơn này. Thì khi chúng ta tính đạo hàm đúng không h phải x thì nó sẽ là đạo hàm của cái phép biến đổi con lưu sơn này. Bây giờ nếu như Hx của mình nó sẽ là bằng con lưu sơn của x cộng thêm x."
        },
        {
          "index": 17,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 792,
          "end_time": 862,
          "text": "Thì khi chúng ta tính đạo hàm đúng không h phải x thì nó sẽ là đạo hàm của cái phép biến đổi con lưu sơn này. Thì khi chúng ta tính đạo hàm đúng không h phải x thì nó sẽ là đạo hàm của cái phép biến đổi con lưu sơn này. Bây giờ nếu như Hx của mình nó sẽ là bằng con lưu sơn của x cộng thêm x. Thì lúc này đạo hàm của H. Lúc này lưu ý là cái H của mình bây giờ mình đang dùng cái ký hiệu ở trên đó.  thì đạo hàm của mình lúc này nó chính là convolution cộng thêm 1 và chính cái thao tác cộng thêm 1 này nó đã giúp cho cái đạo hàm của mình tăng cái giá trị lên tăng cái giá trị lên tăng cái giá trị của từng cái đạo hàm thành phần này lên và các cái đạo hàm thành phần này tăng lên thì cái đà giảm của hàng này nó sẽ giảm xuống, như vậy là cái đà giảm sẽ bị giảm xuống và dẫn đến đó là cái chuỗi đạo hàm này nó sẽ lâu tiến về 0 hơn thì việc sử dụng cái skip connection này"
        },
        {
          "index": 18,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 849,
          "end_time": 910,
          "text": "sẽ bị giảm xuống và dẫn đến đó là cái chuỗi đạo hàm này nó sẽ lâu tiến về 0 hơn thì việc sử dụng cái skip connection này nó sẽ giúp cho chúng ta đó là chống được cái hiện tượng vanishing gradient và chống cái vanishing gradient này thì nó sẽ giúp cho chúng ta huấn luyện nhanh hơn tại vì sao khi cái đạo hàm này nó đủ lớn huấn luyện nhanh hơn nè huấn luyện nhanh hơn thì do là cái thao tác theta là bằng theta trừ cho alpha nhân cho đạo hàm của l theo theta thì cái giá trị này nó lớn nó lâu giảm thì dẫn đến là cái bước nhảy của mình nó sẽ nhanh nó sẽ nhảy nhanh đúng không? nhảy nhanh hơn thì đó chính là cái cải tiến của mạng ResNet và với một cái cải tiến vô cùng bé như thế này thôi thì chúng ta thấy là cái impact của nó cực kỳ cao"
        },
        {
          "index": 19,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 899,
          "end_time": 960,
          "text": "đúng không? nhảy nhanh hơn thì đó chính là cái cải tiến của mạng ResNet và với một cái cải tiến vô cùng bé như thế này thôi thì chúng ta thấy là cái impact của nó cực kỳ cao và cho đến thời điểm hiện giờ là năm 2024 thì những cái mạng CNN mà khi người ta nhắc đến để mà làm một cái backbone để làm một cái nền tảng để cho huấn luyện để giải quyết các bài toán bên địa giác máy tính người ta vẫn nhắc đến ResNet rất là nhiều và bên trái đó chính là cái hình hồi nãy chúng ta show là càng tăng cái số lượng layer lên thì cái độ chính xác hoặc là cái accuracy nó sẽ càng giảm tức là càng tệ đó càng tệ càng tăng cái layer lên thì nó sẽ càng tệ độ lỗi nó rất là cao đúng không? nhưng khi sử dụng với cái ResNet khi sử dụng với cái ResNet thì chúng ta sẽ thấy nè những cái thằng mà nằm có cái độ lỗi thấp nhất đúng không? là 110 layer 56 layer"
        },
        {
          "index": 20,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 949,
          "end_time": 1009,
          "text": "khi sử dụng với cái ResNet thì chúng ta sẽ thấy nè những cái thằng mà nằm có cái độ lỗi thấp nhất đúng không? là 110 layer 56 layer 44 layer 32 layer 20 layer tức là những cái thằng mà nằm ở dưới là những cái thằng mà nằm ở dưới  là những cái thằng có số lượng layer rất là lớn lớn hơn so với những thằng ở trên như vậy nhờ cái module skip connection này hay còn gọi là residual module thì nó đã giúp cho cái mạng của mình có khả năng là càng lúc càng dài hơn rồi và với cái cải tiến rất là đơn giản như vậy thì cái bài ResNet đạt được cái số lượng trích dẫm là 214.000 tức là các cái bài báo trước các cái bài báo trước các bạn thấy ra đều dưới 200.000 riêng cái bài này với cải tiến rất là là đơn giản đúng không? và cái thời điểm mà nó ra cũng là ra sau những cái bài kia là 2016 những bài kia là 2014-2015"
        },
        {
          "index": 21,
          "video_id": "Chương 4_tMKUb4k5nZw",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_3： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/tMKUb4k5nZw",
          "start_time": 999,
          "end_time": 1021,
          "text": "riêng cái bài này với cải tiến rất là là đơn giản đúng không? và cái thời điểm mà nó ra cũng là ra sau những cái bài kia là 2016 những bài kia là 2014-2015 thằng này ra sau nhưng mà cái số lượng trích dẫm còn nhiều hơn và nhiều hơn gần gấp đôi so với lại các cái bài trước thì đủ cho thấy là ResNet nó có một cái cái sức ảnh hưởng kinh khủng khiếp như thế nào"
        }
      ]
    },
    {
      "video_id": "Chương 4_MNHY9TA4fZs",
      "summary": "## 1. Giới thiệu (Introduction)\n- Mục tiêu chính của bài giảng: ôn tập các kiến trúc mạng CNN tiêu biểu và những cải tiến chính nhằm giải quyết hai vấn đề lớn là *overfitting* và *vanishing gradient*, đồng thời trình bày chi tiết MobileNet và phép Depthwise Separable Convolution (DSC) như một biến thể tối ưu cho thiết bị di động. [6][12][13]  \n- Các khái niệm sẽ được đề cập: Conv (convolution), Pooling, ReLU, data augmentation, bottleneck (1x1 conv), Inception module, skip connection (ResNet), Depthwise Separable Convolution (Depthwise + Pointwise), so sánh số lượng tham số và lợi ích cho tốc độ/overfitting, cùng một số thành tựu/đặc điểm của AlexNet, VGG, GoogleNet, ResNet, MobileNet. [7][8][9][10][11][12]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Tổng quan: mục tiêu chung của các cải tiến CNN\n- Hai vấn đề chính các kiến trúc cố gắng giải quyết: *Overfitting* (mô hình quá phức tạp do nhiều tham số) và *Vanishing gradient* (đạo hàm bé làm chậm/không cập nhật tham số). [12][13]  \n- Hai hướng giải quyết chủ yếu: giảm số lượng tham số / tăng dữ liệu để chống overfitting; thiết kế kiến trúc tăng giá trị đạo hàm từng thành phần (ví dụ skip connection) để chống vanishing gradient. [13][14]\n\n### 2.2. Các kiến trúc lịch sử và cải tiến chính\n- CNN cơ bản: Conv và Pooling là hai cải tiến nền tảng; mục tiêu là giảm số tham số và khối lượng tính toán, từ đó giảm overfitting. [7]  \n- AlexNet:\n  - Thay Sigmoid bằng ReLU (giảm vanishing gradient). [7][8]  \n  - Sử dụng data augmentation (tăng dữ liệu) giúp chống overfitting. [8]  \n  - Lần đầu sử dụng GPU để tăng tốc tính toán. [8]\n\n- VGG:\n  - Cải tiến đơn giản: thay các filter lớn (5x5, 7x7) bằng chuỗi nhiều lớp 3x3 liên tiếp, vẫn giữ khả năng trích đặc trưng tương đương nhưng giảm tham số. [8][9]\n\n- GoogleNet (Inception):\n  - Hai cải tiến chính: sử dụng bottleneck 1x1 convolution và Inception module (nhiều kích thước filter song song: 1x1, 3x3, 5x5) để tận dụng đặc trưng ở các tỉ lệ khác nhau và giảm tham số. [9][10]  \n  - Thiết kế giả định là không biết kích thước filter tối ưu nên dùng nhiều kích thước cùng lúc (Inception). [10]\n\n- ResNet:\n  - Cải tiến then chốt: skip connection (residual connection). Biểu diễn: H(x) = Conv(x) + x. [10][11]  \n  - Tác dụng: khi tính đạo hàm, thành phần Conv được cộng thêm 1 (từ phần cộng x), làm tăng giá trị đạo hàm và giảm hiện tượng vanishing gradient; đồng thời có thể hiểu là giữ lại thông tin gốc (x) kết hợp với đặc trưng mới, làm thông tin đầy đủ hơn cho phân loại. [11][14][15]\n\n### 2.3. MobileNet và Depthwise Separable Convolution (DSC)\n- Mục tiêu MobileNet: không nhằm tối đa hóa độ chính xác tuyệt đối mà hướng tới giảm khối lượng tính toán để có thể triển khai trên thiết bị di động với phần cứng nhẹ. MobileNet ra đời khoảng năm 2018. [1][6]  \n- Ý tưởng chính: thay convolution chuẩn bằng Depthwise Separable Convolution (DSC), gồm hai bước liên tiếp:\n  1. Depthwise convolution: áp dụng một kernel (ví dụ 3x3) riêng cho từng kênh đầu vào (thực hiện độc lập theo chiều độ sâu), kết quả ghép lại sẽ có cùng số kênh như input. Ví dụ video dùng input có depth = 32, áp depthwise 3x3 trên từng kênh tạo feature map depth = 32. [1][2][3]  \n  2. Pointwise convolution: 1x1 convolution áp lên toàn bộ depth (ở ví dụ, 64 filter 1x1x32) để kết hợp thông tin kênh và tạo số kênh đầu ra mong muốn (ví dụ 64). [3][5]\n\n- So sánh số lượng tham số (theo ví dụ trong video):\n  - Convolution chuẩn (3x3 conv từ 32 -> 64): số tham số xấp xỉ 18.000 (video nêu ~18k). [4]  \n  - Depthwise separable: \n    - Bước depthwise: 3x3x32 tham số;  \n    - Bước pointwise: 1x1x32 × 64 = (video làm tròn) khoảng phần cộng dẫn tới tổng ~2.000 tham số (video đưa ra ~2k). [5]  \n  - Tỉ lệ giảm: số tham số giảm khoảng 9 lần (từ ~18k xuống ~2k), tức giảm còn 1/9 so với conv chuẩn — giảm 8/9 tham số. [5][11][12]\n\n- Lợi ích của giảm tham số:\n  - Giảm overfitting (ít tham số hơn). [5][6][12]  \n  - Tăng tốc độ tính toán, phù hợp triển khai trên thiết bị di động (MobileNet có thể chạy trên phần cứng nhẹ). [6]  \n  - MobileNet là một kiến trúc nổi tiếng, được trích dẫn nhiều trong tài liệu (video nêu con số “24.000 citation”). [6]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n- Ví dụ cụ thể (từ video) minh họa DSC:\n  - Input depth = 32; thực hiện depthwise 3x3 trên từng kênh -> output depth = 32; sau đó pointwise 1x1 với 64 filter 1x1x32 để ra output 64 kênh. So sánh tham số: conv chuẩn ~18k vs DSC ~2k -> giảm ~9x. [1][2][3][4][5][11][12]\n- Ứng dụng thực tế:\n  - MobileNet: triển khai trên thiết bị di động, nơi tài nguyên tính toán và bộ nhớ hạn chế — mục tiêu là tốc độ và tiết kiệm tham số hơn là tối đa hóa accuracy thuần túy. [6]  \n  - AlexNet: sử dụng GPU để tăng tốc huấn luyện (mốc lịch sử quan trọng). [8]  \n  - Inception/GoogleNet: dùng khi muốn tận dụng đa kích thước filter để trích xuất đặc trưng đa quy mô. [9]\n- Trường hợp sử dụng:\n  - Khi cần mô hình nhẹ, độ trễ thấp cho inference trên thiết bị biên (edge/mobile) -> chọn MobileNet hoặc biến thể dùng DSC. [6]  \n  - Khi gặp khó khăn do vanishing gradient với mạng rất sâu -> sử dụng ResNet (skip connection). [10][11]\n\n## 4. Kết luận (Conclusion)\n- Tóm tắt:\n  - Các kiến trúc CNN chính (AlexNet, VGG, GoogleNet, ResNet, MobileNet) đều cố gắng cân bằng giữa khả năng trích đặc trưng (accuracy) và các vấn đề thực tế như overfitting, vanishing gradient, và chi phí tính toán. [7][8][9][10][11][12]  \n  - MobileNet (2018) dùng Depthwise Separable Convolution (depthwise + pointwise 1x1) để giảm mạnh số tham số (~1/9 so với conv chuẩn trong ví dụ), từ đó giảm overfitting và tăng tốc inference, phù hợp cho thiết bị di động. [1][5][6][11][12]  \n  - ResNet với skip connection (H(x) = Conv(x) + x) là một giải pháp đơn giản nhưng hiệu quả để chống vanishing gradient và giữ lại thông tin gốc. [10][11][14][15]\n\n- Tầm quan trọng:\n  - Hiểu các “mẹo” thiết kế (ví dụ 1x1 bottleneck, Inception, 3x3 thay cho 5x5, skip connections, DSC) giúp bạn lựa chọn hoặc tùy biến kiến trúc phù hợp cho bài toán cụ thể (tối ưu tham số, tốc độ, hoặc ổn định huấn luyện). [8][9][10][11][12][15]\n\n- Liên hệ với các bài giảng khác:\n  - Bài giảng này tổng hợp và so sánh các cải tiến kiến trúc đã học trước đó (các lớp convolution, pooling, activation, regularization, và kỹ thuật huấn luyện như data augmentation/GPU), nhấn mạnh ứng dụng các kỹ thuật này khi thiết kế mô hình sâu tiếp theo. [7][8][13][15]\n\n(End — tóm tắt dựa hoàn toàn trên các đoạn trích được cung cấp.)",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 4_MNHY9TA4fZs",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_4： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/MNHY9TA4fZs",
          "start_time": 1,
          "end_time": 60,
          "text": "Và cuối cùng trong các kiến trúc mạng mà chúng ta sẽ tìm hiểu ngày hôm nay đó chính là MobileNet MobileNet thì nó ra đời khoảng năm 2018 Và cái mục tiêu của nó không phải là để tăng cái độ chính xác Mà mục tiêu của nó đó là để giảm cái khối lượng tính toán Nhưng cái cải tiến của nó đồng thời nó cũng đã giúp cho chúng ta giải quyết được cái hiện tượng Overfitting luôn Thì cái cải tiến của MobileNet đó là gì? Thì MobileNet đó đã thay thế cái phép Conclusion bình thường Bằng cái phép Depth-Wide Separable Conclusion, tức là DSC Thì cái bản chất của cái phép DSC này thì nó thực hiện 2 bước Nó cũng giống như là cái bottleneck của Google Linux Nó gồm 2 bước Đầu tiên nó là Depth-Wide Conclusion Và sau đó nó sẽ thực hiện cái Point-Wide Conclusion Thì chi tiết chúng ta sẽ nói trong cái ví dụ sau Ở đây chúng ta sẽ có Input Input của mình trong trường hợp này Nó có 32 kênh"
        },
        {
          "index": 2,
          "video_id": "Chương 4_MNHY9TA4fZs",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_4： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/MNHY9TA4fZs",
          "start_time": 47,
          "end_time": 109,
          "text": "Và sau đó nó sẽ thực hiện cái Point-Wide Conclusion Thì chi tiết chúng ta sẽ nói trong cái ví dụ sau Ở đây chúng ta sẽ có Input Input của mình trong trường hợp này Nó có 32 kênh Có cái độ sâu là 32 Và chúng ta sẽ lấy cái filter này Cái filter này nó sẽ chia sẻ với filter này Tức là dùng chung cái bộ filter Rồi lấy cái filter này chúng ta sẽ lần lượt thực hiện trên từng cái kênh độc lập nhau Depth-Wide tức là thực hiện một cách độc lập theo cái chiều độ sâu Thực hiện độc lập Lấy filter này nhân với lại cái feature này Để tạo ra cái feature map này Lấy cái filter này nhân với lại cái feature này Filter này, filter này, filter này giống nhau ha Nhân với lại cái kernel này Xin lỗi nhân với feature map này Để tạo ra cái feature Lấy cái filter này nhân với lại cái feature này Để tạo ra cái feature này Và chúng ta Con khác chúng ta nối Tất cả cái kết quả này lại với nhau Thì lúc này chúng ta sẽ tạo ra một cái feature map có kích thước cũng là 32"
        },
        {
          "index": 3,
          "video_id": "Chương 4_MNHY9TA4fZs",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_4： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/MNHY9TA4fZs",
          "start_time": 99,
          "end_time": 160,
          "text": "Và chúng ta Con khác chúng ta nối Tất cả cái kết quả này lại với nhau Thì lúc này chúng ta sẽ tạo ra một cái feature map có kích thước cũng là 32 Thì đây là cái bước đầu tiên Sang cái bước thứ hai á Bước thứ hai Đó là chúng ta sẽ thực hiện Point-Wide Conclusion hay còn gọi là point-wide conclusion Bước thứ hai là 1 x 1 conclusion Thì ở đây Chúng ta sẽ thực hiện cái phép conclusion giống như cái phép conclusion bình thường Và cái kích thước đầu ra của mình Trong trường hợp này đó là 64 kênh đúng không Thì ở đây chúng ta sẽ thực hiện 64 cái conclusion 64 cái filter 1 x 1 x 32 đúng không Tại vì cái depth ở đây là 32 Như vậy là cái filter này là có kích thước là 1 x 1 x 32 1 x 1 x 32 Và 64 cái kernel này 64 cái filter này Như vậy Nếu Chúng ta thực hiện cái phép conclusion bình thường Thì Cái Số lượng tham số"
        },
        {
          "index": 4,
          "video_id": "Chương 4_MNHY9TA4fZs",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_4： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/MNHY9TA4fZs",
          "start_time": 149,
          "end_time": 211,
          "text": "64 cái filter này Như vậy Nếu Chúng ta thực hiện cái phép conclusion bình thường Thì Cái Số lượng tham số Số lượng tham số của mình nó sẽ là 3 x 3 Và cái depth đầu vào của mình là 32 đúng không 3 x 3 x 32 Đây là cái kích thước của cái filter Nhân với lại 64 cái filter như vậy Thì nó sẽ ra là khoảng 18.000 tham số Còn nếu như chúng ta thực hiện depth-wide separable conclusion Thì Ở đây chúng ta sẽ có là Kích thước của cái filter của mình nó sẽ là 32 x 3 x 3 Tức là sao Ở đây cái kích thước của cái filter của mình là 3 x 3 x 3 Và mình sẽ có cái độ sâu tương ứng là 32 đúng không Độ sâu 32 Giá thằng này có cái độ sâu là 32 Cái filter này có độ sâu là 32 và nó sẽ chia sẻ trọng số với mấy này"
        },
        {
          "index": 5,
          "video_id": "Chương 4_MNHY9TA4fZs",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_4： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/MNHY9TA4fZs",
          "start_time": 200,
          "end_time": 261,
          "text": "Và mình sẽ có cái độ sâu tương ứng là 32 đúng không Độ sâu 32 Giá thằng này có cái độ sâu là 32 Cái filter này có độ sâu là 32 và nó sẽ chia sẻ trọng số với mấy này Về tổng số tham số của mình sẽ là 3 x 3 x 32 Cho cái bước số 1 Đối với cái bước số 2 Đây là bước 1 Đối với cái bước số 2 Thì cái kernel filter của mình nó sẽ có kích thước là 1 x 1 x 32 1 x 1 x 32 Và có 64 cái filter như vậy Có 64 filter như vậy Thì cộng lại 2 x 1 x 32 là 32 Thì 2 cái số lượng tham số của bước 1 và bước 2 Thì chúng ta sẽ có Số lượng tham số sẽ là 2000 Như vậy nếu chia ra 2000 Cho 18000 Thì đâu đó nó sẽ sắp xỉ Nó sẽ sắp xỉ là 1 phần 9 Như vậy cái số lượng tham số của mình Nó giảm xuống còn 1 phần 9 Như vậy Nó sẽ giúp cho mình giảm param Giảm param nó sẽ có 2 công dụng"
        },
        {
          "index": 6,
          "video_id": "Chương 4_MNHY9TA4fZs",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_4： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/MNHY9TA4fZs",
          "start_time": 248,
          "end_time": 310,
          "text": "Nó sẽ sắp xỉ là 1 phần 9 Như vậy cái số lượng tham số của mình Nó giảm xuống còn 1 phần 9 Như vậy Nó sẽ giúp cho mình giảm param Giảm param nó sẽ có 2 công dụng Một đó là giảm cái hiện tượng overfit Và hai đó là chúng ta sẽ tăng cái tốc độ lên Tăng cái tốc độ tính toán lên Thì đây mới chính là cái mục tiêu chính của cái mạng mobile net Là để tăng cái tốc độ tính toán Và như cái tên thì mobile net nó có thể triển khai được trên các cái thiết bị di động Một trong những cái mạng CNN mà có khả năng triển khai được trên thiết bị di động Sử dụng những cái phần cứng Không có quá nặng đắt tiền Và có cái khối lượng xử lý lớn Thì cái mobile net này nó Có cái Số lượng citation cũng khá là lớn Đó là 24.000 24.000 citation Thì đây cũng là Một trong những cái Kiến trúc mạng rất là nổi tiếng Rồi như vậy thì chúng ta sẽ cùng Tóm tắt lại một số cái thành tựu"
        },
        {
          "index": 7,
          "video_id": "Chương 4_MNHY9TA4fZs",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_4： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/MNHY9TA4fZs",
          "start_time": 300,
          "end_time": 359,
          "text": "Thì đây cũng là Một trong những cái Kiến trúc mạng rất là nổi tiếng Rồi như vậy thì chúng ta sẽ cùng Tóm tắt lại một số cái thành tựu Của Các cái kiến trúc mạng LNS Cải tiến lớn nhất của nó đó chính là cái phép Convulsions Và cái phép Pooling Mục tiêu của Convulsions đó là để giảm cái số lượng tham số Và giảm tham số này để giúp cho chúng ta giảm cái hiện tượng overfit Pooling Sau này thì nó cũng sẽ giúp cho chúng ta giảm cái số lượng tham số Nhưng đồng thời nó cũng giúp cho chúng ta giảm cái khối lượng tính toán Giảm cái việc tính toán Việc giảm tham số này nó sẽ còn giúp cho chúng ta giảm cái hiện tượng overfit AlexNet Cải tiến lớn nhất của nó Đó chính là nó sẽ thay cái thằng sigmoid bằng relu Và cái relu này thì nó sẽ giúp cho chúng ta giảm cái hiện tượng gọi là vanishing Radiant"
        },
        {
          "index": 8,
          "video_id": "Chương 4_MNHY9TA4fZs",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_4： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/MNHY9TA4fZs",
          "start_time": 349,
          "end_time": 411,
          "text": "Đó chính là nó sẽ thay cái thằng sigmoid bằng relu Và cái relu này thì nó sẽ giúp cho chúng ta giảm cái hiện tượng gọi là vanishing Radiant Đồng thời nó tăng cái dữ liệu lên tăng cường dữ liệu lên Tăng dữ liệu lên này thì sẽ giúp cho chúng ta giảm cái hiện tượng overfitting Đồng thời nó sẽ là lần đầu tiên sử dụng GPU Sử dụng GPU để tăng cái tốc độ lên Tăng cái tốc độ tính toán VGG Đây là một trong những cái kiến trúc mạng có cái cải tiến rất là đơn giản Đó là thay những cái thằng 5x5 7x7 Bỏ hết đi Và thay bằng những cái 3x3 liên tiếp Và cái việc cải tiến này Nó đã giúp cho chúng ta giảm cái số lượng tham số Với cùng một cái mục đích Với cùng một cái việc là rút trích đặc trưng Với cái reset tifu giống nhau"
        },
        {
          "index": 9,
          "video_id": "Chương 4_MNHY9TA4fZs",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_4： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/MNHY9TA4fZs",
          "start_time": 400,
          "end_time": 459,
          "text": "Và cái việc cải tiến này Nó đã giúp cho chúng ta giảm cái số lượng tham số Với cùng một cái mục đích Với cùng một cái việc là rút trích đặc trưng Với cái reset tifu giống nhau Thì nó đã giảm được cái số lượng tham số Mà giảm số lượng tham số thì giúp cho chúng ta giảm được cái hiện tượng overfitting Google Linux có hai cái cải tiến chính Một đó là sử dụng cái bottleneck 1 x 1 Convolution Và hai đó là cái inception Module Thì hai cái cải tiến này nó sẽ giúp cho chúng ta giảm số lượng tham số Đồng thời đó là do giúp cho chúng ta cái inception là giảm số lượng tham số  Và cái inception này sẽ giúp cho chúng ta tận dụng được các cái đặc trưng Từ nhiều cái loại Từ nhiều cái filter Có kích thước khác nhau Ví dụ filter 3x3 Filter 1x1"
        },
        {
          "index": 10,
          "video_id": "Chương 4_MNHY9TA4fZs",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_4： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/MNHY9TA4fZs",
          "start_time": 451,
          "end_time": 511,
          "text": "Từ nhiều cái loại Từ nhiều cái filter Có kích thước khác nhau Ví dụ filter 3x3 Filter 1x1 Filter 5x5 Tại vì với cái assumption của Google Linux là họ không biết Cái filter kích thước bao nhiêu là total thì họ sẽ sử dụng hết Thì đây chính là cái cải tiến của Google Linux hơn  Cái giảm tham số này thì nó sẽ giúp cho chúng ta giảm hiện tượng overfit Rồi ResNet Cải tiến lớn nhất của nó Đơn giản nhất của nó Đó chính là Sử dụng các cái skip Connection Sử dụng cái skip connection Và biểu diễn dưới dạng công thức thì chúng ta sẽ có Hx Sẽ là bằng Convolution Của x Cộng thêm với x Thì đây chính là cái Cải tiến lớn nhất của nó Thì cái việc này nó sẽ giúp cho chúng ta Tăng cái giá trị đạo hàm lên Khi mình tính H phẩy Thì Đạo hàm Convolution nó sẽ cộng thêm 1"
        },
        {
          "index": 11,
          "video_id": "Chương 4_MNHY9TA4fZs",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_4： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/MNHY9TA4fZs",
          "start_time": 499,
          "end_time": 560,
          "text": "Thì đây chính là cái Cải tiến lớn nhất của nó Thì cái việc này nó sẽ giúp cho chúng ta Tăng cái giá trị đạo hàm lên Khi mình tính H phẩy Thì Đạo hàm Convolution nó sẽ cộng thêm 1 Nó sẽ giúp tăng cái giá trị đạo hàm lên Và Việc tăng cái đạo hàm Từng thành phần lên nó sẽ giúp cho chúng ta Giải quyết cái vấn đề là Vanishing Radian Rồi cuối cùng Đó chính là MobileNet Cải tiến lớn nhất của nó đó chính là thay vì chúng ta có cái mobile net  Chúng ta sử dụng 3 x 3 Convolution Đúng không Không sử dụng 3 x 3 Convolution nữa Mà chúng ta sẽ kết hợp Depth y Convolution Cộng với lại cái phép 1 x 1 Convolution Thì Hay còn gọi là Point y Convolution Thì cái việc này nó sẽ giúp cho chúng ta giảm được cái số lượng tham số Và cụ thể ở đây là giảm được 1 phần 9"
        },
        {
          "index": 12,
          "video_id": "Chương 4_MNHY9TA4fZs",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_4： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/MNHY9TA4fZs",
          "start_time": 546,
          "end_time": 606,
          "text": "Cộng với lại cái phép 1 x 1 Convolution Thì Hay còn gọi là Point y Convolution Thì cái việc này nó sẽ giúp cho chúng ta giảm được cái số lượng tham số Và cụ thể ở đây là giảm được 1 phần 9 Chúng ta chỉ còn Giảm được 8 phần 9 đúng không Tại vì Từ 9 xuống 1 phần 9 thì nó đã giảm 8 phần 9 cái số lượng tham số Và cái việc giảm tham số này nó sẽ có 2 cái tác dụng Đó là chống được Overfit Và Đồng thời là nó sẽ tăng cái speed Cái tốc độ tính toán của mình lên Như vậy thì ở trên đây ta đã tóm tắt qua Các cái kiến trúc mạng và những cái cải tiến chính Thì chúng ta thấy ra Hai cái vấn đề lớn nhất Mà các cái kiến trúc mạng Tập trung giải quyết Chúng ta nhìn xuyên xuống đây Chỉ có hai vấn đề lớn nhất thôi Hai vấn đề Vấn đề đầu tiên đó chính là vấn đề Overfitting Và vấn đề thứ hai đó là Vanishing"
        },
        {
          "index": 13,
          "video_id": "Chương 4_MNHY9TA4fZs",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_4： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/MNHY9TA4fZs",
          "start_time": 598,
          "end_time": 660,
          "text": "Vấn đề đầu tiên đó chính là vấn đề Overfitting Và vấn đề thứ hai đó là Vanishing Vanishing, Radiant Cái vấn đề về Overfitting Là xảy ra khi các cái kiến trúc mạng càng lúc càng sâu Thì cái số lượng tham số càng tăng Hoặc là số tham số càng tăng Thì mô hình càng phức tạp Nó sẽ dễ gõ đến những từ Overfitting Và để giải quyết vấn đề này thì chúng ta sẽ phải thiết kế Để làm Giảm cái số lượng tham số Giảm số lượng tham số Giảm số lượng tham số Hoặc chúng ta tăng cái dữ liệu lên Chúng ta tăng cường dữ liệu lên Còn đối với vấn đề về Vanishing, Radiant Nó sẽ gây ra cái việc là Tham số Theta Cập nhật Nó sẽ chọn Tham số Theta nó sẽ cập nhật Do cái giá trị đạo hàm này nó bé Thì để chống cái hiện tượng Vanishing, Radiant này"
        },
        {
          "index": 14,
          "video_id": "Chương 4_MNHY9TA4fZs",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_4： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/MNHY9TA4fZs",
          "start_time": 650,
          "end_time": 710,
          "text": "Cập nhật Nó sẽ chọn Tham số Theta nó sẽ cập nhật Do cái giá trị đạo hàm này nó bé Thì để chống cái hiện tượng Vanishing, Radiant này Thì người ta sẽ có những cái giải pháp Liên quan đến cái việc đó là Tăng cái giá trị đạo hàm của từng cái thành phần Trong cái hàm loss này lên Từng cái thành phần ta tăng lên Và ResNet Chỉ với một cái cải tiến rất là nhỏ Đó là cộng thêm cái x đầu vào Cộng thêm cái dữ liệu đầu vào Thì Nó đã giúp cho chúng ta Tăng cái giá trị đạo hàm và tăng giá trị đạo hàm thì giảm được hiện tượng Vanishing Đối với ResNet thì chúng ta có một cái Cách giải thích khác cho cái việc cộng cái x này ha Cái conclusion này Conclusion này đó là tạo ra một cái feature Nhưng mà cái feature này nó sẽ không còn giữ được cái Thông tin của cái dữ liệu đầu vào nữa Do đó chúng ta cộng thêm x Thì đây chính là cái dữ liệu Cái đặc trưng gốc Thì cái việc cộng này nó sẽ giúp cho chúng ta kết hợp với cái dữ liệu đầu vào này Kết hợp những cái đặc trưng mới"
        },
        {
          "index": 15,
          "video_id": "Chương 4_MNHY9TA4fZs",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 1_4： Ôn tập kiến trúc mạng CNN và một số biến thể phổ biến hiện nay",
          "video_url": "https://youtu.be/MNHY9TA4fZs",
          "start_time": 700,
          "end_time": 739,
          "text": "Thông tin của cái dữ liệu đầu vào nữa Do đó chúng ta cộng thêm x Thì đây chính là cái dữ liệu Cái đặc trưng gốc Thì cái việc cộng này nó sẽ giúp cho chúng ta kết hợp với cái dữ liệu đầu vào này Kết hợp những cái đặc trưng mới Và những cái đặc trưng gốc Để làm cho đầy đủ thông tin hơn cho cái quá trình nhận diện Thì đây là một cái cách giải thích khác theo cái lý thuyết về mặt thông tin Của ResNet Đó là giải thích cái tính hiệu quả của ResNet Như vậy thì Qua những cái kiến trúc mạng này thì chúng ta đã học được Rất nhiều những cái kỹ thuật khác nhau trong cái việc là cải tiến các cái mô hình học sâu Hy vọng rằng là các bạn có thể vận dụng được những cái Những cái mẹo này những cái Kỹ thuật này để cải tiến cho những cái mô hình học sâu tiếp theo Cố mình"
        }
      ]
    },
    {
      "video_id": "Chương 4_0I8uw0ELYj4",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giới thiệu **các cách sử dụng một mạng huấn luyện sẵn (pre-trained model)** để giải quyết bài toán của mình khi việc huấn luyện từ đầu tốn nhiều thời gian và tài nguyên; trình bày 3 cách chính để tận dụng mạng đã huấn luyện sẵn. [1]\n\n- Các khái niệm sẽ được đề cập: *sử dụng trực tiếp (direct use)*, *feature extraction* (dùng mạng như bộ rút trích đặc trưng), và *transfer learning / fine-tuning* (học chuyển tiếp, tinh chỉnh). [1]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Tại sao dùng pre-trained models\n- Huấn luyện một mạng CNN trên tập dữ liệu rất lớn có thể kéo dài nhiều ngày hoặc nhiều tháng và đòi hỏi tài nguyên tính toán lớn; do đó người ta thường sử dụng mô hình huấn luyện sẵn để tiết kiệm thời gian và tài nguyên. [1]\n\n### 2.2. Cách 1 — Sử dụng trực tiếp (Direct use)\n- Ý tưởng: nếu dataset mới có các nhãn trùng với các nhãn mà mô hình đã được huấn luyện (ví dụ: airplane, car, cat, horse) thì có thể **dùng trực tiếp** mô hình đó cho nhiệm vụ của mình. [2]  \n- Hạn chế: dù cùng nhãn, dữ liệu thực tế có thể khác về phân bố (ví dụ giống loài khác vùng miền), nên độ chính xác có thể không đạt kỳ vọng khi domain khác với tập gốc. Đây là cách *ngây thơ/đơn giản* nhất nhưng không luôn hiệu quả. [2][3][11]\n\n### 2.3. Cách 2 — Dùng mạng như bộ rút trích đặc trưng (Feature extraction)\n- Ý tưởng chung: tách phần *feature extractor* (các lớp đầu trong mạng như ResNet50) và loại bỏ lớp phân lớp cuối cùng; dùng phần rút trích để sinh feature vector cho ảnh mới. [3][4]  \n- Tính tổng quát: các đặc trưng học trên tập lớn thường mang tính tổng quát đủ để tái sử dụng cho các dataset khác nhau. [4]  \n- Sau khi lấy feature, ta kết hợp với bộ phân lớp khác (ví dụ K-Nearest Neighbor, SVM) để phân loại dựa trên feature vector thu được. [4][5]  \n  - K-Nearest Neighbor (KNN): với K=3, lấy 3 feature gần nhất so với điểm cần phân loại, dùng cơ chế vote để quyết định nhãn (ví dụ nếu 2 trong 3 là dog thì nhãn là dog). [5][6]  \n  - SVM: dùng để phân lớp (thường nhị phân) bằng cách tìm đường biên phân chia tốt nhất giữa hai lớp trong không gian feature. [6]\n\n### 2.4. Cách 3 — Transfer learning / Fine-tuning (Học chuyển tiếp)\n- Khái niệm: phần đầu của mạng đóng vai trò rút trích đặc trưng, phần cuối là lớp phân lớp được huấn luyện cho tập dữ liệu cũ; khi dùng cho bài toán mới, thường phải thay/làm lại phần phân lớp cuối để phù hợp với số nhãn mới. [7][8]  \n- Chú ý kích thước đầu ra: ví dụ nếu mô hình gốc có output 1000 chiều nhưng dataset mới chỉ có 3 nhãn thì tầng FC cuối cần thay thành vector 3 chiều. [7][8]\n\n- Hai chiến lược tinh chỉnh:\n  - 3.1 Freeze (đóng băng) phần feature extractor và chỉ huấn luyện (tinh chỉnh) phần phân lớp cuối (hoặc các lớp FC mới thêm vào). Phương pháp này phù hợp khi dataset mới **nhỏ**. [8][9]  \n  - 3.2 Fine-tune toàn bộ mạng (mở đóng băng và huấn luyện cả phần feature extractor lẫn phần phân lớp mới). Phương pháp này phù hợp khi dataset mới **lớn** và có đủ dữ liệu để cập nhật tham số toàn mạng. [9][10][12]\n\n- Lựa chọn giữa freeze vs full fine-tune phụ thuộc vào kích thước và tính khác biệt của dữ liệu mới so với dữ liệu gốc: nếu dữ liệu mới nhỏ hoặc tương đồng, thường freeze phần lớn và chỉ train lớp cuối; nếu dữ liệu mới lớn hoặc khác biệt nhiều, nên fine-tune toàn bộ. [9][10][11][12]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ nhãn trùng nhau: dataset gốc có nhãn như *airplane, car, cat, horse*; nếu bài toán người dùng quan tâm đến *cat, dog, horse* mà trùng nhãn, có thể dùng trực tiếp model gốc. [2]  \n- Ví dụ kiến trúc: **ResNet50** được dùng làm ví dụ – phần đầu làm feature extractor, phần cuối là lớp phân lớp cần thay/điều chỉnh khi sang bài toán mới. [3][4]  \n- Ví dụ thuật toán kết hợp: sau khi lấy feature, dùng KNN (vote K=3) hoặc SVM để phân lớp dựa trên feature vectors. [5][6]  \n- Vấn đề thực tế / domain shift: hình ảnh cùng nhãn (ví dụ *dog*) có thể khác nhau theo vùng địa lý (chó ở phương Tây khác với chó ở Việt Nam), dẫn đến giảm hiệu năng nếu dùng trực tiếp model huấn luyện trên dữ liệu chung mà không điều chỉnh. [3][11]  \n- Ứng dụng thực tế: các kỹ sư và nhà phát triển thường dùng các mô hình pre-trained và một trong ba cách trên để triển khai nhanh các bài toán nhìn nhận ảnh trong công nghiệp và nghiên cứu. [13]  \n- Trường hợp sử dụng:\n  - Muốn triển khai nhanh với tiết kiệm tài nguyên → dùng trực tiếp nếu domain tương đồng. [2][11]  \n  - Muốn tận dụng đặc trưng tổng quát, kết hợp với bộ phân lớp nhẹ → feature extraction + SVM/KNN. [4][5][6]  \n  - Muốn đạt hiệu năng cao cho dataset khác biệt hoặc lớn → transfer learning + fine-tune (toàn bộ hoặc một phần tùy kích thước dataset). [9][10][12]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính: có **3 cách** chính để dùng mạng huấn luyện sẵn: (1) sử dụng trực tiếp khi dataset trùng nhãn; (2) lấy mạng làm bộ rút trích đặc trưng và kết hợp với bộ phân lớp khác (KNN, SVM); (3) sử dụng transfer learning bằng cách thay lớp cuối và tinh chỉnh — hoặc chỉ train lớp cuối (freeze phần còn lại) hoặc train toàn bộ mạng tùy kích thước dataset. [1][2][4][5][6][7][8][9][10][12]\n\n- Tầm quan trọng: các kỹ thuật này cho phép tận dụng các mô hình CNN đã được huấn luyện trên tập lớn để giải quyết các bài toán thực tế với ít tài nguyên hơn, đồng thời giúp các engineer áp dụng nhanh trong công nghiệp. [1][13]\n\n- Liên hệ với các bài giảng khác: trong đoạn trích không có đề cập cụ thể đến bài giảng khác; bài này chủ yếu tập trung trình bày ba cách sử dụng mô hình huấn luyện sẵn và các cân nhắc kèm theo. [13]\n\nNếu bạn muốn, tôi có thể:\n- Rút gọn thành checklist triển khai step-by-step (ví dụ: khi nào dùng direct vs feature extraction vs fine-tune).  \n- Viết mẫu code (PyTorch hoặc TensorFlow) cho từng cách: sử dụng trực tiếp, feature extraction + SVM/KNN, freeze & fine-tune toàn mạng.",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 4_0I8uw0ELYj4",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 2： Sử dụng mạng huấn luyện sẵn",
          "video_url": "https://youtu.be/0I8uw0ELYj4",
          "start_time": 0,
          "end_time": 60,
          "text": "Cuối cùng chúng ta sẽ cùng tìm hiểu về các cách để sử dụng một mạng huấn luyện sẵn Thông thường các mạng CNN được huấn luyện trên những tập dữ liệu rất lớn Và việc huấn luyện này đâu đó có thể kéo dài tính bằng ngày Hoặc thậm chí tính bằng tháng Nó có thể kéo dài đến hàng tháng Và không phải ai cũng có khả năng có thể đủ tài nguyên tính toán Để mà có thể thực hiện được công việc huấn luyện này Do đó thì chúng ta sẽ có một kỹ thuật Đó là sử dụng những mô hình huấn luyện sẵn Để đi giải quyết những bài toán của riêng mình Thì ở đây chúng ta sẽ gọi là kỹ thuật sử dụng các pre-trained model Thì ở đây có 3 cách chính Cách đầu tiên đó là chúng ta sẽ sử dụng trực tiếp Chúng ta sẽ sử dụng trực tiếp nghĩa là sao Nếu như tập dataset của mình Đây là tập dataset của mình Nó có các nhãn"
        },
        {
          "index": 2,
          "video_id": "Chương 4_0I8uw0ELYj4",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 2： Sử dụng mạng huấn luyện sẵn",
          "video_url": "https://youtu.be/0I8uw0ELYj4",
          "start_time": 46,
          "end_time": 112,
          "text": "Cách đầu tiên đó là chúng ta sẽ sử dụng trực tiếp Chúng ta sẽ sử dụng trực tiếp nghĩa là sao Nếu như tập dataset của mình Đây là tập dataset của mình Nó có các nhãn Ví dụ như là máy bay, xe, mèo, con ngựa, v.v Và đối tượng cho bài toán mà mình đang quan tâm Đó là cat, dog, horse Tức là mèo, chó, bà ngựa Thì vô tình 3 đối tượng này trùng với các đối tượng Trong tập dataset mà chúng ta đã huấn luyện trước đó Nó đã trùng Thì chúng ta sẽ sử dụng trực tiếp luôn Chúng ta sẽ lấy chính cái model đó ra để sử dụng trực tiếp luôn Thì đây là cái cách ngây thơ nhất, đơn giản nhất để sử dụng Tuy nhiên nó sẽ có một cái vấn đề đó là Cái dữ liệu của mình, cat, dog và horse này nè Nó có khả năng là nó đi theo những cái giống loài Mà ở cái khu vực mà mình đang sinh sống Còn cái tập dataset này thì đó là những cái tập dataset chung Do đó thì có khả năng khi chúng ta sử dụng những cái model"
        },
        {
          "index": 3,
          "video_id": "Chương 4_0I8uw0ELYj4",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 2： Sử dụng mạng huấn luyện sẵn",
          "video_url": "https://youtu.be/0I8uw0ELYj4",
          "start_time": 99,
          "end_time": 159,
          "text": "Nó có khả năng là nó đi theo những cái giống loài Mà ở cái khu vực mà mình đang sinh sống Còn cái tập dataset này thì đó là những cái tập dataset chung Do đó thì có khả năng khi chúng ta sử dụng những cái model Mà đã trend trên cái tập dữ liệu lớn này Các dữ liệu này sẽ có khả năng để sử dụng những cái model này nè Và đồng chí là chúng ta sẽ sử dụng trên chính cái dữ liệu của mình Có khả năng là độ chính xác nó không đạt như chúng ta kỳ vọng Nhưng mà đây là cái cách ngây thơ nhất, đơn giản nhất đầu tiên Khi chúng ta sử dụng với một cái mạng huấn luyện sẵn Rồi, cái cách thức thứ 2 Đó là chúng ta sẽ sử dụng cái mạng CNN Mà đã được huấn luyện sẵn như là một cái bộ rút trích đặc trưng Thì ở đây chúng ta sẽ lấy ra một cái hình ảnh ví dụ thôi ha Đó là một cái mạng ResNet 50 Và cái ResNet 50 này á Nó sẽ có cái phần đầu là cái phần rút trích đặc trưng Nó sẽ là rút trích đặc trưng Cái phần sau là cái phần liên quan đến cái việc là phân lớp"
        },
        {
          "index": 4,
          "video_id": "Chương 4_0I8uw0ELYj4",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 2： Sử dụng mạng huấn luyện sẵn",
          "video_url": "https://youtu.be/0I8uw0ELYj4",
          "start_time": 154,
          "end_time": 210,
          "text": "Nó sẽ là rút trích đặc trưng Cái phần sau là cái phần liên quan đến cái việc là phân lớp Thì các cái nhà khoa học mới phát hiện ra rằng Các cái đặc trưng mà được huấn luyện với những cái tập dữ liệu lớn trước đây á Thì nó khá là tổng quát Sau này chúng ta đưa vô một cái tập dữ liệu bất kỳ Hoặc là đưa vô một cái đối tượng khác Thì các cái đặc trưng này đâu đó vẫn có khả năng sử dụng Tái sử dụng lại được Và chúng ta sẽ kết hợp nó Kết hợp với lại một cái mô hình máy học khác Như vậy thì ở đây chúng ta sẽ loại bỏ đi Chúng ta sẽ loại bỏ đi cái lớp Phân lớp cuối cùng Và ở đây Kết thúc cái bước mà feature extraction này Chúng ta sẽ ra một cái feature Chúng ta sẽ ra cái feature Và chúng ta sẽ sử dụng cái feature này Để đi kết hợp với cái mô hình máy học khác   Kết hợp với một cái bộ phân lớp khác Ví dụ ở đây chúng ta có thể sử dụng cái bộ phân lớp là K-Nearest Neighbor Chúng ta có thể sử dụng với cái bộ phân lớp là"
        },
        {
          "index": 5,
          "video_id": "Chương 4_0I8uw0ELYj4",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 2： Sử dụng mạng huấn luyện sẵn",
          "video_url": "https://youtu.be/0I8uw0ELYj4",
          "start_time": 200,
          "end_time": 260,
          "text": "Và chúng ta sẽ sử dụng cái feature này Để đi kết hợp với cái mô hình máy học khác   Kết hợp với một cái bộ phân lớp khác Ví dụ ở đây chúng ta có thể sử dụng cái bộ phân lớp là K-Nearest Neighbor Chúng ta có thể sử dụng với cái bộ phân lớp là SVM Thì cái feature này nếu mà chiếu trong cái không gian Đúng không? Thì chúng ta sẽ có các cái feature như thế này Rồi Và khi có một cái feature mới Đúng không? Cần phải phân loại Ví dụ ở đây chúng ta sẽ có một cái feature mới Đúng không? Thì chúng ta sẽ chạy cái thuật toán K-Nearest Neighbor Ví dụ trong trường hợp này K là bằng 3 Chúng ta sẽ lấy ra 3 cái feature Mà gần với lại cái điểm Mà mình cần phân loại này nhất Rồi sau đó chúng ta sẽ xem xem Cái nhãn của 3 cái feature này Đó là gì? Ví dụ như nếu đây là dot Đây là dot Đây là cat Như vậy chúng ta sẽ kết luận Cái nhãn của cái điểm này"
        },
        {
          "index": 6,
          "video_id": "Chương 4_0I8uw0ELYj4",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 2： Sử dụng mạng huấn luyện sẵn",
          "video_url": "https://youtu.be/0I8uw0ELYj4",
          "start_time": 249,
          "end_time": 308,
          "text": "Đó là gì? Ví dụ như nếu đây là dot Đây là dot Đây là cat Như vậy chúng ta sẽ kết luận Cái nhãn của cái điểm này Đó chính là dot Dùng cơ chế voting Thì đây chính là dot Chính là cái ý tưởng của Thuật toán K-Lán Diện gần nhất K-Nearest Neighbor Tương tự như vậy cho thuật toán SVM Là thuật toán phân lớp nhị phân Thì chúng ta sẽ có 2 cái tập Ví dụ cái feature ở đây Tương ứng đó là cái điểm này Và chúng ta sẽ có 2 cái tập Là tròn và cộng Rồi chúng ta sẽ nhờ Cái máy phân lớp để tìm ra Cái đường biên tốt nhất Để mà phân loại ra 2 cái tập dữ liệu này Thì đó là thuật toán SVM Thì đây là cái cách sử dụng thứ 2 Và cái cách sử dụng thứ 3 Đó là Chúng ta sẽ sử dụng transfer learning Hay còn gọi là học chuyển tiếp Thì học chuyển tiếp ở đây đó là gì"
        },
        {
          "index": 7,
          "video_id": "Chương 4_0I8uw0ELYj4",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 2： Sử dụng mạng huấn luyện sẵn",
          "video_url": "https://youtu.be/0I8uw0ELYj4",
          "start_time": 300,
          "end_time": 361,
          "text": "Đó là Chúng ta sẽ sử dụng transfer learning Hay còn gọi là học chuyển tiếp Thì học chuyển tiếp ở đây đó là gì Như đã đề cập Đó là Các cái lớp đầu tiên Nó đóng vai trò là cái feature Extraction Còn cái lớp ở cuối Nó đóng vai trò đó là phân lớp Thế thì cái bài toán phân lớp Cái mô hình Cái cái cái Các cái phép biến đổi này Các cái lớp này Là nó dùng cho data cũ Nó dùng cho cái data cũ Đúng không Dùng để phân lớp cho cái dữ liệu cũ Do đó mình sẽ không có thể Tái sử dụng nó được Chúng ta sẽ phải bỏ đi Và thay bằng 1 cái kím mẹ Fully connected Các cái FC khác Đúng không Và chúng ta lưu ý Là chúng ta ở cái lớp cuối cùng Là chúng ta phải điều chỉnh nha Ví dụ như ở đây Chúng ta có 1 ngàn lớp Thì cái output FC này Nó sẽ là 1 cái vector 1 ngàn chiều Nhưng mà giả sử như cái tập dữ liệu của mình"
        },
        {
          "index": 8,
          "video_id": "Chương 4_0I8uw0ELYj4",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 2： Sử dụng mạng huấn luyện sẵn",
          "video_url": "https://youtu.be/0I8uw0ELYj4",
          "start_time": 349,
          "end_time": 407,
          "text": "Và chúng ta lưu ý Là chúng ta ở cái lớp cuối cùng Là chúng ta phải điều chỉnh nha Ví dụ như ở đây Chúng ta có 1 ngàn lớp Thì cái output FC này Nó sẽ là 1 cái vector 1 ngàn chiều Nhưng mà giả sử như cái tập dữ liệu của mình Nó chỉ có 3 object thôi 3 nhãn thôi Thì lúc đó cái vector Cái lớp biến đổi đầu tiên của mình Nó sẽ phải ra cái vector Nó chỉ có 3 chiều thôi Chú ý cái chỗ đó Là nó sẽ tùy thuộc vô cái số lượng Các cái loại object của mình Rồi Sau khi chúng ta bỏ cái lớp Phân lớp Ở đây đi Và nối Với lại Cái lớp FC Các cái lớp FC Hay là gọi là Neural Network ở đây Và lưu ý là phải chỉnh lại Cái tầng cuối cùng Sao cho phù hợp Với lại cái kích thước Của dataset của mình Thì mình sẽ tiến hành Cách đầu tiên Cách 3.1 Đó là chúng ta sẽ đóng băng Đóng băng các cái lớp đồ này đi Tức là chúng ta sẽ Chúng ta sẽ không huấn luyện Không Huấn luyện"
        },
        {
          "index": 9,
          "video_id": "Chương 4_0I8uw0ELYj4",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 2： Sử dụng mạng huấn luyện sẵn",
          "video_url": "https://youtu.be/0I8uw0ELYj4",
          "start_time": 400,
          "end_time": 460,
          "text": "Đóng băng các cái lớp đồ này đi Tức là chúng ta sẽ Chúng ta sẽ không huấn luyện Không Huấn luyện Chúng ta sẽ không huấn luyện Trên cái Cái lớp Cái phần mà Rút chất đặc trưng Mà chúng ta chỉ huấn luyện ở đây Huấn luyện Cái Lớp phân lớp Huấn luyện cái việc phân lớp Đó Thì ở đây chúng ta sẽ chỉ Ở đây Chúng ta sẽ có một cái thuật ngữ Đó gọi là tinh chỉnh Chúng ta sẽ tinh chỉnh lại Tinh chỉnh Hoặc còn gọi là file tool Các cái tham số Ở những cái lớp cuối này thôi Thì đây là cái cách 3.1 Tuy nhiên Nếu như cái dữ liệu của mình Đủ lớn Cái cách 3.1 này Nó chỉ phù hợp Nó chỉ phù hợp Cho cái trường hợp là cái data Cái data mới của mình Cái data mới này của mình Là nhỏ thôi Còn khi mà cái data mới của mình Nó rất là lớn Khi data của mình"
        },
        {
          "index": 10,
          "video_id": "Chương 4_0I8uw0ELYj4",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 2： Sử dụng mạng huấn luyện sẵn",
          "video_url": "https://youtu.be/0I8uw0ELYj4",
          "start_time": 449,
          "end_time": 511,
          "text": "Cái data mới của mình Cái data mới này của mình Là nhỏ thôi Còn khi mà cái data mới của mình Nó rất là lớn Khi data của mình Nó rất là lớn Thì chúng ta Không cần phải đóng băng Cái lớp này Không cần cái đóng băng Chúng ta không cần đóng băng Cái lớp rút chất đặc trưng Mà chúng ta sẽ Huấn luyện luôn Trên toàn bộ Cái mạng này luôn Tức là Chúng ta sẽ huấn luyện Trên cả những cái phần Feature Extraction Lẫn cái phần Mà mình mới thêm vào Thì đây là Hai cái cách thức Để mà học truyền tiếp Và cái cách này Nó sẽ phù hợp Cho cái trường hợp Data của mình Data mới của mình Nó rất là lớn Data mới Rồi Như vậy thì hy vọng là Qua Cái phần số 3 này Chúng ta sẽ được giới thiệu Chúng ta hiểu qua Các cái cách thức Để mà Sử dụng Một cái mạng huấn luyện sẵn Trên những cái tập dữ liệu Rất là lớn Để đi giải quyết Cho các cái bài toán Của cá nhân mình Trên những cái dữ liệu lớn"
        },
        {
          "index": 11,
          "video_id": "Chương 4_0I8uw0ELYj4",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 2： Sử dụng mạng huấn luyện sẵn",
          "video_url": "https://youtu.be/0I8uw0ELYj4",
          "start_time": 499,
          "end_time": 560,
          "text": "Chúng ta hiểu qua Các cái cách thức Để mà Sử dụng Một cái mạng huấn luyện sẵn Trên những cái tập dữ liệu Rất là lớn Để đi giải quyết Cho các cái bài toán Của cá nhân mình Trên những cái dữ liệu lớn Thì trong cái cách số 1 Đó là nếu như Cái dữ liệu của mình Nó trùng với lại Cái đối tượng Mà mình quan tâm Nó trùng với lại Cái tập dữ liệu Mà mình đã huấn luyện Trước đó Thì chúng ta sử dụng trực tiếp Trong trường hợp Mà cái dữ liệu của mình Nó không giống Với lại cái dữ liệu Mà đã được huấn luyện Trước đó Nhưng mà đồng thời Hoặc là Dữ liệu của mình Nó giống Nhưng mà nó Rất khác Về cái thể loại Ví dụ như chó Ở phương Tây Nó sẽ khác với chó Ở Việt Nam Ví dụ vậy Thì chúng ta sẽ sử dụng Đến cái cách số 2 Và cách số 3 Cái cách số 2 Đó là chúng ta sẽ Sử dụng Kết hợp Cái cách 2 Là chúng ta sẽ kết hợp Với các cái mô hình Khác Đúng không Và cái cách 2"
        },
        {
          "index": 12,
          "video_id": "Chương 4_0I8uw0ELYj4",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 2： Sử dụng mạng huấn luyện sẵn",
          "video_url": "https://youtu.be/0I8uw0ELYj4",
          "start_time": 549,
          "end_time": 610,
          "text": "Là chúng ta sẽ kết hợp Với các cái mô hình Khác Đúng không Và cái cách 2 Cái cách số 3 Đó là chúng ta sẽ Học chuyển tiếp Transfer learning Chúng ta sẽ Thực hiện gọi là Transfer Learning Tức là chúng ta sẽ Thiết kế cái phần sau Của Cái kiến trúc mạng CNN Của mình Sao cho nó phù hợp Với lại cái loại dữ liệu mới Sau đó chúng ta sẽ Tinh chỉnh Và có 2 cách tinh chỉnh Đó là Cái cách 3.1 Đó là chúng ta sẽ Tinh chỉnh Phần cuối Cái cách 3.1 Đó là cái phần Phân lớp Và cái cách 3.2 Đó là chúng ta sẽ Đi tinh chỉnh Toàn bộ Toàn bộ cái mạng Thì đây là 3 cái cách thức Để mà chúng ta có thể sử dụng Một cái mạng công nghiệp sở"
        },
        {
          "index": 13,
          "video_id": "Chương 4_0I8uw0ELYj4",
          "chapter": "Chương 4",
          "video_title": "[CS431 - Chương 4] Part 2： Sử dụng mạng huấn luyện sẵn",
          "video_url": "https://youtu.be/0I8uw0ELYj4",
          "start_time": 599,
          "end_time": 633,
          "text": "Đi tinh chỉnh Toàn bộ Toàn bộ cái mạng Thì đây là 3 cái cách thức Để mà chúng ta có thể sử dụng Một cái mạng công nghiệp sở Thì Hy vọng là Qua cái bài học này Thì giúp cho chúng ta Sẽ có cái góc nhìn Toàn dị hơn Về những cái thành tựu Của cái mạng CNN Và Một cái mạng CNN Và nắm mắt được Một trong những cái cách thức Mà Các cái nhà Gọi là Các cái engineer Đang sử dụng Để Ứng dụng trong các cái công việc của mình Đó chính là sử dụng Cái phương pháp là Trên các cái mô hình hữu luyện sở"
        }
      ]
    },
    {
      "video_id": "Chương 5_RVFApjx4KKI",
      "summary": "## 1. Giới thiệu (Introduction)\n- Mục tiêu chính của bài giảng: khảo sát các ứng dụng phổ biến của mạng CNN trong **bài toán phân loại đối tượng** và **bài toán truy vấn ảnh (image retrieval / image query)**, đồng thời trình bày các thách thức và kỹ thuật chính để giải quyết những bài toán này. [1][15]  \n- Các khái niệm sẽ được đề cập: kiến trúc cơ bản của CNN (feature learning bằng convolution/ReLU/pooling + phân lớp bằng fully connected/softmax), phân loại ở mức độ *fine-grained* (mịn), các cải tiến hàm loss cho face recognition, yêu cầu trong ảnh y tế (explainability, domain khác biệt), và phương pháp truy vấn ảnh dựa trên đặc trưng local (DELF) cùng bước geometric verification. [2][3][10][11][9][20][22]\n\n## 2. Các điểm chính (Main Points)\n\n### A. Kiến trúc căn bản của bài toán phân loại đối tượng\n- Bài toán: cho một ảnh chứa (giả định) duy nhất một đối tượng, mô hình phải dự đoán phân lớp của đối tượng đó (one-label classification). [1][2]  \n- Kiến trúc CNN cơ bản gồm hai thành phần chính: (1) feature learning (convolution → ReLU → pooling) để học đặc trưng của object, và (2) classifier (fully connected + softmax) để phân lớp các đặc trưng đó. [2][3]  \n\n### B. Phân loại *fine-grained* (mịn)\n- Khái niệm: *fine-grained* classification (giảng viên nhắc là “five grand — mịn”) là phân biệt các lớp có khác biệt rất nhỏ nội bộ (ví dụ: các loài hoa, các dòng xe trong cùng hãng, cá thể gương mặt…). [3]  \n- Ví dụ bộ dữ liệu: *passport flower data set* cho hoa; *Stanford Car Data Set* cho phân loại các dòng xe; face recognition với bộ *Webface 260M* cho nhiều tư thế/chủng tộc/trạng thái. [4][4][5][6]  \n- Thách thức: các lớp có đặc điểm tổng thể giống nhau (ví dụ cùng có mắt/mũi/miệng ở gương mặt) nên cần học đặc trưng rất phân biệt (subtle details) như màu da, tỷ lệ, độ dài bộ phận… để phân biệt các cá thể/loại rất gần nhau. [7][8]\n\n### C. Ứng dụng trong y tế và các vấn đề domain\n- Ví dụ: phân loại tổn thương da là lành tính hay ác tính (skin cancer detection) — hình ảnh y tế có domain khác (CT, MRI, ảnh chụp chuyên dụng) so với ảnh thế giới thực (e.g. ImageNet/MNIST), do đó khó tái sử dụng trực tiếp các đặc trưng đã huấn luyện trước; có khi phải huấn luyện lại từ đầu. [8][9]  \n- Yêu cầu đặc thù: trong y tế, dù độ chính xác tổng thể cao (ví dụ 99%) thì bác sĩ vẫn cần **tính giải thích** (explainability) và bằng chứng cụ thể (vị trí, khu vực có dấu hiệu) chứ không chỉ kết quả đầu cuối. Vì vậy các phương pháp như CAM/Grad-CAM (giúp giải thích) hoặc thay đổi thiết kế hàm loss có vai trò quan trọng để tăng tính tin cậy và khả năng thuyết phục người dùng chuyên môn. [13][14][15]\n\n### D. Cải tiến hàm loss cho Face Recognition (vấn đề *fine-grained*)\n- Lý do: face recognition là một trường hợp *fine-grained* nhiều thách thức vì các gương mặt khác nhau có các bộ phận rất giống nhau; cần ép mô hình học đặc trưng phân biệt cao. [10][11]  \n- Các cải tiến nổi bật: ArcFace, SphereFace và các dạng *angular margin loss* — mục tiêu là điều chỉnh hàm loss để tối đa hóa khoảng cách (góc/biên độ) giữa các feature tương ứng với các lớp khác nhau trên không gian đặc trưng (ví dụ minh họa các điểm đặc trưng trên một vòng/hypersphere và tối đa hóa góc giữa các lớp). [11][12]\n\n### E. Bài toán truy vấn ảnh (Image retrieval / image query)\n- Định nghĩa: có một cơ sở dữ liệu ảnh (database) và một ảnh truy vấn (query); nhiệm vụ là tính độ tương đồng giữa ảnh query và tất cả ảnh trong database, trả về danh sách ảnh sắp xếp giảm dần theo mức độ tương đồng. [15][16]  \n- Thách thức thực tế: object trong ảnh có thể chiếm vùng nhỏ, bị che khuất, chịu biến đổi do ánh sáng/góc nhìn, làm giảm độ tương đồng đo bằng đặc trưng toàn cục. [16][17]\n\n### F. Hai hướng tiếp cận chính cho truy vấn ảnh\n- 1) Đặc trưng toàn cục: rút trích feature của toàn ảnh bằng CNN tiền huấn luyện, sau đó so sánh bằng độ tương đồng / khoảng cách; cách này đơn giản nhưng dễ bỏ sót các chi tiết vùng cục bộ quan trọng. [18][19]  \n- 2) Đặc trưng local có attention (DELF): mô tả ý tưởng DELF — rút trích đặc trưng dày đặc (dense features) trên từng ô/vùng cục bộ, kèm một lớp tính *attention score* để chọn những vùng có thông tin (score cao) và loại bỏ vùng ít thông tin (score thấp), giữ lại tập các local features tiêu biểu cho ảnh. [20][21]  \n  - Sau đó index các local features của toàn bộ database, với ảnh query ta rút trích features tương tự và truy vấn trong chỉ mục để tìm các feature gần nhất. [22]  \n  - Bước geometric verification: thực hiện kiểm tra hình học (ví dụ loại bỏ cặp điểm outlier bằng thuật toán “grand chart” theo bài giảng) để giữ lại các cặp inlier thực sự tương đồng; số cặp inlier càng nhiều → mức độ giống nhau càng cao → dùng để xếp hạng kết quả. [22][23]  \n- Kết luận về hướng này: DELF và các cải tiến dựa trên nó đã đem lại kết quả ấn tượng cho bài toán truy vấn ảnh, và nhiều phương pháp hiện đại xây dựng tiếp từ ý tưởng này. [23][24]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n- Bộ dữ liệu minh họa:\n  - *passport flower data set* cho phân loại loài hoa (fine-grained). [4]  \n  - *Stanford Car Data Set* cho phân loại các dòng/kiểu xe. [4]  \n  - *Webface 260M* (Webface 260 triệu) cho face recognition với đa dạng tư thế, chủng tộc, trạng thái (đeo khẩu trang, ảnh cổ, trắng đen, cảm xúc…). [6][7]  \n- Ứng dụng thực tế:\n  - Nhận diện gương mặt (face recognition) ở cấp độ cá nhân hóa — cần hàm loss đặc biệt (ArcFace, SphereFace, angular margin) để tăng tính phân biệt. [10][11][12]  \n  - Ứng dụng y tế: phát hiện/ phân loại ung thư da (benign vs malignant) — yêu cầu giải thích (localization) và độ tin cậy cao trước khi được chuyên gia chấp nhận. [8][13][14]  \n  - Truy vấn ảnh lớn (large-scale image retrieval) — ví dụ: tìm các ảnh tương tự trong database lớn bằng cách dùng feature extraction (global/local), index và geometric verification (DELF + inlier counting). [15][18][20][22]\n\n## 4. Kết luận (Conclusion)\n- Tóm tắt các ý chính:\n  - Kiến trúc CNN cơ bản (convolution/ReLU/pooling + fully connected/softmax) là nền tảng cho phân loại đối tượng. [2][3]  \n  - *Fine-grained* classification (hoa, xe, gương mặt) đòi hỏi học các đặc trưng rất phân biệt; điều này dẫn đến các cải tiến ở cấp hàm loss (ArcFace, SphereFace, angular margin) và/hoặc kỹ thuật giải thích (CAM/Grad-CAM). [3][10][11][12][15]  \n  - Ứng dụng y tế đặt ra yêu cầu đặc biệt về domain và explainability — chỉ tối ưu accuracy không đủ; cần cung cấp bằng chứng/lưu đồ vùng liên quan. [8][9][13][14]  \n  - Trong truy vấn ảnh, việc dùng **đặc trưng local có attention** (DELF) kết hợp indexing và geometric verification là hướng hiệu quả để xử lý các trường hợp object nhỏ, che khuất, hoặc biến đổi mạnh. Nhiều phương pháp hiện đại đã phát triển tiếp từ DELF. [18][19][20][21][22][23][24]\n- Tầm quan trọng: những kỹ thuật và cải tiến nêu trong bài giúp CNN ứng dụng hiệu quả hơn ở các bài toán thực tế có độ khó cao (fine-grained, domain khác biệt, truy vấn quy mô lớn), đồng thời nhấn mạnh vai trò của explainability trong các lĩnh vực nhạy cảm như y tế. [7][9][13][14][23]  \n- Liên hệ với các bài giảng khác / hướng tiếp theo: phần trước đã bàn về các dạng phân loại ở các cấp độ khác nhau; phần tiếp theo (sẽ trình bày trong bài tiếp theo) sẽ đi sâu vào các mô hình và kỹ thuật cụ thể cho bài toán truy vấn ảnh. [14][15]\n\n--- \n\nGhi chú: các trích dẫn [1]…[24] tương ứng với các đoạn thời gian trong video (các chunk được cung cấp). Bạn có thể click vào mỗi citation để nhảy tới đoạn tương ứng trong video.",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 14,
          "end_time": 59,
          "text": "Trong bài hôm nay thì chúng ta sẽ tiến hành khảo sát số ứng dụng của mạng CNN trong lĩnh vực về sử dụng dạng dưới đây là 4 ứng dụng phổ biến trong lĩnh vực sử dụng dạng có sử dụng Deep Learning nói chung và CNN nói riêng Đầu tiên đó chính là phân loại đối tượng thì đây có vẻ như là một trong những bài toán khởi nguồn cho mạng CNN Deep Learning đầu vào của chúng ta sẽ có ảnh của một cái object và đầu ra mình sẽ cần phải phân loại xem cái object trong cái ảnh đầu vào này nó thuộc cái phân lớp là gì thì ở đây với một cái giả định rằng là trong cái tấm hình này nó chỉ chứa duy nhất"
        },
        {
          "index": 2,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 47,
          "end_time": 109,
          "text": "và đầu ra mình sẽ cần phải phân loại xem cái object trong cái ảnh đầu vào này nó thuộc cái phân lớp là gì thì ở đây với một cái giả định rằng là trong cái tấm hình này nó chỉ chứa duy nhất một đối tượng thì như vậy thì output của mình nó chỉ có duy nhất một cái đáp án thôi thì đó là cho bài toán phân loại đối tượng và cái kiến trúc của mình thì nó sẽ bao gồm 2 cái thành phần thành phần đầu tiên đó là học object đặc trưng và thành phần thứ hai đó là mình sẽ phân lớp các cái đặc trưng đó vào các cái class thì đối với cái thành phần mà feature learning object đặc trưng thì chúng ta sẽ có các cái module chính chính là cái module về convolution relu và pooling còn đối với cái module về phân loại thì chúng ta sẽ có các cái phép biến đổi là fully connected và softmax"
        },
        {
          "index": 3,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 99,
          "end_time": 161,
          "text": "relu và pooling còn đối với cái module về phân loại thì chúng ta sẽ có các cái phép biến đổi là fully connected và softmax thì đây chính là cái kiến trúc căn bản của mạng cnn và ngoài cái bài toán mà phân loại đối tượng mà chúng ta được tìm hiểu đó là object classification thì chúng ta sẽ có một cái chủ đề cũng là phân loại đối tượng nhưng mà nó ở cấp độ gọi là five grand five grand có nghĩa là mịn có nghĩa là mịn nghĩa là sao ví dụ trước đây thì chúng ta chỉ phân biệt là hoa rồi cây cối rồi chó mèo v..v nhưng mà bây giờ trong các cái loại hoa thì nó có rất nhiều những cái loại hoa mà thuộc các cái chi các cái nhánh trong giới sinh vật hoa lài, hoa hỏe, hoa hồng rồi thậm chí là trong hoa hồng"
        },
        {
          "index": 4,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 148,
          "end_time": 212,
          "text": "nhưng mà bây giờ trong các cái loại hoa thì nó có rất nhiều những cái loại hoa mà thuộc các cái chi các cái nhánh trong giới sinh vật hoa lài, hoa hỏe, hoa hồng rồi thậm chí là trong hoa hồng nó cũng có rất nhiều cái giống hoa hồng hoa cúc nó cũng có rất nhiều cái giống hoa cúc thì ở đây chính là five grand object classification và trong cái hình ở đây thì chúng ta có rất nhiều cái dấu hoa hồng  chúng ta có nó thuộc cái bộ data set đó là passport flower data set tương tự như vậy cho cái bài toán phân loại xe thì có rất nhiều cái loại xe khác nhau và thậm chí là trong cùng một cái hãng xe thì chúng ta sẽ có rất nhiều cái dòng xe hạng A, hạng B, hạng C, v.v rồi ứng với từng cái hạng thì nó cũng sẽ có các cái đời xe thế thì ở đây chúng ta sẽ có một cái bộ data set đó là Stanford Car Data Set để thu thập và phân loại các cái loại xe các cái dòng xe từ xưa đến nay rồi ngoài ra thì nó cũng có một cái bài toán"
        },
        {
          "index": 5,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 198,
          "end_time": 261,
          "text": "đó là Stanford Car Data Set để thu thập và phân loại các cái loại xe các cái dòng xe từ xưa đến nay rồi ngoài ra thì nó cũng có một cái bài toán dạng phân loại đối tượng và dạng five grand tức là mịn đó chính là face recognition trước đây thì chúng ta chỉ cần detect cái face tức là chúng ta sẽ đi so sánh cái gương mặt với lại các cái bộ phận khác trong cơ thể ví dụ như là tay ví dụ như là chân hoặc là với những cái đối tượng khác ví dụ như là xe ví dụ như là cây thì trong cái nội bộ cái face tức là cái gương mặt này nè thì chúng ta sẽ có rất nhiều những cái định danh rất nhiều những cái định danh rồi và mục tiêu của mình đó chính là làm sao để mà phân biệt được cái định danh số 1 với cái định danh số 2 thì đó chính là face recognition thì ở đây chúng ta sẽ có một cái bộ data set"
        },
        {
          "index": 6,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 250,
          "end_time": 311,
          "text": "rồi và mục tiêu của mình đó chính là làm sao để mà phân biệt được cái định danh số 1 với cái định danh số 2 thì đó chính là face recognition thì ở đây chúng ta sẽ có một cái bộ data set đó là webface 260 me data set và cái bộ data set này thì có chứa rất nhiều những cái ảnh gương mặt ở rất nhiều những cái tư thế ví dụ như là chúng ta nhìn trực diện nè nhìn về bên tay phải nè nhìn xuống dưới nè nhìn về bên tay trái nè rồi có rất nhiều những cái chủng tộc ví dụ có người da trắng người da màu rồi có người châu Á người châu Âu người châu Phi v.v và có rất nhiều cái trạng thái ví dụ ở đây chúng ta sẽ có trạng thái đó là đeo khẩu trang rồi sẽ có cái tình huống đó là cái ảnh này là được chụp từ thời xưa ảnh trắng đen rồi trạng thái ở đây thì cũng bao gồm là trạng thái về cảm xúc"
        },
        {
          "index": 7,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 298,
          "end_time": 360,
          "text": "ví dụ ở đây chúng ta sẽ có trạng thái đó là đeo khẩu trang rồi sẽ có cái tình huống đó là cái ảnh này là được chụp từ thời xưa ảnh trắng đen rồi trạng thái ở đây thì cũng bao gồm là trạng thái về cảm xúc ví dụ như đang là tươi cười hoặc là đang giận dữ ví dụ vậy và từ đó thì chúng ta sẽ thấy là cái tập Webfile 260 triệu dataset này là thể hiện được cái tính khó của cái bài toán phân loại đối tượng nó rất là khác so với lại cái bài toán phân loại đối tượng bình thường mà mình đã được tiềm hiện trước đây ở đây là các cái đối tượng ở đây nó sẽ có cùng những cái đặc điểm tổng thể đó là đều có mắt nè có mũi nè có miệng nè có tóc nè nhưng mà làm sao có thể phân biệt được những cái chi tiết nhỏ để có thể giúp phân biệt được người này với người kia ví dụ để phân biệt được người này với người kia thì người ta hay so sánh về màu da so sánh về tỷ lệ phân bố và độ dài"
        },
        {
          "index": 8,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 349,
          "end_time": 412,
          "text": "nhỏ để có thể giúp phân biệt được người này với người kia ví dụ để phân biệt được người này với người kia thì người ta hay so sánh về màu da so sánh về tỷ lệ phân bố và độ dài của các cái bộ phận trên cái gương mặt của mình thì tất cả những cái yếu tố đó nó tạo nên cái độ khó của cái bài toán này và ứng dụng trong cái lĩnh vực về y tế thì chúng ta sẽ có cái bài toán đó là phân loại ung thư da ví dụ như ở đây là chúng ta sẽ có một cái vết đúng không thì nếu mà nhìn bề ngoài mà người bình thường thì có thể chúng ta sẽ xem đây là có thể hiểu đây là cái nốt ruồi nhưng thực tế thì nó có thể là một cái dấu hiệu của cái bệnh ung thư thì ở đây chúng ta sẽ phải phân biệt xem làm hai loại đó là cái dấu hiệu của mình đó là lành tính hay là ác tính thì đây là cái bài toán ứng dụng trong lĩnh vực về y tế và trong ảnh y tế thì một số cái loại ảnh nó có cái tính phức tạp cao hơn và có cái domain"
        },
        {
          "index": 9,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 400,
          "end_time": 460,
          "text": "lành tính hay là ác tính thì đây là cái bài toán ứng dụng trong lĩnh vực về y tế và trong ảnh y tế thì một số cái loại ảnh nó có cái tính phức tạp cao hơn và có cái domain không giống với lại cái domain của lĩnh vực mà mình hay thú luyện trên tập dữ liệu MNS ví dụ như là chụp trên ảnh CT Scan hoặc là chụp trên ảnh MRI thì đây tất cả những cái này đều là những cái định dạng ảnh và nó không phổ biến trong thế giới thực dẫn đến đó là cái khi mà chúng ta huấn luyện các cái mạng CNN trên các cái domain này thì có khi chúng ta sẽ phải huấn luyện lại từ đầu chúng ta cũng không có thể tái sử dụng được nhiều những cái đặc trưng trong ảnh ảnh màu, ảnh thế giới thực của mình và một số cái kỹ thuật mà ứng dụng của cái mạng CNN cho cái bài toán phân lớp ở những cái bài toán như vừa đề cập thì nó có rất nhiều những cái kỹ thuật khác nhau"
        },
        {
          "index": 10,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 449,
          "end_time": 511,
          "text": "và một số cái kỹ thuật mà ứng dụng của cái mạng CNN cho cái bài toán phân lớp ở những cái bài toán như vừa đề cập thì nó có rất nhiều những cái kỹ thuật khác nhau ví dụ đối với cái bài toán nhận diện ngư mặt thì ở đây người ta sẽ tập trung vào cái việc là cải tiến các cái hàm loss như chúng ta đã biết là trong một cái mô hình máy học thì nó sẽ có cái hàm là hàm mô hình rồi và chúng ta sẽ có cái hàm loss thì đối với cái hàm mô hình thì họ vẫn sử dụng các cái kiến trúc mạng hoặc là các cái thành phần như là conclusion pooling hoặc là activation nhưng mà khi tính toán các cái độ sai lệch giữa cái mẫu dữ liệu của mình với lại những cái mẫu dữ liệu của cái ngư mặt của mình với lại những cái ngư mặt khác thì chúng ta sẽ phải sử dụng cái hàm loss đặc biệt tại vì ngư mặt là một cái loại đối tượng đặc biệt nó có những cái bộ phận rất là giống nhau"
        },
        {
          "index": 11,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 500,
          "end_time": 558,
          "text": "của cái ngư mặt của mình với lại những cái ngư mặt khác thì chúng ta sẽ phải sử dụng cái hàm loss đặc biệt tại vì ngư mặt là một cái loại đối tượng đặc biệt nó có những cái bộ phận rất là giống nhau nhưng mà đồng thời nó cũng khác nhau ở các cái yếu tố rất là nhỏ và cái giải pháp của các cái hướng tiếp cận mà tiên tiến nhất hiện nay cho bài toán nhận diện ngư mặt nó đều là đến từ các cái cải tiến cho hàm loss ví dụ như chúng ta có các cái mô hình như là arc phase sphere phase và các cái độ đo là angular margin loss và ý tưởng của các cái hàm loss này đó là nó sẽ ép để cho cái mô hình nó cố gắng học được những cái đặc trưng phân biệt cao những cái đặc trưng phân biệt cao để tách biệt giữa những cái gương mặt tương tự nhau ví dụ như chúng ta thấy ở trên cái hình tròn này thì mỗi một cái chấm ở đây nó đại diện cho cái đặc trưng của một phay của một gương mặt"
        },
        {
          "index": 12,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 549,
          "end_time": 608,
          "text": "ví dụ như chúng ta thấy ở trên cái hình tròn này thì mỗi một cái chấm ở đây nó đại diện cho cái đặc trưng của một phay của một gương mặt mình sẽ viết bằng tiếng anh để cho nó gọn ha rồi thì mỗi cái này là một cái gương mặt và những cái gương mặt nào mà gần giống nhau thực tế chúng ta thấy là có những cái gương mặt hai người khác nhau nhưng mà có cái nét gương mặt đó nó ná ná giống nhau thế thì những cái phay đó nó sẽ đặt nằm ở gần nhau trên cái cung hình tròn này và nhiệm vụ của các cái hàm loss này là cố gắng tách các cái phay tương tự nhau tách ra xa cái phay tương tự nhau tách ra xa ví dụ chúng ta thấy ở đây trên cái cung góc này thì nó sẽ maximize tức là cực đại hóa cái góc này để tách các cái điểm ở trên cái hình tròn này đối với những cái gương mặt khác nhau thì nó sẽ tách càng càng xa nhau ra và đó chính là cái ý tưởng của cái việc cải tiến hàm loss"
        },
        {
          "index": 13,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 599,
          "end_time": 660,
          "text": "ở trên cái hình tròn này đối với những cái gương mặt khác nhau thì nó sẽ tách càng càng xa nhau ra và đó chính là cái ý tưởng của cái việc cải tiến hàm loss mà khác trong một số cái lĩnh vực ví dụ như trong lĩnh vực y khoa thì cái kết quả của mình khi mà chúng ta phân loại mà ra được cái độ chính xác là ví dụ 99% thì đôi khi các bác sĩ họ sẽ không có sử dụng cái kết quả của mình kể cả khi mình chứng minh với họ là cái phương pháp cái mô hình của mình có thể nhận diện được chính xác các cái bệnh đến 99% mà họ chỉ có thể sử dụng cái hệ thống của mình khi mình chỉ là ra được những cái khu vực mà có khả năng là xác định được đó là bệnh, tức là bên cạnh việc đưa ra cái kết quả đầu cuối là có bệnh hay không bệnh, thì ở đây mình phải thuyết phục là cái người sử dụng rằng à tôi đưa ra cái nhận diện được cái người này bị bệnh, khi nó có những cái dấu hiệu rõ ràng"
        },
        {
          "index": 14,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 648,
          "end_time": 710,
          "text": "là có bệnh hay không bệnh, thì ở đây mình phải thuyết phục là cái người sử dụng rằng à tôi đưa ra cái nhận diện được cái người này bị bệnh, khi nó có những cái dấu hiệu rõ ràng và những cái dấu hiệu đó thì nó thể hiện ở những cái khu vực nhất định tức là chúng ta sẽ có cái tính là giải thích được, cái tính giải thích được trong AI hiện nay rất là quan trọng, tại vì nó không chỉ chứng minh được rằng là AI có khả năng gọi là tiếp cận được đến những cái tri thức của con người nhưng mà đồng thời nó có khả năng suy luận logic và đưa ra được những cái cao cứ, cách khách quan để giúp cho chúng ta đưa ra được cái kết quả phán xét cuối cùng như vậy thì trong cái phần trước chúng ta đã nghiên cứu về các cái bài toán phân loại đối tượng ở các cái cấp độ khác nhau đó là bài toán về file range đó là phân loại đối tượng ở mức độ mịn ví dụ như bài toán về phân loại các cái loài hoa, phân loại biệt các cái loài xe, các cái"
        },
        {
          "index": 15,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 699,
          "end_time": 760,
          "text": "ở các cái cấp độ khác nhau đó là bài toán về file range đó là phân loại đối tượng ở mức độ mịn ví dụ như bài toán về phân loại các cái loài hoa, phân loại biệt các cái loài xe, các cái chủng loại xe, rồi gương mặt v.v và có những cái hướng tiếp cận như là thay đổi các cái giá trị hàm loss, hoặc là hướng tiếp cận RATCAM, hoặc là CAM trong cái việc là giải thích được các cái mô hình của mình, trong phần tiếp theo thì chúng ta sẽ cùng tìm hiểu về các cái mô hình của mình và tìm hiểu về cái ứng dụng của mạng Deep Neural Network trong cái việc đó là tri vấn hình ảnh thì ý tưởng bài toán tri vấn hình ảnh đó là cho trước một cái cơ sở dữ liệu ảnh, chúng ta sẽ có một cái ảnh tri vấn hay còn gọi là query, và nhiệm vụ của mình đó là sẽ xác định cái độ tương đồng giữa cái ảnh tri vấn với tất cả những cái ảnh trong dataset và chúng ta sẽ trả về danh sách danh sách đã được sắp xếp giảm dần theo cái mức độ tương đồng giảm dần"
        },
        {
          "index": 16,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 749,
          "end_time": 810,
          "text": "ảnh tri vấn với tất cả những cái ảnh trong dataset và chúng ta sẽ trả về danh sách danh sách đã được sắp xếp giảm dần theo cái mức độ tương đồng giảm dần thì ở đây chúng ta sẽ có một cái ví dụ ở đây chúng ta sẽ có một cái ảnh query query số 1, đây là ảnh query thứ 2 và vâng đây là ảnh tri vấn, và ứng với một ảnh tri vấn thì chúng ta sẽ lục tìm trong cái database để trả về mỗi một cái hàng này sẽ là một cái kết quả trả về, trong đó cái nhóm đầu tiên được tô bởi màu xanh đậm, chính là những cái ảnh có cái mức độ tương đồng rất là cao đối với cái ảnh tri vấn chúng ta sẽ xét cái kết quả theo từng hàng nha, và ở cột ở giữa thì chúng ta sẽ thấy là đây là những cái kết quả tri vấn có cái độ tương đồng cao, bên đây là rất cao còn đây là cao, thì chúng ta có thể thấy là cái object trong cái ảnh tri vấn ở đây, thì nó xuất hiện đâu đó ở một cái khu vực tương đối là"
        },
        {
          "index": 17,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 799,
          "end_time": 860,
          "text": "cao, bên đây là rất cao còn đây là cao, thì chúng ta có thể thấy là cái object trong cái ảnh tri vấn ở đây, thì nó xuất hiện đâu đó ở một cái khu vực tương đối là khu vực tương đối là nhỏ trong cái phạm vi của tấm hình và nó sẽ bị ảnh hưởng bởi những cái yếu tố như là bị che khuất rồi ví dụ như là chúng ta sẽ bị ảnh hưởng bởi yếu tố về ánh sáng và góc nhìn đó thì nó sẽ làm cho cái kết quả so sánh tương đồng nó sẽ bị giảm xuống đáng kể và đối với cái ảnh màu vàng thì đây là những cái ảnh có độ tương đồng rất là thấp, tại vì cái khu vực có cái sự xuất hiện của cái object của mình nó rất là nhỏ và nó đã bị biến đổi rất là nhiều thì đây là cái bài toán tri vấn ảnh và ý tưởng chính để mà giải quyết cái bài toán tri vấn ảnh này đó chính là chúng ta sẽ sử dụng các cái mạng CNN đã được huấn luyện sẵn"
        },
        {
          "index": 18,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 851,
          "end_time": 911,
          "text": "và ý tưởng chính để mà giải quyết cái bài toán tri vấn ảnh này đó chính là chúng ta sẽ sử dụng các cái mạng CNN đã được huấn luyện sẵn như là những cái bộ rút trích đặc trưng thì cái ý tưởng đơn giản nhất đó chính là chúng ta sẽ fit tất cả những cái ảnh trong cái database vào bên trong cái mạng CNN và mỗi một cái ảnh này nó sẽ tương ứng là một cái điểm một cái feature có cái ảnh query thì chúng ta sẽ đưa vào cái mạng CNN một lần nữa và chúng ta sẽ ra được cái feature một cam này và chúng ta sẽ dùng các cái giải thuật về độ tương đồng hoặc là độ đo khoảng cách để xác định tốt những cái ảnh có cái sự tương đồng hoặc là có cái khoảng cách đến cái đặc trưng tri vấn này là tốt nhất và tương ứng từng cái đặc trưng này nó chính là cái kết quả trả về là những cái tấm ảnh chúng ta trả về thì đây là cái ý tưởng cho cái bài toán tri vấn ảnh tuy nhiên cái cách tiếp cận này"
        },
        {
          "index": 19,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 899,
          "end_time": 959,
          "text": "và tương ứng từng cái đặc trưng này nó chính là cái kết quả trả về là những cái tấm ảnh chúng ta trả về thì đây là cái ý tưởng cho cái bài toán tri vấn ảnh tuy nhiên cái cách tiếp cận này thì nó sẽ xem toàn bộ cái tấm hình này như là một cái đặc trưng và nó sẽ bị một số vấn đề đó là nó sẽ bỏ sót những cái đặc trưng chi tiết ở bên trong đôi khi chúng ta nhìn một cái tấm hình thì chúng ta sẽ xác định xem cái hình đó có tương đồng có giống với lại cái ảnh tri vấn hay không dựa vào những cái bộ phận bên trong ví dụ như chúng ta nhìn tấm hình này thì đôi khi chúng ta sẽ chỉ cần một cái bộ phận này thôi để đi so sánh chứ không có sử dụng những cái thông tin ở cái vùng màu đen những cái vùng mà chiếm rất là nhiều tỉ trọng rất là nhiều diện tích như vậy thì đòi hỏi là đặc trưng CNN, đặc trưng Deep Learning của mình sẽ phải đi sâu vô đến mức độ chi tiết từng cái khu vực nhỏ và đó chính là cái ý tưởng của cái hướng tiếp cận là đặc trưng DELF"
        },
        {
          "index": 20,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 949,
          "end_time": 1010,
          "text": "sẽ phải đi sâu vô đến mức độ chi tiết từng cái khu vực nhỏ và đó chính là cái ý tưởng của cái hướng tiếp cận là đặc trưng DELF và ý tưởng của cái mạng CNN cho cái việc rút trích đặc trưng này đó chính là chúng ta sẽ fit cái tấm ảnh này vào và chúng ta sẽ rút trích ra đặc trưng dày đặc với mỗi một cái cột ở đây nó sẽ tương ứng là một cái đặc trưng trên một cái vùng cục bộ ở bên trong cái tấm hình tuy nhiên thì trong cái tấm hình của mình nó sẽ có những cái vùng không có chứa nhiều thông tin ví dụ như trong cái tấm hình vừa rồi chúng ta thấy là những cái vùng mà đồng màu cái khu vực này nè thì nó sẽ có ít thông tin để giúp cho mình phân biệt do đó thì cái khu vực này sẽ có ít thông tin để giúp cho mình phân biệt nó sẽ có cái attention thấp hoặc là có cái trọng số thấp nó sẽ có cái score thấp nó sẽ có cái score thấp và những cái khu vực nào mà có cái sự thay đổi về mặt hình ảnh nhiều"
        },
        {
          "index": 21,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 1000,
          "end_time": 1061,
          "text": "nó sẽ có cái score thấp nó sẽ có cái score thấp và những cái khu vực nào mà có cái sự thay đổi về mặt hình ảnh nhiều thì chỗ đó nó sẽ có trọng số cao thì chỗ đó nó sẽ có trọng số cao thế thì để huấn luyện và xác định được cái khu vực nào có trọng số thấp, khu vực nào có trọng số cao thì chúng ta sẽ có một cái lớp để tính cái attention score và dựa trên cái attention score này nó sẽ bỏ đi những cái feature nào mà không có cái bỏ đi những cái feature nào mà không có cái vai trò quan trọng và chỉ chừa lại những cái feature tốt, những cái feature nào đặc trưng cho tấm ảnh và gom lại thành Dell Feature và chúng ta đại diện một cái tấm ảnh này bằng danh sách những cái feature có cái vai trò quan trọng trên những cái khu vực mà có cái sự biến động, những cái khu vực mà có những cái điểm đặc trưng và dựa trên cái đặc trưng mà đã rút trích được từ tấm ảnh này thì chúng ta sẽ làm tương tự như vậy cho toàn bộ tấm ảnh"
        },
        {
          "index": 22,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 1048,
          "end_time": 1110,
          "text": "có cái sự biến động, những cái khu vực mà có những cái điểm đặc trưng và dựa trên cái đặc trưng mà đã rút trích được từ tấm ảnh này thì chúng ta sẽ làm tương tự như vậy cho toàn bộ tấm ảnh rồi sau đó chúng ta sẽ đánh chỉ mục vào trong một cái cơ sở dữ liệu, chỉ mục rồi đối với ảnh query thì chúng ta cũng sẽ có các cái feature sau khi đã thực hiện với attention score chúng ta sẽ có những cái đặc trưng đại diện cho một cái tấm ảnh và chúng ta sẽ đi so sánh đi tra cứu trong cái large scale index này để từ đó xác định ra những cái feature nào mà gần với lại những cái feature này nhất rồi đồng thời chúng ta sẽ thực hiện cái thao tác là geometric verification tức là chúng ta sẽ tinh chỉnh lại cái yếu tố về mặt hình học để cho cái tấm hình của mình xác định xem là vị trí thực sự của cái object nó nằm ở đâu ví dụ trong cái hình này chúng ta thấy là qua cái phép geometric verification thì chúng ta xác định được cái object thực sự của mình là nằm trong cái khung màu vàng"
        },
        {
          "index": 23,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 1099,
          "end_time": 1159,
          "text": "xác định xem là vị trí thực sự của cái object nó nằm ở đâu ví dụ trong cái hình này chúng ta thấy là qua cái phép geometric verification thì chúng ta xác định được cái object thực sự của mình là nằm trong cái khung màu vàng và chúng ta sẽ thực hiện một số cái thuật toán kinh điển ví dụ như thuật toán grand chart để loại bỏ những cái cặp điểm out layer những cặp điểm mà không thực sự tương đồng và chừa lại những cái cặp điểm in layer và đếm cái số điểm đó rồi chúng ta sẽ số cặp điểm tương đồng nào mà càng cao thì cái mức độ giống nhau giữa ảnh tri vấn và cái ảnh đó sẽ là càng tốt và chúng ta sẽ sắp kích hoạt nó càng cao đó thì cái ý tưởng chính của thuật toán NELF là như vậy và các cái mô hình tri vấn mà có sử dụng deep learning thì cũng sẽ dựa trên cái ý tưởng này để phát triển tiếp và hiện nay thì cũng có rất nhiều những cái phương pháp đã cải tiến từ cái phương pháp của Dell và cho những cái kết quả rất là ấn tượng trong cái bài toán tri vấn hình ảnh"
        },
        {
          "index": 24,
          "video_id": "Chương 5_RVFApjx4KKI",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 1-2： Ứng dụng trong bài toán phân loại và bài toán truy vấn ảnh",
          "video_url": "https://youtu.be/RVFApjx4KKI",
          "start_time": 1149,
          "end_time": 1159,
          "text": "để phát triển tiếp và hiện nay thì cũng có rất nhiều những cái phương pháp đã cải tiến từ cái phương pháp của Dell và cho những cái kết quả rất là ấn tượng trong cái bài toán tri vấn hình ảnh"
        }
      ]
    },
    {
      "video_id": "Chương 5_Til9AdPO7JE",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Trình bày cách **ứng dụng mạng CNN cho bài toán phát hiện đối tượng (object detection)** — từ ý tưởng dùng Feature Map của CNN để xác định vị trí (bounding box) và lớp (class) của các đối tượng trong ảnh, đến hai hướng tiếp cận chính (two-stage như Faster R-CNN và one-stage như YOLO/SSD) cùng so sánh hiệu năng (tốc độ vs độ chính xác). [1][8][12]\n\n- Các khái niệm sẽ được đề cập:\n  - Tính bất biến của Feature Map (dịch chuyển không gian, tỉ lệ) và cách tận dụng để localize đối tượng. [2][3][4]\n  - Ý tưởng nội suy từ Feature Map về bounding box trên ảnh gốc. [4]\n  - Phân biệt bài toán detection so với classification; xử lý nhiều đối tượng, nhiều lớp, hoặc không có đối tượng. [6][7]\n  - Kiến trúc hai giai đoạn (Region Proposal Network + detector) — ví dụ Faster R-CNN. [8][9][10][11]\n  - Kiến trúc một giai đoạn (end-to-end) — ví dụ YOLO, SSD; cấu trúc output dưới dạng tensor lưới. [12][13][14][15]\n  - So sánh tốc độ (fps) và độ chính xác (ví dụ mAP) giữa các hướng tiếp cận. [17][19]\n\n(Các nội dung trên dựa trực tiếp vào các phần trình bày trong video.) [1][2][8][12]\n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### A. Động cơ và ứng dụng thực tế\n- Ứng dụng điển hình: xe tự hành cần phát hiện xe, phương tiện, người bộ hành, chướng ngại vật từ các camera để đưa ra quyết định điều khiển. [1]\n\n### B. Feature Map của CNN: bất biến không gian và tỉ lệ\n- Quan sát từ Deep Visualization Toolbox: một số filter/feature map sáng lên khi phát hiện concept (ví dụ *gương mặt*) và vị trí \"đèn sáng\" này dịch chuyển theo đối tượng khi đối tượng di chuyển trong khung hình (bất biến theo dịch chuyển không gian). [2][3]\n- Tương tự, nếu đối tượng xuất hiện với kích thước nhỏ hơn (ví dụ người mặc áo đen ở xa bằng khoảng nửa kích thước mặt so với người gần), thì activation tương ứng cũng có cường độ và kích thước phù hợp — thể hiện **bất biến theo tỉ lệ** ở mức độ nào đó. [3][4]\n\n### C. Từ Feature Map sang Bounding Box trên ảnh gốc\n- Ý tưởng sơ khởi: dùng các vùng kích hoạt (hot-spot) trên Feature Map làm đại diện cho vị trí đối tượng, ước lượng bounding box trên Feature Map rồi nội suy về toạ độ trên ảnh gốc (biết kích thước width/height của ảnh gốc và Feature Map). [4]\n- Để mạng học trực tiếp nhiệm vụ dự đoán bounding box từ ảnh đến ảnh (end-to-end), cần điều chỉnh/thiết kế lại kiến trúc CNN để thêm khả năng dự đoán vị trí và kích thước hộp — vì object thường chỉ chiếm một vùng nhỏ trong ảnh. [4][5]\n\n### D. Định nghĩa bài toán phát hiện đối tượng\n- Khác với classification (gán nhãn cho toàn ảnh), detection yêu cầu xác định:\n  - Có những đối tượng nào (class labels),\n  - Ở vị trí nào (bounding boxes),\n  - Có thể có nhiều đối tượng khác nhau, nhiều đối tượng cùng lớp, hoặc không có đối tượng nào. [6][7]\n\n### E. Hướng tiếp cận hai giai đoạn (Two-stage): Faster R-CNN\n- Ý tưởng chính: tách bài toán thành hai bước:\n  1. Region Proposal Network (RPN) — xác định các vùng có khả năng chứa đối tượng (các đề xuất vùng). RPN khai thác deep feature map và tìm các vị trí có phản hồi mạnh. [8][9][10]\n  2. Detector — với mỗi proposal, trích xuất phần tương ứng của feature map (feature sharing giữa RPN và detector), rồi tinh chỉnh bounding box và phân loại class. [10][11]\n- Ưu/nhược:\n  - Ưu: độ chính xác cao (ví dụ Faster R-CNN có độ chính xác cao hơn YOLO trong ví dụ), phù hợp khi cần chất lượng detection. [17][18]\n  - Nhược: chậm do quy trình hai bước (RPN → detector), khó thực thi real-time. [11][17]\n\n### F. Hướng tiếp cận một giai đoạn (One-stage): YOLO / SSD\n- Ý tưởng chính: làm *end-to-end* — trực tiếp từ ảnh đầu vào đầu ra một tensor mã hoá vị trí + nhãn, loại bỏ bước region proposal tách rời. [12]\n- Cách biểu diễn output (theo ví dụ trong video):\n  - Chia ảnh thành lưới S x S (ví dụ S = 7). Mỗi cell chứa một vector chiều sâu D (ví dụ D ≈ 30 trong ví dụ) để encode thông tin về class, toạ độ (x,y) và kích thước (w,h) cho các object có thể chồng lấp. Tổng thể output là một tensor S x S x D. [13][14][15]\n  - Mỗi cell có thể mã hoá nhiều thông tin để xử lý hiện tượng occlusion (object chồng lên object). [14]\n- Các phiên bản YOLO liên tục cải tiến (đến YOLO v10 vào thời điểm 2024), nhưng ý tưởng cơ bản là vẫn fit ảnh → tensor chứa bounding boxes và class. [12][16]\n- Ưu/nhược:\n  - Ưu: rất nhanh, phù hợp real-time (ví dụ YOLO v3 ≈ 45 fps). [17]\n  - Nhược: thường đánh đổi một phần độ chính xác so với hai giai đoạn (ví dụ chênh lệch ~10% so với Faster R-CNN trong ví dụ). [17][18]\n\n### G. So sánh tốc độ và độ chính xác; các lựa chọn thực tế\n- Ví dụ đo lường từ video:\n  - YOLO v3: khoảng 45 fps (thời gian thực), độ chính xác (mAP50) trong ví dụ ~51%. [17][19]\n  - Faster R-CNN: ~7 fps, độ chính xác cao hơn YOLO ~10% (con số tương đối trong video). [17][18]\n- Kết luận thực tế: chọn mô hình tuỳ theo yêu cầu:\n  - Nếu cần độ chính xác cao hơn, không bắt buộc real-time → chọn two-stage (Faster R-CNN hoặc biến thể). [18]\n  - Nếu cần tốc độ real-time và chấp nhận đánh đổi độ chính xác → chọn one-stage (YOLO, SSD) và các phiên bản cải tiến. [18][16]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa từ video:\n  - Visualisation: activation map “sáng” tương ứng *gương mặt* khi người di chuyển, activation cũng dịch chuyển theo; khi có người khác ở xa hơn, activation nhỏ hơn tương ứng (thể hiện tính bất biến tỉ lệ/không gian). [2][3]\n  - Mô tả pipeline Faster R-CNN: RPN tìm các vùng có khả năng (region proposals) trên feature map → trích feature tương ứng → detector tinh chỉnh bounding box và phân lớp. [8][9][10][11]\n  - Mô tả YOLO: chia lưới (ví dụ 7x7), mỗi cell output một vector (khoảng 30 chiều trong ví dụ) chứa thông tin class + x,y,w,h (và đủ để biểu diễn hiện tượng chồng lấp). [13][14][15]\n\n- Ứng dụng thực tế:\n  - Xe tự hành: phát hiện xe, người, chướng ngại vật từ camera nhiều hướng để ra quyết định điều khiển. [1]\n  - Các hệ thống cần real-time (giám sát video, robotics) thường ưu tiên YOLO/SSD; các hệ cần độ chính xác cao (phân tích ảnh y tế, hệ thống phân tích hình ảnh tĩnh chất lượng cao) có thể chọn Faster R-CNN hoặc biến thể. [1][17][18]\n\n- Trường hợp sử dụng / Lựa chọn:\n  - Nhu cầu real-time + chấp nhận giảm một chút độ chính xác → one-stage (YOLO/SSD). [17][18]\n  - Ưu tiên độ chính xác trên tốc độ → two-stage (Faster R-CNN / FPN variants). [18][19]\n\n---\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Feature Map của CNN có tính bất biến không gian và tỉ lệ, có thể dùng làm cơ sở để localize đối tượng và ước lượng bounding box. [2][3][4]\n  - Bài toán detection yêu cầu không chỉ phân lớp mà còn xác định vị trí (bounding boxes) của một hoặc nhiều đối tượng trong ảnh. [6][7]\n  - Hai hướng tiếp cận chính:\n    - Two-stage (Faster R-CNN): chính xác cao nhưng chậm do tách RPN và detector. [8][9][11]\n    - One-stage (YOLO/SSD): end-to-end, rất nhanh nhưng thường đánh đổi một phần độ chính xác; tiếp tục được cải tiến qua nhiều phiên bản (đến YOLO v10 vào 2024). [12][16]\n  - Lựa chọn mô hình tuỳ thuộc yêu cầu ứng dụng: real-time vs độ chính xác. [17][18]\n\n- Tầm quan trọng:\n  - Phát hiện đối tượng là bài toán cốt lõi trong thị giác máy tính với nhiều ứng dụng thực tế (xe tự hành, giám sát, robotics, v.v.). Việc tận dụng đặc trưng của CNN (feature maps) và thiết kế output phù hợp là then chốt. [1][4][12]\n\n- Liên hệ với các bài giảng khác:\n  - Video nhắc trở lại bài Deep Visualization Toolbox (đã thảo luận trước đây) để minh họa hành vi của feature maps (activation dịch chuyển theo đối tượng, bất biến tỉ lệ). Người học nên tham khảo nội dung đó để hiểu rõ cơ sở trực quan của việc dùng feature maps cho detection. [1][2]\n\n---\n\nGhi chú ngắn về biểu diễn tensor output (theo nội dung video):\n- Mô hình one-stage có thể biểu diễn output dưới dạng tensor S x S x D, với S là số cell lưới (ví dụ 7) và D là chiều sâu mã hoá thông tin class + bbox (ví dụ ~30 trong video): Tensor_output ∈ R^{S×S×D}. Mỗi cell chứa các thông tin x,y,w,h và class probabilities (và đủ chiều để xử lý occlusion). [13][14][15]\n\n(Các trích dẫn trong tóm tắt này tham chiếu trực tiếp tới các đoạn thời gian trong video: [1]…[19].)",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 1,
          "end_time": 63,
          "text": "ứng dụng tiếp theo của mạng CNN trong các bài toán liên quan đến thị giác máy tính chính là phát hiện đối tượng và đây có thể nói là một trong những bài toán mà có sức ảnh hưởng rất lớn về mặt ứng dụng nó có ứng dụng trong xe tự hành ví dụ như khi chiếc xe trên đường sẽ được trang bị các camera đặt ở tất cả các hướng nhìn của xe và nó sẽ phát hiện xung quanh có những xe hoặc các phương tiện đi lại hoặc là những người bộ hành và những vật cản nào để từ đó nó đưa ra quyết định là xe nên đi theo hướng nào và bài toán phát hiện đối tượng này thì bắt gồm từ một trực quan hóa của mạng CNN trước đây chúng ta đã từng thảo luận về bài Deep Visualization Toolbox và nó có một số tính chấp của mạng CNN của cái Feature Map trong cái mạng CNN đó chính là tính bất biến về trình tự không gian"
        },
        {
          "index": 2,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 47,
          "end_time": 111,
          "text": "trước đây chúng ta đã từng thảo luận về bài Deep Visualization Toolbox và nó có một số tính chấp của mạng CNN của cái Feature Map trong cái mạng CNN đó chính là tính bất biến về trình tự không gian tính bất biến về trình tự không gian và cái sự bất biến về tỷ lệ nghĩa là sao ví dụ trong cái tấm hình này thì chúng ta thấy cái người đàn ông ngồi trước cái màn hình thì đây chính là cái Feature Map và cái đốn sáng này là được đốn sáng này thì nó có cái concept có cái ý nghĩa đó chính là thể hiện được những cái concept là gương mặt chính cái tấm ảnh này là chính cái tấm ảnh mà làm cho cái Feature Map này là phát sáng nhất thì chúng ta thấy điểm chung của tất cả các cái ảnh này đó chính là có cái gương mặt và khi cái người này di chuyển thì chúng ta sẽ thấy là cái đốn sáng này cũng di chuyển theo và sau đó sẽ có một cái người đàn ông khác"
        },
        {
          "index": 3,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 99,
          "end_time": 160,
          "text": "đó chính là có cái gương mặt và khi cái người này di chuyển thì chúng ta sẽ thấy là cái đốn sáng này cũng di chuyển theo và sau đó sẽ có một cái người đàn ông khác mặc áo màu đen đi vào khung hình thì chúng ta thấy là cái người này nằm ở phía tay trái này đúng không thì cái đốn sáng tương ứng nó cũng nằm phía tay trái so với cái đốn sáng lớn này tức là trình tự về không gian cái người mặc áo đen đứng về phía đằng xa và đứng sau thì cái đốn sáng này cũng đứng về đằng xa và phía sau nó tương ứng với lại cái vị trí của cái người áo đen đồng thời về mặt tỷ lệ chúng ta thấy là cái gương mặt của cái người mặc áo đen nó bằng khoảng một nửa theo một chiều nữa hoặc là bằng một phần nữa  bằng một phần tư về mặt diện tích thôi bây giờ chúng ta sẽ nói về chiều ngang chiều dọc đi thì cái mặt của cái người áo đen bằng khoảng một nửa so với lại cái người mặc áo tím ở đây thì cái đốn sáng tương ứng cũng sẽ bằng một nửa như vậy chính là"
        },
        {
          "index": 4,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 149,
          "end_time": 210,
          "text": "chiều ngang chiều dọc đi thì cái mặt của cái người áo đen bằng khoảng một nửa so với lại cái người mặc áo tím ở đây thì cái đốn sáng tương ứng cũng sẽ bằng một nửa như vậy chính là đây chính là cái tính bất biến về mặt tỷ lệ và dựa trên cái đặc điểm này thì chúng ta ứng dụng nó ứng dụng các cái Feature Map của mạng CNN để giải quyết các cái bài toán về khái hiện đối tượng ví dụ như đốn sáng này là đại diện cho cái concept về mặt gương mặt và chúng ta sẽ dùng các cái thuật ván để ước lượng cái Bounding Box để ước lượng cái Bounding Box xung quanh các cái đốn sáng này sau khi đã ước lượng xong cái đốn sáng, cái Bounding Box này chúng ta sẽ nội suy ra cái Bounding Box ở cái ảnh gốc tại vì chúng ta đã biết cái bề ngang và bề cao của cái tấm ảnh này rồi chúng ta biết cái bề ngang và bề cao"
        },
        {
          "index": 5,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 200,
          "end_time": 260,
          "text": "ở cái ảnh gốc tại vì chúng ta đã biết cái bề ngang và bề cao của cái tấm ảnh này rồi chúng ta biết cái bề ngang và bề cao của cái Feature Map rồi thì khi đó chúng ta hoàn toàn có thể thực hiện được cái thao tác nội suy tuy nhiên thì đây chỉ là cái ý tưởng sơ khởi và làm sao để cho cái mạng của mình có khả năng học và đoán được cái vị trí cái Bounding Box từ đầu tới cuối thì như vậy chúng ta sẽ phải thiết kế lại cái mạng CNN chúng ta phải điều chỉnh lại cái mạng CNN một chút để mà đạt được cái cái nhiệm vụ đó là phát hiện đối tượng và lưu ý là bài toán phát hiện đối tượng nó sẽ có một cái tính chất là cái object của mình à nó sẽ không xuất hiện trọn vẹn bên trong cái khung hình mà đâu đó đó chỉ xuất hiện ở một cái khu vực nhỏ nào đó thôi và nhiệm vụ của mình sẽ là phải tìm ra cái vị trí đó và trong cái hình này nó có thể có"
        },
        {
          "index": 6,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 248,
          "end_time": 313,
          "text": "bên trong cái khung hình mà đâu đó đó chỉ xuất hiện ở một cái khu vực nhỏ nào đó thôi và nhiệm vụ của mình sẽ là phải tìm ra cái vị trí đó và trong cái hình này nó có thể có rất nhiều cái object khác hoặc là có thể có rất nhiều có rất nhiều những loại đối tượng khác và thậm chí là nó không có đối tượng nào cả thì đấy chính là cái bài toán phát hiện đối tượng và nó khác như thế nào so với bài toán phân loại đối tượng và đây là khái niệm thì như chúng ta đã đề cập rồi ha tức là trong cái tấm hình này thì có rất nhiều những cái loại đối tượng khác nhau và có đồng thời cũng có khả năng là có hai đối tượng cùng loại ví dụ ở đây là cái chai thì ở đây cũng có xuất hiện là cái chai rồi và ở đây là có cái ly rồi cũng có những cái đối tượng mà một loại ví dụ như là cái tô cái laptop thì đây là nhiệm vụ chính của cái bài toán phát hiện đối tượng đó chính là chúng ta sẽ xác định cái vùng hình hộp cái vùng hình hộp hay"
        },
        {
          "index": 7,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 292,
          "end_time": 360,
          "text": "ví dụ ở đây là cái chai thì ở đây cũng có xuất hiện là cái chai rồi và ở đây là có cái ly rồi cũng có những cái đối tượng mà một loại ví dụ như là cái tô cái laptop thì đây là nhiệm vụ chính của cái bài toán phát hiện đối tượng đó chính là chúng ta sẽ xác định cái vùng hình hộp cái vùng hình hộp hay còn gọi là cái vùng bao riêng bóc nếu có sự xuất hiện của một hoặc nhiều đối tượng trong trong tấm hình này chúng ta sẽ phải phải phải tính đến cả cái tình huống đó là những cái tình huống đó là những cái tình huống đó là những cái tình huống đó là những cái tình huống trong cái vùng hình mà trong tấm hình không có nó là những cái tình huống đó là những cái tình huống đó là những cái tình huống đó là những cái tình huống đó là những cái tình hình không có âm chấp nào Thì để ứng dụng cái mạng CNN cho cái bài toán phát hiện đối tượng này thì có rất nhiều cái mà ứng dụng cái mạng CNN cho cái bài toán phát hiện đối tượng này thì có rất nhiều cái mà ứng dụng cái mạng CNN cho cái bài toán phát hiện đối tượng này thì có rất nhiều cái mô hình mà nổi tiếng và gần nhất trong 3 cái mô hình này và ý tưởng của nó cũng được sử dụng cho rất nhiều những cái"
        },
        {
          "index": 8,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 346,
          "end_time": 411,
          "text": "mà ứng dụng cái mạng CNN cho cái bài toán phát hiện đối tượng này thì có rất nhiều cái mô hình mà nổi tiếng và gần nhất trong 3 cái mô hình này và ý tưởng của nó cũng được sử dụng cho rất nhiều những cái thuật toán phát hiện đối tượng về sau kể cả có sử dụng những cái mô hình tiên tiến nhất của Deep Learning như là Vision Transformer thì đầu tiên đó là cái giai đoạn số 1 chúng ta sẽ phải xác định xem cái vùng có khả năng đối tượng tức là trong cái tấm hình này mình sẽ chỉ ra là những cái khu vực nào là có khả năng có đối tượng nhưng đối tượng đó là đối tượng gì thì hạ vội phân giải chúng ta sẽ tính sau thì sang giai đoạn số 2 chúng ta sẽ phân loại xem ứng với từng cái Bounding Box đó thì ở đây nó sẽ là cái Class của nó là cái gì cái tên của cái đối tượng trong cái Bounding Box này là gì đồng thời chúng ta có thể sẽ phải tinh chỉnh lại cái Bounding Box sao cho nó khớp với đối tượng hơn"
        },
        {
          "index": 9,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 397,
          "end_time": 460,
          "text": "nó sẽ là cái Class của nó là cái gì cái tên của cái đối tượng trong cái Bounding Box này là gì đồng thời chúng ta có thể sẽ phải tinh chỉnh lại cái Bounding Box sao cho nó khớp với đối tượng hơn và cái vùng màu đen này chỉ là khu vực tạm thời thôi để localize cái vị trí có khả năng có đối tượng rồi sau đó chúng ta sẽ thực hiện là ước lượng cái Bounding Box một cách chính xác nhất vào cái đối tượng của mình thì đó chính là cái ý tưởng của ý tưởng của cái phương pháp mà phát hiện đối tượng 2 giai đoạn thì đối với thực quán FasterACNN nó sẽ khai thác cái đặc trưng Deep Feature và ở hai bước bước đầu tiên đó chính là Reason Proposal Network RPN nhiệm vụ của cái bước này đó chính là xác định những cái khu vực có khả năng có đối tượng và cái cách thức để xác định những cái vùng có khả năng đối tượng đó là dựa trên cái quan sát khi chúng ta rút trích ra các cái Feature Map"
        },
        {
          "index": 10,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 450,
          "end_time": 509,
          "text": "nhiệm vụ của cái bước này đó chính là xác định những cái khu vực có khả năng có đối tượng và cái cách thức để xác định những cái vùng có khả năng đối tượng đó là dựa trên cái quan sát khi chúng ta rút trích ra các cái Feature Map chúng ta sẽ thấy có những cái chỗ có Respawn thì đây chính là những cái chỗ có khả năng có đối tượng và từ những cái đốn sán này những cái chỗ Respawn này mình sẽ đưa qua một cái mạng Neural Network để chỉ ra những cái Billing Box chỉ ra được những cái Billing Box là chỗ đó có khả năng có đối tượng sau đó với cái Billing Box này chúng ta sẽ kết hợp với một cái Feature Map và lưu ý là Feature Map này nó được chia sẻ nó share Feature tức là Feature Map này và Feature Map này là một Feature Map này kết hợp với 말� bar  với lại cái bounding box mà qua cái mạng region proposal network nó sẽ khoanh vùng cái feature map này nó sẽ trích cái feature map này ra"
        },
        {
          "index": 11,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 499,
          "end_time": 561,
          "text": "với lại cái bounding box mà qua cái mạng region proposal network nó sẽ khoanh vùng cái feature map này nó sẽ trích cái feature map này ra và từ cái feature map này đến đến thực hiện cái công đoạn nó gọi là detector chỉ ra cái vị trí chính xác hơn chúng ta sẽ chỉ ra cái vị trí chính xác hơn cái bounding box chính xác đồng thời là chúng ta sẽ phải có thêm cái class cái class name tức là cái tên của cái đối tượng đó là gì thì đây chính là cái ý tưởng của faster acnn và cái hướng tiếp cận faster acnn thì nó sẽ có một cái điểm yếu là nó sẽ chậm và nó phải tách ra làm 2 giai đoạn thì bây giờ người ta có cái ý tưởng là làm sao trend từ đầu đến cuối tức là chúng ta sẽ thực thi từ đầu đến cuối chỉ cần fit vào một tấm ảnh đầu ra nó sẽ ra"
        },
        {
          "index": 12,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 549,
          "end_time": 608,
          "text": "và nó phải tách ra làm 2 giai đoạn thì bây giờ người ta có cái ý tưởng là làm sao trend từ đầu đến cuối tức là chúng ta sẽ thực thi từ đầu đến cuối chỉ cần fit vào một tấm ảnh đầu ra nó sẽ ra được cái bounding box các cái object luôn mà không cần phải chia ra làm 2 bước tại vì chia ra làm 2 bước thì nó sẽ có tình trạng là bước này phải chờ bước kia nó sẽ chậm còn hướng tiếp cận mà một giai đoạn thì nó sẽ loại bỏ hoàn toàn cái bước đối tượng tức là cái region proposal network mà nó sẽ thực thi từ đầu đến cuối hay là end to end một cái mạng cnn luôn rồi và cái ý tưởng của này của cái hướng giai đoạn này à nổi tiếng nhất chính là yolo và cái yolo thì ở đây chúng ta đang nói là yolo phiên bản đầu tuy nhiên thì yolo cho đến thời điểm hiện nay năm 2024 là nó đã có yolo phiên bản 10 tức là cứ cải thiến rất nhiều nhưng mà cái ý tưởng chính nhất của nó vẫn chính là làm sao fit một cái tấm ảnh đầu vào"
        },
        {
          "index": 13,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 597,
          "end_time": 661,
          "text": "năm 2024 là nó đã có yolo phiên bản 10 tức là cứ cải thiến rất nhiều nhưng mà cái ý tưởng chính nhất của nó vẫn chính là làm sao fit một cái tấm ảnh đầu vào đây chính là cái ảnh ảnh thô đầu vào và cái output đầu ra của mình nó sẽ là một cái tensor và cái tấm ảnh đầu ra của mình nó sẽ là một cái tensor cái tensor này nó có thể encode tức là nó có chứa đủ được cái thông tin về mặt class name rồi về mặt bounding box thì ở đây nó sẽ có cái trick là mỗi một cái ảnh của mình thì nó giả sử là nó chia ra một cái ô lưới ví dụ như trong trường hợp này là nó nghĩ ra là cái ô lưới của mình sẽ là kích thước là 7x7 tức là object của mình đâu đó chỉ xuất hiện"
        },
        {
          "index": 14,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 653,
          "end_time": 711,
          "text": "ví dụ như trong trường hợp này là nó nghĩ ra là cái ô lưới của mình sẽ là kích thước là 7x7 tức là object của mình đâu đó chỉ xuất hiện trong những cái khu vực 7x7 này mà thôi và nó sẽ có cái tình huống đó là với một cái ô này thì nó có khả năng là có hiện tượng chồng đối tượng tức là hiện tượng mẹ bụng con một đối tượng ở đằng trước và một đối tượng ở đằng sau thì nó sẽ thiết kế cái tensorflow này làm sao đó đủ để có thể encode được cả những cái tình huống đó tức là có những cái object này nó chồng lên cái object kia và tất cả mọi thứ nó sẽ encode trong cái 30 chiều độ sâu này trong cái 30 chiều độ sâu này nó sẽ phải có đầy đủ là class name nó sẽ phải có đầy đủ là tạo độ x, tạo độ y rồi width và height của các cái object này và với mỗi một cái cell ở đây với mỗi một cái cell ở đây"
        },
        {
          "index": 15,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 698,
          "end_time": 762,
          "text": "nó sẽ phải có đầy đủ là tạo độ x, tạo độ y rồi width và height của các cái object này và với mỗi một cái cell ở đây với mỗi một cái cell ở đây thì chúng ta sẽ có được cái thông tin vị trí của một cái object của mình trong đó đó thì như vậy là ý tưởng của YOLO là biến một cái ảnh đầu vào fit qua để tạo thành một tensor tensor tensor tensor rồi cuối cùng chúng ta sẽ ra được một cái tensor và cái tensor này nó có khả năng encode được cái thông tin vả độ và vị trí cũng như là cái cái tên, cái nhãn của cái object ở bên trong cái khu vực đó thì các cái phiên bản sau của YOLO có rất nhiều những cái cải thiến nó cũng kế thừa rất nhiều những cái thành tựu của Deep Learning trong cái việc thay đổi cái kiến trúc rồi trong cái việc là thiết kế cái output làm sao cho nó tiện nhất và có khả năng giải quyết được cái bài toán object detection trong cái tình huống đó là"
        },
        {
          "index": 16,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 748,
          "end_time": 810,
          "text": "kế thừa rất nhiều những cái thành tựu của Deep Learning trong cái việc thay đổi cái kiến trúc rồi trong cái việc là thiết kế cái output làm sao cho nó tiện nhất và có khả năng giải quyết được cái bài toán object detection trong cái tình huống đó là object của mình nó nhỏ tức là cái vấn đề về scale rồi cái vấn đề về trồng lấp occlusion trồng lấp còn tuy nhiên là cái tốc độ luôn luôn là điểm mạnh của các cái hướng tiếp cận vào một giai đoạn thì nó vẫn luôn luôn là làm sao cho cải tiến cái tốc độ càng lúc càng nhanh nhưng đồng thời là nó vẫn phải đảm bảo được cái độ chính xác ngang bằng hoặc là thậm chí là cố gắng để tốt hơn các cái hướng tiếp cận 2 giai đoạn ok, rồi đây là cái tốc độ của các cái hướng tiếp cận 2 giai đoạn và nếu so với cái hướng tiếp cận YOLO v3 tức là một cái hướng tiếp cận mà cách đây cũng 5-6 năm á thì so với lại cái faster acnn"
        },
        {
          "index": 17,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 798,
          "end_time": 861,
          "text": "và nếu so với cái hướng tiếp cận YOLO v3 tức là một cái hướng tiếp cận mà cách đây cũng 5-6 năm á thì so với lại cái faster acnn thì YOLO v3 cho tốc độ nhanh hơn faster acnn rất là nhiều lần ví dụ YOLO v3 thì có cái frame per second của mình là 45 frames per second tức là như vậy là nó đã có thể thực thi được thời gian thực thời gian thực trong khi đó FasterACNN là 7 frames per second nó dưới mức là 24 fps để đủ tạm gọi là có thể thực hiện được cái thời gian thực nhưng đồng thời nó sẽ đánh đổi cái độ chính xác nó sẽ đánh đổi cái độ chính xác là FasterACNN thì cho cái độ chính xác cao hơn YOLO đến hơn 10% thật sự mà nói thì trong cái bài toán Detection thì 10% là một con số rất là lớn và tùy vào cái nhu cầu cũng như là cái ngưỡng cảnh mà mình sẽ quyết định xem chọn được cái mô hình nào nếu như"
        },
        {
          "index": 18,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 849,
          "end_time": 914,
          "text": "thật sự mà nói thì trong cái bài toán Detection thì 10% là một con số rất là lớn và tùy vào cái nhu cầu cũng như là cái ngưỡng cảnh mà mình sẽ quyết định xem chọn được cái mô hình nào nếu như chúng ta không cần phải thực hiện cái thực toán quá nhanh real time thì chúng ta có và chúng ta cần độ chính xác thì chúng ta sẽ sử dụng cái tiếp cận 2 giai đoạn và cụ thể là FasterACNN cũng như là các cái biến thể của FasterACNN như vậy còn nếu như chúng ta cần một cái biến thể của FasterACNN thực thi theo thời gian thực thì lúc đó và chúng chúng ta cũng phải cân bằng được kiểu có về độ chính xác thì lúc đó YOLO các phiên bản của YOLO cũng như là SSD đây là một cái tên của một cái thực toán khác thì chúng ta sẽ chọn cái hướng tiếp cận là một giai đoạn để mà sử dụng và với cái sô đồ này thì chúng ta thấy là cái sự tương quan giữa các hướng tiếp cận thì YOLO là cho cái cái mp tức mp50 tức là một cái độ đo thể hiện cái độ chính khác, time tức là thời gian để mà thực thi"
        },
        {
          "index": 19,
          "video_id": "Chương 5_Til9AdPO7JE",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 3： Ứng dụng mạng CNN cho bài toán Phát hiện đối tượng",
          "video_url": "https://youtu.be/Til9AdPO7JE",
          "start_time": 899,
          "end_time": 936,
          "text": "cái sô đồ này thì chúng ta thấy là cái sự tương quan giữa các hướng tiếp cận thì YOLO là cho cái cái mp tức mp50 tức là một cái độ đo thể hiện cái độ chính khác, time tức là thời gian để mà thực thi đó thì YOLO V3 là cho cái tốc độ là cao nhất trong số cái hướng tiếp cận đây và à về độ chính xác thì YOLO V3 là 51% và thua so với lại Feature Pyramid Network cũng là một trong những cái hướng tiếp cận của Rất là nổi tiếng."
        }
      ]
    },
    {
      "video_id": "Chương 5_4p0L74qD7Lg",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- **Mục tiêu chính của bài giảng**  \n  Trình bày *phân đoạn ngữ nghĩa đối tượng* (semantic segmentation) — bài toán xác định vị trí và nhãn của các đối tượng đến mức *pixel* (khác với object detection chỉ trả về bounding box). [1][2]\n\n- **Các khái niệm sẽ được đề cập**  \n  - Định nghĩa phân đoạn ngữ nghĩa đối tượng và khác biệt so với object detection (pixel-level labeling vs. bounding box). [1][2]  \n  - Kiến trúc **UNet** (encoder–decoder với skip connections) và lý do cần skip connections. [2][3][5]  \n  - Quá trình *downsampling* và *upsampling*, các kỹ thuật unpooling, deconvolution (transposed convolution). [3][4][6][7][8]  \n  - Ý tưởng của **DeepLab v3**: *dilated/atrous convolution*, các *rate* khác nhau và *Atrous Spatial Pyramid Pooling (ASPP)*, concat + 1×1 conv, bilinear upsampling. [8][9][10][11][12][13][14]  \n  - Các ứng dụng thực tế của CNN liên quan (classification, retrieval, detection, super-resolution, style transfer, tracking) và liên hệ ý tưởng với nghiên cứu hiện đại. [15][16][17][18][19][20][21]\n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Định nghĩa phân đoạn ngữ nghĩa đối tượng\n- Phân đoạn ngữ nghĩa đối tượng là gán nhãn từng pixel: mỗi pixel được xác định thuộc đối tượng nào (ví dụ: pixel thuộc con bò hay thuộc cỏ). Đây là mức chi tiết hơn object detection (bounding box) và giúp loại bỏ vùng không phải đối tượng nằm trong bounding box. [1][2]\n\n### 2.2. Kiến trúc UNet: encoder–decoder và skip connections\n- UNet là một kiến trúc phổ biến cho phân đoạn: gồm giai đoạn **encode** (downsampling) để trích xuất feature có tính chịu nén và giai đoạn **decode** (upsampling) để tái tạo lại bản đồ đặc trưng với độ phân giải giống input. [2][3][4]  \n- Khi downsample liên tiếp, kích thước không gian giảm (ví dụ từ 572×572 → 284×284 → ... → ~28×28) làm mất thông tin chi tiết. [3][4]  \n- *Skip connections* nối giữa các tầng encode và decode dùng để tận dụng thông tin gốc ở độ phân giải cao, giữ lại chi tiết không gian khi tái tạo (concat feature map từ encode vào decode). Việc này giúp giữ đường nét và độ phân giải của output. [3][5]  \n- Skip connections cũng tương tự ý tưởng residual của ResNet: hỗ trợ huấn luyện, giúp tránh *vanishing gradient* và làm quá trình huấn luyện nhanh và ổn định hơn. [5][6]\n\n### 2.3. Upsampling: vấn đề mất nét và các phép toán thực hiện\n- Quá trình upsampling cố tăng kích thước không gian (ví dụ từ 28×28 → 54×54 → 104×104 → 200×204 → 392×392), nhưng việc phóng to này thường làm giảm độ sắc nét do thiếu thông tin ban đầu. [4]  \n- Có nhiều cách thực hiện upsampling:\n  - **Unpooling**: khi pooling lưu lại vị trí của giá trị lớn nhất (switches); unpooling chép ngược giá trị đó về vị trí đã đánh dấu; các ô không có thông tin được điền 0. [7]  \n  - **Deconvolution / Transposed convolution**: từ một pixel (1×1) lan truyền thông tin ra vùng lớn hơn (ví dụ 3×3) bằng filter học được; về bản chất là phép nghịch đảo/lan tỏa của convolution. [7][8]  \n- Cần lưu vị trí khi pooling để unpooling có thể khôi phục đúng vị trí chứa giá trị tối ưu; nếu không có vị trí sẽ chỉ điền 0 cho các ô còn lại. [7]\n\n### 2.4. Dilated / Atrous convolution và DeepLab v3\n- **Atrous (dilated) convolution** mở rộng receptive field bằng cách “bỏ qua” các ô ở giữa khi lấy mẫu, cho phép tổng hợp thông tin từ vùng lớn hơn mà không tăng số tham số nhiều hoặc giảm kích thước feature map. (Trong video gọi là *address/dilated commission*). [8][9]  \n- Thay vì áp dụng nhiều lần convolution 3×3 liên tiếp để mở rộng receptive field (tốn chi phí), dilated conv dùng *rate* (bước nhảy) để lấy mẫu cách nhau: ví dụ rate = 1 (không bỏ ô nào), rate = 2 (nhảy 2 ô), rate = 6, 12, 18... cho các khoảng cách khác nhau. [10][11][12]  \n- Một trường hợp đặc biệt: 1×1 convolution có thể coi là rate = 0 (không nhảy), giữ nguyên điểm mẫu. [11][12]  \n- Ý tưởng của **DeepLab v3** là thực hiện nhiều phép atrous convolution với các rate khác nhau (ASPP — Atrous Spatial Pyramid Pooling), concat các feature map kết quả rồi dùng 1×1 conv để tổng hợp thành một feature map duy nhất, sau đó upsampling (ví dụ bilinear upsampling) để đưa về độ phân giải cao. [12][13][14]\n\n### 2.5. Tổng hợp tính năng và các thao tác cuối\n- Sau khi thu được các feature map từ nhiều rate, thực hiện concat dọc theo chiều kênh rồi dùng 1×1 convolution để hợp nhất thông tin thành feature map duy nhất trước khi upsample. [12][13][14]  \n- Up-sampling cuối cùng trong DeepLab v3 thường là bilinear (không có tham số) để tạo output với độ phân giải cao hơn. [14]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n### 3.1. Ví dụ minh họa trong bài giảng\n- Ví dụ phân đoạn con bò: khác với bounding box, phân đoạn ngữ nghĩa gán rõ pixel nào thuộc con bò, pixel nào thuộc nền (cỏ), tránh sai nhãn trong bounding box. [1][2]  \n- Các kích thước ví dụ khi down/up sampling được nêu: 572×572 → 284×284 → ... → 28×28, rồi upsample dần lên 54×54 → 104×104 → 200×204 → 392×392, minh họa mất chi tiết khi upsampling. [3][4]  \n- Ví dụ lưu vị trí khi pooling và chép ngược khi unpooling (các ô không có thông tin đặt 0). [7]\n\n### 3.2. Ứng dụng thực tế\n- Phân loại (classification), đặc biệt *fine-grained classification* (ví dụ phân loại các loài hoa, các dòng xe, niên đại xe). [15]  \n- Nhận dạng khuôn mặt (face identification): cần phân biệt cá thể tới mức nhận diện định danh. [15]  \n- Content-based image retrieval / embedding: ảnh được chuyển thành embedding vector để so sánh tương đồng (cosine, khoảng cách) và truy vấn ảnh tương tự. [16]  \n- Phát hiện đối tượng (object detection): xác định vị trí bằng bounding box (một bước hoặc hai bước). [16][20]  \n- Phân đoạn ngữ nghĩa (semantic segmentation): xác định vị trí đối tượng tới cấp pixel; tiếp cận UNet/encoder–decoder rất phổ biến cho nhiệm vụ này. [17]  \n- Super Resolution (tăng độ phân giải ảnh): từ ảnh nhỏ (ví dụ 200×200) tạo ảnh lớn hơn (ví dụ 1000×1000) giữ sắc nét (không chỉ dùng upsampling phi tham số như bilinear lặp lại). [18]  \n- Style transfer (chuyển phong cách ảnh): chuyển domain (thực → hoạt hình, v.v.), có liên quan tới Generative AI. [18][19]  \n- Tracking: theo dõi cùng một đối tượng qua nhiều frame, gán ID liên tục cho đối tượng đó trong video. [19]  \n\n### 3.3. Trường hợp sử dụng / Lưu ý\n- Kiến trúc UNet và các biến thể vẫn được dùng rộng rãi cho segmentation; các ý tưởng cốt lõi (skip connections, encoder–decoder) tiếp tục ảnh hưởng tới các phương pháp mới. [2][17][20]  \n- DeepLab v3 đặc biệt hữu ích khi cần mở rộng receptive field để bao quát các đối tượng có kích thước khác nhau nhờ các rate khác nhau trong atrous conv (ASPP). [8][9][12][14]\n\n---\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:  \n  - Phân đoạn ngữ nghĩa đối tượng gán nhãn tới từng pixel, chính xác hơn object detection bằng bounding box. [1][2]  \n  - UNet (encoder–decoder) với skip connections giữ lại thông tin chi tiết không gian trong quá trình upsampling và giúp huấn luyện ổn định. [3][5][6]  \n  - Upsampling có thể làm mất nét; để khắc phục có thể dùng unpooling (với switches), deconvolution (transposed conv) hoặc dùng thông minh atrous conv để mở rộng receptive field mà không giảm kích thước. [4][7][8]  \n  - DeepLab v3 dùng nhiều atrous rates (ASPP), concat kết quả và 1×1 conv để tổng hợp, sau đó upsample (bilinear) cho output độ phân giải cao. [9][12][13][14]  \n  - Các kỹ thuật này được ứng dụng rộng rãi: classification (fine-grained), retrieval, detection, segmentation, super-resolution, style transfer, tracking, và tiếp tục là nền tảng cho nghiên cứu mới. [15][16][17][18][19][20][21]\n\n- Tầm quan trọng:  \n  Những kiến trúc và thao tác (skip connections, residual idea, atrous convolution, ASPP, unpooling/deconv) là các công cụ căn bản để giải quyết bài toán phân đoạn và nhiều bài toán thị giác máy khác; dù nhiều ý tưởng đã xuất hiện vài năm trước, chúng vẫn là nền tảng cho các công trình mới nhất. [2][5][8][20][21]\n\n- Liên hệ với các bài giảng khác:  \n  - Ý tưởng các kiến trúc phát hiện đối tượng (one-stage, two-stage) và những kỹ thuật từ chương này tiếp tục được kế thừa trong các phương pháp hiện đại. [20]  \n\n(End of summary — toàn bộ nội dung tóm tắt dựa trên các đoạn trích trong video.)",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 1,
          "end_time": 62,
          "text": "Trong phần cuối cùng thì chúng ta sẽ cùng tìm hiểu về phân đoạn ngữ nghĩa đối tượng Thì ở đây chúng ta sẽ có cái định nghĩa bài tán phân đoạn ngữ nghĩa đối tượng Là chúng ta sẽ xác định cái vị trí của đối tượng, các cái đối tượng cần quan tâm Và chính xác đến cấp độ điểm ảnh Tức là trước đây nếu như chúng ta phát hiện đối tượng, object detection Thì ở đây chúng ta chỉ cần chỉ ra cái bounding box Bào xung quanh cái đối tượng Thì ở đây chúng ta sẽ phải chỉ đến cái cấp độ đó là pixel Tức là pixel này thì nó sẽ thuộc về đối tượng là con bò Pixel này thì sẽ thuộc về đối tượng là cái bài cỏ Tương tự như vậy ở đây Trước đây thì chúng ta sẽ chỉ ra cái bounding box Còn bây giờ thì chúng ta sẽ chỉ ra chi tiết Đến từ cầm đến từng cái pixel Như vậy thì cái bài toán phân đoạn ngữ nghĩa đối tượng"
        },
        {
          "index": 2,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 49,
          "end_time": 112,
          "text": "Tương tự như vậy ở đây Trước đây thì chúng ta sẽ chỉ ra cái bounding box Còn bây giờ thì chúng ta sẽ chỉ ra chi tiết Đến từ cầm đến từng cái pixel Như vậy thì cái bài toán phân đoạn ngữ nghĩa đối tượng Nó sẽ giúp cho chúng ta giải quyết triệt để hơn cái bài toán object detection Tại vì trong object detection thì nó sẽ có cái vùng của cái bounding box Mà không thực sự là thuộc cái đối tượng Nhưng đó, ví dụ như trong cái điểm ở đây Chúng ta thấy mặc dù nó nằm trong cái bounding box Nhưng nó không thực sự là nằm trong cái đối tượng là con bò Thì trong số những cái lúc tiếp cận cho Phân đoạn ngữ nghĩa đối tượng là con bò  Thì kiến trúc UNED Mặc dù nó ra đời từ khoảng năm 2015-2016 Rất là lâu rồi Nhưng có thể nói cho đến nay Đây là một trong những kiến thức rất Một trong những kiến trúc rất là tổng quát Và được tái sử dụng cho rất nhiều cái mô hình Và cái ý tưởng của nó Đó chính là sử dụng các cái skip collection"
        },
        {
          "index": 3,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 97,
          "end_time": 160,
          "text": "Nhưng có thể nói cho đến nay Đây là một trong những kiến thức rất Một trong những kiến trúc rất là tổng quát Và được tái sử dụng cho rất nhiều cái mô hình Và cái ý tưởng của nó Đó chính là sử dụng các cái skip collection Rồi Thì tại sao lại như vậy Đầu tiên nếu như không có cái skip collection này Nếu như không có cái skip collection này Thì cái ảnh của mình sẽ được Downsampled Tức là được giảm cái độ phân giải xuống Ví dụ ban đầu là 572 x 572 Sau đó sẽ giảm xuống còn 284 x 284 Rồi kéo xuống một hồi sẽ còn là 30 x 30 28 x 28 Và đây là kết thúc cái quá trình encode Sang cái giai đoạn decode Để mà có thể tái tạo lại Cái feature map Hoặc là cái kết quả có cái độ phân giải Giống với lại cái độ phân giải ban đầu"
        },
        {
          "index": 4,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 149,
          "end_time": 210,
          "text": "Sang cái giai đoạn decode Để mà có thể tái tạo lại Cái feature map Hoặc là cái kết quả có cái độ phân giải Giống với lại cái độ phân giải ban đầu Thì buộc Là cái feature map ở đây Nó sẽ phải tăng cái Kích thước của hai cái chiều không gian bề ngang và bề cao lên Ví dụ ở đây chúng ta thấy là từ 28 x 28 Tăng lên là 54 x 54 Rồi Sau đó là tăng lên 104 Tăng lên 200 x 204 Tăng lên 392 x 392 Thì cái thao tác mà up sample Hay là up sampling Là Nó sẽ làm giảm Cái độ phân giải Của tấm hình của mình Nó sẽ làm giảm cái độ phân giải tức là cái đường nét của mình nó không còn sắc nét nữa Thì cái này là một cái Việc rất là bình thường Khi chúng ta Từ nhiều Từ một cái không gian mà nhiều thông tin nén xuống không gian x thông tin Xong từ không gian x thông tin Mở rộng rừng trở lại Thì nó sẽ bị Thiếu sóc thông tin"
        },
        {
          "index": 5,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 199,
          "end_time": 260,
          "text": "Việc rất là bình thường Khi chúng ta Từ nhiều Từ một cái không gian mà nhiều thông tin nén xuống không gian x thông tin Xong từ không gian x thông tin Mở rộng rừng trở lại Thì nó sẽ bị Thiếu sóc thông tin Do đó Nó sẽ có cái skip connection này Skip connection này nó sẽ tận dụng được cái Thông tin gốc Tận dụng được cái độ Và nó sẽ giữ được cái độ phân giải Từ đó là Nó sẽ kết nối Với lại cái Feature map ở các lớp Đã được up sampling từ cái giai đoạn encode Sau đó nó sẽ concat Nó sẽ concat Kết nối với lại cái Feature map Tại lớp trước đó Lớp mà Trước khi thực hiện cái quá trình encode Như vậy thì ở đây Nó sẽ giúp cho chúng ta giữ được cái Độ phân giải Về lý thuyết của ResNet Với các cái residual block Thì nó cũng sẽ có các cái skip connection"
        },
        {
          "index": 6,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 249,
          "end_time": 309,
          "text": "Độ phân giải Về lý thuyết của ResNet Với các cái residual block Thì nó cũng sẽ có các cái skip connection Và Skip connection này ngoài cái việc giữ được cái độ phân giải Của cái Feature map Output Thì mình sẽ Còn có một cái tính năng nữa đó là Giúp cho cái quá trình huấn luyện nhanh hơn Nó đỡ tránh được cái hiện tượng Vanishing gradient Không bị cái hiện tượng Vanishing Radian Rồi Và để có thể thực hiện được các cái thao tác mà Up sampling này lên á Thì chúng ta sẽ có các cái phép là Unpooling Và deconvolution Nếu như pooling Thì chúng ta lưu cái giá trị nhỏ nhất hoặc giá trị lớn nhất hoặc giá trị trung bình Tại đây"
        },
        {
          "index": 7,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 300,
          "end_time": 361,
          "text": "Unpooling Và deconvolution Nếu như pooling Thì chúng ta lưu cái giá trị nhỏ nhất hoặc giá trị lớn nhất hoặc giá trị trung bình Tại đây Thì khi chúng ta tái tạo Chúng ta sẽ không biết là Phải Thế cái giá trị này Vào cái vị trí nào Do đó trong cái quá trình mà Pooling Thì chúng ta sẽ lưu các cái Squid variable Để lưu cái vị trí của cái giá trị lớn nhất hoặc giá trị nhỏ nhất hoặc giá trị trung bình đó Ví dụ Ở đây Chúng ta biết giá trị này là giá trị lớn nhất Thì chúng ta sẽ Đưa cái giá trị đó vào cái pool map nhưng đồng thời đánh dấu Là cái vị trí này Là chứa cái giá trị Mà mình vừa mới được thực hiện pooling Thì khi cái quá trình unpooling Chúng ta sẽ lấy cái giá trị này Chép ngược trở lại về cái vị trí này Chép ngược trở lại về cái vị trí này Và lưu ý là 3 cái giá trị ở đây nó sẽ để là 3 con số 0 Tại vì nó không có thông tin để một con số 0 ở đây Nó sẽ trả cái giá trị này Rồi đối với phép decommission"
        },
        {
          "index": 8,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 348,
          "end_time": 410,
          "text": "Chép ngược trở lại về cái vị trí này Và lưu ý là 3 cái giá trị ở đây nó sẽ để là 3 con số 0 Tại vì nó không có thông tin để một con số 0 ở đây Nó sẽ trả cái giá trị này Rồi đối với phép decommission Phép decommission Thì nó tổng hợp thông tin từ một cái vùng Giả sử như đây là kích thước 3x3 Nó sẽ tổng hợp thông tin về một cái pixel Thì decommission là cái công việc ngược lại Tức là từ một cái O có kích thước là 1x1 Nó sẽ lan truyền cái thông tin này Đến cái vùng có kích thước là 3x3 Ở cái feature map output Và nó cũng sẽ trở lại là 3x3 Nó sẽ có các cái bộ filter để thực hiện cái phép decommission Tương tự như là phép decommission Một cái kiến trúc khác cũng rất là nổi tiếng Và đó chính là cái DeepLab V3 Ý tưởng của DeepLab V3 nó sẽ dựa trên cái phép tính toán Đó là address commission Hoặc là Hoặc là cái tên khác đó là dilated commission"
        },
        {
          "index": 9,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 400,
          "end_time": 461,
          "text": "Ý tưởng của DeepLab V3 nó sẽ dựa trên cái phép tính toán Đó là address commission Hoặc là Hoặc là cái tên khác đó là dilated commission Nếu như Cái phép biến đổi Commission Ví dụ đây là output ha Đây là output nè Đây là input nè Rồi Thì nó sẽ tổng hợp thông tin của cái vùng có kích thước là 3x3 Để tổng hợp thông tin Và điền vô Một cái điểm Ở trên feature map ở đây Thì cái việc này nó sẽ dẫn đến cái vấn đề đó là Nó sẽ không có tổng hợp được thông tin Ở những cái vùng có kích thước lớn hơn Đó Muốn Tổng hợp được thông tin ở những vùng lớn hơn Thì chúng ta sẽ phải thực hiện cái phép commission liên tiếp nhiều lần Còn cái phép address commission Và đương nhiên cái việc mà chúng ta thực hiện nhiều lần như vậy Thì nó sẽ tăng cái chi phí tính toán đồng thời là không giải quyết được cái vấn đề về"
        },
        {
          "index": 10,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 448,
          "end_time": 509,
          "text": "Thì chúng ta sẽ phải thực hiện cái phép commission liên tiếp nhiều lần Còn cái phép address commission Và đương nhiên cái việc mà chúng ta thực hiện nhiều lần như vậy Thì nó sẽ tăng cái chi phí tính toán đồng thời là không giải quyết được cái vấn đề về Scale Vấn đề về độ bất biến Trong cái Phân đoạn ngữ nghĩa với những cái đối tượng nhỏ hoặc là những đối tượng rất là lớn Thì ở đây Cái phép address commission Thay vì chúng ta Chỉ lấy các cái vùng 3 x 3 Liên tiếp nhau Thì chúng ta có thể skip Chúng ta sẽ bỏ qua Những cái ô ở giữa Thì ở trong cái ví dụ này Thì cái ray Ray nó thể hiện là cái skip của mình là bằng 2 Trong cái ví dụ này Thì cái ray của mình Ray là bằng 1 Tức là nó chỉ nhảy cốc Một đơn vị Còn cái này là nhảy cốc 2 đơn vị để lấy giá trị để mà tổng hợp thông tin lên đây Và cái phép address commission là cái giá trị để tổng hợp thông tin lên đây  Rồi cái ray commission Nó là sự kết hợp Concrete Của"
        },
        {
          "index": 11,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 498,
          "end_time": 560,
          "text": "Còn cái này là nhảy cốc 2 đơn vị để lấy giá trị để mà tổng hợp thông tin lên đây Và cái phép address commission là cái giá trị để tổng hợp thông tin lên đây  Rồi cái ray commission Nó là sự kết hợp Concrete Của Thông tin Tại rất nhiều cái vùng Với các cái kích thước khác nhau Ví dụ như là từ một cái vùng 1 x 1 Từ cái vùng là 3 x 3 Thật ra cái 1 x 1 commission này nếu như mình nhìn một cái góc độ nào đó Nó chính là ray bằng 0 Ray bằng 0 Tức là Các cái giá trị của mình nó không có Nhảy ra Để lấy tổng hợp thông tin Mà nó cứ đứng yên ở đó Ray bằng 0 Còn commission 3 x 3 commission Với cái ray là bằng 6 tức là khoảng cách giữa các cái điểm mà mình Lấy mẫu Để mà tổng hợp thông tin Thì khoảng cách này là 6 Ray bằng 12 tức là cái khoảng cách này là 12 Ray là 18"
        },
        {
          "index": 12,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 549,
          "end_time": 611,
          "text": "Để mà tổng hợp thông tin Thì khoảng cách này là 6 Ray bằng 12 tức là cái khoảng cách này là 12 Ray là 18 Thì là khoảng cách là Giữa 2 cái điểm ảnh 2 cái điểm trên cái feature map để mà mình tổng hợp thông tin Đó là 18 Thì cái nhắc lại là nữa là cái 1 x 1 commission này Hiểu một cách Gọi là extreme decay Thì nó chính là 3 x 3 Nhưng mà Ray bằng 0 Là cái điểm mà mình lấy mẫu không có nhảy ra ngoài Mà nó cứ đứng yên một chỗ Đồng thời Nó sẽ kết hợp với lại cái Phép biến đổi là imitator Nó sẽ pull link như bình thường Nó sẽ downsampling như bình thường Như vậy thì Cả 5 Cái feature map này Nó sẽ concat lại với nhau Tức là nó sẽ trồng lại lên nhau Và đồng thời Nó sẽ trồng lên nhau Và đồng thời nó sẽ thực hiện cái phép 1 x 1 commission để tổng hợp thông tin về 1 cái feature map"
        },
        {
          "index": 13,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 599,
          "end_time": 658,
          "text": "Nó sẽ concat lại với nhau Tức là nó sẽ trồng lại lên nhau Và đồng thời Nó sẽ trồng lên nhau Và đồng thời nó sẽ thực hiện cái phép 1 x 1 commission để tổng hợp thông tin về 1 cái feature map Tức là ban đầu Các cái phép tính 1 x 1 commission Nó sẽ tạo ra một cái Feature map như thế này Rồi Cái phép 3 x 3 commission với rate là bằng 6 Đúng không Thì nó lại tiếp tục Tạo ra một cái Nó sẽ tạo ra một cái feature map mới Rồi 12 x 12 18 x 18 Rồi Thì sau khi chúng ta thực hiện cái phép commission 1 x 1 Thì nó sẽ tổng hợp lại Thành duy nhất"
        },
        {
          "index": 14,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 652,
          "end_time": 710,
          "text": "Thì sau khi chúng ta thực hiện cái phép commission 1 x 1 Thì nó sẽ tổng hợp lại Thành duy nhất Nó sẽ tổng hợp lại thành duy nhất một cái feature map mà thôi Rồi Thì Sau đó thì nó sẽ thực hiện cái phép upsampling Binary Binary upsampling Tức là không có tham số Để tải tạo ra Một cái Thổng Kết quả mà Có cái độ phân giải cao Thì đây là cái ý tưởng của DeepLav3 Thì cái ý chính của cái DeepLav3 chính là cái phép address commission hay là dilated commission Và nó nằm ở trong cái address Special pyramid pooling Nó sẽ concat thông tin Khi chúng ta thực hiện cái address commission Với rất nhiều cái rate khác nhau"
        },
        {
          "index": 15,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 697,
          "end_time": 763,
          "text": "Và nó nằm ở trong cái address Special pyramid pooling Nó sẽ concat thông tin Khi chúng ta thực hiện cái address commission Với rất nhiều cái rate khác nhau Rồi sau đó chúng ta tổng hợp lại thông qua cái phép Concat kết hợp với lại 1 x 1 commission Rồi Như vậy thì Trên đây Đó là chúng ta đã Tóm tắt Rất nhiều những cái ứng dụng Kinh điển Điển hình của mạng CNN Từ các cái Ứng dụng liên quan đến bài toán Phân loại đối tượng Trên những cái đối Trên những cái loại đối tượng mà Có cái Rất là mịn tức là Thay vì chúng ta nhận diện Hoa So với lại các cái đối tượng khác Như là Cây cối Thì ở đây hoa Chúng ta sẽ phân ra rất là nhiều cái loại hoa Tương tự như vậy đối với xe hơi Đúng không? Thì chúng ta cũng sẽ có rất nhiều Những cái loại xe hơi Các cái dòng xe hơi Các cái Niên đại của nó Rồi Đối với cái bài toán mà nhận diện gương mặt Thì chúng ta sẽ phải phân biệt được cái định danh của người này với người kia"
        },
        {
          "index": 16,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 749,
          "end_time": 810,
          "text": "Đúng không? Thì chúng ta cũng sẽ có rất nhiều Những cái loại xe hơi Các cái dòng xe hơi Các cái Niên đại của nó Rồi Đối với cái bài toán mà nhận diện gương mặt Thì chúng ta sẽ phải phân biệt được cái định danh của người này với người kia Thì đó là cái ứng dụng trong bài toán Classification nhưng mà ở cấp độ là Miệng File Grand Classification Và cái ứng dụng tiếp theo đó là cho cái bài toán tri vấn tức là Tấm ảnh của mình nó sẽ được Convert sang cái dạng Emitting Vector Và cái Emitting Vector này sẽ được sử dụng để đi so sánh với lại các cái Emitting Vector của Những cái tấm ảnh khác trong cái sử dụng Và cái việc so sánh này thì cũng tương tự như là các cái file thao tác tri vấn bình thường Đó là chúng ta có thể sử dụng các cái độ đo tích phố hướng Cô sinh Hoặc là sử dụng độ đo khoảng cách Rồi sau đó lấy tốt các cái giá trị mà Có cái độ tương đồng cao thì chúng ta trả về Và cái ứng dụng Nữa đó chính là Có thể thực hiện các cái thao tác liên quan đến phát hiện đối tượng"
        },
        {
          "index": 17,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 799,
          "end_time": 862,
          "text": "Có cái độ tương đồng cao thì chúng ta trả về Và cái ứng dụng Nữa đó chính là Có thể thực hiện các cái thao tác liên quan đến phát hiện đối tượng Tức là chúng ta sẽ chỉ ra chính xác Chúng ta có thể chỉ ra được cái vị trí của đối tượng đến cái cấp độ là Bounding Box Và Đối với bài toán Semantic Segmentation tức là phân đoạn nghĩa đối tượng Thì chúng ta có thể chỉ ra được cái vị trí Của đối tượng đến cấp độ là Pixel Và Trong cái các hướng tiếp cận thì hướng tiếp cận Unet Với Cấu trúc Encoder và Decoder đó là một trong những cái kiến trúc mà cho đến bây giờ Vẫn được sử dụng rất là nhiều Có rất nhiều những cái biến thể khác nhau nhưng mà ý tưởng chung Đó là có cái skip connection Giữa cái lớp encode sang lớp decode để đảm bảo được cái Độ phân giải Giữa Cái ảnh đầu vào Với lại cái ảnh output nó có cái độ phân giải Và cái đường nét sắc đét Và độ chính xác cao Và bên cạnh các ứng dụng trên thì còn rất nhiều những ứng dụng khác Ví dụ như là ứng dụng tăng độ phân giải ảnh"
        },
        {
          "index": 18,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 849,
          "end_time": 910,
          "text": "Với lại cái ảnh output nó có cái độ phân giải Và cái đường nét sắc đét Và độ chính xác cao Và bên cạnh các ứng dụng trên thì còn rất nhiều những ứng dụng khác Ví dụ như là ứng dụng tăng độ phân giải ảnh Tức là từ một cái ảnh Có độ Có cái kích thước Và ví dụ như 200 Nhân 200 Thì sau khi thực hiện cái phép Super Resolution xong Nó có thể tạo thành cái cái ảnh Mà có kích thước lên đến là 1000 Nhân với 1000 Ví dụ vậy nó sẽ tăng cái độ phân giải của ảnh lên Và đương nhiên là nó vẫn phải giữ được cái Tính chất Cái sự sắc nét của Hệ ảnh chứ nó không phải là chỉ sử dụng các cái thao tác mà Upsampling Theo kiểu là Billionaire tức là Song tuyến Một cách gọi là phi tham số Để mà Repeat tức là nó sẽ lặp lại các cái điểm ảnh nhiều lần Tạo ra cái ảnh nó không có được sắc nét cho lắm Chuyển đổi phong cách ảnh Tức là chúng ta sẽ có thể chuyển đổi một cái Domain Một cái không gian của ảnh"
        },
        {
          "index": 19,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 899,
          "end_time": 962,
          "text": "Repeat tức là nó sẽ lặp lại các cái điểm ảnh nhiều lần Tạo ra cái ảnh nó không có được sắc nét cho lắm Chuyển đổi phong cách ảnh Tức là chúng ta sẽ có thể chuyển đổi một cái Domain Một cái không gian của ảnh Tại cái domain hiện đại sang một cái domain khác Ví dụ như chúng ta có được một cái tấm ảnh Trong đời thật Của mình Chúng ta có thể chuyển sang cái phong cách đó là ảnh hoạt hình Thì đây có lẽ là một trong những cái ứng dụng mà có rất nhiều Cái sự chú ý của cộng đồng trong thời gian gần đây liên quan đến cái mảng là Generative AI Tiếp theo đó là các cái bài toán liên quan đến theo với đối tượng tức là Một cái đối tượng Nó sẽ xuất hiện Ở trong Xuyên suốt Rất nhiều cái frame của tấm ảnh Và làm sao mình biết Cái đối tượng này Nó cũng chính là cái đối tượng này ID của đối tượng thông qua ID của đối tượng Khi di chuyển qua nhiều frame Đối tượng này cũng chính là đối tượng này Thì đó là bài toán Tracking Các kiến trúc mạng trong các cái bài toán này"
        },
        {
          "index": 20,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 949,
          "end_time": 1000,
          "text": "Khi di chuyển qua nhiều frame Đối tượng này cũng chính là đối tượng này Thì đó là bài toán Tracking Các kiến trúc mạng trong các cái bài toán này Thì có cái mức độ ảnh hưởng lớn Và nó sẽ dẫn dắt ý tưởng chủ đạo cho các kiên cố về sau tức là mặc dù Những cái kiến trúc Những cái Phương pháp, những cái thuật toán, những cái mô tả Những cái bài toán của Quỳnh mà được giới thiệu trong cái bài học ngày hôm nay Nó cũng Có từ cách đây 4-5 năm trở về trước Tuy nhiên Những cái ý tưởng này Nó vẫn như như đã đề cập đối với cái kiến trúc UNED Hoặc là một số cái kiến trúc như là Phát hiện đối tượng Với Một giai đoạn hoặc phát hiện đối tượng với 2 giai đoạn Thì Các hướng tiếp cận gần đây nó vẫn kế thừa những cái ý tưởng chủ đạo đó để mà tạo ra những cái công trình Nghiên cứu mới nhất"
        },
        {
          "index": 21,
          "video_id": "Chương 5_4p0L74qD7Lg",
          "chapter": "Chương 5",
          "video_title": "[CS431 - Chương 5] Part 4： Ứng dụng mạng CNN cho bài toán Phân đoạn ngữ nghĩa đối tượng",
          "video_url": "https://youtu.be/4p0L74qD7Lg",
          "start_time": 1000,
          "end_time": 1000,
          "text": "Nghiên cứu mới nhất"
        }
      ]
    },
    {
      "video_id": "Chương 6_30kCjQ0BdUc",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (Natural Language Processing - NLP), trình bày các định nghĩa cơ bản, tiềm năng ứng dụng, **hướng tiếp cận học sâu trong NLP**, tầm quan trọng của việc biểu diễn từ dưới dạng vector và giới thiệu mô hình *Word2Back* để biểu diễn từ. [1][2]\n\n- Bối cảnh khóa học: Trước đây các bài học tập trung vào dữ liệu ảnh; từ chương này chuyển sang dữ liệu văn bản và cách biểu diễn từ ngữ cho mô hình học sâu. [1]\n\n- Các khái niệm sẽ được đề cập: định nghĩa NLP; hai nhiệm vụ chính là Natural Language Understanding (NLU) và Natural Language Generation (NLG); vấn đề biểu diễn từ (Word2Back); những thách thức của ngôn ngữ tự nhiên (nhập nhằng, tách từ, ngôn ngữ mạng xã hội, thành ngữ, ngữ cảnh và kiến thức thực tế); các ứng dụng điển hình (spell-check, spam detection, POS tagging, NER, search, synonym finding, sentiment analysis, coreference resolution, parsing, machine translation, information extraction, chatbots như ChatGPT). [2][3][4][5][6][8][11][12][13][14][15][16][17][18]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Định nghĩa và phạm vi của NLP\n- NLP (Natural Language Processing) là lĩnh vực giao thoa giữa khoa học máy tính, trí tuệ nhân tạo và ngôn ngữ học, với mục tiêu làm cho máy tính có khả năng ngôn ngữ giống như con người. [2][3]\n\n- Hai khía cạnh chính của NLP:\n  - **Natural Language Understanding (NLU)**: khả năng máy hiểu ngôn ngữ đầu vào (thường ở dạng văn bản trong phạm vi bài giảng này). [4][5]\n  - **Natural Language Generation (NLG)**: khả năng sinh ngôn ngữ đầu ra, có thể ở dạng âm thanh hoặc văn bản. [4][5]\n\n### 2.2. Tầm quan trọng của biểu diễn từ dưới dạng vector (Word representation)\n- Biểu diễn từ dưới dạng vector là một phần quan trọng để máy học xử lý văn bản; *Word2Back* được giới thiệu như một phương pháp biểu diễn từ dưới dạng vector. [1][2]\n  - Nội dung chính nhắc tới: Word2Back là một phương pháp biểu diễn một từ dưới dạng một vector, và bài sẽ tìm hiểu mô hình Word2Back cùng các khái niệm liên quan. [1][2]\n\n### 2.3. Những thách thức cơ bản của ngôn ngữ tự nhiên\n- **Tính nhập nhằng (ambiguity)**: Một câu có thể hiểu theo nhiều cách nếu thiếu ngữ cảnh. Ví dụ minh họa câu \"Ông già đi nhanh quá\" có ít nhất ba cách hiểu khác nhau (ông già = người lớn tuổi; \"đi\" = di chuyển hoặc đã mất; nghĩa hàm ẩn tùy ngữ cảnh). [5][6][7]\n\n- **Vấn đề tách từ và đơn vị ngữ nghĩa**: Một cụm như \"tốc độ truyền thông tin\" cần được nhận diện là các đơn vị ngữ nghĩa (ví dụ \"tốc độ\", \"thông tin\") thay vì tách rời các từ thành \"tốc\", \"độ\", \"truyền\", \"thông\", \"tin\" gây mất nghĩa. [8][9]\n\n- **Ngôn ngữ mạng xã hội / slang**: Ngôn ngữ trên mạng xã hội hay dùng kỹ thuật chơi chữ, viết tắt, lẫn tiếng Anh và kí tự thay thế (ví dụ \"More the ui Hôm nay con không về\" có kết hợp English + ký tự số/biểu tượng), gây khó khăn cho xử lý tự động. [10]\n\n- **Thành ngữ và ngữ nghĩa không theo chữ**: Ví dụ \"ra ngô ra khoai\" không nói về trái ngô/trái khoai mà nghĩa là làm việc đến nơi đến chốn; nghĩa không trực tiếp liên quan tới mặt chữ. [11]\n\n- **Phụ thuộc vào ngữ cảnh thực tế và kiến thức nền**: Hiểu đúng một câu còn cần biết chủ đề/ngữ cảnh thảo luận và kiến thức chuyên môn; hai người cùng hiểu từ vựng nhưng thiếu kiến thức nền vẫn có thể không hiểu ý nhau. [12]\n\n### 2.4. Các bài toán và khái niệm kỹ thuật chính\n- **POS tagging (gán nhãn từ loại)**: gán nhãn danh từ/động từ/tính từ... cho từng token trong câu. [13]\n- **NER (Name Entity Recognition)**: xác định từ/ngữ là tên người/tổ chức/địa điểm... trong câu. [14]\n- **Spam detection**: phát hiện thư rác trong hệ thống email. [13]\n- **Spell-check (kiểm tra chính tả)**: ứng dụng truyền thống lâu đời. [13]\n- **Search / Keyword search**: hệ thống tìm kiếm theo từ khóa (Search Engine). [14]\n- **Synonym finding (tìm từ đồng nghĩa)**: tìm các từ có nghĩa tương tự. [14]\n- **Sentiment Analysis / Opinion Mining**: phân loại nội dung (tích cực/tiêu cực/trung tính) — ứng dụng phổ biến cho social listening. [15]\n- **Coreference resolution (xác định đồng tham chiếu)**: xác định từ (ví dụ \"he\") tham chiếu tới thực thể nào trong ngữ cảnh. Ví dụ: trong \"Cater, Thôn, Mubarak He shouldn't run again\", \"he\" tham chiếu tới Mubarak chứ không phải Cater. [16]\n- **Parsing (phân tích cú pháp), constituency/ dependency parsing**: phân tích cấu trúc câu. [16]\n- **Machine Translation (dịch máy)**: hệ thống dịch hiện đã đạt kết quả ấn tượng và được thương mại hóa. [16][17]\n- **Information Extraction (rút trích thông tin)**: tự động rút các thực thể như thời gian, địa điểm từ email để dùng cho các tác vụ (ví dụ đặt lịch). [16][17]\n\n### 2.5. Hướng tiếp cận học sâu trong NLP\n- Bài sẽ đề cập tới hướng tiếp cận bằng học sâu để giải quyết các bài toán NLP (được nêu là phần nội dung sẽ tìm hiểu). [1][2]\n\n(Ngoài ra không có công thức toán học cụ thể nào được nêu trong các đoạn trích đã cung cấp.)\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ về **nhập nhằng (ambiguity)**: câu \"Ông già đi nhanh quá\" có thể hiểu là:\n  - Ông già (người lớn tuổi) di chuyển nhanh. [6]\n  - Ông già đã mất đột ngột (đi = chết). [6]\n  - \"Ông già\" là biệt danh/ám chỉ bạn bè, nghĩa hàm khác (ví dụ trông tiều tụy). [7]\n  - Ở một số vùng miền \"ông già\" có thể hàm ý là \"bố\", dẫn tới các cách hiểu khác. [7][8]\n\n- Ví dụ về **tách từ / đơn vị ngữ nghĩa**: cụm \"tốc độ truyền thông tin\" phải được nhận diện là các đơn vị ngữ nghĩa (\"tốc độ\", \"truyền thông tin\") để giữ đúng ý nghĩa, nếu tách rời sẽ mất nghĩa. [8][9]\n\n- Ví dụ về **ngôn ngữ mạng xã hội**: văn bản có lẫn tiếng Anh, ký tự số thay cho chữ (ví dụ \"More the\" + dùng số 0 thay cho chữ 'o'), các cách viết tắt/phi chuẩn gây khó khăn cho hệ thống xử lý. [10]\n\n- Ví dụ về **thành ngữ**: \"ra ngô ra khoai\" mang nghĩa là làm việc triệt để, không phải nghĩa đen của trái ngô/trái khoai. [11]\n\n- Ứng dụng thực tế (liệt kê và minh họa):\n  - Spell-check (kiểm tra lỗi chính tả). [13]\n  - Spam detection trong email. [13]\n  - POS tagging (gán nhãn từ loại) và NER (nhận dạng tên riêng). [13][14]\n  - Search engine (tìm kiếm theo từ khóa) và tìm từ đồng nghĩa. [14]\n  - Sentiment Analysis / Social Listening (phân tích cảm xúc từ mạng xã hội). [15]\n  - Coreference resolution (xác định tham chiếu của đại từ). [16]\n  - Parsing, machine translation (dịch máy) với kết quả ấn tượng đã được thương mại hóa. [16][17]\n  - Information extraction từ email (rút thời gian, địa điểm để đặt lịch tự động). [17]\n  - ChatGPT như ví dụ nổi bật của hệ thống tích hợp nhiều khả năng NLP: question answering, paraphrase (viết lại văn bản), sửa lỗi chính tả, tóm tắt văn bản, hội thoại/chatbot. [17][18][19]\n\n- Trường hợp sử dụng cụ thể cho ChatGPT:\n  - Question answering: trả lời câu hỏi dựa trên kiến thức quá khứ. [17]\n  - Paraphrase + spell correction: viết lại hoặc sửa lỗi cho văn bản đầu vào. [18]\n  - Summarization: tóm tắt văn bản dài thành nội dung chính. [18]\n  - Chatbot / hội thoại: trò chuyện, trả lời trong phạm vi cho phép. [18][19]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính: Bài giảng giới thiệu tổng quan về NLP — định nghĩa, hai nhiệm vụ chính NLU/NLG, tầm quan trọng của biểu diễn từ dạng vector (Word2Back), những thách thức lớn của ngôn ngữ tự nhiên (nhập nhằng, tách từ, ngôn ngữ mạng xã hội, thành ngữ, phụ thuộc ngữ cảnh và kiến thức nền) và nhiều ứng dụng thực tiễn từ các tác vụ cơ bản đến các hệ thống hiện đại như ChatGPT. [1][2][3][4][5][6][8][9][10][11][12][13][14][15][16][17][18][19]\n\n- Tầm quan trọng: Hiểu những thách thức và cách biểu diễn (vector) là nền tảng để áp dụng các phương pháp học sâu giải quyết bài toán NLP, và các ứng dụng NLP đang ngày càng phổ biến, có đóng góp lớn cho xã hội (ví dụ dịch máy, chatbot, rút trích thông tin, phân tích cảm xúc...). [1][2][16][17][19]\n\n- Liên hệ với các bài giảng khác: Khóa học đã làm việc với dữ liệu ảnh trước đây; từ chương này chuyển hướng sang dữ liệu văn bản và sẽ tiếp tục với các kỹ thuật học sâu trong NLP (sẽ được trình bày trong các phần sau). [1][2]\n\n(Đã sử dụng toàn bộ các đoạn trích được cung cấp để xây dựng tóm tắt này.)",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 15,
          "end_time": 63,
          "text": "Chào các bạn, hôm nay chúng ta sẽ đến với bài về xử lý ngôn ngữ tự nhiên Trong các bài trước thì chúng ta đã làm việc trên loại dữ liệu đó là dữ liệu ảnh Còn từ nay cho về sau thì chúng ta sẽ đến với loại dữ liệu đặc biệt Đó chính là loại dữ liệu văn bản Và làm sao để có thể biểu diễn được các từ trong văn bản Thì chúng ta có thể sử dụng một công cụ rất hiệu quả Đó chính là Word2Back Word2Back là một phương pháp để giúp cho chúng ta biểu diễn một từ dưới dạng một vector Và nội dung chính của chúng ta sẽ bao gồm các thành phần như sau Đầu tiên là chúng ta sẽ cùng tìm hiểu về một số định nghĩa, khái niệm Cũng như là một số tìm năng ứng dụng của lĩnh vực xử lý ngôn ngữ tự nhiên Thứ hai đó là chúng ta sẽ tìm hiểu về hướng tiếp cận học sâu trong xử lý ngôn ngữ tự nhiên"
        },
        {
          "index": 2,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 47,
          "end_time": 113,
          "text": "Đầu tiên là chúng ta sẽ cùng tìm hiểu về một số định nghĩa, khái niệm Cũng như là một số tìm năng ứng dụng của lĩnh vực xử lý ngôn ngữ tự nhiên Thứ hai đó là chúng ta sẽ tìm hiểu về hướng tiếp cận học sâu trong xử lý ngôn ngữ tự nhiên Và thứ ba đó là chúng ta tìm hiểu về tầm quan trọng của việc biểu diễn một từ dưới dạng một vector Cuối cùng đó là chúng ta sẽ tìm hiểu về mô hình Word2Back Đây là một trong những mô hình rất phổ biến để biểu diễn một từ dưới dạng một vector hiện nay Đầu tiên đó là chúng ta sẽ tìm hiểu về mô hình Word2Back Và chúng ta sẽ tìm hiểu về một số khái niệm Thì xử lý ngôn ngữ tự nhiên thì tên tiếng Anh đó là Natural Language Processing Rồi, thì đây là một lĩnh vực nghiên cứu mà thuộc lĩnh vực về khoa học bái tính, trí tuệ nhân tạo và ngôn ngữ học"
        },
        {
          "index": 3,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 106,
          "end_time": 163,
          "text": "Rồi, thì đây là một lĩnh vực nghiên cứu mà thuộc lĩnh vực về khoa học bái tính, trí tuệ nhân tạo và ngôn ngữ học Như vậy thì chúng ta có thể thấy là nó có một phần giao thoa Với lại một cái lĩnh vực thuộc về khoa học xã hội Đó chính là ngôn ngữ học Bên cạnh những cái lĩnh vực nghiên cứu về trí tuệ nhân tạo và khoa học máy tính Thì mục tiêu của cái lĩnh vực này đó là làm sao cho máy tính có khả năng ngôn ngữ giống như con người Thế thì khả năng ngôn ngữ như con người nó sẽ bao gồm khả năng gì? Đầu tiên đó chính là khả năng hiểu Con người chúng ta thì sẽ nhận cái thông tin đầu vào là một cái ngôn ngữ Và ngôn ngữ sẽ thông qua hai cái phương tiện chính Phương tiện đầu tiên đó chính là văn bản Cái công cụ thứ hai, cái phương tiện thứ hai đó chính là qua âm thanh Khi chúng ta nghe một người nào đó đứng trước mặt nói chuyện"
        },
        {
          "index": 4,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 147,
          "end_time": 211,
          "text": "Và ngôn ngữ sẽ thông qua hai cái phương tiện chính Phương tiện đầu tiên đó chính là văn bản Cái công cụ thứ hai, cái phương tiện thứ hai đó chính là qua âm thanh Khi chúng ta nghe một người nào đó đứng trước mặt nói chuyện Thì cái ngôn ngữ nó được thể hiện qua âm thanh Thì máy tính cũng tương tự như vậy Máy tính cũng sẽ nhận cái thông tin đầu vào là một cái ngôn ngữ Tuy nhiên trong phạm vi của cái bài học này Chúng ta sẽ tiếp cận ngôn ngữ ở dưới dạng là văn mẹ Và sau khi máy tính có khả năng hiểu được cái ngôn ngữ đầu vào Thì nó sẽ đưa ra cái phản hồi Và cái phản hồi này thì nó sẽ nằm trong cái giai đoạn gọi là generation Và phản hồi này nó sẽ tạo ra cái gì? Nó cũng sẽ tạo ra chính ngôn ngữ Ngôn ngữ này thì nó có thể ở dạng là âm thanh Ví dụ như máy nó có thể phát ra một cái đoạn âm thanh Để thể hiện cái sự phản hồi của mình Hoặc máy cũng có thể tạo ra dưới dạng là một cái đoạn văn mẹ Thì trong phạm vi của ngôn ngữ này"
        },
        {
          "index": 5,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 197,
          "end_time": 260,
          "text": "Ngôn ngữ này thì nó có thể ở dạng là âm thanh Ví dụ như máy nó có thể phát ra một cái đoạn âm thanh Để thể hiện cái sự phản hồi của mình Hoặc máy cũng có thể tạo ra dưới dạng là một cái đoạn văn mẹ Thì trong phạm vi của ngôn ngữ này Chúng ta cũng sẽ tiếp cận sự hiểu ngôn ngữ tự nhiên dưới dạng là văn mẹ Rồi, và tương ứng với hai cái giai đoạn Hiểu ngôn ngữ và tạo sinh ngôn ngữ Thì chúng ta sẽ có các cái từ khóa đó là Natural Language Understanding Và Natural Language Generation Thì cái gì này là bắt đầu? Thì bắt đầu từ chữ Generation và chữ U này Thì bắt đầu từ chữ Understanding Thế thì lĩnh vực xử lý ngôn ngữ tự nhiên Là một trong những cái lĩnh vực khó Nó khó vì rất nhiều những cái lý do sau Đầu tiên đó chính là Cái tính nhập nhằn của ngôn ngữ Nếu như chúng ta không có đầy đủ thông tin về mặt ngữ cảnh Mà chúng ta đọc cái câu này Thì chúng ta có thể hiểu bằng bao nhiêu cách"
        },
        {
          "index": 6,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 249,
          "end_time": 310,
          "text": "Đầu tiên đó chính là Cái tính nhập nhằn của ngôn ngữ Nếu như chúng ta không có đầy đủ thông tin về mặt ngữ cảnh Mà chúng ta đọc cái câu này Thì chúng ta có thể hiểu bằng bao nhiêu cách Cái câu đó là Ông già đi nhanh quá Thì cái câu ông già đi nhanh quá Nó có thể được hiểu bằng không dưới là 3 cách Đầu tiên đó là Nếu như chúng ta quan sát cái từ ông già Như là một cái cụm từ Thì cái từ ông già này có thể ám chỉ là một người lớn tuổi nào đó Có thể ám chỉ một người lớn tuổi nào đó Và cái từ đi này á Thì cũng có thể hiểu hai cái nghĩa Nghĩa đầu tiên đó chính là di chuyển Và nghĩa thứ hai đó là có thể người này đã mất Còn nhanh quá thì chắc chắn là không được hiểu được Thì tương ứng đó là thể hiện cái tốc độ đúng không Như vậy là ông già di chuyển nhanh quá Hoặc là ông già mất đột ngột quá Thì đây là hai cái cách hiểu đầu tiên Nhưng nếu như cũng cái con này"
        },
        {
          "index": 7,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 298,
          "end_time": 361,
          "text": "Như vậy là ông già di chuyển nhanh quá Hoặc là ông già mất đột ngột quá Thì đây là hai cái cách hiểu đầu tiên Nhưng nếu như cũng cái con này Chúng ta nhìn nó ở một cái cách chia khác Thì cái từ ông là mình đang muốn nói đến cái Người mà mình đang muốn nói chuyện ở trước mặt Cái người bạn của mình Mình hay xin gọi ông Gia Thì ở đây sẽ hàm ý đó là tiêu tụy Hoặc là người dạo này là pha phai quá Thì ý của cái câu này đó là Cái người bạn của mình là dạo này nhìn sau tiêu tụy đi nhanh quá Trước đây thì rất là trẻ trung Và bây giờ thì lại gọi là tiêu tụy đi Và còn rất nhiều những cái cách nghĩa hiểu khác Ví dụ như cũng là cái từ ông già này Nhưng mà ở một số nơi một số vùng miền Thì hàm ý đó là để nói về phụ thân của mình Tức là ba của mình Như vậy thì với một cái cách hiểu ông già là ba của mình Thì nó lại có rất nhiều những cái cách hiểu khác nữa"
        },
        {
          "index": 8,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 348,
          "end_time": 410,
          "text": "Nhưng mà ở một số nơi một số vùng miền Thì hàm ý đó là để nói về phụ thân của mình Tức là ba của mình Như vậy thì với một cái cách hiểu ông già là ba của mình Thì nó lại có rất nhiều những cái cách hiểu khác nữa Thì như vậy đó chính là cái sự nhập nhằn của ngôn ngữ Nếu như không có đầy đủ thông tin về mặt tự cảnh Mà chúng ta chỉ có một cái đoạn văn Một cái câu văn như vậy Thì rõ ràng là sẽ tạo ra cái cách hiểu Rất nhiều cái cách hiểu khác nhau Và một trong những cái vấn đề để gây ra cái sự nhập nhằn đó Chính là cái vấn đề về tách từ Ví dụ như chúng ta có cái từ là tốc độ truyền thông tin Nếu như chúng ta nhìn cái câu này Cái cụm từ này dưới dạng là những cái từ rời rạc Được ngăn cách bởi cái dấu khoảng trắng Thì cái từ tốc Rồi cái từ độ Rồi từ truyền Rồi từ thông Và từ tin Đó là những cái từ Mà không có mang nghĩa đúng Của cái nội hàm Mà cái câu này muốn truyền đạt Cái nghĩa đúng của nó"
        },
        {
          "index": 9,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 400,
          "end_time": 461,
          "text": "Và từ tin Đó là những cái từ Mà không có mang nghĩa đúng Của cái nội hàm Mà cái câu này muốn truyền đạt Cái nghĩa đúng của nó Nó phải đi theo một cái cụm Nó sẽ phải đi theo một cái cụm Ví dụ ở đây là thông tin Phải là một cái cụm Và một cái đơn vị ngữ nghĩa Tốc độ Sẽ là một cái đơn vị ngữ nghĩa Như vậy là tốc độ truyền thông tin Thì nó sẽ phải là Chi ra thành các cái đơn vị ngữ nghĩa như vậy Thì lúc đó chúng ta mới hiểu đúng được cái ý của cái câu này Còn nếu chúng ta tách ra từ tốc riêng Từ độ riêng Từ truyền riêng Từ thông riêng Ví dụ từ thông chúng ta có thể nghĩa là Hiểu rất nhiều nghĩa khác nhau Rồi từ tốc Hoặc là từ độ Nếu mà đứng riêng mình nói Thì chúng ta cũng có thể hiểu rất nhiều cái cách hiểu khác nhau Cái thứ ba Đó chính là cái ngôn ngữ tin Ngôn ngữ Mình hay nói đùa đó là trẻ trâu bây giờ đó Thì các bạn trẻ thì có thể nhấn các cái đoạn văn bản"
        },
        {
          "index": 10,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 447,
          "end_time": 510,
          "text": "Thì chúng ta cũng có thể hiểu rất nhiều cái cách hiểu khác nhau Cái thứ ba Đó chính là cái ngôn ngữ tin Ngôn ngữ Mình hay nói đùa đó là trẻ trâu bây giờ đó Thì các bạn trẻ thì có thể nhấn các cái đoạn văn bản Với rất nhiều những cái cách thức sáng tạo khác nhau Ví dụ như ở đây chúng ta thấy là More the ui Hôm nay con không về Thì ở đây đã phối hợp Rất nhiều những cái kỹ thuật Ví dụ nó đã có lòng ghép cái tiếng Anh Ở trong đó Là more the Rồi cái thứ hai đó là Với cái từ more the này thì cái số không Người ta không gõ là Người ta không gõ là chữ o Mà người ta lại gõ là số không Tức là lợi dụng cái việc là cái tượng hình của nó Cái số không và chữ o nó giống nhau Rồi thậm chí là Nói giọng Ví dụ như là hông Yeah Hoặc là nay Thì đây chính là cái sự khó khăn Khi chúng ta phải làm việc với những cái ngôn ngữ mà không có Chủng Và xuất phát từ Cái mạng xã hội Tiếp theo Đó là vấn đề về Cái ngữ nghĩa của câu nó sẽ không liên quan trực tiếp"
        },
        {
          "index": 11,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 499,
          "end_time": 560,
          "text": "Khi chúng ta phải làm việc với những cái ngôn ngữ mà không có Chủng Và xuất phát từ Cái mạng xã hội Tiếp theo Đó là vấn đề về Cái ngữ nghĩa của câu nó sẽ không liên quan trực tiếp Đến cái mặt chữ của nó Ví dụ như Ở đây chúng ta có một cái thành ngữ là ra ngô ra khoai Hở Ví dụ như là làm ra ngô ra khoai Thì Không có nghĩa là Chúng ta sẽ tạo ra Các Cái Trái ngô trái khoai Mà ở đây Ý của nó đó chính là Làm việc đến nơi đến chỗ đó Làm việc triệt để Làm việc cho ra thành quả Thì đó là cái ý nghĩa của cái câu ra ngô ra khoai Chứ không hề nhắc gì đến cái Hai cái Cái cái cái loại trái cây ở đây đó là Trái ngô và trái khoai Và Ngoài ra thì Tất cả những cái Yếu tố trên nó sẽ còn Đi chung với lại một cái khó khăn thách thức nữa đó là Mình phải đi kèm với cái ngữ cảnh thực tế Cái ngữ cảnh thực tế Thì Ví dụ như là mình đang bàn về"
        },
        {
          "index": 12,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 548,
          "end_time": 610,
          "text": "Yếu tố trên nó sẽ còn Đi chung với lại một cái khó khăn thách thức nữa đó là Mình phải đi kèm với cái ngữ cảnh thực tế Cái ngữ cảnh thực tế Thì Ví dụ như là mình đang bàn về Cái Một cái chủ đề nào đó Thì Các cái câu của mình nó sẽ xoay xung quanh cái chủ đề mà mình đang bàn Thì nếu mình biết được cái ngữ cảnh đó Thì cái câu của mình có thể nó sẽ hiểu dễ dàng hơn Ngoài ra Đó là cái Kiến thức thực tế Dạ Ví dụ như trong một số tình huống Chúng ta Nói chuyện với nhau nhưng mà Từng từ chúng ta đều hiểu Nhưng mà ráp lại chúng ta không hiểu tại vì Chúng ta không có cái kiến thức chuyên môn Chúng ta không có kiến thức chuyên môn ví dụ như Chúng ta nói chuyện với một cái người về Công nghệ thông tin Chúng ta nói chuyện công nghệ thông tin về một cái người Và làm trong một cái lĩnh vực về Tài chính chẳng hạn Thì có khi là họ sẽ không hiểu cái câu chuyện mình đang nói là gì Vậy là Cũng là cái mặt trực đó nhưng mà Nó sẽ còn hiểu hay không phụ thuộc vô cái kiến thức trong thực tế nữa Rồi Và Lĩnh vực xử lý ngôn ngữ tự nhiên thì có rất nhiều"
        },
        {
          "index": 13,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 599,
          "end_time": 660,
          "text": "Cũng là cái mặt trực đó nhưng mà Nó sẽ còn hiểu hay không phụ thuộc vô cái kiến thức trong thực tế nữa Rồi Và Lĩnh vực xử lý ngôn ngữ tự nhiên thì có rất nhiều Những cái ứng dụng khác nhau Ví dụ như những cái ứng dụng kinh điển Như là kiểm tra Nỗ chính tả Thì đây là một trong những cái ứng dụng mà đã có từ rất là lâu đời nè Cái thiết kế này Thứ hai Đó chính là phát hiện Thư giác Thì bất cứ một cái email nào Một cái hệ thống email nào Đều có trang bị cái hệ thống là Spam Detection Ví dụ như đây là một cái Lạng tên nhóm là Spam Rồi gán nhãn từ loại Với mỗi từ trong một cái câu Chúng ta sẽ biết là từ nào là động từ Danh từ, tính từ Rồi Từ nào là Bổ nghĩa Cho từ nào Vân vân Thì đó là Nhận dạng Sự gắn nhãn từ Gắn nhãn từ loại Còn nhận dạng tên riêng Thì ở đây là bài toán NER Là Name Entity Recognition Thì chúng ta cho trước một câu Và chúng ta cần phải xác định xem là Ứng với từng từ"
        },
        {
          "index": 14,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 649,
          "end_time": 710,
          "text": "Sự gắn nhãn từ Gắn nhãn từ loại Còn nhận dạng tên riêng Thì ở đây là bài toán NER Là Name Entity Recognition Thì chúng ta cho trước một câu Và chúng ta cần phải xác định xem là Ứng với từng từ Thì nó sẽ là cái tên riêng cho Người Hay là tên riêng cho một cái tổ chức Hoặc là tên riêng cho một cái địa điểm Tên riêng cho một cái địa điểm Và đây là một cái bài toán cũng có rất nhiều những cái ứng dụng Có rất nhiều những cái ứng dụng Rồi tìm kiếm từ khóa Chắc là chúng ta thấy rằng là các cái hệ thống Search Engine hiện nay Là minh họa cho cái ứng dụng về tìm kiếm theo từ khóa Rồi tìm từ đồng nghĩa Vân vân Rồi gần đây hơn thì chúng ta sẽ có các cái ứng dụng ví dụ như là phân tích cảm xúc Thì cái này là ứng dụng trong cái mạng xã hội ví dụ như là Social Listening Social Listening"
        },
        {
          "index": 15,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 697,
          "end_time": 760,
          "text": "Thì cái này là ứng dụng trong cái mạng xã hội ví dụ như là Social Listening Social Listening Tức là gì? Chúng ta sẽ có các cái con bot có thể draw các cái dữ liệu trên mạng xã hội Và nó sẽ biết rằng là các cái nội dung mà mình đang trao đổi Rồi các cái đoạn tin nhắn, các cái comment Vân vân Thì nó có cái trạng thái là tích cực, tiêu cực hay là trung tính Đó là một cái ngữ cảnh ứng dụng của bài toán Sentiment Analysis Vậy thì Sentiment Analysis hoặc là một cái từ khác là Opinion Mining Tức là chúng ta đang Phân loại xem một cái đoạn văn hoặc là một cái cụm từ Là nó mang tính chất là tích cực, tiêu cực hay là trung tính Xác định độc thanh chiếu Thì cái bài toán này đó là chúng ta sẽ xem coi Các cái từ trong một cái câu nó sẽ được thanh chiếu đến cái từ nào"
        },
        {
          "index": 16,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 748,
          "end_time": 810,
          "text": "Xác định độc thanh chiếu Thì cái bài toán này đó là chúng ta sẽ xem coi Các cái từ trong một cái câu nó sẽ được thanh chiếu đến cái từ nào Ví dụ như là Cater, Thôn, Mubarak He shouldn't run again Thì cái từ he này Nó có thể là thanh chiếu đến cái từ Cater Hoặc cũng có thể là thanh chiếu đến từ Mubarak Nhưng mà trong cái ngữ cảnh của cái câu này Thì nó sẽ không có thanh chiếu đến Cater mà nó sẽ thanh chiếu đến Mubarak Rồi, ngoài ra thì chúng ta có các cái ứng dụng khác Ví dụ như là phân tích cú pháp, Bạc sinh hoặc là dịch máy Rồi rút trích thông tin Thì trong những năm gần đây thì các cái hệ thống dịch thuật là Rồi rút trích thông tin thì trong những năm gần đây thì các cái hệ thống dịch thuật là Coi cho những cái kết quả rất là ấn tượng Và đã được thương hoại hóa bởi rất nhiều những cái công ty Rồi rút trích thông tin thì chúng ta thấy là trong các cái email bây giờ là"
        },
        {
          "index": 17,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 798,
          "end_time": 862,
          "text": "Coi cho những cái kết quả rất là ấn tượng Và đã được thương hoại hóa bởi rất nhiều những cái công ty Rồi rút trích thông tin thì chúng ta thấy là trong các cái email bây giờ là Nó đã có thể tự động rút trích được là nội dung của một cái email Có thể rút trích được thông tin về mặt thời gian, nơi trốn Và từ đó để có thể giúp cho chúng ta đạt lịch một cách dễ dàng Rồi các cái ứng dụng khác mà Rồi các cái ứng dụng khác mà Và gần đây, có những cái ứng dụng rất là nổi tiếng Chính là ChatGPT Cái ứng dụngmat chatgpt này là Điển hình cho những cái ứng dụng sau Tức là chatGPT là một cái hệ thống mà nó tích hợp rất nhiều những cái khả năng về mặt ngôn ngữ Ví dụ, nó có khả năng question answering tức là trả lời câu hỏi Mình có thể hỏi chatGPT các cái câu hỏi Và nó sẽ đi lột tìm những cái kiến thức ở trong quá khứ Để từ đó là nó có thể trả lời cho mình Cái thứ hai đó là paraphrase, tức là viết lại một cái văn bản,"
        },
        {
          "index": 18,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 848,
          "end_time": 913,
          "text": "Mình có thể hỏi chatGPT các cái câu hỏi Và nó sẽ đi lột tìm những cái kiến thức ở trong quá khứ Để từ đó là nó có thể trả lời cho mình Cái thứ hai đó là paraphrase, tức là viết lại một cái văn bản, mình đưa cho chat GPT một cái đoạn văn, và mình yêu cầu nó viết lại theo một cái cách viết khác, và đồng thời là nó cũng có thể thực hiện luôn các cái thao tác sửa lỗi chính tả cho mình luôn. Thì paraphrase cộng với lại cái sửa lỗi chính tả. Như vậy chúng ta thấy là chat GPT nó rất là mạnh và hiệu quả trong việc ứng dụng các cái thành tựu mới của sự liên ngục về tự nhiên. Tóm tắt nội dung, tức là chúng ta sẽ cung cấp một cái đoạn văn rất là dài hoặc là cung cấp rất nhiều thông tin, nó sẽ tự tóm tắt lại thành cái nội dung chính. Rồi hội thoại chat bot, tức là chúng ta có thể tám dẫu, tám gẫu và hỏi thăm, tức là chúng ta sẽ hỏi con bot về bất cứ thứ gì mà nó có thể trả lời trong cái phạm vi mà nó cho phép. Như vậy thì trong cái phần vừa rồi thì chúng ta đã tìm hiểu qua cái ứng dụng,"
        },
        {
          "index": 19,
          "video_id": "Chương 6_30kCjQ0BdUc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 1： Giới thiệu lĩnh vực xử lý ngôn ngữ tự nhiên (NLP)",
          "video_url": "https://youtu.be/30kCjQ0BdUc",
          "start_time": 897,
          "end_time": 928,
          "text": "Rồi hội thoại chat bot, tức là chúng ta có thể tám dẫu, tám gẫu và hỏi thăm, tức là chúng ta sẽ hỏi con bot về bất cứ thứ gì mà nó có thể trả lời trong cái phạm vi mà nó cho phép. Như vậy thì trong cái phần vừa rồi thì chúng ta đã tìm hiểu qua cái ứng dụng, cái tiềm năng ứng dụng của sự liên ngục. Vì tự nhiên, và chúng ta thấy là sự liên ngục của tự nhiên đang ngày càng trở nên phổ biến và trong cuộc sống của chúng ta, và nó đang có những cái đóng góp to lớn cho cái sự phát triển của xã hội hiện nay."
        }
      ]
    },
    {
      "video_id": "Chương 6_utOha-d0prc",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Trình bày **hướng tiếp cận của học sâu (Deep Learning)** cho các bài toán xử lý ngôn ngữ tự nhiên (NLP), nhấn mạnh cách học sâu là một nhánh của *Representation Learning* cho phép học trực tiếp từ dữ liệu thô thay vì dựa vào các đặc trưng do chuyên gia thiết kế. [1]\n\n- Các khái niệm sẽ được đề cập:\n  - Representation Learning / học biểu diễn và lợi ích khi đưa dữ liệu thô vào mô hình thay vì hand-crafted features. [1][8]\n  - Những yếu tố dẫn đến thành tựu của Deep Learning: lượng dữ liệu lớn, nguồn dữ liệu từ Internet và mạng xã hội, sức mạnh tính toán (đặc biệt GPU), và cải tiến về mô hình/thuật toán. [2][3][4][5][6]\n  - Các mô hình nổi bật trong NLP hiện nay, đặc biệt **Transformer** và các Large Language Models (ví dụ: GPT / ChatGPT). [8][9]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 Representation Learning — học từ dữ liệu thô\n- Học sâu là một nhánh của *Representation Learning*; thay vì đưa vào các feature vector do chuyên gia thiết kế, học sâu cho phép đưa dữ liệu nguyên bản (raw input) và tự động học biểu diễn/đặc trưng từ dữ liệu đó. [1][8]\n- Kết quả: mô hình tự rút trích đặc trưng hữu ích từ dữ liệu khi được cung cấp đủ lượng dữ liệu. [1]\n\n### 2.2 Yếu tố dữ liệu: nguồn và quy mô\n- Internet đã tồn tại từ những năm 1990 nhưng gần đây phủ sóng rộng hơn, khiến rất nhiều người có thể tạo nội dung giá trị trên mạng, đóng góp kho dữ liệu cho AI. [2]\n- Nhiều nguồn dữ liệu quan trọng cho NLP hiện nay bao gồm Wikipedia, Stack Overflow (cho dữ liệu lập trình), và các nội dung trên YouTube (caption/ subtitle do tình nguyện viên tạo) — tất cả đều là nguồn huấn luyện hữu ích cho mô hình ngôn ngữ. [3][4]\n\n### 2.3 Sức mạnh tính toán: CPU vs GPU\n- CPU có giới hạn về khả năng thực hiện các phép toán đồng thời; GPU là phần cứng cho phép tính toán song song nhiều phép toán tương tự và độc lập cùng lúc, do đó thúc đẩy khả năng huấn luyện mô hình học sâu lớn. [4][5]\n\n### 2.4 Cải tiến mô hình & thuật toán\n- Các mô hình và thuật toán hiện đại giúp học được nhiều dữ liệu hơn trong thời gian huấn luyện ít hơn, đồng thời giảm các hiện tượng như overfitting so với các phương pháp cũ. [5]\n- Lược sử phương pháp: trước đây nhiều hệ thống dựa trên rule hoặc feature do chuyên gia thiết kế (hand-design programs / classic machine learning). Ví dụ trên ảnh: dùng bộ lọc (filters) do chuyên gia thiết kế; trong NLP có các trick/hacks và các mô hình như LSTM từng được thiết kế để xử lý chuỗi, hoặc dùng thống kê dựa trên bias để ước lượng xác suất từ tiếp theo. [6][7]\n- Với Representation Learning/Deep Learning, ta thường đưa vào các feature đơn giản hoặc dữ liệu thô và để mô hình tự học trọng số/đặc trưng. [8]\n\n  - Ghi chú về xác suất từ tiếp theo (như được mô tả trong video): các phương pháp thống kê truyền thống ước lượng xác suất xuất hiện của từ kế tiếp dựa trên từ hiện tại; có thể biểu diễn sơ lược bằng P(w_{t+1} | w_t) (khái niệm mô tả trong video), nhưng video chỉ nêu khái quát về thống kê xác suất mà không trình bày công thức chi tiết. [7]\n\n### 2.5 Transformer và vai trò trung tâm trong NLP hiện đại\n- Kiến trúc **Transformer** là nền tảng cho hầu hết các tiến bộ gần đây trong NLP; nhiều mô hình nổi bật như GPT, ChatGPT có chữ \"T\" (Transformer) trong tên, và GPT-4 là một ví dụ của large language model dựa trên Transformer. [8][9]\n- Transformer và các kiến trúc dẫn xuất cho phép xử lý ngôn ngữ tự nhiên vượt trội so với các phương pháp truyền thống. [8]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ về nguồn dữ liệu:\n  - Wikipedia cung cấp kho văn bản lớn cho huấn luyện. [3]\n  - Stack Overflow là nguồn dữ liệu quan trọng cho các tác vụ liên quan đến lập trình. [3]\n  - YouTube: video kèm caption/subtitle, cộng đồng tình nguyện viên tạo bản dịch/phiên âm sang nhiều ngôn ngữ (ví dụ tiếng Việt), đóng góp dữ liệu song ngữ và đa ngôn ngữ cho các hệ thống dịch và nhận dạng. [3][4]\n\n- Ví dụ ứng dụng thực tế của mô hình học sâu trong NLP:\n  - Dịch máy (machine translation). [8]\n  - Chatbot / conversational agents (ví dụ ChatGPT, GPT-4). [8][9]\n  - Gợi ý nội dung trong email (autocomplete / drafting assistance). [8][9]\n  - Công cụ hỗ trợ lập trình (ví dụ Co-Pilot của Microsoft) tự động gợi ý hoàn thiện mã, dịch đoạn văn từ tiếng Anh sang tiếng Việt, hoặc soạn email dựa trên ngữ cảnh trước đó. [9][10]\n  - Cơ chế hoạt động điển hình: hệ thống dựa trên lịch sử trao đổi để dự đoán hoặc hoàn thiện lần gõ tiếp theo (ví dụ gợi ý nhấn Tab để hoàn thiện nội dung). [9][10]\n\n- Trường hợp sử dụng:\n  - Huấn luyện mô hình dịch máy bằng corpora đa ngôn ngữ (ví dụ caption YouTube được dịch tay). [3][4]\n  - Tạo trợ lý soạn thảo văn bản tự động dựa trên mô hình ngôn ngữ lớn. [9]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Học sâu trong NLP tập trung vào học biểu diễn từ dữ liệu thô thay vì phụ thuộc vào đặc trưng do con người thiết kế. [1][8]\n  - Các yếu tố then chốt làm nên thành tựu hiện nay: dữ liệu khổng lồ từ Internet và mạng xã hội, sức mạnh tính toán (đặc biệt GPU), và cải tiến về mô hình/thuật toán (ví dụ Transformer). [2][3][4][5][6][8]\n  - Transformer là kiến trúc cốt lõi cho nhiều thành tựu NLP hiện nay (GPT, ChatGPT, v.v.), và các ứng dụng thực tiễn rất đa dạng (dịch máy, chatbot, trợ lý viết email, hỗ trợ lập trình). [8][9]\n\n- Tầm quan trọng của nội dung:\n  - Hiểu được mối liên hệ giữa dữ liệu, phần cứng và mô hình là thiết yếu để nắm bắt cách và lý do học sâu thay đổi mạnh mẽ lĩnh vực NLP. [2][4][5][6]\n\n- Liên hệ với các bài giảng khác:\n  - Video không đề cập cụ thể đến các bài giảng khác trong chuỗi; nội dung chủ yếu dừng lại ở việc giới thiệu hướng tiếp cận học sâu cho NLP và những yếu tố thúc đẩy sự phát triển của nó. [1][2][8]\n\n(Đã sử dụng toàn bộ các đoạn trích từ video: [1], [2], [3], [4], [5], [6], [7], [8], [9], [10].)",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 6_utOha-d0prc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 2： Hướng tiếp cận Deep learning cho các bài toán NLP",
          "video_url": "https://youtu.be/utOha-d0prc",
          "start_time": 1,
          "end_time": 67,
          "text": "Trong phần tiếp theo thì chúng ta sẽ cùng tìm hiểu về hướng tiếp cận của học sâu trong lĩnh vực xử lý ngôn ngữ tự nhiên Thì học sâu chúng ta nhắc lại đó là một cái nhánh của Representation Learning Tức là chúng ta sẽ tìm cách học từ dữ liệu thô Dữ liệu đồ vào trước đây của chúng ta nó phải là các cái feature vector Tức là chúng ta đã dựa trên cái trí thức chuyên gia để mà đưa vào bên trong các cái mạng neuron Thì bây giờ với mạng học sâu chúng ta chỉ cần đưa dữ liệu đồ vào là dữ liệu nguyên bản Và chúng ta không cần phải có cái kiến thức của chuyên gia để rút trích ra thành những cái đặc trưng quan trọng nữa Mà tự mô hình nó sẽ đi tìm cách để học và biểu diễn các cái dữ liệu hoặc là các cái đặc trưng đó cho mình Và đương nhiên là chúng ta sẽ phải cung cấp cho các cái mô hình máy học này rất nhiều dữ liệu Và khi cung cấp cho nó rất nhiều dữ liệu thì nó sẽ đút kết ra được những cái trí thức trung của tất cả những cái dữ liệu đó Và tất cả những cái thành tựu liên quan đến học sâu hiện nay mà có được là nhờ những cái yếu tố sau"
        },
        {
          "index": 2,
          "video_id": "Chương 6_utOha-d0prc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 2： Hướng tiếp cận Deep learning cho các bài toán NLP",
          "video_url": "https://youtu.be/utOha-d0prc",
          "start_time": 46,
          "end_time": 111,
          "text": "Và đương nhiên là chúng ta sẽ phải cung cấp cho các cái mô hình máy học này rất nhiều dữ liệu Và khi cung cấp cho nó rất nhiều dữ liệu thì nó sẽ đút kết ra được những cái trí thức trung của tất cả những cái dữ liệu đó Và tất cả những cái thành tựu liên quan đến học sâu hiện nay mà có được là nhờ những cái yếu tố sau Đầu tiên đó là yếu tố dữ liệu ngày càng lớn Trước đây thì thật ra internet thì nó đã có từ những năm 1990 trở về trước Nó đã có từ những năm 1990 Tuy nhiên cho đến những năm gần đây chúng ta thấy là mạng internet nó đã có những thành tựu Rất là đáng kể, thứ nhất đó là nó đã phủ sóng được cho rất là nhiều người có thể cùng tiếp cận Và với việc phủ sóng được rất nhiều người có khả năng tiếp cận thì mỗi người trong chúng ta Sẽ có khả năng là một cái người tạo ra một cái nội dung có giá trị trên mạng internet Thông qua các cái mạng xã hội Và các cái mạng xã hội hiện nay nó đã góp phần tạo ra rất nhiều cái kho dữ liệu quan trọng"
        },
        {
          "index": 3,
          "video_id": "Chương 6_utOha-d0prc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 2： Hướng tiếp cận Deep learning cho các bài toán NLP",
          "video_url": "https://youtu.be/utOha-d0prc",
          "start_time": 98,
          "end_time": 161,
          "text": "Sẽ có khả năng là một cái người tạo ra một cái nội dung có giá trị trên mạng internet Thông qua các cái mạng xã hội Và các cái mạng xã hội hiện nay nó đã góp phần tạo ra rất nhiều cái kho dữ liệu quan trọng Cho các cái hệ thống AI hiện nay Để có thể sử dụng các cái dữ liệu đó để huấn luyện cho các cái mô hình máy học Và chúng ta có thể kể đến một số cái mạng xã hội quan trọng Đã góp phần cho cái thành tựu của sử dụng ngôn ngữ tự nhiên ví dụ như là wikipedia Rồi các cái trang mạng xã hội khác ví dụ như là stack overflow Cho những cái bạn nào mà làm về lập trình thì sẽ biết rõ trang này Rồi các cái mạng xã hội khác mà cung cấp rất nhiều những cái dữ liệu dịch thuật Ví dụ như là YouTube Trên trang YouTube thì chúng ta thấy là bên cạnh cái video gốc Nó sẽ có các cái caption Tức là các cái bảng phiên ra cái lời thoại"
        },
        {
          "index": 4,
          "video_id": "Chương 6_utOha-d0prc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 2： Hướng tiếp cận Deep learning cho các bài toán NLP",
          "video_url": "https://youtu.be/utOha-d0prc",
          "start_time": 151,
          "end_time": 211,
          "text": "Trên trang YouTube thì chúng ta thấy là bên cạnh cái video gốc Nó sẽ có các cái caption Tức là các cái bảng phiên ra cái lời thoại Và có rất nhiều những cái tình nguyện viên họ đã phiên ra những cái ngôn ngữ khác nhau Ví dụ như video gốc góc nói về tiếng anh Và cái nội dung của nó đúng như vậy. Ví dụ như video gốc nói về tiếng Anh lời thoại là đương nhiên là họ sẽ tạo ra là tiếng Anh rồi đồng thời sẽ có một số người cộng tác họ tạo ra những cái lời thoại cho rất nhiều những ngôn ngữ khác trong đó có tiếng Việt. Như vậy thì cái kênh Youtube này cũng góp phần là cung cấp cho các cái kho dữ liệu về mấy học hiện nay một cách đáng kể. Và một cái lý do nữa để khiến cho học sâu nó phát triển trong những năm gần đây chính là cái sức mạnh tính toán ngày càng tăng. Trước đây thì chúng ta có các cái CPU và các cái CPU này thì cho dù nó tăng tốc độ đến độ nào đi tăng nữa thì tại một thời điểm nó cũng chỉ có thể thực hiện được khoảng 100. Còn GPU là một cái thiết bị"
        },
        {
          "index": 5,
          "video_id": "Chương 6_utOha-d0prc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 2： Hướng tiếp cận Deep learning cho các bài toán NLP",
          "video_url": "https://youtu.be/utOha-d0prc",
          "start_time": 199,
          "end_time": 256,
          "text": "và các cái CPU này thì cho dù nó tăng tốc độ đến độ nào đi tăng nữa thì tại một thời điểm nó cũng chỉ có thể thực hiện được khoảng 100. Còn GPU là một cái thiết bị phần cưng khác cho phép mình có thể tính toán song song rất nhiều cái phép toán tương tự và độc lập nhau. Tại một thời điểm đó ta có thể tính các cái phép toán tương tự độc lập nhau. Rồi, đồng thời không thể không kể đến các cái mô hình cũng như là các cái thuật toán ngày nay cũng đã cải tiến rất là nhiều. Các cái mô hình có thể giúp cho chúng ta học được nhiều dữ liệu hơn với cái thời gian huấn luyện ít hơn và tránh được rất nhiều những cái hiện tượng đó là overfitting, tức là chỉ là tốt cho dữ liệu trend nhưng mà không tốt cho dữ liệu test. Như vậy thì cái thành tựu của các cái mạng học sâu hiện nay là đã giúp cho Deep Learning phát triển một cách vượt mập."
        },
        {
          "index": 6,
          "video_id": "Chương 6_utOha-d0prc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 2： Hướng tiếp cận Deep learning cho các bài toán NLP",
          "video_url": "https://youtu.be/utOha-d0prc",
          "start_time": 249,
          "end_time": 310,
          "text": "thành tựu của các cái mạng học sâu hiện nay là đã giúp cho Deep Learning phát triển một cách vượt mập. Rồi, và sơ đồ ở bên đây thì chúng ta có thể thấy là trước đây các cái hệ thống của mình nó sẽ dựa trên rule hoặc là những cái hệ thống và gọi là kinh điển thì nó đều phải có những cái gọi là hand design program tức là các cái chương trình này sẽ do những cái chi thức của các cái chuyên gia thi kế, thi kế ra. Và ở cái mức độ là classic machine learning thì nó sẽ có các cái feature, có cái công cụ để mapping, các cái feature và thậm chí là các cái chuyên gia họ sẽ phải thiết kế các cái đặc trưng này. Ví dụ khi chúng ta làm việc trên hình ảnh thì chúng ta biết là mối quan hệ giữa các cái pixel với các cái nguồn đơn chậm chúng ta sẽ thiết kế các cái phép biến đổi là filter và trọng số của các filter sẽ là do"
        },
        {
          "index": 7,
          "video_id": "Chương 6_utOha-d0prc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 2： Hướng tiếp cận Deep learning cho các bài toán NLP",
          "video_url": "https://youtu.be/utOha-d0prc",
          "start_time": 298,
          "end_time": 361,
          "text": "việc trên hình ảnh thì chúng ta biết là mối quan hệ giữa các cái pixel với các cái nguồn đơn chậm chúng ta sẽ thiết kế các cái phép biến đổi là filter và trọng số của các filter sẽ là do chuyên gia họ thiết kế. Tương đương như vậy trong lĩnh vực xử lý ngôn ngược thuyền nhiên chúng ta sẽ có những cái trick, những cái mẹo để giúp cho học các cái mô hình. Ví dụ như LSTM là mô hình học sâu. Tuy nhiên thì trước đây người ta không có sử dụng các cái mô hình mà tự huấn luyện để tạo ra các cái trọng số mà họ phải thi kế trước các trọng số dựa trên một số cái luật. Ví dụ như là mô hình dựa trên bias để thống kê xem là cái từ này xuất hiện thì sát xuất của cái từ tiếp theo sẽ là bao nhiêu họ sẽ còn thống kê. Rồi và gần đây thì Representation Learning và điển hình đó là Deep Learning thì nó sẽ đưa vào những cái simple feature và thậm chí như thầy có đề cập đó là"
        },
        {
          "index": 8,
          "video_id": "Chương 6_utOha-d0prc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 2： Hướng tiếp cận Deep learning cho các bài toán NLP",
          "video_url": "https://youtu.be/utOha-d0prc",
          "start_time": 349,
          "end_time": 411,
          "text": "Rồi và gần đây thì Representation Learning và điển hình đó là Deep Learning thì nó sẽ đưa vào những cái simple feature và thậm chí như thầy có đề cập đó là chúng ta không cần phải đưa đặc trưng của nó mà chúng ta có thể đưa dữ liệu thô vào thì máy vẫn có thể học được. Rồi và lĩnh vực học sâu nó đã có những cái thành tựu vượt bậc trong một số bài toán, không phải trong một số bài toán mà trong rất nhiều bài toán. Và nổi tiếng nhất chính là các cái bài toán về dịch máy, về chatbot, về gợi ý nội dung trong email. Và một số cái mô hình nổi tiếng gần đây chúng ta được nghe rất là nhiều đó chính là Transformer. Tất cả các cái mô hình mà trong sự ý ngôn ngữ tự nhiên hiện nay đều có cái gốc là từ cái kiến trúc Transformer. Ví dụ như cái con chatbot rất là nổi tiếng hiện nay đó là chatgpt, đúng không? chatgpt thì cái chữ T này chính là Transformer. gpt 4.0. là một trong những cái mô hình ngôn ngữ lớn"
        },
        {
          "index": 9,
          "video_id": "Chương 6_utOha-d0prc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 2： Hướng tiếp cận Deep learning cho các bài toán NLP",
          "video_url": "https://youtu.be/utOha-d0prc",
          "start_time": 399,
          "end_time": 452,
          "text": "Ví dụ như cái con chatbot rất là nổi tiếng hiện nay đó là chatgpt, đúng không? chatgpt thì cái chữ T này chính là Transformer. gpt 4.0. là một trong những cái mô hình ngôn ngữ lớn và có cái chữ T thì T ở đây cũng chính là Transformer. Và ở trên đây đó là những cái hình ảnh chụp ra từ một cái con bot của Co-Pilot được phát triển bởi Microsoft. Thì chúng ta có thể yêu cầu dịch một cái đoạn văn mạn từ tiếng Anh sang tiếng Việt. Chúng ta có thể soạn email một cách dễ dàng hơn bằng cách chúng ta chỉ cần gõ vài điều khóa là cái hệ thống nó sẽ tự nhắc cho chúng ta. Cái từ tiếp theo sẽ đi là gì? Chúng ta chỉ cần nhấn phím Tab là lập tức nó có thể hoàn thiện cái nội dung cho mình. Các cái nội dung mà nó sẽ test ở đây nó sẽ dựa trên những cái nội dung trao đổi trước đó của mình một cách tự động."
        },
        {
          "index": 10,
          "video_id": "Chương 6_utOha-d0prc",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 2： Hướng tiếp cận Deep learning cho các bài toán NLP",
          "video_url": "https://youtu.be/utOha-d0prc",
          "start_time": 447,
          "end_time": 452,
          "text": "nó sẽ dựa trên những cái nội dung trao đổi trước đó của mình một cách tự động."
        }
      ]
    },
    {
      "video_id": "Chương 6_O57P9YHZOE0",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giải thích cách **biểu diễn từ bằng vector** và tại sao biểu diễn này quan trọng cho các mô hình máy học (vì các mô hình chỉ thao tác được trên số/vec-tơ/ma trận/tensor). [1]  \n- Các khái niệm sẽ được đề cập: *one-hot vector*, biểu diễn theo *ngữ cảnh* (context-based), ý tưởng rằng \"bạn sẽ biết một từ bằng các từ xung quanh nó\" (câu nói của Ford, 1957), khái niệm **word vector** (còn được nhắc theo thuật ngữ tiếng Anh trong video là *Work Abandoning* hoặc *Work Representation*), và trực quan hóa không gian biểu diễn (embedding space). [1][2][6][10]\n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Tại sao cần biểu diễn từ dưới dạng vector\n- Mọi mô hình machine learning hiện nay tính toán trên các con số (vector × vector, vector × ma trận, ma trận × ma trận, tensor × tensor), nên cần biểu diễn từ dưới dạng các vector số. [1]\n\n### 2.2. One-hot vector — định nghĩa và hạn chế\n- Định nghĩa: với one-hot, mỗi từ trong tập từ điển được biểu diễn bằng một vector có duy nhất một phần tử bằng 1 ở vị trí tương ứng với vị trí của từ trong từ điển, còn lại là 0. [1][4]  \n- Tính chất: các vector one-hot thường trực giao với nhau (orthogonal), tức là không có sự tương đồng nội tại giữa các từ khác nhau. [4][6]  \n- Hạn chế minh họa: khi tìm kiếm ví dụ \"Saigon Hotel\", nếu biểu diễn \"motel\" và \"hotel\" bằng one-hot thì hai vector này trực giao (tương đồng = 0) nên hệ thống không coi chúng là liên quan, dẫn đến kết quả truy vấn thiếu các mục có ý nghĩa tương tự. [5][6]\n\n### 2.3. Biểu diễn theo ngữ cảnh — ý tưởng cơ bản\n- Ý tưởng trung tâm (distributional hypothesis): \"bạn sẽ biết một từ bằng các từ xung quanh nó\" (Ford, 1957) — nghĩa là hai từ có thể thay thế lẫn nhau trong cùng ngữ cảnh sẽ có biểu diễn tương tự. [6][7]  \n- Ví dụ khái quát: trong câu \"I love ___ so much\", các từ you, him, her có thể thế chỗ cho nhau; do đó các từ này có cùng mối quan hệ ngữ cảnh và nên có vector tương tự. [7]\n\n### 2.4. Word vector (Word Representation) và đo độ tương đồng\n- Định nghĩa: vector biểu diễn cho một từ gọi là *word vector* (video cũng nhắc đến các thuật ngữ tiếng Anh *Work Abandoning* / *Work Representation*). [8][10]  \n- Nguyên tắc: các từ có cùng ngữ cảnh sẽ được biểu diễn gần nhau trong không gian vector (tương đồng cao). [8]  \n- Đo độ tương đồng: thường dùng **tích vô hướng (dot product)** để đo mức tương đồng giữa hai vector (được video nhắc là *dot product*). [8]  \n- Minh hoạ số: hai vector của \"hotel\" và \"motel\" có nhiều phần tử cùng dấu (dương/dương, âm/âm) nên tích vô hướng cho thấy mức tương đồng tương đối cao; cũng có các thành phần khác khác dấu phản ánh sự khác biệt ngữ nghĩa giữa hai từ. [9]\n\n(Theo ký hiệu toán: dot product giữa u và v là <u, v> = sum_i u_i * v_i — video nêu khái niệm *tích vô hướng* / *dot product* để đo tương đồng.) [8]\n\n### 2.5. Không gian biểu diễn (embedding space) — trực quan hóa\n- Trong không gian biểu diễn, các từ có vai trò ngữ cảnh giống nhau sẽ nằm gần nhau (ví dụ: Pink, White, Blue nằm gần nhau khi xét câu như \"I like Pink/White/Blue\"). [10][11]  \n- Các lớp từ khác (ví dụ: các động từ) sẽ tạo thành cụm riêng; từ hotel và motel nằm gần nhau; những từ có ngữ cảnh khác biệt (ví dụ: màu sắc so với động từ \"to be\") sẽ nằm rất xa nhau trong không gian. [10][11][12]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa one-hot: biểu diễn một từ là một vector có đúng một phần tử bằng 1 tại vị trí của từ trong từ điển. [1][4]  \n- Ví dụ vấn đề truy vấn: tìm kiếm \"Saigon Hotel\" — nếu dùng one-hot, motel và hotel được coi là độc lập (trực giao) nên các kết quả chứa \"motel\" có thể không được trả về dù ý nghĩa gần nhau. [5][6]  \n- Ví dụ ngữ cảnh thay thế: câu \"I love ___ so much\" có thể điền you/him/her — những từ này có cùng ngữ cảnh nên được biểu diễn tương đồng. [7]  \n- Ví dụ số: hai vector của hotel và motel có các thành phần dương/âm tương ứng, cho thấy tương đồng tương đối chứ không hoàn toàn giống nhau. [9]  \n- Ví dụ cụm từ (clustering): Pink/White/Blue gần nhau trong không gian biểu diễn khi xét ngữ cảnh mẫu \"I like ...\", còn động từ (verbs) nằm thành cụm khác; từ có ngữ cảnh khác biệt sẽ ở vị trí xa. [11][12]\n\nỨng dụng thực tế (theo nội dung video):\n- Việc biểu diễn từ bằng vector là thiết yếu cho mọi mô hình ML/NLP vì các mô hình cần số để tính toán. [1]  \n- Biểu diễn theo ngữ cảnh (word vectors) giúp cải thiện các tác vụ như tìm kiếm/IR, phân loại, mô hình ngôn ngữ... (video minh họa bằng ví dụ truy vấn và các ví dụ thay thế ngữ cảnh). [5][7][8]\n\nNgoài ra video đề cập một phương án khác: có thể dùng một vector để biểu diễn cho cả câu hoặc đoạn văn (mô hình *backward* được nhắc đến là một cách để đưa vector biểu diễn cho câu/đoạn). [2]\n\n---\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt:  \n  - Biểu diễn từ bằng vector là yêu cầu cơ bản vì mô hình ML thao tác trên số; one-hot là cách đơn giản nhưng có nhiều hạn chế (không thể hiện tương đồng ngữ nghĩa). [1][4][6]  \n  - Biểu diễn theo ngữ cảnh (word vector / word representation) dựa trên ý tưởng rằng từ nào xuất hiện trong ngữ cảnh tương tự sẽ có vector tương tự; tương đồng giữa hai vector thường đo bằng tích vô hướng (dot product). [6][7][8]  \n  - Không gian biểu diễn cho phép trực quan hóa các cụm từ có vai trò ngữ cảnh tương đồng (màu sắc, động từ, hotel/motel, v.v.). [10][11][12]\n\n- Tầm quan trọng: Biểu diễn từ bằng vector chuyển thông tin ngữ nghĩa từ dạng rời rạc (từ) sang dạng số liên tục, từ đó cho phép các mô hình học được các mối quan hệ, tương đồng và áp dụng cho nhiều tác vụ NLP. [1][6][8]\n\n- Liên hệ với tài liệu/bài giảng khác: Video nhắc rằng khi đọc tài liệu sau này sẽ gặp các thuật ngữ tương tự về *word vector / word representation* và cách dùng chúng trong các tài liệu chuyên sâu hơn. [10]",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 6_O57P9YHZOE0",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 3： Biểu diễn từ bằng Vector",
          "video_url": "https://youtu.be/O57P9YHZOE0",
          "start_time": 0,
          "end_time": 63,
          "text": "Trong phần tiếp theo thì chúng ta sẽ cùng tìm hiểu về biểu diễn từ với vector Thì biểu diễn từ bằng vector là có một vai trò cực kỳ quan trọng trong các mô hình máy học Tại sao? Tất cả các mô hình máy học hiện nay đều phải xử lý tính toán dưới dạng là các con số Và các con số này thì nó sẽ có thể tính toán như là vector x vector, vector x ma trận hoặc là ma trận x ma trận Và thậm chí là chúng ta có thể tính toán trên khối lượng lớn, ví dụ như là tensor x tensor Như vậy thì nhu cầu là làm sao biểu diễn được một từ dưới dạng các vector đó là một trong những nhu cầu rất là phức thiết Và các kỹ thuật hiện nay thường được sử dụng đó chính là Đầu tiên là chúng ta có thể biểu diễn dưới dạng one-hot vector Tức là một từ của mình trong tập từ điển nó xuất hiện ở vị trí nào Thì tương ứng là trong cái vector Cái vector của mình, cái vị trí đó sẽ bật lên là 1 và những phần của từ còn lại sẽ bật là 0"
        },
        {
          "index": 2,
          "video_id": "Chương 6_O57P9YHZOE0",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 3： Biểu diễn từ bằng Vector",
          "video_url": "https://youtu.be/O57P9YHZOE0",
          "start_time": 50,
          "end_time": 110,
          "text": "Tức là một từ của mình trong tập từ điển nó xuất hiện ở vị trí nào Thì tương ứng là trong cái vector Cái vector của mình, cái vị trí đó sẽ bật lên là 1 và những phần của từ còn lại sẽ bật là 0 Chút nữa chúng ta sẽ nói rõ hơn cái ví dụ này Cái thứ hai đó là chúng ta có thể sử dụng cái mô hình backward Tức là cái vector của mình nó có thể biểu diễn cho một câu hoặc là một đoạn văn Rồi và thứ ba tiếp theo đó chính là chúng ta có thể biểu diễn từ bằng ngưỡng cảnh Tức là một từ nếu như đứng một mình nó Thì nó sẽ không có được ý nghĩa trọng mẹ Mà chúng ta phải đối chiếu nó trong cái ngưỡng cảnh xung quanh Ví dụ như cái từ ông già hồi nãy đó Trong cái từ tiếng Việt Cái từ ông già này nếu mà mình để một nghĩa thông thường Thì mình sẽ hiểu đó là một người lớn tuổi Nhưng mà nếu như mình đang nói về hai người bạn thân với nhau Đang trao đổi về chuyện gia đình"
        },
        {
          "index": 3,
          "video_id": "Chương 6_O57P9YHZOE0",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 3： Biểu diễn từ bằng Vector",
          "video_url": "https://youtu.be/O57P9YHZOE0",
          "start_time": 96,
          "end_time": 160,
          "text": "Cái từ ông già này nếu mà mình để một nghĩa thông thường Thì mình sẽ hiểu đó là một người lớn tuổi Nhưng mà nếu như mình đang nói về hai người bạn thân với nhau Đang trao đổi về chuyện gia đình Thì có thể là chúng ta hiểu ông già này hàm ý là cái người Nhưng mà nếu như mình đang nói về hai người bạn thân với nhau Thì người cha của mình Như vậy thì muốn biết cái ý nghĩa của cái từ ông già này như thế nào Thì chúng ta phải xem cái bối cảnh xung quanh Để biết được cái ý nghĩa thực sự của cái từ này Rồi và chúng ta sẽ đến với cái cách biểu diễn đầu tiên Đó là One Hot Rector Thì trước đây chúng ta xem một cái từ Thì được xem là một phần tử trong một cái từ điển Trước đây ví dụ như chúng ta có Cái cách biểu diễn như vậy One Hot Rector như sao Thì chúng ta sẽ biết là một cái từ này là một cái từ điển Của One Hot đó là cái từ One Hot Chính là có một cái con số một Giờ xem cái việc nó gọi là vector đơn trội Tiếng Việt của mình nó là vector đơn trội"
        },
        {
          "index": 4,
          "video_id": "Chương 6_O57P9YHZOE0",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 3： Biểu diễn từ bằng Vector",
          "video_url": "https://youtu.be/O57P9YHZOE0",
          "start_time": 149,
          "end_time": 211,
          "text": "Chính là có một cái con số một Giờ xem cái việc nó gọi là vector đơn trội Tiếng Việt của mình nó là vector đơn trội Tức là chỉ có một thằng trội lên thôi Và còn lại là sẽ là số không Và ý nghĩa của cái số một này Đó chính là cái vị trí của cái số một này    Vì trí của cái từ trong cái từ điển của mình Là vị trí của từ trong từ điển Ví dụ như ở đây chúng ta sẽ có hai từ là motel và hotel Thì cái vị trí của cái từ motel là nó xuất hiện ở đây Trong tập từ điển Còn vị trí của từ hotel thì nó xuất hiện ở đây Nên ở đây nó sẽ bật vào một Tất cả những cái vị trí còn lại sẽ để là số không Thì đây là một cái cách biểu diễn rất là đơn giản và dễ hiểu Tuy nhiên với cái cách biểu diễn này thì nó vẫn Chứa trong nó những cái vấn đề Vấn đề đầu tiên đó là Khi chúng ta tìm kiếm với từ khóa trên mạng internet đi Ví dụ vậy Và chúng ta tìm kiếm với từ là Saigon Hotel"
        },
        {
          "index": 5,
          "video_id": "Chương 6_O57P9YHZOE0",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 3： Biểu diễn từ bằng Vector",
          "video_url": "https://youtu.be/O57P9YHZOE0",
          "start_time": 199,
          "end_time": 260,
          "text": "Tuy nhiên với cái cách biểu diễn này thì nó vẫn Chứa trong nó những cái vấn đề Vấn đề đầu tiên đó là Khi chúng ta tìm kiếm với từ khóa trên mạng internet đi Ví dụ vậy Và chúng ta tìm kiếm với từ là Saigon Hotel Chúng ta tìm kiếm với từ là Saigon Hotel Thì như vậy thì cái từ khóa Saigon Hotel Là lẽ ra nó sẽ phải trả về các cái văn bản Hoặc là các cái trang web Mà có chứa cả hai từ khóa là Saigon Hotel và Saigon Hotel Tại vì xét về mặt ý nghĩa là chúng ta đang tìm Một cái nơi để mà mình dừng chân Mình nghỉ ngơi Thì Saigon Hotel và Motel thì nó cũng sẽ giống nhau Về mặt ý nghĩa đó Thì lẽ ra cái kết quả nó sẽ phải trả về Nhưng nếu như chúng ta sử dụng Hai cái từ đó Và dưới dạng là cái vector one hot Nếu như chúng ta biểu diễn hai cái từ Motel và Hotel Dưới dạng là cái one hot vector Thì cả hai cái vector này Nó sẽ có cái tính chất trực giao với nhau Nên cái sự tương đồng là bằng không"
        },
        {
          "index": 6,
          "video_id": "Chương 6_O57P9YHZOE0",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 3： Biểu diễn từ bằng Vector",
          "video_url": "https://youtu.be/O57P9YHZOE0",
          "start_time": 249,
          "end_time": 311,
          "text": "Nếu như chúng ta biểu diễn hai cái từ Motel và Hotel Dưới dạng là cái one hot vector Thì cả hai cái vector này Nó sẽ có cái tính chất trực giao với nhau Nên cái sự tương đồng là bằng không Mà một cái Hai cái từ khóa Mà cái từ khóa là một cái từ khóa Có cái độ tương đồng là không Tức là khi chúng ta tìm kiếm Thì rõ ràng là cái kết quả trả về Những cái từ Motel nó sẽ không được trả về Và nếu như chúng ta search với từ khóa là Hotel Rồi như vậy thì với những cái điểm yếu Của cái cách biểu diễn one hot and one day Thì Các cái từ nó đã xem như là Độc lập nhau Hotel và Motel nó xem như là độc lập nhau Nó không xem xét đến cái ý nghĩa của cái từ đó Là gì Thì chúng ta sẽ tìm kiếm Và chúng ta sẽ nảy sinh ra Cái cách biểu diễn thứ hai Đó là biểu diễn từ bằng ngữ cảnh Và chúng ta có một cái câu nói rất là nổi tiếng Của ông Ford Vào năm 1957 Đó là bạn sẽ biết được một cái từ Bằng cách Là những cái từ xung quanh nó"
        },
        {
          "index": 7,
          "video_id": "Chương 6_O57P9YHZOE0",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 3： Biểu diễn từ bằng Vector",
          "video_url": "https://youtu.be/O57P9YHZOE0",
          "start_time": 299,
          "end_time": 361,
          "text": "Và chúng ta có một cái câu nói rất là nổi tiếng Của ông Ford Vào năm 1957 Đó là bạn sẽ biết được một cái từ Bằng cách Là những cái từ xung quanh nó Bằng những cái từ xung quanh nó Lấy ví dụ ha Chúng ta sẽ Lấy ví dụ là Có một cái câu tiếng Anh là I love so much I love so much Thì cái từ mà chúng ta có thể điền vô ở đây Được á Thì nó sẽ có cùng cái Cửa cảnh với nhau Nó sẽ có cùng ngữ cảnh với nhau Ví dụ I love you so much I love him so much Và I love her so much Thì các cái từ you, him, her Đều có thể thay thế để điền vô Cái vị trí ở giữa này Do đó chúng ta có thể nói là từ you, him, her Nó sẽ có cùng Cái mối quan hệ về mặt ngữ cảnh Tức là từ này có thể thay thế được cho từ kia Và Như vậy thì Một cái vector Khi biểu diễn Thì nó sẽ gọi là word vector"
        },
        {
          "index": 8,
          "video_id": "Chương 6_O57P9YHZOE0",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 3： Biểu diễn từ bằng Vector",
          "video_url": "https://youtu.be/O57P9YHZOE0",
          "start_time": 348,
          "end_time": 410,
          "text": "Cái mối quan hệ về mặt ngữ cảnh Tức là từ này có thể thay thế được cho từ kia Và Như vậy thì Một cái vector Khi biểu diễn Thì nó sẽ gọi là word vector Vector biểu diễn cho một từ Thì nó gọi là word vector Và đó là cái vector biểu diễn cho từ Trong đó Các cái từ có cùng ngữ cảnh Thì được biểu diễn tương đồng nhau Nghĩa là sao Những cái từ nào mà có thể thay thế cho nhau Trong cùng một ngữ cảnh Thì nó có khả năng là Phải có cái tính tương đồng nhau Để đo được cái sự tương đồng Của hai cái vector Thì chúng ta sẽ sử dụng cái công thức Nó là tích vô hướng Hay là tên tiếng Anh nó là dot product Và chúng ta sẽ có một cái ví dụ như sau Hai cái vector hotel và hotel Thì hai cái vector này Chúng ta sẽ thấy nó có cái sự tương đồng Tại sao? Tại vì ở cái phần tử đầu tiên Chúng ta thấy đó là một cái giá trị dương Là khoảng 0.28 Thì ở đây cũng là một cái giá trị dương 0.4"
        },
        {
          "index": 9,
          "video_id": "Chương 6_O57P9YHZOE0",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 3： Biểu diễn từ bằng Vector",
          "video_url": "https://youtu.be/O57P9YHZOE0",
          "start_time": 399,
          "end_time": 459,
          "text": "Chúng ta sẽ thấy nó có cái sự tương đồng Tại sao? Tại vì ở cái phần tử đầu tiên Chúng ta thấy đó là một cái giá trị dương Là khoảng 0.28 Thì ở đây cũng là một cái giá trị dương 0.4 Ở đây là một cái giá trị dương là 0.79 Ở đây là một cái giá trị dương là 0.79 Thì ở đây là một cái giá trị dương 0.58 như vậy là có cái sự tương đồng Và khi đến cái giá trị thứ ba Nó rớt xuống còn là giá trị âm Thì ở đây nó cũng ra giá trị âm Rồi riêng cái phần tử thứ Tư Thì nó sẽ khác nhau, đây là dương Còn đây là âm Thì điều này cũng dễ hiểu thôi tại vì bản chất là hai từ hotel và motel Nó cũng mặc dù nó giống nhau Nó có một số chỗ giống nhau Về mặt ý nghĩa nhưng Nó cũng sẽ có những cái điểm khác biệt Chứ không thể nào mà hai cái thần Hai vector này nó tương đồng Tương đồng nhau hoàn toàn Rồi thành phần thứ năm Cũng là các con số dương là 0.1 Rồi đây là 0.2 Rồi đây là con số âm Thì đây cũng là con số âm Như vậy là hai vector này có cái tính tương đồng tương đối là cao Xét về cái cách biểu diễn Theo nguồn cảnh này"
        },
        {
          "index": 10,
          "video_id": "Chương 6_O57P9YHZOE0",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 3： Biểu diễn từ bằng Vector",
          "video_url": "https://youtu.be/O57P9YHZOE0",
          "start_time": 449,
          "end_time": 512,
          "text": "Rồi đây là 0.2 Rồi đây là con số âm Thì đây cũng là con số âm Như vậy là hai vector này có cái tính tương đồng tương đối là cao Xét về cái cách biểu diễn Theo nguồn cảnh này Và một vector nó còn một cái tên gọi khác Trong các thuật ngữ tiếng Anh Đó chính là Work Abandoning hoặc là Work Representation Thì nếu như sau này chúng ta Xem các cái tài liệu Thì chúng ta thấy là khi nói về Work Vector Hoặc là khi nói về Work Abandoning Hoặc là khi nói về Work Representation Thì tất cả đều có chung một cái ý nghĩa Đó chính là làm sao có thể biểu diễn Một cái từ dưới dạng một cái vector Và để trực quan hóa Cái không gian Abandoning Ở đây chúng ta đang sử dụng đến cái khái niệm ở đây ha Sử dụng cái cách dùng từ ở đây ha Để trực quan hóa một cái từ Trong một cái không gian Abandoning Thì chúng ta sẽ có cái ví dụ này là Ví dụ như sau Ví dụ như các cái từ Pink, White, Blue Thì đây là những cái từ Mà đều Có cái vai trò ngưỡng cảnh giống nhau"
        },
        {
          "index": 11,
          "video_id": "Chương 6_O57P9YHZOE0",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 3： Biểu diễn từ bằng Vector",
          "video_url": "https://youtu.be/O57P9YHZOE0",
          "start_time": 498,
          "end_time": 560,
          "text": "Trong một cái không gian Abandoning Thì chúng ta sẽ có cái ví dụ này là Ví dụ như sau Ví dụ như các cái từ Pink, White, Blue Thì đây là những cái từ Mà đều Có cái vai trò ngưỡng cảnh giống nhau Ví dụ như là Mình thích màu vàng I like Pink với lại mình thích màu Xin lỗi mình thích màu vàng I like Yellow, I like Pink, I like White I like Blue Thì tất cả những cái từ Pink, White, Blue Nó đều có thể thay thế được cho nhau trong cái cảnh đó Do đó thì cái từ Pink, White và Blue Nó sẽ nằm gần nhau với nhau Trong cái không gian Abandoning Tương tự như vậy các cái động từ M, E, R, World, World Thì nằm gần nhau Rồi Từ Hotel và Motel Sẽ nằm gần nhau Và những cái từ nào mà nó Khác biệt nhau về mặt ngữ cảnh Thì chúng ta sẽ thấy là Cái vị trí của nó trong không gian Cách xa nhau Ví dụ như Pink, White, Blue"
        },
        {
          "index": 12,
          "video_id": "Chương 6_O57P9YHZOE0",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 3： Biểu diễn từ bằng Vector",
          "video_url": "https://youtu.be/O57P9YHZOE0",
          "start_time": 547,
          "end_time": 573,
          "text": "Và những cái từ nào mà nó Khác biệt nhau về mặt ngữ cảnh Thì chúng ta sẽ thấy là Cái vị trí của nó trong không gian Cách xa nhau Ví dụ như Pink, White, Blue Thì nó sẽ rất là xa Đối thoái với lạy các cái động từ Tobe ở đây Vì chúng nó có Cái mối quan hệ về mặt ngữ cảnh Rất là khác nhau Ừ Các bạn có thể nhìn những cái hình ảnh của tôi"
        }
      ]
    },
    {
      "video_id": "Chương 6_UJNyIptbcNM",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- **Mục tiêu chính của bài giảng**: Giới thiệu và phân tích mô hình Word2Vec — một mô hình biểu diễn từ rất phổ biến — bao gồm hai phương pháp con (submodels) là SkipRam và *Continuous Better Work* (như trình bày trong video) và đi sâu vào kiến trúc SkipRam, hàm mục tiêu và cách ước lượng tham số. [1]\n\n- **Các khái niệm sẽ được đề cập**:\n  - Ý tưởng cơ bản của SkipRam: dự đoán các từ ngữ cảnh xung quanh một từ ở giữa (WT). [2][3]\n  - Cửa sổ ngữ cảnh (window size) và cách xác định các vị trí ngữ cảnh (t−2, t−1, t+1, t+2 trong ví dụ). [4]\n  - Hàm khả năng (likelihood) tổng quát cho toàn bộ câu và chuyển sang hàm loss bằng âm log-likelihood trung bình. [6][8][9]\n  - Kiến trúc mạng neural đơn giản (một lớp ẩn duy nhất) với đầu vào one-hot, ma trận tham số W (kích thước V×N), vector ẩn H = W × X và đầu ra Softmax. [10][11][12][13]\n  - Cách ánh xạ tham số θ sang các ma trận trọng số (W trái / W phải) trong mô hình. [16][17]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 Ý tưởng cơ bản của SkipRam\n- SkipRam học mối quan hệ ngữ cảnh bằng cách **dự đoán các từ xung quanh** khi biết một từ ở giữa (ký hiệu là WT). Ví dụ, khi biết WT, ta dự đoán w_{t−2}, w_{t−1}, w_{t+1}, w_{t+2}. [2][3]\n- Cửa sổ ngữ cảnh (window size) là một tham số cố định M (ví dụ M = 2 trong bài giảng). Các từ càng xa tâm (WT) thì khó dự đoán hơn, vì vậy thường dùng một bán kính ngắn. [4]\n\n### 2.2 Mô hình hóa xác suất (Conditional probabilities) và likelihood\n- Mỗi dự đoán được mô hình hóa dưới dạng xác suất có điều kiện, ví dụ P(w_{t−2} | w_t), P(w_{t−1} | w_t), P(w_{t+1} | w_t), P(w_{t+2} | w_t). [3]\n- Với một câu có T từ, likelihood của toàn bộ dữ liệu theo SkipRam (với cửa sổ M) là tích của các xác suất ngữ cảnh cho mỗi vị trí t và mỗi g ∈ {−M,...,M}, g ≠ 0:\n  - L = ∏_{t=1}^{T} ∏_{g=−M, g≠0}^{M} P(w_{t+g} | w_t) . [6][7]\n  - (Lưu ý: g ≠ 0 vì khi g = 0 thì tử vụ P(w_t | w_t) = 1 là vô nghĩa để đưa vào tích.) [7]\n\n### 2.3 Chuyển sang hàm Loss (negative average log-likelihood)\n- Vì tích nhiều xác suất nhỏ có xu hướng làm giá trị rất nhỏ và gây vấn đề số học, ta lấy log của likelihood. Log của tích là tổng các log; do đó thường tối ưu **âm của log-likelihood trung bình**:\n  - Loss = −(1/(T * 2M)) ∑_{t} ∑_{g≠0} log P(w_{t+g} | w_t)  (công thức tổng quát theo nội dung video). [8][9]\n- Mục tiêu huấn luyện là minimize hàm loss này trên toàn bộ dữ liệu. [8][9]\n\n### 2.4 Kiến trúc mạng để ước lượng P(·|·)\n- Để ước lượng P(w_{t+g} | w_t), ta dùng một mạng neural rất đơn giản: **một lớp ẩn duy nhất** và hàm đầu ra là **Softmax**. [10]\n- Cách biểu diễn và phép biến đổi:\n  - Input X là vector one-hot có chiều V (V = kích thước vocabulary), ví dụ X = [0,0,0,1,0,...] với 1 tại vị trí của w_t. V có thể rất lớn (có thể lên đến ~1 triệu theo ví dụ). [12][13]\n  - Ma trận trọng số W (gọi là W trái) có kích thước V × N (V hàng, N cột). N là chiều embedding (vector biểu diễn từ). [11]\n  - Tính vector ẩn H = W^T × X (trong video trình bày đơn giản H = W × V theo ký hiệu, ý nghĩa là lấy hàng/col tương ứng với one-hot để ra embedding H). Vì X one-hot, H tương ứng là hàng (hoặc cột) embedding của từ WT. [11][12][13]\n  - Lớp output sử dụng một ma trận W' (ký hiệu là W phải, kích thước N × V) để tính điểm cho mỗi từ trong vocabulary: score_i = (W'_{:,i})^T · H. Sau đó áp Softmax lên các score để ra phân bố xác suất trên từ (tức các giá trị trong khoảng [0,1] và tổng = 1). [13][14]\n- Do đó P(w_{t+g} = word_i | w_t) = softmax_i( scores ) với scores = W'^T · H. [13][14]\n\n### 2.5 Tham số mô hình và ký hiệu θ\n- Toàn bộ tham số của mô hình (θ) thực chất là các ma trận trọng số: θ₁ tương ứng với W (embedding từ input), θ₂ tương ứng với W' (ma trận đầu ra). Trong bài giảng, ký hiệu θ được thay bằng W và W' trong hình minh họa. [16][17]\n- Sau khi tối ưu, hàng/cột của ma trận W (tùy quy ước) chính là **vector embedding** của từng từ. [16]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa dịch chuyển vị trí trung tâm: khi dịch tâm sang từ \"banking\" (là WT tại thời điểm nào đó), ta sẽ dự đoán xác suất các từ xung quanh như \"into\" (t−1), \"crisis\" (t+1), ... và ước lượng P cho từng vị trí ngữ cảnh theo kiến trúc nêu trên. [5]\n- Ứng dụng thực tế: từ việc tối ưu hóa hàm loss trên một tập văn bản, ta thu được embedding (trong ma trận W) có thể dùng cho nhiều tác vụ NLP (mặc dù video chỉ trình bày quá trình học và kiến trúc chứ không liệt kê chi tiết các ứng dụng). [1][16]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Word2Vec gồm hai phương pháp con (SkipRam và *Continuous Better Work* theo video), và phần trình bày tập trung vào SkipRam: dự đoán từ ngữ cảnh khi biết từ trung tâm WT. [1][2]\n  - Mô hình hóa bằng xác suất có điều kiện P(w_{t+g}|w_t), xây dựng likelihood dưới dạng tích các xác suất theo tất cả vị trí t và g ∈ {−M..M}\\{0}, rồi tối ưu bằng cách minimize âm log-likelihood trung bình để tránh vấn đề số học. [6][7][8][9]\n  - Kiến trúc sử dụng mạng neural một lớp ẩn với input one-hot, ma trận trọng số W (V×N) cho embedding, vector ẩn H = W × X, và đầu ra Softmax qua một ma trận W' để ra phân bố xác suất. Tham số θ tương ứng với các ma trận W và W'. [10][11][12][13][16][17]\n\n- Tầm quan trọng của nội dung:\n  - Mô hình Word2Vec (với kiến trúc và hàm mục tiêu như trên) là nền tảng để học vector biểu diễn từ hiệu quả từ dữ liệu thô, giúp chuyển thông tin rời rạc (từ) sang không gian liên tục (embedding) và là bước cơ bản cho nhiều mô hình NLP sau này. (Ý tưởng này là trọng tâm của bài giảng). [1][10][16]\n\n- Liên hệ với các bài giảng khác:\n  - Video nhắc đến Word2Vec là một phần trong chương 6; phần tiếp theo trong loạt bài sẽ đào sâu hai biến thể (SkipRam và *Continuous Better Work*) và chi tiết hơn về từng phương pháp (đã nêu khái quát trong phần mở đầu). [1]\n\n---\n\nGhi chú: Tất cả nội dung tóm tắt trên được trích trực tiếp từ các đoạn video đã cho (các chunk [1]..[17]) và giữ nguyên các ký hiệu/thuật ngữ theo bản gốc trong video.",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 6_UJNyIptbcNM",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_1： Mô hình Word2Vec",
          "video_url": "https://youtu.be/UJNyIptbcNM",
          "start_time": 0,
          "end_time": 62,
          "text": "Và trong phần cuối cùng thì chúng ta sẽ cùng tìm hiểu về một trong những mô hình rất là nổi tiếng, phổ biến hiện nay đó chính là mô hình Word2Vec. Mô hình Word2Vec thì được Thomas Piccolo và các cộng sự giới thiệu vào năm 2013 tính đến thời điểm hiện nay thì nó đã được hơn 11 năm. Và mô hình này thì nó sẽ bao gồm hai cái mô hình CON, tức là hai cái phương pháp hai hướng tiếp cận. CON đó chính là SkipRam và Continuous Better Work. Thì chúng ta sẽ nói chi tiết hơn về hai cái mô hình này trong những cái phần tiếp theo. Đầu tiên đó là cái mô hình SkipRam. Thì ý tưởng của SkipRam đó chính là chúng ta sẽ dự đoán các cái từ xung quanh. Chúng ta sẽ tìm hiểu về mô hình. Chúng ta sẽ dự đoán các cái từ xung quanh khi có một từ ở giữa. Có một từ ở giữa. Thì ở đây cái từ ở giữa mà chúng ta tô vàng ở đây chính là cái từ thứ WT."
        },
        {
          "index": 2,
          "video_id": "Chương 6_UJNyIptbcNM",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_1： Mô hình Word2Vec",
          "video_url": "https://youtu.be/UJNyIptbcNM",
          "start_time": 44,
          "end_time": 111,
          "text": "Thì ý tưởng của SkipRam đó chính là chúng ta sẽ dự đoán các cái từ xung quanh. Chúng ta sẽ tìm hiểu về mô hình. Chúng ta sẽ dự đoán các cái từ xung quanh khi có một từ ở giữa. Có một từ ở giữa. Thì ở đây cái từ ở giữa mà chúng ta tô vàng ở đây chính là cái từ thứ WT. Và cái từ ở giữa này chúng ta sẽ phải đoán xem cái từ thứ T trừ 2 là gì. Đoán cái từ thứ T trừ 1 là gì. Đoán cái từ thứ T cộng 1 là gì. Và đoán cái từ thứ T cộng 2 là gì. Như vậy từ trái sang phải chúng ta thấy cái chỉ số của mình là chạy từ T trừ 2, T trừ 1. Đoán cái từ thứ T cộng 2 là gì. Và T cộng 1, T cộng 2. Như vậy chúng ta sẽ phải đoán cái từ thứ T khi chúng ta sẽ có cho trước cái từ thứ WT. Và chúng ta sẽ phải đoán cái từ thứ T trừ 1, T trừ 2, T cộng 1, T cộng 2. Thì đó là cái ý tưởng của SkipRam khi học cái mối quan hệ về mặt ngữ cảnh của từ. Rồi. Và ở đây chúng ta sẽ mô hình hóa"
        },
        {
          "index": 3,
          "video_id": "Chương 6_UJNyIptbcNM",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_1： Mô hình Word2Vec",
          "video_url": "https://youtu.be/UJNyIptbcNM",
          "start_time": 99,
          "end_time": 160,
          "text": "Thì đó là cái ý tưởng của SkipRam khi học cái mối quan hệ về mặt ngữ cảnh của từ. Rồi. Và ở đây chúng ta sẽ mô hình hóa cái việc dự đoán này dưới dạng là một cái công thức sát xuất. Để dự đoán cái từ thứ T trừ 2 thì chúng ta sẽ đưa về cái biểu diễn là cái công thức sát xuất là P của WT trừ 2. Tức là đoán cái từ thứ T trừ 2 cho trước hay là khi biết trước cái từ thứ T. Như vậy là đây là một cái công thức sát xuất có điều kiện. Công thức sát xuất có điều kiện. Và điều kiện ở đây là chúng ta phải biết trước cái từ WT. Tương tự như vậy thì cho cái từ thứ T trừ 1 chúng ta sẽ có là phải tính được cái sát xuất của cái từ thứ T trừ 1 khi cho trước cái từ thứ T. Rồi. P của WT cộng 1 khi biết trước từ thứ T."
        },
        {
          "index": 4,
          "video_id": "Chương 6_UJNyIptbcNM",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_1： Mô hình Word2Vec",
          "video_url": "https://youtu.be/UJNyIptbcNM",
          "start_time": 146,
          "end_time": 210,
          "text": "Tương tự như vậy thì cho cái từ thứ T trừ 1 chúng ta sẽ có là phải tính được cái sát xuất của cái từ thứ T trừ 1 khi cho trước cái từ thứ T. Rồi. P của WT cộng 1 khi biết trước từ thứ T. Vân vân. Và hai cái từ, hai cái từ này thì tương ứng. Nó chính là cái ngữ cảnh của WT.  Nó chính là cái ngữ bên ngoài với cái cửa sổ. Cái cửa sổ là bằng 2. Cái kích thước của cửa sổ là bằng 2. Nghĩa là sao? Một từ nếu mà chúng ta đoán ra càng xa thì rất là khó. Cái từ thứ 10, 15, 20 rất là khó. Thông thường khi có một từ ở giữa chúng ta có thể đoán được những cái từ phía trước đó trong một cái bán kính tương đối nhỏ thôi. Thì trong trường hợp này cái bán kính của mình nó được thể hiện là Windows size là bằng 2. Tương tự như vậy cho WT. Nếu mà chúng ta đoán ra ngữ cảnh bên ngoài bên đây bên tay phải nó sẽ là Windows size bằng 2. Và cái từ mà mình ở giữa thì nó sẽ là tại cái thời điểm thứ 3."
        },
        {
          "index": 5,
          "video_id": "Chương 6_UJNyIptbcNM",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_1： Mô hình Word2Vec",
          "video_url": "https://youtu.be/UJNyIptbcNM",
          "start_time": 200,
          "end_time": 261,
          "text": "Tương tự như vậy cho WT. Nếu mà chúng ta đoán ra ngữ cảnh bên ngoài bên đây bên tay phải nó sẽ là Windows size bằng 2. Và cái từ mà mình ở giữa thì nó sẽ là tại cái thời điểm thứ 3. Rồi và tương tự như vậy chúng ta sẽ dịch chuyển, dịch chuyển sang cái từ tiếp theo. Như vậy là T của mình sẽ được dịch chuyển sang cái từ là từ banking. Và chúng ta sẽ phải dự đoán xem cái từ kết đó hai bước đó là sát xuất là bao nhiêu phần trăm. Đó là từ banking. Sát xuất bao nhiêu phần trăm cái từ thứ T trừ 1 đó là into. Rồi sát xuất của cái từ tiếp theo, ngay tiếp theo là WT cộng 1 là crisis. Rồi sát xuất của cái từ thứ T cộng 2 là S là bao nhiêu phần trăm. Như vậy là nhiệm vụ của chúng ta là phải đi xây dựng một cái mô hình để làm sao ước lượng được cái cái sát xuất này. Và với mỗi cái thời điểm thứ T, với mỗi cái thời điểm thứ T. Với T chạy từ mỗi cái thời điểm thứ T."
        },
        {
          "index": 6,
          "video_id": "Chương 6_UJNyIptbcNM",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_1： Mô hình Word2Vec",
          "video_url": "https://youtu.be/UJNyIptbcNM",
          "start_time": 246,
          "end_time": 311,
          "text": "Như vậy là nhiệm vụ của chúng ta là phải đi xây dựng một cái mô hình để làm sao ước lượng được cái cái sát xuất này. Và với mỗi cái thời điểm thứ T, với mỗi cái thời điểm thứ T. Với T chạy từ mỗi cái thời điểm thứ T. Một cho đến T lớn. Thì một ở đây là cái từ đầu tiên của câu. Và T lớn là cái từ cuối cùng của câu ha. Thì chúng ta cho biết trước cái từ ở giữa. Chúng ta cho biết trước cái từ ở giữa. Là WT. Và chúng ta sẽ dự đoán các cái từ ngữ cảnh xung quanh với một cái cửa sổ cố định là M. Thì trong trường hợp ví dụ ở trên M là bằng 2. Như vậy thì ta sẽ có cái công thức cho cái likelihood. Đó là L. Thế ta là bằng tích. Với T chạy từ 1 cho đến T. Với T chạy từ 1 cho đến T hoa. Và ở đây chúng ta sẽ phải làm sao đó. Để nhân sát xuất của những cái từ ngữ cảnh bên ngoài. Với G là chạy từ trừ M cho đến M. Với G là phải khác không. Tại vì trong trường hợp G bằng 0."
        },
        {
          "index": 7,
          "video_id": "Chương 6_UJNyIptbcNM",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_1： Mô hình Word2Vec",
          "video_url": "https://youtu.be/UJNyIptbcNM",
          "start_time": 300,
          "end_time": 363,
          "text": "Để nhân sát xuất của những cái từ ngữ cảnh bên ngoài. Với G là chạy từ trừ M cho đến M. Với G là phải khác không. Tại vì trong trường hợp G bằng 0. Thì T cộng G chính là bằng T. Tức là cái từ. Tức là P của WT khi cho trước. Tức là P của WT khi cho trước.  T. Đó là mình đoán cái từ ở giữa. Khi biết từ ở giữa thì cái này là một cái điều rất là vô lý. Sát xuất này thì nó luôn luôn là bằng 1 rồi. Do đó chúng ta cũng không cần thiết phải đưa vào đây nữa. Do đó thì G sẽ phải khác không. Và cái công thức likelihood này. M thê ta này. Thì mình sẽ phải làm sao để thực đại hóa. Mình sẽ đi tính. Tính Max. Được. Và cái hàm mục tiêu của mình thông thường nó sẽ là hàm Loss. Và mình phải đi Minimize. Và cộng với việc là cái việc tính tích này. Thì các cái giá trị sát xuất này thông thường là những con số rất là bé."
        },
        {
          "index": 8,
          "video_id": "Chương 6_UJNyIptbcNM",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_1： Mô hình Word2Vec",
          "video_url": "https://youtu.be/UJNyIptbcNM",
          "start_time": 348,
          "end_time": 410,
          "text": "Và cái hàm mục tiêu của mình thông thường nó sẽ là hàm Loss. Và mình phải đi Minimize. Và cộng với việc là cái việc tính tích này. Thì các cái giá trị sát xuất này thông thường là những con số rất là bé. Nó bé 1. Và tích của các sát xuất này. Giangô. Thì nó sẽ có xu hướng là tiến đến 0. Dẫn đến là. Cái khả năng biểu diễn của máy tính của mình. Khi làm việc với tích của các cái con số. Nhỏ không? Rất là là là thấp. Tức là nó sẽ dần làm tròn thành số 0. Do đó thì chúng ta sẽ thiết kế lại. Là hàm mục tiêu Loss. Thê ta. Là âm. Của. Trung bình. Lock likelihood. Của ta. Của L. Tức là chúng ta sẽ đi tính Lock của hôm nay. Chúng ta sẽ đi tính Lock của L. Đi tính Lock của L. Và cái việc là tính Lock của hàm tích. Nó sẽ đưa về tổng của các hàm Lock. Lực âm. Của trung bình. Đây là dấu âm. Còn trung bình."
        },
        {
          "index": 9,
          "video_id": "Chương 6_UJNyIptbcNM",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_1： Mô hình Word2Vec",
          "video_url": "https://youtu.be/UJNyIptbcNM",
          "start_time": 400,
          "end_time": 461,
          "text": "Và cái việc là tính Lock của hàm tích. Nó sẽ đưa về tổng của các hàm Lock. Lực âm. Của trung bình. Đây là dấu âm. Còn trung bình. Đây chính là trung bình. Rồi. Và Lock của tích sẽ là bằng tổng của các cái Lock. Do đó thì cái công thức ở trên nó sẽ được chuyển về như dạng này. Và thay vì chúng ta sẽ đi tìm giá trị lớn nhất. Thì chúng ta sẽ phải đi tìm cái giá trị nhỏ nhất. Đi tìm cái giá trị nhỏ nhất của cái Lock Setsup này. Và như vậy chúng ta đặt ra một cái câu hỏi đó là làm sao. Chúng ta tính được cái. Cái P. Làm sao chúng ta tính được cái Setsup P của T cộng Z. Cho chiếc WT. Với. Theta là cái tham số của mô hình của mình. Là tham số của mô hình. Thì ý tưởng đó là chúng ta sẽ sử dụng một cái mạng Neural Network. Với một lớp ẩn duy nhất thôi."
        },
        {
          "index": 10,
          "video_id": "Chương 6_UJNyIptbcNM",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_1： Mô hình Word2Vec",
          "video_url": "https://youtu.be/UJNyIptbcNM",
          "start_time": 451,
          "end_time": 508,
          "text": "Là tham số của mô hình. Thì ý tưởng đó là chúng ta sẽ sử dụng một cái mạng Neural Network. Với một lớp ẩn duy nhất thôi. Và cái đầu ra của mình sẽ là một cái hàm Softmax. Thì ở bên đây. Chúng ta sẽ có cái kiến trúc của cái mạng Neural Network. Rõ ràng là cái mạng Neural Network này. Nó cũng là một cái mạng học sâu. Nhưng mà nó rất là gắn. Chỉ có duy nhất một lớp ẩn thôi. Duy nhất một lớp ẩn. Và toàn bộ cái H1, H2, Hn này. Thì người ta sẽ ký diệu là H. Và để mà từ cái Input Layer. Chuyển tính ra được cái XN Layer. Thì chúng ta sẽ có một cái ma trận là ma trận W. Ma trận W này thì sẽ có kích thước là V x N. Trong đó V, chỉ số B. Là cái số từ trong từ điện."
        },
        {
          "index": 11,
          "video_id": "Chương 6_UJNyIptbcNM",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_1： Mô hình Word2Vec",
          "video_url": "https://youtu.be/UJNyIptbcNM",
          "start_time": 498,
          "end_time": 559,
          "text": "Trong đó V, chỉ số B. Là cái số từ trong từ điện. Số từ trong từ điện. Còn N là cái số chiều của cái Output của mình. Hay nói cách khác. Đây chính là cái số chiều của cái vector biểu diễn. Của cái từ. Của cái từ đầu vào. Như vậy thì chúng ta sẽ có cái công thức cho cái mạng Neural Network này. Ở cái Layer biến đổi đầu tiên. Đó là tính Hit the Layer. Thì chúng ta sẽ có công thức là H. Là bằng W x V. Như vậy thì chúng ta thấy là. Cái công thức này nó rất là đơn giản."
        },
        {
          "index": 12,
          "video_id": "Chương 6_UJNyIptbcNM",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_1： Mô hình Word2Vec",
          "video_url": "https://youtu.be/UJNyIptbcNM",
          "start_time": 550,
          "end_time": 609,
          "text": "Thì chúng ta sẽ có công thức là H. Là bằng W x V. Như vậy thì chúng ta thấy là. Cái công thức này nó rất là đơn giản. Và nó không có cái hàm Signal. Hoặc là không có cái hàm kích hoạt. Mà nó chỉ đơn giản là một cái phép biến đổi tiến tính. Chỉ là khác. Sẽ là bằng W x V. Rồi. Và với X là một cái One-Hop. Encode của cái từ thứ T. Của cái từ WT. Như vậy thì đầu vào của mình nó sẽ là có cái dạng như sau. Là 000010. Trong đó. Đây chính là cái vị trí. Vị trí. Của. Cái từ WT. Của cái từ WT. Trong từ điển. Vị trí của cái từ WT trong từ điển."
        },
        {
          "index": 13,
          "video_id": "Chương 6_UJNyIptbcNM",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_1： Mô hình Word2Vec",
          "video_url": "https://youtu.be/UJNyIptbcNM",
          "start_time": 599,
          "end_time": 661,
          "text": "Trong từ điển. Vị trí của cái từ WT trong từ điển.   Là như vậy thì. Cái vector này. Nó sẽ là vector One-Hop. Và có số chiều rất là lớn. Thông thường W. Cái V này. V này thì có khả năng là lên đến 1 triệu. Rồi. Cái lớp tiếp theo. Đó là lớp Output. Thì nó sẽ có cái công thức đó là giá trị dự đoán. Đây là giá trị dự đoán. Tại một cái này nó sẽ là giá trị dự đoán. Rồi. Sẽ là bằng Soap Max. Của W. W thì sẽ là ngược lại của. W. Thì nó sẽ có kích thước là N. Nhân với lại V. Và. Chúng ta cũng lần nữa chúng ta sẽ nhân. Tích vô hướng với lại H. Và đưa vào cái hàm Soap Max. Thì mục tiêu của cái việc đưa về. Hàm Soap Max. Đó chính là nó sẽ đưa về cái không gian sát xuất. Nó sẽ đưa về cái không gian sát xuất."
        },
        {
          "index": 14,
          "video_id": "Chương 6_UJNyIptbcNM",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_1： Mô hình Word2Vec",
          "video_url": "https://youtu.be/UJNyIptbcNM",
          "start_time": 649,
          "end_time": 712,
          "text": "Tích vô hướng với lại H. Và đưa vào cái hàm Soap Max. Thì mục tiêu của cái việc đưa về. Hàm Soap Max. Đó chính là nó sẽ đưa về cái không gian sát xuất. Nó sẽ đưa về cái không gian sát xuất. Trong đó từng cái phần tử. Trong cái I này. Nó sẽ có cái giá trị là từ 0 cho đến 1. Và tổng tất cả các phần tử ở đây. Thì nó sẽ là bằng 1. Rồi. Và nhiệm vụ của chúng ta. Đó chính là chúng ta sẽ phải đi. Tối ưu hóa cái hàm loss. Theta. Là bằng cái công thức là trung bình. Âm của trung bình. Locked Line Input hồi nãy đúng không. Thì cái. PW. T cộng Z. Khi biết trước. Khi biết trước. Khi biết trước cái WT. Thì X này của mình nó chính là WT. Nó chính là cái đầu vào. Và đưa ra cái sát xuất dự đoán. Cho cái từ thứ T cộng Z. Thì nó sẽ được thể hiện. Ở trong cái phân bố sát xuất của cái thần y ngã này."
        },
        {
          "index": 15,
          "video_id": "Chương 6_UJNyIptbcNM",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_1： Mô hình Word2Vec",
          "video_url": "https://youtu.be/UJNyIptbcNM",
          "start_time": 699,
          "end_time": 761,
          "text": "Thì X này của mình nó chính là WT. Nó chính là cái đầu vào. Và đưa ra cái sát xuất dự đoán. Cho cái từ thứ T cộng Z. Thì nó sẽ được thể hiện. Ở trong cái phân bố sát xuất của cái thần y ngã này. Đúng không. Và các cái giá trị này. Giá trị nào mà càng cao. Thì nó sẽ càng thể hiện cái sát xuất của. Cái việc dự đoán đó. Rồi. Và cái ở đây chúng ta lưu ý. Là nó sẽ có một cái hiệu. Đó là IND. Tức là IND là vi tác của chữ INDEX. Cho thấy. Là INDEX của cái từ thứ T cộng Z. Tức là cho biết cái vị trí. Của cái từ thứ T cộng Z. Trong từ điển của mình. Ví dụ như là từ thứ T cộng Z của mình. Đó là ở vị trí số 3. Vị trí thứ 100. Rồi vị trí thứ 120. Vị trí thứ 130. Đó là các cái giá trị. Chỉ là. Chỉ số của các cái từ. Thứ T trừ 1. T trừ 2. Rồi T cộng 1. T cộng 2. Đó. Thì chúng ta sẽ chỉ tính."
        },
        {
          "index": 16,
          "video_id": "Chương 6_UJNyIptbcNM",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_1： Mô hình Word2Vec",
          "video_url": "https://youtu.be/UJNyIptbcNM",
          "start_time": 749,
          "end_time": 811,
          "text": "Vị trí thứ 130. Đó là các cái giá trị. Chỉ là. Chỉ số của các cái từ. Thứ T trừ 1. T trừ 2. Rồi T cộng 1. T cộng 2. Đó. Thì chúng ta sẽ chỉ tính. Tổng của các cái lốc. Của các cái giá trị output này. Các cái giá trị output này. Như vậy thì. Từ cái công thức này thì chúng ta sẽ đi. Tối U hóa. Tìm Theta sao cho cái tổng này là nhỏ nhất. Tìm Theta sao cho cái tổng này là nhỏ nhất. Hay là tìm Min á. Hay là tìm Min á. Và sau khi chúng ta tối U xong. Thì chúng ta sẽ có các cái trọng số. Là các cái giá trị W. Và chúng ta lưu ý là. Ở đây thì cái hệ thống ký hiệu nó hơi. Khác một chút xíu. Các bạn sẽ hỏi là W với Theta là gì. Thì mình cũng xin lỗi đó là. Trong trường hợp này. Cái hệ thống ký hiệu của mình. Trước đây mình hay sử dụng cho các cái mô hình đó là Theta. Thì trong cái hình này mình lấy từ cái bài bao gốc. Thì thật ra Theta của mình. Chính là cái W này. Theta 1 của mình. Chính là cái W này. Và Theta 2 của mình. Chính là cái W phải này."
        },
        {
          "index": 17,
          "video_id": "Chương 6_UJNyIptbcNM",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_1： Mô hình Word2Vec",
          "video_url": "https://youtu.be/UJNyIptbcNM",
          "start_time": 799,
          "end_time": 824,
          "text": "Thì trong cái hình này mình lấy từ cái bài bao gốc. Thì thật ra Theta của mình. Chính là cái W này. Theta 1 của mình. Chính là cái W này. Và Theta 2 của mình. Chính là cái W phải này. Chính là cái mộ tham số Theta. Rồi. Ở đây thì. Nếu mà theo cái ký hiệu ở đây. Thì lẽ ra nó phải là loss của W. Rồi."
        }
      ]
    },
    {
      "video_id": "Chương 6_AkHEcgasvkw",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Trình bày chi tiết các mô hình Word2Vec — cụ thể là biến thể *continuous* (CBOW-like) và *skip* (skip-ram/skip-ramp), cách xây dựng hàm loss, cấu trúc mạng, cách huấn luyện và các tính chất thú vị của embedding (ví dụ phép toán vector cho analogies). [1][2][6][9]  \n- Các khái niệm sẽ được đề cập: mô hình continuous (dự đoán từ ở giữa từ các từ xung quanh), mô hình skip (skip-ram), cách tính vector ẩn h từ các vector từ ngữ cảnh, hàm softmax ở lớp output, hàm loss (âm trung bình log xác suất), ma trận trọng số W và W', cách huấn luyện và nguồn dữ liệu, trực quan hóa embedding và phép toán ngữ nghĩa (analogy). [1][2][3][4][5][6][7][9][10]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Ý tưởng cơ bản của hai mô hình (continuous vs skip)\n- Mô hình *continuous* (CBOW-like): dự đoán từ ở giữa w_t khi biết trước các từ xung quanh w_{t-m} ... w_{t+m} (không bao gồm w_t). Hàm loss của mô hình là âm trung bình của log xác suất p(w_t | context). [1]  \n  - Cụ thể: L = - (1/T) Σ_t log p(w_t | w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m}). [1]\n- Mô hình *skip* (gọi trong lời giảng là \"skip ramp/skip ram\"): tương tự nhưng mục tiêu là dùng từ trung tâm để dự đoán các từ ngữ cảnh (được nhắc là cùng cấu trúc mạng neural nhưng chiều mũi tên khác so với continuous). [1][2]\n\n### 2.2. Kiến trúc mạng và cách tính vector ẩn h\n- Cả hai mô hình sử dụng một mạng neural đơn giản: ánh xạ mỗi từ vào một vector (embedding) rồi tổng hoặc trung bình các vector của từ ngữ cảnh để tạo vector ẩn h. [2][3]  \n  - Minh họa công thức (theo lời giảng): h được tính bằng tích chuyển vị ma trận W với tổng các vector one-hot x của các từ ngữ cảnh: h = W^T (x1 + x2 + ... + x_k). [3]  \n- Lớp output: ŷ (y ngã) được tính bằng softmax của (W' nhân với h) (giải thích tương tự như skip-ram). [3][4]  \n  - Nghĩa là ŷ = softmax(W' · h). [3][4]\n\n### 2.3. Hàm loss và tối ưu hóa\n- Với biến thể continuous, hàm lỗi đơn giản hơn vì mô hình chỉ cần dự đoán tại thời điểm t (từ ở giữa) thay vì dự đoán nhiều lần từ các vị trí ngữ cảnh. [4]  \n- Giá trị dùng để tối ưu hóa là phần tử ŷ ở chỉ số tương ứng với từ w_t (lấy phần tử tương ứng với index của từ ở giữa) và tối ưu hóa để thu được bộ trọng số W và W' (ký hiệu θ gồm W và W'). [4][5]  \n- Tóm lại: tối ưu θ = {W, W'} bằng cách giảm hàm loss dựa trên log-xác suất của từ đúng. [5]\n\n### 2.4. Ma trận embedding và kích thước\n- Sau khi huấn luyện, tồn tại hai ma trận trọng số W và W'. Ma trận W chứa toàn bộ word vectors (mỗi hàng là vector của một từ trong từ điển). [7][8]  \n  - Kích thước chung được mô tả là V × n (V = số từ trong từ điển, n = chiều embedding). Mỗi hàng (vector độ dài n) là biểu diễn của một từ. [7][8]\n\n### 2.5. Dữ liệu huấn luyện và yêu cầu tính toán\n- Dữ liệu huấn luyện thường lấy từ kho văn bản lớn như Wikipedia và các trang web uy tín khác; việc huấn luyện cần nhiều tuần và rất nhiều GPU / server do kích thước dữ liệu và mô hình. [6]  \n- Các tập đoàn công nghệ với hạ tầng lớn (siêu máy tính, cluster GPU) thường là những thực thể có khả năng huấn luyện mô hình này ở quy mô lớn. [6]  \n- Ví dụ thư viện/nguồn embedding phổ biến: fastText (thuộc Facebook) cung cấp nhiều word embedding cho các ngôn ngữ phổ biến. [7]\n\n### 2.6. Tính chất không gian embedding và các mối quan hệ ngữ nghĩa\n- Khi trực quan hóa các vector từ trong không gian embedding, người ta thấy xuất hiện các mối quan hệ tuyến tính thú vị giữa từ (ví dụ king, queen, princess, prince, he, she). Những quan hệ này cho phép thực hiện phép toán vector để tìm analogies. [9][10]  \n- Phép toán analogies mẫu (theo lời giảng): nếu lấy vector(woman) - vector(man) và cộng vector(king), kết quả hướng đến vector(queen). Công thức biểu diễn trong giảng: x = woman - man + king, và x gần nhất về cosine/ khoảng cách với vector tương ứng của từ sẽ là queen. [11][12][13]  \n- Các mối quan hệ khác cũng được tìm thấy: số ít — số nhiều, tính từ, trạng từ, quan hệ đất nước — thủ đô (ví dụ: Đức → Berlin, Pháp → Paris), so sánh hơn, mối quan hệ giữa công ty và sản phẩm, món ăn, kim loại, v.v. Các mối quan hệ này được mô tả là “lưu” trong không gian embedding khi huấn luyện trên kho dữ liệu lớn. [14][15][16]\n\n### 2.7. Vai trò của Word2Vec trong pipeline NLP và các mô hình khác\n- Word2Vec (word to vector) là bước quan trọng để biến từ thành vector biểu diễn; các mô hình máy học/vật toán sau đó hoạt động trên các vector này (thực hiện các phép toán tuyến tính, nhân, cộng, v.v.). Word2Vec trở thành input tiêu chuẩn cho nhiều mô hình NLP sau này. [16][17]  \n- Ngoài Word2Vec, có các phương pháp embedding khác nổi tiếng (giảng viên nêu tên: GloVe — global vector) và thư viện Python hiện hỗ trợ GloVe cũng như các công cụ embedding khác. [17][18]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ analogies:\n  - woman - man + king ≈ queen (minh họa phép toán vector để tìm quan hệ giới tính và quyền lực). [11][12][13]  \n  - Từ số ít — số nhiều: phép toán tương tự cho ra dạng số nhiều tương ứng. [13][14]  \n  - Thủ đô — đất nước: ví dụ Germany → Berlin nên France → Paris có thể thu được bằng phép toán vector tương tự. [14][15]  \n- Ứng dụng thực tế:\n  - Tạo word embeddings dùng làm input cho mô hình phân loại, dịch máy, tìm kiếm ngữ nghĩa, clustering từ, v.v. (nói chung: mọi mô hình ML cần biểu diễn từ dưới dạng vector). [16]  \n  - Thư viện fastText/ Facebook cung cấp embedding đã huấn luyện sẵn cho nhiều ngôn ngữ, giúp ứng dụng ngay trong hệ thống sản phẩm. [7]\n- Trường hợp sử dụng:\n  - Phân tích ngữ nghĩa, xây dựng hệ thống trả lời câu hỏi, công cụ tìm kiếm semantically-aware, hệ thống gợi ý từ, và bất kỳ hệ thống NLP nào cần biểu diễn từ dưới dạng vector để thực hiện các phép toán đại số tuyến tính. [16][17]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Hai biến thể chính của Word2Vec được thảo luận: mô hình continuous (dự đoán từ ở giữa từ ngữ cảnh) và mô hình skip (skip-ram) với kiến trúc mạng tương tự; vector ẩn h thu được bằng tổng/ trung bình các embedding ngữ cảnh; output dùng softmax; tối ưu θ = {W, W'} bằng hàm loss dạng âm trung bình log xác suất. [1][2][3][4][5]  \n  - Sau huấn luyện, ma trận W (V × n) chứa embedding (mỗi hàng là vector một từ) và embedding này biểu diễn nhiều mối quan hệ ngữ nghĩa có thể khai thác bằng phép toán vector (ví dụ analogies). [7][8][9][11][12]  \n  - Huấn luyện cần dữ liệu rất lớn (Wikipedia, các trang uy tín) và tài nguyên tính toán mạnh (nhiều GPU, server); có thư viện sẵn như fastText để sử dụng embedding đã huấn luyện. [6][7]\n- Tầm quan trọng: Word2Vec là bước nền tảng cho nhiều hệ thống NLP hiện đại vì nó chuyển không gian rời rạc của từ thành không gian vector liên tục, cho phép các mô hình học máy thực hiện phép toán đại số trên biểu diễn từ. [16][17]  \n- Liên hệ với các bài giảng khác: Giảng viên đề cập GloVe như một phương pháp embedding khác (global vector) và nhắc đến thư viện Python hỗ trợ GloVe; Word2Vec/GloVe đều là công cụ tiền xử lý quan trọng cho các bài học mô hình tiếp theo. [17][18]\n\n--- \n\nGhi chú: Tóm tắt trên dựa hoàn toàn vào nội dung được trích từ các đoạn video cung cấp (các citation [1]–[18] trỏ tới các đoạn tương ứng).",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 6_AkHEcgasvkw",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_2： Mô hình Word2Vec",
          "video_url": "https://youtu.be/AkHEcgasvkw",
          "start_time": 0,
          "end_time": 60,
          "text": "và tương tự như vậy cho cái mô hình continuous variable continuous variable nó sẽ đi dự đoán cái từ ở giữa dự đoán cái từ ở giữa khi cho trước những từ xung quanh trước đây là cho trước cái từ ở giữa đoán những từ xung quanh bây giờ là mình sẽ đi đoán cái từ ở giữa khi biết trước những cái từ xung quanh thì lúc này là cái mũi tên của mình nó sẽ là ngược lại từ những cái từ thứ t trừ 2 t trừ 1 chúng ta sẽ đi đoán cái từ thứ t và chúng ta sẽ có cái công thức của cái hàm loss của mình là bằng trừ là bằng trung bình cộng là âm của trung bình cộng của lốc của pwt khi biết trước cái wt trừ m wt trừ m cộng 1 cho đến wt cộng m value và lưu ý là từ đây ở cái chỗ này nè"
        },
        {
          "index": 2,
          "video_id": "Chương 6_AkHEcgasvkw",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_2： Mô hình Word2Vec",
          "video_url": "https://youtu.be/AkHEcgasvkw",
          "start_time": 49,
          "end_time": 110,
          "text": "khi biết trước cái wt trừ m wt trừ m cộng 1 cho đến wt cộng m value và lưu ý là từ đây ở cái chỗ này nè thì chúng ta sẽ không có cái wt chúng ta sẽ không có wt từ t trừ 1 nó sẽ nhảy lên t cộng 1 luôn rồi và ở đây thì tương tự như skip ram thì chúng ta cũng sẽ sử dụng một cái mạng neural network cho cái mô hình continuous bfwork và dựa trên các cái đầu vào thì chúng ta sẽ tính cái vector h bằng cách đó là lấy tổng hoặc là trung bình cộng của các cái vector tương ứng như vậy thì ở đây trong cái shadow này chúng ta thấy nè với mỗi cái khối này nè nó sẽ ánh xạ cái từ 1k x1k về một cái dạng vector nó sẽ ánh xạ cái từ x2k về một dạng vector ánh xạ cái từ thứ ck về một cái dạng vector"
        },
        {
          "index": 3,
          "video_id": "Chương 6_AkHEcgasvkw",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_2： Mô hình Word2Vec",
          "video_url": "https://youtu.be/AkHEcgasvkw",
          "start_time": 99,
          "end_time": 161,
          "text": "1k x1k về một cái dạng vector nó sẽ ánh xạ cái từ x2k về một dạng vector ánh xạ cái từ thứ ck về một cái dạng vector và chúng ta sẽ tính tổng tất cả các vector đó để tạo ra cái h chúng ta sẽ tính tổng cái đó để tạo ra cái h và như vậy thì chúng ta sẽ có công thức là h sẽ là bằng w chuyển vị nhân với lại x1 cộng cho x2 cộng cho xt 1 rồi và qua đó thì chúng ta sẽ h tính cái lớp output layer chúng ta sẽ tính cái lớp output layer bằng một cái hàm tương tự như skip ramp đó là y ngã ở đây là y ngã sẽ được tính là bằng shopmark của w nhân với lại h nhân với lại w phải chuyển vị nhân với h"
        },
        {
          "index": 4,
          "video_id": "Chương 6_AkHEcgasvkw",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_2： Mô hình Word2Vec",
          "video_url": "https://youtu.be/AkHEcgasvkw",
          "start_time": 148,
          "end_time": 210,
          "text": "sẽ được tính là bằng shopmark của w nhân với lại h nhân với lại w phải chuyển vị nhân với h thì cái này là tương tự skip ramp rồi và khi đó hàm lỗi của mình lúc này nó sẽ đơn giản hơn tại vì nó sẽ không phải tính trên cái tổng của các cái dự đoán từ t trừ 1 t trừ 2 cho đến t cộng 1 t cộng 2 nữa mà nó chỉ đoán tại cái thời điểm thứ t và do đó thì cái vector này nó sẽ là bằng cái y ngã index của w t thì nó sẽ là lấy cái phần tử thứ lấy cái phần tử thứ có cái chỉ số là chỉ số của cái từ thứ w t tức là"
        },
        {
          "index": 5,
          "video_id": "Chương 6_AkHEcgasvkw",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_2： Mô hình Word2Vec",
          "video_url": "https://youtu.be/AkHEcgasvkw",
          "start_time": 198,
          "end_time": 258,
          "text": "thì nó sẽ là lấy cái phần tử thứ lấy cái phần tử thứ có cái chỉ số là chỉ số của cái từ thứ w t tức là cái từ ở giữa cái từ ở giữa nó sẽ có một cái chỉ số nó sẽ có một cái chỉ số ví dụ như từ đó nó nằm ở đây từ đó nằm ở đây thì ở đây chúng ta sẽ tính là lóc của y ngã ví dụ trong vị trí này là 123 à không 12 chỉ số của mình là 2 là tương ứng là cái vị trí của cái từ w t thì khi đó chúng ta sẽ lấy y ngã của 2 rồi và tối chúng ta tối ưu hóa cái hàm loss này thì chúng ta sẽ tìm được cái bộ trọng số w và w ngã thì tương tự như trong cái ký hiệu hồi nãy chúng ta có nói theta của mình nó sẽ bao gồm một cái bộ là w và w' theta của mình sẽ bao gồm một cái bộ như vậy"
        },
        {
          "index": 6,
          "video_id": "Chương 6_AkHEcgasvkw",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_2： Mô hình Word2Vec",
          "video_url": "https://youtu.be/AkHEcgasvkw",
          "start_time": 248,
          "end_time": 308,
          "text": "chúng ta có nói theta của mình nó sẽ bao gồm một cái bộ là w và w' theta của mình sẽ bao gồm một cái bộ như vậy rồi và bây giờ thì công việc đó là huấn luyện chúng ta đã có cái hàm mô hình cho từng cái mô hình là skip ramp và continuous better work chúng ta có hàm loss cho từng cái mô hình vậy thì dữ liệu lấy ở đâu dữ liệu được thu thập từ các cái trang wikipedia và những cái trang web mà uy tín khác rồi sau đó thì chúng ta sẽ huấn luyện trong nhiều tuần với rất nhiều gpu thì ở đây thực sự mà nói thì để huấn luyện được các cái mô hình skip ramp và continuous better work này thì phải dựa trên cái sức mạnh tính toán của các cái tập đoàn công nghệ được trang bị rất nhiều những cái con server siêu máy tính mới có thể thực hiện được thì cụ thể ở đây là chúng ta sẽ biết là cái thư viện của fasttech"
        },
        {
          "index": 7,
          "video_id": "Chương 6_AkHEcgasvkw",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_2： Mô hình Word2Vec",
          "video_url": "https://youtu.be/AkHEcgasvkw",
          "start_time": 299,
          "end_time": 363,
          "text": "được trang bị rất nhiều những cái con server siêu máy tính mới có thể thực hiện được thì cụ thể ở đây là chúng ta sẽ biết là cái thư viện của fasttech là của facebook là cái nơi cung cấp rất nhiều cái word embedding cho các cái từ của các cái ngôn ngữ phổ biến nhất hiện nay rồi và sau khi huấn luyện xong thì cái ma trận w à chúng ta lưu ý là có ma trận w và w' thì cái ma trận w nó sẽ chứa toàn bộ các cái word vector của các cái từ trong từ điển của các cái từ trong từ điển thì ví dụ cái ma trận w của mình có kích thước là v nhân n v nhân n v nhân n v nhân n v  v v v v v v v   v  v v v v v tr x"
        },
        {
          "index": 8,
          "video_id": "Chương 6_AkHEcgasvkw",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_2： Mô hình Word2Vec",
          "video_url": "https://youtu.be/AkHEcgasvkw",
          "start_time": 344,
          "end_time": 411,
          "text": "v nhân n v  v v v v v v v   v  v v v v v tr x r y  x t x r x     cca x tr x              y y            z y z z z z và cái vector này nó sẽ có n phần tử thì đây chính là cái word vector của từ k thì đó chính là cái mô hình word to vector"
        },
        {
          "index": 9,
          "video_id": "Chương 6_AkHEcgasvkw",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_2： Mô hình Word2Vec",
          "video_url": "https://youtu.be/AkHEcgasvkw",
          "start_time": 401,
          "end_time": 461,
          "text": "của từ k thì đó chính là cái mô hình word to vector và lưu ý là với mỗi mô hình skip ramp hoặc là continuous better word thì chúng ta sẽ có một cái mô hình trận riêng nha tức là mỗi mô hình chúng ta có thể tiếp cận bằng 2 cách khác nhau thì mỗi mô hình sẽ cho sản sinh ra một cái mộ trọng số thì cứ mỗi trọng số này thì chúng ta sẽ lấy cái vector biểu diễn cho cái từ đó và mỗi một cái vector biểu diễn của một từ nó tương ứng là một hàng trong cái ma trận W rồi và và , khi mà người ta trực quan hóa các cái vector biểu diễn của các cái từ trong cái không gian thì người ta mới thấy là có một cái mối quan hệ rất là thú vị ví dụ người ta vẽ cái người ta biểu diễn các cái từ như là king, queen trong không gian"
        },
        {
          "index": 10,
          "video_id": "Chương 6_AkHEcgasvkw",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_2： Mô hình Word2Vec",
          "video_url": "https://youtu.be/AkHEcgasvkw",
          "start_time": 449,
          "end_time": 511,
          "text": "thì người ta mới thấy là có một cái mối quan hệ rất là thú vị ví dụ người ta vẽ cái người ta biểu diễn các cái từ như là king, queen trong không gian princess hero heroine rồi he, she v.v thì chúng ta thấy là các cái từ mà có thể hiện cái vector ánh xạ từ queen sang king, từ princess sang prince, rồi từ sea sang hay, hình như nó đều có cái vector giống nhau nó đều có cái vector giống nhau và bây giờ người ta sẽ nảy sinh ra một cái ý tưởng, đó là vậy thì nếu như mình lấy cái vector từ man sang woman, đúng không mình lấy cái vector woman trừ cho vector man mình có cái vector này, và lấy cái vector này đem xuống đây rồi sau đó chúng ta sẽ lấy cái vector biểu diễn của từ king, vector biểu diễn của từ king, ánh xạ lên"
        },
        {
          "index": 11,
          "video_id": "Chương 6_AkHEcgasvkw",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_2： Mô hình Word2Vec",
          "video_url": "https://youtu.be/AkHEcgasvkw",
          "start_time": 499,
          "end_time": 561,
          "text": "mình có cái vector này, và lấy cái vector này đem xuống đây rồi sau đó chúng ta sẽ lấy cái vector biểu diễn của từ king, vector biểu diễn của từ king, ánh xạ lên với cùng cái vector giống như là từ man sang woman, thì hỏi cái x ở đây nó sẽ là cái từ nào trong không gian em bắt đi nó sẽ là cái từ nào trong không gian em bắt đi và cái công thức chúng ta sẽ mua hình hóa cái ý tưởng này dưới dạng cái công thức sau, đó là woman trừ man giả sử như đây là hai cái vector biểu diễn nha đây chính là cái vector đây là cái word vector của từ woman nha đây là word vector của từ man rồi woman trừ cho man sẽ là bằng x trừ cho king thì từ đó chúng ta sẽ đem cái trừ king này qua bên kia thì chúng ta sẽ có là x sẽ là bằng woman trừ man cộng king"
        },
        {
          "index": 12,
          "video_id": "Chương 6_AkHEcgasvkw",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_2： Mô hình Word2Vec",
          "video_url": "https://youtu.be/AkHEcgasvkw",
          "start_time": 549,
          "end_time": 609,
          "text": "x trừ cho king thì từ đó chúng ta sẽ đem cái trừ king này qua bên kia thì chúng ta sẽ có là x sẽ là bằng woman trừ man cộng king thì chúng ta sẽ có là x sẽ là bằng woman trừ man cộng king và như vậy thì chúng ta sẽ lấy vector biểu diễn của từ man lấy vector biểu diễn của từ woman từ man và từ king chúng ta thực hiện với lại cái công thức là trừ woman trừ man sau đó cộng cho king chúng ta sẽ được vector và với cái vector này thì chúng ta sẽ xem xem từ nào nằm gần với cái từ mà biểu diễn bởi cái vector x này nhất thì rất là thú vị, đó chính là từ queen tức là nữ hoàng đó thì ở đây nếu mà dịch sang một cái ngữ nghĩa nào đó thì chúng ta có thể thấy là nếu như cái người đàn ông mặt quyền lực là vua gọi là vua thì hỏi người đàn bà"
        },
        {
          "index": 13,
          "video_id": "Chương 6_AkHEcgasvkw",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_2： Mô hình Word2Vec",
          "video_url": "https://youtu.be/AkHEcgasvkw",
          "start_time": 599,
          "end_time": 661,
          "text": "thấy là nếu như cái người đàn ông mặt quyền lực là vua gọi là vua thì hỏi người đàn bà quyền lực thì gọi là gì thì người đàn bà quyền lực đó chính là queen là nữ hoàng thì đây chính là một cái mối quan hệ về mặt ngữ nghĩa rất là tốt nhất của người đàn bà thì cái mối quan hệ ở đây đó chính là cái mối quan hệ về mặt giới tính mối quan hệ về mặt giới tính rồi và tương tự như vậy thì chúng ta sẽ có các cái mối quan hệ ngữ nghĩa khác lấy ví dụ Apple và Apple thì ở đây đó chính là cái mối quan hệ về số ít, số nhiều số nhiều, bên đây là số ít của một cái từ, danh từ và với cái công thức này đúng không? x sẽ bằng Apple trừ cho Apple cộng k"
        },
        {
          "index": 14,
          "video_id": "Chương 6_AkHEcgasvkw",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_2： Mô hình Word2Vec",
          "video_url": "https://youtu.be/AkHEcgasvkw",
          "start_time": 649,
          "end_time": 710,
          "text": "số nhiều, bên đây là số ít của một cái từ, danh từ và với cái công thức này đúng không? x sẽ bằng Apple trừ cho Apple cộng k như vậy thì câu hỏi đặt ra đó là cái từ nào mà gần với cái vector biểu diễn của từ x này nhất của cái vector biểu diễn x này nhất thì rất là thú vị, đó chính là từ k và tương tự như vậy cho các cái mối quan hệ về tính từ rồi và adverb và adverb rồi mối quan hệ về đất nước và thủ đô đúng không? đây là thủ đô nè đây là đất nước nè đây là đất nước nè thì nếu như chúng ta có thể tìm ra những cái mối quan hệ nếu như thủ đô của Đức là Berlin thì thủ đô của Pháp là gì? thì cái vector z này cũng"
        },
        {
          "index": 15,
          "video_id": "Chương 6_AkHEcgasvkw",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_2： Mô hình Word2Vec",
          "video_url": "https://youtu.be/AkHEcgasvkw",
          "start_time": 702,
          "end_time": 761,
          "text": "thì nếu như chúng ta có thể tìm ra những cái mối quan hệ nếu như thủ đô của Đức là Berlin thì thủ đô của Pháp là gì? thì cái vector z này cũng cho ra được một kết quả rất là thú vị đó chính là Paris đó chính là Paris rồi và chúng ta sẽ còn rất rất rất nhiều những cái mối quan hệ ngữ nghĩa khác và nó cũng đều thoải mãn được cái kiến thức trong thực tế như vậy, một cái mô hình skip, mô hình skip mô hình word to word khi được trend trên một cái kho dữ liệu cực kì lớn thì nó vẫn sẽ lưu được những cái thông tin cái mối quan hệ khác bên cạnh cái mối quan hệ về mặt vũ pháp nó vẫn có những cái mối quan hệ khác nữa và mối quan hệ về thủ đô đất nước mối quan hệ về so sánh hơn rồi mối quan hệ về đất nước rồi mối quan hệ về đất nước rồi món ăn mối quan hệ về kim loại và biểu diễn"
        },
        {
          "index": 16,
          "video_id": "Chương 6_AkHEcgasvkw",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_2： Mô hình Word2Vec",
          "video_url": "https://youtu.be/AkHEcgasvkw",
          "start_time": 748,
          "end_time": 810,
          "text": "và mối quan hệ về thủ đô đất nước mối quan hệ về so sánh hơn rồi mối quan hệ về đất nước rồi mối quan hệ về đất nước rồi món ăn mối quan hệ về kim loại và biểu diễn và kí hiệu trong cái bản phân loại tình huống rồi mối quan hệ về các cái công ty sản phẩm thì cái mô hình word to word nó thể hiện được trong cái không gian emedding và như vậy thì chúng ta đã tìm hiểu về cái mô hình một trong những cái mô hình rất là quan trọng cho cái lĩnh vực xử lý ngôn ngữ tự nhiên trở về sau tại vì word to word sẽ là cái đầu vào cho các cái mô hình máy học chúng ta sẽ phải biến các cái từ chúng ta sẽ biến các từ thành một cái dạng vector biểu diễn và có cái vector biểu diễn này rồi thì các cái mô hình máy học bản chất đó chính là các cái phép toán các cái hàm toán học trên đại số tiến tính"
        },
        {
          "index": 17,
          "video_id": "Chương 6_AkHEcgasvkw",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_2： Mô hình Word2Vec",
          "video_url": "https://youtu.be/AkHEcgasvkw",
          "start_time": 799,
          "end_time": 861,
          "text": "biểu diễn và có cái vector biểu diễn này rồi thì các cái mô hình máy học bản chất đó chính là các cái phép toán các cái hàm toán học trên đại số tiến tính thực hiện cộng trừ nhân chia thì nó phải thực hiện trên đối tượng vector này nó không thể nào thực hiện cái phép cộng trừ nhân chia với các cái từ ở dưới dạng là chuỗi được, nó phải chuyển sang cái dạng vector và từ nay trở về sau thì chúng ta sử dụng word to word như là một trong những cái công cụ để mà làm đầu vào cho các cái mô hình về sau và có rất nhiều những cái mô hình word to word hiện nay và nổi tiếng và cho cái độ chính xác cao đó chính là Glob là viết tắt của chữ là global vector thì các cái thư viện của python hiện giờ là đều cho phép hỗ trợ cái word emitting Glob và đây là các cái tài liệu"
        },
        {
          "index": 18,
          "video_id": "Chương 6_AkHEcgasvkw",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 4_2： Mô hình Word2Vec",
          "video_url": "https://youtu.be/AkHEcgasvkw",
          "start_time": 849,
          "end_time": 863,
          "text": "của python hiện giờ là đều cho phép hỗ trợ cái word emitting Glob và đây là các cái tài liệu tham khảo sử dụng trong cái bài học của ngày hôm nay"
        }
      ]
    },
    {
      "video_id": "Chương 6_WAiLM7OFU9A",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: hiểu và thực hành biểu diễn từ dùng mô hình Word2Pack (Word2Vec-like), bao gồm (1) tạo vector biểu diễn từ và tính độ tương đồng giữa hai từ, (2) khai thác các quan hệ ngữ nghĩa/ngữ pháp giữa từ dựa trên embedding[1].  \n  [1]\n\n- Các khái niệm / công cụ sẽ được đề cập: thư viện GenSim để tải và sử dụng mô hình embedding tiền huấn luyện; mô hình embedding tiền huấn luyện (ví dụ Vector English trên dữ liệu Wikinews); thao tác lấy vector bằng chỉ mục; đo độ tương đồng bằng tích vô hướng (dot product) hoặc cosine similarity; phân tích sự khác biệt giữa tương đồng ngữ nghĩa và tương đồng vai trò ngữ pháp[1][2][3][9][10].  \n  [1][2][3][9][10]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Cài đặt và khởi tạo môi trường (GenSim và model tiền huấn luyện)\n- Cài đặt GenSim: có thể dùng pip hoặc conda theo hướng dẫn trong video (pip install gensim hoặc conda install -c ara-conda gensim được đề cập)[1][2].  \n  [1][2]\n\n- Import GenSim và thời gian khởi tạo: cần import gensim trước khi dùng; thao tác import và khởi tạo model có thể tốn vài giây đến vài phút để load[2][8].  \n  [2][8]\n\n- Mô hình không có sẵn trong GenSim: các mô hình pre-trained phải tải từ các kho chứa mô hình của cộng đồng (ví dụ trang rất nổi tiếng được nêu trong video là FastTech (fasttext.cc) nơi lưu các mô hình cho nhiều ngôn ngữ)[2][3].  \n  [2][3]\n\n### 2.2. Ví dụ mô hình tiền huấn luyện (Wikinews 300D)\n- Ví dụ mô hình được sử dụng: “Vector English” được huấn luyện trên Wikinews, vocab ~1 triệu token, embedding dimension = 300 (300‑D) và file tên tương tự “Wikinews‑300D‑1M.vec” trong video[3][4].  \n  [3][4]\n\n- Kích thước ma trận embedding và dung lượng: ma trận embedding kích thước khoảng 1,000,000 × 300 = ~300 triệu tham số (ứng với vocab × embedding_dim); file sau khi nén còn khoảng 600MB (video nói ~600B nhưng ngữ cảnh đề cập đến file lớn đã nén)[7][8].  \n  [7][8]\n\n- Thời gian tải / load: việc download + load model có thể mất từ ~1 phút để tải (tùy đường truyền) và ~3–4 phút để load model vào bộ nhớ; huấn luyện mô hình như vậy từ đầu cần nhiều tài nguyên (GPU, thời gian vài tuần) và dữ liệu lớn (hàng triệu–hàng tỷ văn bản), nên khuyến nghị sử dụng model đã tiền huấn luyện cho người dùng cá nhân[4][5][6].  \n  [4][5][6]\n\n### 2.3. Truy xuất vector từ và tính độ tương đồng\n- Lấy vector của một từ: dùng toán tử chỉ mục trên model (ví dụ model['king'] để lấy vector của \"king\"); vector thu được có kích thước 300 chiều trong ví dụ[8][9][11].  \n  [8][9][11]\n\n- So sánh hai từ = so sánh hai vector: video minh họa dùng tích vô hướng (dot product) để đánh giá tương đồng: winvec.dot(charvec) tương ứng với tích vô hướng giữa hai vector; giá trị tích vô hướng càng lớn → tương đồng càng cao[9][10].  \n  [9][10]\n\n- Các phép đo khác: có thể dùng cosine similarity (độ đo cosine) hoặc các độ đo khoảng cách khác ngoài nhóm similarity để đánh giá mối quan hệ giữa vectors[10].  \n  [10]\n\n- Công thức (tóm tắt toán học):\n  - Tích vô hướng: a · b = Σ_i a_i * b_i  (được video nhắc tới qua thao tác dot product) [10].  \n    [10]\n  - Cosine similarity: cos(a,b) = (a · b) / (||a|| ||b||) (video đề cập tới độ đo cosine như lựa chọn thay thế dot product) [10].  \n    [10]\n\n- Ví dụ giá trị thực nghiệm: trong phần chạy thử, giá trị tương đồng giữa một số cặp được in ra (ví dụ win và kai ≈ 1.51; win và king ≈ 3.2 — win/king có độ tương đồng cao gấp đôi so với win/kai trong ví dụ)[11][12][13].  \n  [11][12][13]\n\n### 2.4. Ý nghĩa của độ tương đồng trong embedding: ngữ pháp vs ngữ nghĩa\n- Embedding phản ánh vai trò ngữ pháp (syntactic/usage similarity) nhiều hơn là nghĩa đối lập hay đồng nghĩa thuần túy: video giải thích rằng Word2Pack (word embedding) chú trọng vào vai trò và vị trí sử dụng của từ trong câu — hai từ có thể thay thế cho nhau trong cùng ngữ cảnh sẽ có similarity cao[14][17].  \n  [14][17]\n\n- Minh họa với love, like, hate:\n  - Intuition: theo cảm quan, \"love\" và \"like\" có vẻ gần nghĩa hơn (dịch sang VN là thích) và \"love\" vs \"hate\" là đối nghịch; tuy nhiên model cho thấy \"love\" và \"hate\" có similarity cao hơn \"love\" và \"like\" trong ví dụ[15][16][17].  \n    [15][16][17]\n  - Lý giải: vì \"love\" và \"hate\" đóng chung vai trò ngữ pháp (cùng là động từ trong ngữ cảnh mẫu) nên embedding của chúng gần nhau; trong khi \"like\" có nhiều vai trò ngữ pháp (động từ, tính từ, ...), khiến vector \"like\" ít giống \"love\" hơn về mặt vai trò ngữ pháp[17][18][19].  \n    [17][18][19]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ cụ thể trong video:\n  - Tải và load mô hình Wikinews 300D; lấy vector của từ \"king\" và \"queen\" bằng chỉ mục; hiển thị kích thước vector là 300 chiều[3][4][8][11].  \n    [3][4][8][11]\n  - Tính dot product giữa vectors (ví dụ winvec.dot(charvec)) để đánh giá similarity; in các giá trị similarity cho các cặp như (win, kai) và (win, king) và so sánh[9][11][12][13].  \n    [9][11][12][13]\n  - Phân tích bộ ba (love, like, hate) để minh họa khác biệt giữa similarity về vai trò ngữ pháp và similarity ngữ nghĩa[15][16][17][18][19].  \n    [15][16][17][18][19]\n\n- Ứng dụng thực tế được đề cập:\n  - Sử dụng embedding như bước tiền xử lý (embedding layer) cho các bài toán Deep Learning: phân loại văn bản, dịch máy, tóm tắt văn bản, v.v.; embedding biến từ từ chuỗi token thành vector (MSN / embedding) để các mô hình deep learning tiếp theo sử dụng[6].  \n    [6]\n  - Lý do dùng pre-trained models: tài nguyên huấn luyện lớn (GPU, nhiều tuần, dữ liệu khổng lồ) khiến việc tự huấn luyện trở nên khó khăn cho người dùng cá nhân, cho nên dùng mô hình đã huấn luyện sẵn là phương án khả thi[5][6].  \n    [5][6]\n\n- Trường hợp sử dụng:\n  - So sánh từ/cụm từ trong bài toán phân tích ngôn ngữ (semantic search, clustering từ, tìm từ tương đồng) bằng dot product hoặc cosine similarity[9][10][11].  \n    [9][10][11]\n  - Làm feature đầu vào cho các mô hình NLP (classification, translation, summarization) thay vì dùng biểu diễn chuỗi thuần túy[6].  \n    [6]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính: bài hướng dẫn trình bày cách dùng GenSim để tải và sử dụng mô hình Word2Pack/Word2Vec tiền huấn luyện (ví dụ Wikinews 300D), cách truy xuất vector từ, các phương pháp đo similarity (dot product, cosine), và phân tích thực nghiệm cho thấy embedding phản ánh nhiều về vai trò/ngữ pháp sử dụng của từ hơn là nghĩa trực tiếp; do đó pre-trained embeddings là công cụ quan trọng trong pipeline NLP/Deep Learning[1][2][3][9][10][14][17].  \n  [1][2][3][9][10][14][17]\n\n- Tầm quan trọng: embeddings tiền huấn luyện giúp tiết kiệm tài nguyên (không phải train từ đầu), cung cấp vector biểu diễn hữu ích cho nhiều bài toán downstream (text classification, MT, summarization), và giúp khám phá quan hệ ngữ pháp/usage giữa từ[5][6].  \n  [5][6]\n\n- Liên hệ với các bài giảng khác: video đề cập đến vai trò của embedding như bước đầu trong mô hình deep learning cho các bài toán như phân loại văn bản, dịch máy, tóm tắt (khớp với nội dung chương về embedding và ứng dụng trong các mô hình sâu) nhưng không nêu cụ thể bài giảng khác trong chuỗi[6].  \n  [6]\n\n--- \n\nGhi chú: tất cả thông tin trong bản tóm tắt trên được trích trực tiếp từ các đoạn (chunks) của video theo timestamp tương ứng: [1], [2], [3], ... [19] (các đoạn đã được dùng tương ứng trong nội dung).",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 0,
          "end_time": 63,
          "text": "Trong bài hướng dẫn này thì chúng ta sẽ cùng tìm hiểu về phương pháp biểu diễn từ với mô hình Word2Pack. Ở đây thì chúng ta sẽ có 2 phần. Phần đầu tiên đó là biểu diễn từ và tính toán sự tương đồng giữa 2 từ với nhau. Tiếp theo đó là chúng ta sẽ cùng khai thác một số quan hệ về mặt ngữ nghĩa mà mô hình biểu diễn từ như là Word2Pack có khả năng thực hiện được. Đối với phần về biểu diễn từ và tính toán sự tương đồng giữa các từ với nhau thì chúng ta sẽ sử dụng thư viện GenSim. Nếu như chúng ta sử dụng Google Colab thì mặc nhìn là Google Colab đã cài trước thư viện GenSim rồi. Vì đó thì chúng ta không cần phải cài đặt lại. Nếu như chúng ta sử dụng trên máy tính cá nhân của mình thì mình sẽ phải cài bằng 1 trong 2 cách sau. Một là chúng ta có thể sử dụng lệnh Pip Install GenSim. Hai là chúng ta có thể sử dụng lệnh Pip Install GenSim. Hai đó là chúng ta nếu sử dụng mini-Conda thì chúng ta sẽ dùng lệnh Conda Install-C, ARA-Conda, GenSim."
        },
        {
          "index": 2,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 49,
          "end_time": 110,
          "text": "Một là chúng ta có thể sử dụng lệnh Pip Install GenSim. Hai là chúng ta có thể sử dụng lệnh Pip Install GenSim. Hai đó là chúng ta nếu sử dụng mini-Conda thì chúng ta sẽ dùng lệnh Conda Install-C, ARA-Conda, GenSim. Và để có thể sử dụng được cái thư viện GenSim này thì chúng ta sẽ khai báo là Import GenSim. Và thời gian để mà chúng ta khởi động và import GenSim thì có thể tốn khoảng vài giây. Rồi, thế thì cái thư viện GenSim này thì nó sẽ không có chứa sẵn cái mô hình đã được huấn luyện sẵn. Do đó thì chúng ta sẽ phải download từ các cái trang web mà đã được cung cấp bởi các cái nhóm nghiên cứu trên thế giới. Ví dụ như là ở đây có một cái nơi rất là nổi tiếng để chứa các cái mô hình huấn luyện sẵn cho cái việc biểu diễn từ của Tô Bác."
        },
        {
          "index": 3,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 99,
          "end_time": 164,
          "text": "Ví dụ như là ở đây có một cái nơi rất là nổi tiếng để chứa các cái mô hình huấn luyện sẵn cho cái việc biểu diễn từ của Tô Bác. Nó chính là FastTech. Chấm cc. Thì đây là cái trang web của Facebook, của nhóm nghiên cứu trong Facebook. Họ đã huấn luyện sẵn các cái mô hình cho các cái ngôn ngữ tiếng Anh, tiếng Việt, rồi tiếng Trung, v.v. Rất nhiều thứ tiếng ở trên thế giới nổi tiếng phổ biến. Và ở đây thì chúng ta chỉ việc lên đây để tải cái mô hình về. Thì ở trong cái code block này chúng ta đã có sẵn một cái đường link để có thể tải được cái mô hình Word2Vec. Tên của cái mô hình này là Vector English. Tức là biểu diễn từ cho các cái ngôn ngữ, cái từ của ngôn ngữ tiếng Anh. Và tập dữ liệu này thì được huấn luyện từ Wikinews. Và tổng số token của mình, hay tổng số từ của mình đã là 1 triệu tờ, 1 triệu token."
        },
        {
          "index": 4,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 147,
          "end_time": 217,
          "text": "Tức là biểu diễn từ cho các cái ngôn ngữ, cái từ của ngôn ngữ tiếng Anh. Và tập dữ liệu này thì được huấn luyện từ Wikinews. Và tổng số token của mình, hay tổng số từ của mình đã là 1 triệu tờ, 1 triệu token. Và sau khi chúng ta đã biến về cái vector biểu diễn thì cái vector biểu diễn của mình sẽ là 300 triệu. 300D ở đây có nghĩa là mỗi một cái token, mỗi một từ sẽ biểu diễn dạng một cái vector 300 triệu. Và sau khi chúng ta tải và dạy nén thì nó sẽ có được một cái file là Wikinews 300D 1V.Vec. Và cái file này thì sẽ được import ở trong cái đoạn code ở dưới đây. Thì cái thời gian để tải và dạy nén thì nó có thể Cái tốn của mình là khoảng 1 phút. Thì cũng nói luôn đó là cái mô hình World Tour Back này thì được trend trên hàng tỷ, hàng trăm triệu cái tài liệu."
        },
        {
          "index": 5,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 199,
          "end_time": 260,
          "text": "Thì cái thời gian để tải và dạy nén thì nó có thể Cái tốn của mình là khoảng 1 phút. Thì cũng nói luôn đó là cái mô hình World Tour Back này thì được trend trên hàng tỷ, hàng trăm triệu cái tài liệu. Ở đây thì chúng ta sẽ tranh thủ thời gian. Chúng ta sẽ chạy cái lệnh này. Lệnh này thì có thể tốn từ 3 đến 4 phút. Đó là loát cái mô hình lên. Thì tập dữ liệu Wikinews này. 300D 1V.Vec. 1 triệu. 1 triệu. Tốt cần này. Thì được trend trên hàng tỷ cái văn bản. Và nó cần sử dụng đến rất nhiều những cái GPU. Và trend trong khoảng thời gian là vài tuần. Thế thì với cái tài nguyên tính toán của chúng ta. Còn những cái người dùng cá nhân. Thì rất khó để mà chúng ta có thể trend được cái mô hình World Tour Back này. Do đó thì cái việc sử dụng một cái pre-trend model."
        },
        {
          "index": 6,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 250,
          "end_time": 308,
          "text": "Còn những cái người dùng cá nhân. Thì rất khó để mà chúng ta có thể trend được cái mô hình World Tour Back này. Do đó thì cái việc sử dụng một cái pre-trend model. Cho một cái mô hình World Tour Back này.  Mình đã huấn luyện sẵn trước đó. Đó là khả thi hơn. Và chúng ta sẽ khai thác cái World Tour Back này. Để giải quyết một số cái bài toán về sau. Có thể nói ví dụ như là bài toán phân loại văn bản. Bài toán dịch máy. Bài toán tóm tắt văn bản. Thì tất cả những cái bài toán đó. Thì cái mô hình Deep Learning mà sử dụng. Cho các cái bài toán đó thì đều. Phải có một cái mước nó gọi là MSN. Làm World MSN. Tức là các cái từ của mình thay vì chúng ta xử lý dưới dạng chuỗi. Thì chúng ta sẽ đưa nó về. Cái vector biểu diễn. Đó thì World Tour Back là một trong những cái phương pháp biểu diễn. Phổ biến. Và được sử dụng. Rất là nhiều trong các cái mô hình học sâu."
        },
        {
          "index": 7,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 299,
          "end_time": 360,
          "text": "Đó thì World Tour Back là một trong những cái phương pháp biểu diễn. Phổ biến. Và được sử dụng. Rất là nhiều trong các cái mô hình học sâu. Rồi. Thì ở đây chúng ta sẽ chờ thêm 2 phút nữa. Để có thể mô hình có thể. Loát được. Thì bản chất của cái mô hình này. Nó chính là một cái ma trận. Thì nếu như ở đây chúng ta thấy có 2 cái thông số. Là 300D. Một triệu. Thì ý đó là. Đây sẽ là một cái ma trận. Có kích thước là. Một triệu nhân ba trăm. Một triệu nhân ba trăm tức là khoảng 300 triệu. Đây là một cái mô hình. Tham số. Là một cái ma trận có kích thước là một triệu.    Một triệu nhân ba trăm triệu. Thì chúng ta thấy là một cái file này. Nó rất là nợ. Trong đây chúng ta thấy là. Cái file của mình nó nặn."
        },
        {
          "index": 8,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 349,
          "end_time": 411,
          "text": "Là một cái ma trận có kích thước là một triệu.    Một triệu nhân ba trăm triệu. Thì chúng ta thấy là một cái file này. Nó rất là nợ. Trong đây chúng ta thấy là. Cái file của mình nó nặn. Khi đã nén thì nó là khoảng 600B. Khoảng 600B. Và đương nhiên cái thời gian. Để mà loát cái mô hình này lên. Thì cũng tốn rất là lâu. Cụ thể ở đây là từ 3 đến 4 phút. Rồi chúng ta đã đi được khoảng một nửa đường. Rồi thì. Trên thủ thời gian thì chúng ta sẽ nói luôn các cái đoạn code phía sau. Thì để cái mô hình này. Có thể. Sử dụng. Thì chúng ta sử dụng cái. Toán tử đó là chỉ mục. Mở ngoặt vô. Và ở đây thì chúng ta sẽ truyền vào cái từ. Mà chúng ta muốn biểu diễn. Thì ví dụ như trong đây. Chúng ta muốn biểu diễn cái từ king. Tức là vua. Thì chúng ta sẽ truyền vào cái chuỗi là king. Sau đây ví dụ như nếu chúng ta muốn truyền."
        },
        {
          "index": 9,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 396,
          "end_time": 462,
          "text": "Và ở đây thì chúng ta sẽ truyền vào cái từ. Mà chúng ta muốn biểu diễn. Thì ví dụ như trong đây. Chúng ta muốn biểu diễn cái từ king. Tức là vua. Thì chúng ta sẽ truyền vào cái chuỗi là king. Sau đây ví dụ như nếu chúng ta muốn truyền. Trừ chuỗi khác. Như là queen. Thì chúng ta chỉ việc thay cái chuỗi king. Và chuỗi queen. Rất là đơn giản. Và cái. Kết quả trả về. Của cái model king này. Đó chính là cái vector biểu diễn của từ king. Và vector này. Cái king vector này. Nó sẽ là một cái vector 300 triệu. Và việc so sánh. Tất cả các cái từ. Với nhau thì nó sẽ tương đương với cái việc. So sánh các cái vector biểu diễn. Của các cái từ này. Ví dụ nếu chúng ta có cái từ queen. Ở dưới đây nè. Chúng ta thấy là có từ queen. Chúng ta có cái queen vector. Chúng ta có cái từ char. Thì chúng ta sẽ có char vector. Và muốn so sánh hai cái từ queen với char với nhau. Thì chúng ta sẽ sử dụng là. Winvec.dot charvec."
        },
        {
          "index": 10,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 449,
          "end_time": 510,
          "text": "Chúng ta có cái queen vector. Chúng ta có cái từ char. Thì chúng ta sẽ có char vector. Và muốn so sánh hai cái từ queen với char với nhau. Thì chúng ta sẽ sử dụng là. Winvec.dot charvec. Thì ở đây đó chính là cái thao tác. Tích vô hướng. Tích vô hướng tức là chúng ta sẽ tính cái sự tương đồng. Giữa hai cái vector. Thì nếu mà cái độ tương đồng này mà càng cao. Thì cái giá trị tích vô hướng này sẽ càng cao. Còn nếu như độ tương đồng mà thấp. Thì hai. Thì cái tích vô hướng này nó sẽ thấp. Tức là hai cái từ này nó không có tương đồng nhau. Bên cạnh đó thì chúng ta cũng hoàn toàn có thể sử dụng. Những cái độ đo khác. Cái độ đo về độ tương đồng. Ví dụ như là chúng ta có thể sử dụng độ độc cosine. Cô xin.  Hoặc là sử dụng những cái độ đo không thuộc nhóm similarity. Độ tương như là độ đo khoảng cách. Rồi. Thì ở đây chúng ta thấy là nó đã tốn hết 4 phút. Để load cái mô hình này lên."
        },
        {
          "index": 11,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 498,
          "end_time": 562,
          "text": "Hoặc là sử dụng những cái độ đo không thuộc nhóm similarity. Độ tương như là độ đo khoảng cách. Rồi. Thì ở đây chúng ta thấy là nó đã tốn hết 4 phút. Để load cái mô hình này lên. Và bây giờ chúng ta sẽ tiến hành chạy thử. Rồi chúng ta sẽ in ra cái key word này là gì. Và chúng ta thấy là nó sẽ ra một cái vector. Thì nếu như bằng mắt thường chúng ta nhìn vô đây. Chúng ta sẽ không thể hiểu được cái ý nghĩa của cái vector này. Đúng không? Ở đây chúng ta sẽ thử quan sát xem là cái kích thước của king vector này là gì. Thì đó là một cái vector 300 chiều. Rồi. Và như vậy thì. Lại chúng ta có đề cập đến cái việc đó là để so sánh giữa 2 cái từ với nhau. Thì chúng ta sẽ tính cái vector biểu diễn của nó. Và sau đó chúng ta sẽ tính tích phối hướng. Thì ở đây chúng ta đã có sảo là win và kai. Thì chúng ta sẽ xem win và kai nó như thế nào. Đó. Thì chúng ta sẽ thấy là. Nó sẽ ra là sự tương đồng giữa từ win và từ kai là 1,5."
        },
        {
          "index": 12,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 549,
          "end_time": 610,
          "text": "Thì ở đây chúng ta đã có sảo là win và kai. Thì chúng ta sẽ xem win và kai nó như thế nào. Đó. Thì chúng ta sẽ thấy là. Nó sẽ ra là sự tương đồng giữa từ win và từ kai là 1,5. 1,51. Thế thì nếu như chúng ta nhìn vô 1,51. Chúng ta không thể biết được rằng. Đây là 2 cái từ có cái sự tương đồng cao hay không. Đúng không? Như vậy thì muốn so sánh được thì chúng ta sẽ phải có thêm một cái từ khác. Thì ở đây chúng ta sẽ có thêm một từ nữa là từ king. Từ king thì chúng ta đã khai báo ở đây rồi. Do đó thì chúng ta sẽ tính thêm là win và king vector. Thì để xem coi win và kai tức là nữ hoàng và xe hơi. Với lại nữ hoàng và vua thì từ nào nó sẽ. Cái cặp từ nào nó sẽ có cái sự tương đồng không. Ở đây thì chúng ta sẽ in ra là similarity của win và kai."
        },
        {
          "index": 13,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 598,
          "end_time": 662,
          "text": "Cái cặp từ nào nó sẽ có cái sự tương đồng không. Ở đây thì chúng ta sẽ in ra là similarity của win và kai. Và bằng. Rồi. Ở đây thì chúng ta sẽ có là similarity của win và king. Để xem coi cái giá trị này sẽ là bằng bao nhiêu. Và ở đây thì chúng ta sẽ kiểm tra cái sự tương đối. Đó thì ở đây chúng ta thấy. Giữa win và king. Chúng ta thấy là độ tương đồng là 3,2. Cao gấp đôi. Hơn gấp đôi. So với lại win và kai. Thì điều này nó cũng thể hiện đó là. Hai từ vua và nữ hoàng nó có tương đồng cao hơn. So với lại nữ hoàng và xe. Thì cái này cũng khá là dễ hiểu tại vì. Win và king. Nếu mà chúng ta xét trong cái phạm vi về đời sống của mình đúng không."
        },
        {
          "index": 14,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 646,
          "end_time": 708,
          "text": "Hai từ vua và nữ hoàng nó có tương đồng cao hơn. So với lại nữ hoàng và xe. Thì cái này cũng khá là dễ hiểu tại vì. Win và king. Nếu mà chúng ta xét trong cái phạm vi về đời sống của mình đúng không. Thì đây là hai từ thường được sử dụng đi chung với nhau. Trong khi đó win và kai. Thì hai cái từ này nó ít đi chung với nhau. Nên cái độ tương đồng của nó thấp hơn. Thì nhắc lại. Thì đó là cái sự tương đồng giữa hai từ. Nó sẽ thể hiện bởi cái chức năng ngữ pháp. Ví dụ như win. Những cái câu nào. Mà chúng ta dùng cái từ win. Thì đều có thể thay thế. Không phải đều có thể thay thế. Mà là chúng ta có thể thay thế với từ king. Mà nó vẫn đảm bảo được cái yếu tố về mặt ngữ pháp. Và ý nghĩa của cái câu. Thế thì bây giờ chúng ta. Sẽ thử xét trên ba cái từ đặc biệt. Đó là từ love, từ like và từ hate. Để minh họa cái vai trò về mặt ngữ pháp. Thì chúng ta sẽ. Có ba cái vector ha. Đó là love."
        },
        {
          "index": 15,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 698,
          "end_time": 760,
          "text": "Đó là từ love, từ like và từ hate. Để minh họa cái vai trò về mặt ngữ pháp. Thì chúng ta sẽ. Có ba cái vector ha. Đó là love. Rồi like. Và hate. Rồi. Thì bây giờ. Nếu như. Về mặt cảm quan. Thì chắc chắn các bạn sẽ. Cảm nhận rằng là. Love và like. Nó đều dịch sang tiếng Việt. Nó đều là thích. Thì hai cái từ này. Nó sẽ có cái mối quan tương đồng. Cao hơn. So với lại. Love và hate. Yêu một cái yêu một cái hate. Thì hai cái từ này. Các bạn cảm nhận được là nó sẽ. Là đối nhật nhau về mặt ngữ nghĩa."
        },
        {
          "index": 16,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 749,
          "end_time": 810,
          "text": "So với lại. Love và hate. Yêu một cái yêu một cái hate. Thì hai cái từ này. Các bạn cảm nhận được là nó sẽ. Là đối nhật nhau về mặt ngữ nghĩa. Thế thì bây giờ chúng ta sẽ cùng xem. Cái điều đó nó có đúng hay không. Giữa love. Và like. Rồi. Thì ở đây sẽ là love. Rồi. Giữa love và hate. Thì ở đây sẽ là love.    Thì nếu như chúng ta. Sử dụng ngữ nghĩa. Thì chúng ta sẽ xem. Love và hate rõ ràng là độ tương đồng. Nó sẽ không cao bằng love và hate. Thì bây giờ chúng ta sẽ chạy ra thử nè. Love, hate. Mình chưa chạy cái lệnh này. Rồi. Thì giữa love và like. Chúng ta thấy là độ tương đồng."
        },
        {
          "index": 17,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 801,
          "end_time": 861,
          "text": "Love, hate. Mình chưa chạy cái lệnh này. Rồi. Thì giữa love và like. Chúng ta thấy là độ tương đồng. Thấp. Trong khi đó. Love và hate. Thì lại độ tương đồng cao hơn. Nó đối nghịch nhau về mặt nghĩa. Tại vì hồi nãy chúng ta cảm nhận. Chúng ta đoán rằng là love và hate. Nó đối nghịch nhau về mặt nghĩa. Thì lại ra độ tương đồng phải thấp hơn. Thế thì điều này sẽ giải thích như thế nào. Ở đây thì chúng ta sẽ thấy. Love và hate. Ở trong cái mô hình word emitting. Thì nó sẽ không quan tâm về mặt nghĩa. Mà nó quan tâm về mặt. Vai trò ngữ pháp. Trong cái câu nào. Mà cái từ A. Có thể thay được cho từ B. Một cách dễ dàng. Và không thay đổi về mặt cấu trúc ngữ pháp. Cũng như là về mặt nghĩa. Không phải thay đổi về mặt nghĩa. Mà là có thể thay đổi. Và không có vi phạm những nguyên tác về mặt ngữ pháp. Thì hai cái từ đó. Có độ tương đồng cao. Thế thì cái từ love và cái từ hate."
        },
        {
          "index": 18,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 849,
          "end_time": 910,
          "text": "Không phải thay đổi về mặt nghĩa. Mà là có thể thay đổi. Và không có vi phạm những nguyên tác về mặt ngữ pháp. Thì hai cái từ đó. Có độ tương đồng cao. Thế thì cái từ love và cái từ hate. Thì nó có cái vai trò ngữ pháp. Giống nhau. Đó là điều là động từ. Nhưng mà riêng cái từ like. Nếu như các bạn tra trên tựa điển. Thì cái từ like này có rất nhiều vai trò. Like này có thể vừa là ép tính. Vừa có thể là love. Vừa có thể là love. Vừa có thể là. Các loại rất nhiều. Những loại từ khác. Đó. Thì. Adjective nữa. Như vậy thì. Nếu nói về mặt ngữ pháp. Thì cái từ love. Nó không có tương đồng với từ like. Nhiều như là từ love với từ hate. Rồi thì đây là một cái ví dụ. Và. Được là. Hai cái từ love, hate và like. Nó có cái mối quan hệ về mặt ngữ pháp như thế nào."
        },
        {
          "index": 19,
          "video_id": "Chương 6_WAiLM7OFU9A",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_1： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/WAiLM7OFU9A",
          "start_time": 900,
          "end_time": 918,
          "text": "Rồi thì đây là một cái ví dụ. Và. Được là. Hai cái từ love, hate và like. Nó có cái mối quan hệ về mặt ngữ pháp như thế nào. Love và hate. Có cái mối quan hệ ngữ pháp giống nhau. Nhiều hơn so với lại love và like. Nên cái sự tương đồng của nó sẽ cao hơn."
        }
      ]
    },
    {
      "video_id": "Chương 6_UfLLBOPvgOU",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- **Mục tiêu chính của bài giảng**: Hướng dẫn lập trình sử dụng Word2Vec để kiểm tra và khai thác các mối quan hệ ngữ nghĩa giữa từ bằng phép toán trên vector (ví dụ: analogies như king : queen :: man : woman) và triển khai các truy vấn tương tự bằng code (sử dụng model.most_similar). [1][3][6]\n\n- **Các khái niệm sẽ được đề cập**:\n  - Quan hệ ngữ nghĩa được biểu diễn bằng sai phân vector (vector differences / vector arithmetic). [1][3]\n  - Cách xây dựng và giải bài toán analogy bằng cách cộng/trừ vector và dùng hàm most_similar với tập positive/negative. [3][6]\n  - Các kiểu quan hệ minh họa: giới tính (gender), thủ đô - quốc gia (capital-country), số nhiều - số ít (plural-singular), và một số quan hệ động từ (verb tense/inflection) được đề cập. [4][8][11][12]\n\n## 2. Các điểm chính (Main Points)\n\n### A. Ý tưởng cơ bản: relation ≈ vector difference\n- Quan sát: vector(queen) - vector(king) ≈ vector(woman) - vector(man). Điều này cho thấy phép hiệu giữa hai từ biểu diễn *mối quan hệ* (ở đây là mối quan hệ về giới tính). [1]\n- Công thức biểu diễn:\n  - queen - king ≈ woman - man. [1]\n  - Để tìm từ x sao cho x có cùng quan hệ với king như queen, ta đặt:\n    x - king = woman - man  ⇒  x = king + (woman - man) = woman + king - man. [3][6]\n\n### B. Triển khai lập trình (sử dụng most_similar)\n- Kỹ thuật thực thi: gọi model.most_similar với danh sách *positive* và *negative* theo cách:\n  - positive = [woman, king], negative = [man]  (tương đương tìm x gần với woman + king - man). [3][6]\n- Giải thích dấu + / - trong truy vấn:\n  - Các từ trong positive có hiệu lực *cộng* (tăng similarity), từ trong negative có hiệu lực *trừ*. [3]\n- Ví dụ in/hiển thị kết quả:\n  - Trong code, lưu kết quả trung gian vào biến (ví dụ `result`) rồi in ra thông điệp dạng “if king is a man then queen is a …” lấy phần tử `result[0]` làm từ đứng gần nhất. [6][7]\n\n### C. Kết quả kiểm chứng cho quan hệ giới tính\n- Khi chạy truy vấn analogue (positive=[woman,king], negative=[man]) kết quả từ gần nhất trả về là *queen* — xác nhận giả thuyết rằng phép hiệu vector bảo toàn quan hệ giới tính. Các từ gần tiếp theo có thể là *nomad*, *princess*, … (kết quả cụ thể theo mô hình). [4]\n\n### D. Quan hệ thủ đô - quốc gia (capital-country)\n- Công thức analogously:\n  - Nếu “Hà Nội is capital of Vietnam” thì để hỏi “the capital of Germany is ?” ta xây dựng:\n    x - Vietnam = Hanoi - Vietnam? (diễn đạt trong video là đưa Vietnam và Hanoi qua bên phải) và cuối cùng biểu diễn truy vấn như:\n    positive = [Germany, Hanoi], negative = [Vietnam]  ⇒ x ≈ Germany + Hanoi - Vietnam. [8][9]\n- Kết quả minh họa:\n  - Truy vấn trả về *Berlin* cho mệnh đề “if Hà Nội is capital of Vietnam then the capital of Germany is …”. [10]\n- Có thể thay thế các cặp khác như (China, Beijing) tương tự. [10]\n\n### E. Quan hệ số nhiều - số ít (plural-singular)\n- Ví dụ nêu trong bài:\n  - Muốn tìm dạng số nhiều của “box” từ “books”: sử dụng dạng x = box + (books_s) - (books) (ý là đưa books có -s và books không -s vào positive/negative) để nhận về *boxes*. [11]\n\n### F. Quan hệ động từ (verb relations) — phần chưa rõ ràng\n- Bài giảng đề cập sẽ có một vài ví dụ về động từ (3 ví dụ), nhưng phần ghi âm/ghi chép lại chứa nhiều phần không rõ / lặp ký tự, nên nội dung chi tiết và ví dụ chính xác không thể xác định từ các chunk được cung cấp. [12][13]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Analogy giới tính:\n  - king : queen :: man : woman — kiểm tra bằng phép toán vector và hàm most_similar; kết quả xấp xỉ cho thấy queen đứng đầu danh sách kết quả. [1][3][4][6]\n\n- Câu hỏi dạng logic (if ... then ...):\n  - Ví dụ in ra dạng câu: “if king is a man then queen is a …” và lấy phần tử result[0] (từ gần nhất) để hiển thị. Đây là cách trình bày kết quả cho người dùng. [6][7]\n\n- Thủ đô – quốc gia:\n  - “If Hà Nội is capital of Vietnam then the capital of Germany is …” → model trả về *Berlin*. Có thể áp dụng cho các cặp khác như (China, Beijing). [8][9][10]\n\n- Số nhiều – số ít:\n  - Từ ví dụ: books (số nhiều) và books (số ít) được dùng để tạo truy vấn tìm boxes (kết quả dạng số nhiều của box). [11]\n\n- Ứng dụng thực tế / trường hợp sử dụng:\n  - Phát hiện và khai thác quan hệ ngữ nghĩa trong corpus (analogy solving).\n  - Ứng dụng trong tìm từ tương đồng, mở rộng từ khóa, kiểm tra consistency của embedding. (Các ví dụ minh họa cụ thể trong video: giới tính, thủ đô-quốc gia, plural). [1][4][10][11]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Word2Vec biểu diễn từ dưới dạng vector sao cho các quan hệ ngữ nghĩa xuất hiện như các phép hiệu vector; từ đó ta có thể giải bài toán analogy bằng cách cộng/trừ vector và dùng model.most_similar với danh sách positive/negative. [1][3][6]\n  - Các ví dụ cụ thể trong video chứng minh: quan hệ giới tính (king↔queen vs. man↔woman), thủ đô–quốc gia (Vietnam:Hanoi → Germany:Berlin), số nhiều–số ít (box↔boxes) đều được mô hình nắm bắt. [4][10][11]\n\n- Tầm quan trọng:\n  - Phương pháp này cho phép kiểm tra và khai thác cấu trúc ngữ nghĩa ẩn trong embedding, hỗ trợ nhiều ứng dụng xử lý ngôn ngữ tự nhiên như trả lời câu hỏi dạng analogy, mở rộng từ khóa, phân tích semântic similarity. (Các minh họa cụ thể được trình bày trong video). [1][4][10]\n\n- Liên hệ với các bài giảng khác:\n  - Trong các chunk được cung cấp không có liên hệ cụ thể tới bài giảng khác ngoài việc nhắc đến “các bài tập tương tự” và “bài học của chúng ta” (tức là bài tập/ví dụ tiếp theo sẽ mở rộng các kiểu quan hệ). Nội dung chi tiết các liên hệ này không được nêu rõ trong các đoạn hiện có. [7][11]\n\nGhi chú: một số đoạn cuối chứa nhiều ký tự lặp/không rõ nghĩa, nên các ví dụ động từ trong phần cuối không thể tái tạo chính xác từ các chunk cung cấp. [12][13]",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 6_UfLLBOPvgOU",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_2： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/UfLLBOPvgOU",
          "start_time": 0,
          "end_time": 63,
          "text": "tiếp theo thì chúng ta sẽ xem xét về mối quan hệ về mặt ngữ nghĩa của các từ này ở đây chúng ta sẽ xét 4 từ là king, queen, man và woman nếu như chúng ta vẽ 2 các vector chúng ta lấy vector queen trừ cho king, chúng ta sẽ có vector màu đỏ ở đây chúng ta lấy woman trừ cho man, chúng ta sẽ có vector màu đỏ ở đây và người ta quan sát rằng là hình như 2 cái vector này sắp xỉ nhau nó sắp xỉ nhau, đó là cái vector nó thể hiện mối quan hệ về mặt giới tính giữa giới tính nam và giới tính nữ, nếu như giới tính nam gọi là man thì giới tính nữ gọi là woman thì tương đương như vậy cái người đàn ông mà quyền lực là king thì người phụ nữ quyền lực là queen thì ở đây chúng ta sẽ xét 1 cái ví dụ để kiểm tra xem cái tính mối quan hệ về mặt của người đàn ông là king, người phụ nữ quyền lực là queen, nữ hoàng là x đó là king, chúng ta sẽ xét xem là các cái vector này có gần nhau hay không"
        },
        {
          "index": 2,
          "video_id": "Chương 6_UfLLBOPvgOU",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_2： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/UfLLBOPvgOU",
          "start_time": 49,
          "end_time": 110,
          "text": "thì ở đây chúng ta sẽ xét 1 cái ví dụ để kiểm tra xem cái tính mối quan hệ về mặt của người đàn ông là king, người phụ nữ quyền lực là queen, nữ hoàng là x đó là king, chúng ta sẽ xét xem là các cái vector này có gần nhau hay không thì nếu như chúng ta đặt queen này là x, đặt queen này là x thì x trừ king nó cốc bằng woman trừ cho man hay không thì ở đây là x này rồi và chúng ta xem coi x chúng ta sẽ kiểm tra xem x có sắp xỉ hoặc là có gần với từ queen hay không thì nếu như ở đây là x ha, queen là x, x trừ king"
        },
        {
          "index": 3,
          "video_id": "Chương 6_UfLLBOPvgOU",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_2： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/UfLLBOPvgOU",
          "start_time": 87,
          "end_time": 155,
          "text": "chúng ta sẽ kiểm tra xem x có sắp xỉ hoặc là có gần với từ queen hay không thì nếu như ở đây là x ha, queen là x, x trừ king chúng ta sẽ có cái dạng triển khai x trừ king sẽ là bằng woman trừ man rồi thì khi chúng ta muốn tính x thì chúng ta sẽ đem cái vết king này qua bên tay phải và thay dấu trừ thành dấu cộng như vậy thì chúng ta sẽ thấy là woman và king sẽ là positive tức là dấu cộng man sẽ là âm thì ở đây chúng ta sẽ dùng là modal.most similarity most similar và positive tức là dấu cộng là woman và king và negative sẽ là man negative thì chúng ta sẽ là man thì để xem coi cái kết quả nó ra như thế nào"
        },
        {
          "index": 4,
          "video_id": "Chương 6_UfLLBOPvgOU",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_2： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/UfLLBOPvgOU",
          "start_time": 150,
          "end_time": 212,
          "text": "negative thì chúng ta sẽ là man thì để xem coi cái kết quả nó ra như thế nào rồi kết quả là cái từ mà gần với cái x này nhất ý nghĩa của nó là gì cái từ gần với lại cái x này nhất chính là queen cái từ mà gần tiếp theo sẽ là queen sẽ là nomad rồi cái từ tiếp theo là princess thì từ gần nhất sẽ là queen như vậy thì rõ ràng là cái mạch đề này là đúng tức là cái giả thuyết này là đúng cái vector màu đỏ này là trùng nhau hoặc là có cái giá trị sắp xịn nhau rồi bây giờ chúng ta sẽ xem cái góc độ của cái câu này cái góc độ này tí dạng là câu hỏi nếu như chúng ta có một cái câu hỏi này thì chúng ta sẽ xem cái góc độ của cái câu này tí dạng là câu hỏi nếu như người đàn ông mà quyền lực thì gọi là vua thì hỏi người phụ nữ quyền lực là gì"
        },
        {
          "index": 5,
          "video_id": "Chương 6_UfLLBOPvgOU",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_2： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/UfLLBOPvgOU",
          "start_time": 199,
          "end_time": 260,
          "text": "cái góc độ này tí dạng là câu hỏi nếu như chúng ta có một cái câu hỏi này thì chúng ta sẽ xem cái góc độ của cái câu này tí dạng là câu hỏi nếu như người đàn ông mà quyền lực thì gọi là vua thì hỏi người phụ nữ quyền lực là gì thì đây chính là cái dạng formulae cái dạng mà đưa về cái câu hỏi giả thuyết nếu như người đàn ông quyền lực người đàn ông nè quyền lực là vua thì người phụ nữ quyền lực là gì thì chúng ta sẽ có được cái công thức này vậy thì bây giờ chúng ta sẽ thử trả lời câu hỏi sau nếu vua là vua mà là đàn ông thì hỏi nữ hoàng là gì thì ở đây chúng ta sẽ có nếu vua nè king trừ man tức là king là đàn ông thì hỏi win là gì tức là x như vậy thì từ cái này chúng ta sẽ suy ra là x sẽ là bằng x đem qua trái đúng không win thì giữ nguyên"
        },
        {
          "index": 6,
          "video_id": "Chương 6_UfLLBOPvgOU",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_2： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/UfLLBOPvgOU",
          "start_time": 249,
          "end_time": 308,
          "text": "thì hỏi win là gì tức là x như vậy thì từ cái này chúng ta sẽ suy ra là x sẽ là bằng x đem qua trái đúng không win thì giữ nguyên rồi cộng cho man đang trừ man thì chúng ta sẽ đem qua bên phải nó sẽ là cộng cho man và king đang là chỗ cộng qua đây sẽ là chỗ trừ rồi như vậy thì chúng ta sẽ copy cái câu này ở đây và win positive sẽ là win và man và negative sẽ là king thì ở đây chúng ta nên có một cái ờ cái câu lệnh in ra sao cho nó hợp lý một chút cho nó dễ hiểu một chút thì ở đây gọi là kết quả trung gian này chúng ta sẽ gọi là result rồi và ở đây chúng ta sẽ in ra màn hình là if à xin lỗi"
        },
        {
          "index": 7,
          "video_id": "Chương 6_UfLLBOPvgOU",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_2： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/UfLLBOPvgOU",
          "start_time": 299,
          "end_time": 360,
          "text": "kết quả trung gian này chúng ta sẽ gọi là result rồi và ở đây chúng ta sẽ in ra màn hình là if à xin lỗi if king is a man then win is a rồi thì chúng ta sẽ lấy cái kết quả cuối cùng result 0 à xin lỗi cái kết quả đầu tiên tức là cái vector nào cái từ nào mà gần với cái x này nhất rồi nếu vua là đàn ông thì nữ hoàng là phụ nữ woman thì đây là một trong những cái tính chất mà bảo toàn về mặt quan hệ về giới tính rồi    thế thì bây giờ chúng ta sẽ làm các cái ví dụ tương tự các cái bài tập cho cái mối quan hệ về thủ đô quốc gia rồi danh từ"
        },
        {
          "index": 8,
          "video_id": "Chương 6_UfLLBOPvgOU",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_2： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/UfLLBOPvgOU",
          "start_time": 349,
          "end_time": 409,
          "text": "quan hệ về giới tính rồi    thế thì bây giờ chúng ta sẽ làm các cái ví dụ tương tự các cái bài tập cho cái mối quan hệ về thủ đô quốc gia rồi danh từ danh từ số nhiều động từ động từ vớt ba thì đầu tiên đó là cho cái quan hệ thủ đô quốc gia rồi thì ở đây chúng ta sẽ đặt một cái mệnh đề đó là nếu thủ đô của Việt Nam thủ đô của Việt Nam là Hà Nội thì thủ đô của Đức là gì đúng không thì thủ đô của Đức Đức là Germany là gì thì ở đây là x và đây chúng ta sẽ để dưới trừ rồi đây là mối quan hệ về đất nước nè thủ đô rồi thì hỏi đất nước Germany thì thủ đô sẽ là gì thì chúng ta sẽ đưa cái công thức này ra là x sẽ là bằng Germany"
        },
        {
          "index": 9,
          "video_id": "Chương 6_UfLLBOPvgOU",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_2： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/UfLLBOPvgOU",
          "start_time": 398,
          "end_time": 468,
          "text": "đây là mối quan hệ về đất nước nè thủ đô rồi thì hỏi đất nước Germany thì thủ đô sẽ là gì thì chúng ta sẽ đưa cái công thức này ra là x sẽ là bằng Germany rồi Việt Nam và Hà Nội đem qua bên tay phải đúng không Việt Nam và Hà Nội đem qua bên tay phải trừ x đem qua bên tay trái rồi như vậy thì ở đây sẽ là Germany cộng cho Hà Nội và trừ cho Việt Nam rồi như vậy thì chúng ta sẽ copy cái nội dung ở trên đây xuống rồi thì Germany Hà Nội và negative là x thì sẽ có là Việt Nam và ở đây chúng ta sẽ có cái câu là if Hà Nội is capital of Vietnam then the capital of Germany is"
        },
        {
          "index": 10,
          "video_id": "Chương 6_UfLLBOPvgOU",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_2： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/UfLLBOPvgOU",
          "start_time": 445,
          "end_time": 510,
          "text": "if Hà Nội is capital of Vietnam then the capital of Germany is rồi thì chúng ta sẽ xem cái kết quả của mình là gì như vậy nếu Hà Nội là thủ đô của Việt Nam thì thủ đô của Đức sẽ là Berlin và chúng ta hoàn toàn có thể thay thế cho những cái đất nước khác ví dụ như là China Trung Quốc và Bắc Kinh thì đây là cái mối quan hệ về thủ đô quốc gia rồi tương tự như vậy chúng ta sẽ có cái danh từ số ít số nhiều thì ở đây để cho nhanh thì chúng ta sẽ có là books thì tương ứng nó sẽ là books có S thì chúng ta sẽ xem cái từ box thì tương ứng nó là cái gì"
        },
        {
          "index": 11,
          "video_id": "Chương 6_UfLLBOPvgOU",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_2： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/UfLLBOPvgOU",
          "start_time": 498,
          "end_time": 561,
          "text": "thì ở đây để cho nhanh thì chúng ta sẽ có là books thì tương ứng nó sẽ là books có S thì chúng ta sẽ xem cái từ box thì tương ứng nó là cái gì thì ở đây chúng ta sẽ đem x qua trên tay trái x sẽ là bằng box cộng cho books có S trừ cho books rồi như vậy là box đây sẽ là books và đây sẽ là books không có S rồi để cho nhanh thì chúng ta khỏi phải ghi cái câu dẫn nhập nó sẽ ra là boxes rồi tương tự như vậy cho mối quan hệ về đọc từ và bài học của chúng ta thì chúng ta sẽ có 1 vài động từ với 3 ví dụ như nếu như động từ thì mình sẽ có là"
        },
        {
          "index": 12,
          "video_id": "Chương 6_UfLLBOPvgOU",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_2： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/UfLLBOPvgOU",
          "start_time": 553,
          "end_time": 601,
          "text": "mối quan hệ về đọc từ và bài học của chúng ta thì chúng ta sẽ có 1 vài động từ với 3 ví dụ như nếu như động từ thì mình sẽ có là gold thì tương ứng nó sẽ là gold thì hỏi text thì nó sẽ là gì như vậy thì chúng ta sẽ có là x sẽ là bằng text cộng cho gold trừ cho gold rồi x x sẽ là bằng text cộng cho gold trừ cho gold x x x x x x x x x x x x x x x x x x   x x x   x  x  x x x x"
        },
        {
          "index": 13,
          "video_id": "Chương 6_UfLLBOPvgOU",
          "chapter": "Chương 6",
          "video_title": "[CS431 - Chương 6] Part 5_2： Hướng dẫn lập trình với Word2Vec",
          "video_url": "https://youtu.be/UfLLBOPvgOU",
          "start_time": 599,
          "end_time": 617,
          "text": "x  x x x x x x x       x x x x x x x x x"
        }
      ]
    },
    {
      "video_id": "Chương 7__KvZN8-SyvQ",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- **Mục tiêu chính của bài giảng:** Giới thiệu mạng Recurrent Neural Network (RNN) — một kiến trúc mạng thần kinh tuần tự phổ biến trong xử lý ngôn ngữ tự nhiên — và khảo sát loại dữ liệu *dạng chuỗi*, tính chất của nó, liệu các kiến trúc NN truyền thống có thể áp dụng trực tiếp hay không, sau đó sẽ đi sâu vào kiến trúc RNN và các vấn đề/giải pháp liên quan. [1][2]\n\n- **Các khái niệm sẽ được đề cập:** dữ liệu dạng chuỗi (text, audio, giá chứng khoán), mô hình hóa theo chỉ số thời gian (x_t, x_{t+1}), biểu diễn dữ liệu (string, ma trận, tensor, vector), tính phụ thuộc theo trình tự (order/temporal dependency), khó khăn khi dùng NN cố định kích thước và các cách biểu diễn từ (one-hot / bag-of-words), và preview về cấu phần/nhược điểm của RNN cùng các giải pháp. [1][2][3][16][17]\n\n## 2. Các điểm chính (Main Points)\n\n### A. Dạng dữ liệu chuỗi — định nghĩa và ví dụ\n- Dữ liệu chuỗi xuất hiện ở nhiều hình thức: văn bản, âm thanh, giá chứng khoán, v.v. [3]  \n- Mô hình hóa chuỗi bằng các chỉ số theo thời điểm: x_t, x_{t+1}, …; giá trị tiếp theo phụ thuộc vào giá trị trước đó (không độc lập) — tức là tồn tại sự phụ thuộc theo trình tự. [3][4]\n\n### B. Biểu diễn dữ liệu theo loại\n- **Văn bản:** thường biểu diễn dưới dạng danh sách các từ (string) hoặc mảng ký tự; ký hiệu thường dùng w1, w2, …, w_t (t là độ dài câu) — lưu ý t có thể thay đổi (ngắn hoặc lên đến hàng ngàn). [6][7][8]  \n- **Hình ảnh:** biểu diễn bằng ma trận 2 chiều (grayscale) hoặc tensor 3 chiều (RGB channels); kích thước (chiều ngang, chiều dọc) có thể thay đổi giữa các ảnh. [6][7][8][9]  \n- **Đặc trưng (feature vectors):** thường biểu diễn bằng vector có kích thước cố định (n phần tử), ví dụ các thuộc tính học sinh: lớp, tuổi, điểm toán, điểm văn, điểm trung bình. Kích thước này phải cố định để duy trì ý nghĩa. [6][8][9]\n\n### C. Tính chất thứ tự và kiểu phụ thuộc\n- **Trình tự quan trọng trong chuỗi (temporal order):** thứ tự các phần tử điều khiển ý nghĩa tổng thể; ví dụ thay đổi thứ tự từ “do you understand?” → “you do understand?” làm thay đổi từ câu hỏi sang khẳng định. Điều này nhấn mạnh rằng vị trí tương đối theo thời gian quyết định nghĩa. [4][5]  \n- **Nguồn gốc “thời gian”:** với ngôn ngữ (tương tự âm thanh), dữ liệu xuất phát từ tín hiệu thời gian nên yếu tố thời gian/chuỗi là tự nhiên. [10][11]  \n- **So sánh với ảnh:** ảnh phụ thuộc theo *hai* chiều không gian (bề ngang, bề cao); thay đổi vị trí các đối tượng (đám mây, mặt trời) làm thay đổi ý nghĩa ảnh. [11][12]  \n- **So sánh với vector đặc trưng:** thành phần trong vector thường độc lập về thứ tự — thay đổi thứ tự các thuộc tính không làm thay đổi nội dung thông tin (khác với chuỗi). [11][12]\n\n### D. Khó khăn khi áp dụng NN truyền thống cho chuỗi\n- **Độ dài biến thiên vs input cố định của NN:** mạng NN cổ điển yêu cầu đầu vào kích thước cố định (ví dụ 4 neuron đầu vào), trong khi văn bản/các chuỗi có độ dài thay đổi; điều này là rào cản trực tiếp để dùng NN “ngay lập tức” cho chuỗi. [14]  \n- **Hình ảnh có cách giải quyết (scaling) mà giữ được ý nghĩa:** với ảnh ta có thể scale/rescale để phù hợp kích thước đầu vào của CNN mà vẫn giữ được ý nghĩa nhìn nhận; nhưng văn bản không có phép nén tương đương mà giữ nguyên thứ tự/ý nghĩa. [15][16]  \n- **Biểu diễn one-hot / bag-of-words gây mất thứ tự:** nếu biểu diễn từ bằng one-hot vector (ví dụ từ “tuyệt” → [0 1 0 0], “quá” → [0 0 1 0]) rồi trộn/chồng các vector để tạo biểu diễn cố định cho câu, thì các câu có cùng tập từ nhưng khác thứ tự sẽ có cùng vector biểu diễn — tức là *mất thông tin về trình tự*. Ví dụ “do you understand” và “you do understand” có thể nhận cùng vector, làm mất thông tin thứ tự cần thiết. Đây là nguyên nhân khiến NN thông thường không đảm bảo được tính thứ tự của văn bản. [16][17][18][19]\n\n### E. Preview: RNN và các vấn đề sắp bàn\n- Bài sẽ tiếp tục phân tích **cấu phần của mạng RNN** (các thành phần và cách tính toán) và **một số vấn đề mà RNN đang gặp phải cùng các giải pháp** (được đề cập sẽ ở phần tiếp theo). [2]\n\n(Chú thích: ký hiệu và công thức nhỏ được sử dụng trong bài — chuỗi x_t, x_{t+1}; ký hiệu từ w1…w_t — đều được nêu ở trên.) [3][7]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- **Ví dụ minh họa thứ tự trong văn bản:** “do you understand?” (câu hỏi) vs “you do understand?” (câu khẳng định) — thay đổi thứ tự làm thay đổi nghĩa. [4][5]  \n- **Ví dụ văn bản tiếng Việt dùng trong bài:** câu “bầu trời xanh và bãi biển óng ánh” được dùng để minh họa chuỗi từ. [5][6][17]  \n- **Ví dụ hình ảnh:** ảnh có mây và mặt trời — thay đổi vị trí các đối tượng (đám mây xuống dưới, mặt trời lên trên) làm thay đổi ý nghĩa hình ảnh; ảnh có thể được scale để khớp đầu vào CNN mà vẫn giữ nghĩa nhìn. [12][15]  \n- **Ví dụ dữ liệu đặc trưng:** vector thuộc tính học sinh (lớp, tuổi, điểm toán, điểm văn, điểm trung bình) — thứ tự các đặc trưng mang ít ý nghĩa thay đổi, nên biểu diễn dạng vector cố định phù hợp. [6][11]  \n- **Ứng dụng thực tế:** xử lý ngôn ngữ tự nhiên (NLP), xử lý âm thanh, mô hình hóa giá chứng khoán — những lĩnh vực mà RNN từng là kiến trúc chủ đạo trong giai đoạn đầu của deep learning. [1][3]\n\n## 4. Kết luận (Conclusion)\n\n- **Tóm tắt các ý chính:** Dữ liệu dạng chuỗi (text, audio, chứng khoán) có đặc trưng *phụ thuộc theo trình tự* (temporal dependency) và độ dài biến thiên; biểu diễn chuỗi khác biệt so với ảnh (2D spatial) và vector đặc trưng (thứ tự không quan trọng). Những khác biệt này gây ra rào cản khi áp dụng NN cố định kích thước và các biểu diễn như bag-of-words dẫn tới mất thông tin thứ tự. Do đó cần các mô hình chuyên biệt (ví dụ RNN) để xử lý chuỗi. [3][10][11][14][16][18]\n\n- **Tầm quan trọng:** Hiểu rõ đặc tính chuỗi và hạn chế của biểu diễn cố định là bước căn bản để thiết kế/ứng dụng các mô hình phù hợp (như RNN và các biến thể) trong NLP, âm thanh, và các bài toán thời gian khác. RNN từng là kiến trúc rất phổ biến và là nền tảng cho nhiều công trình trong giai đoạn đầu của DL cho NLP. [1][2]\n\n- **Liên hệ với bài giảng tiếp theo:** Phần sau của bài (được nhắc tới trong bài giảng) sẽ trình bày chi tiết kiến trúc RNN, các thành phần và công thức tính toán, cùng các vấn đề mà RNN gặp phải và các giải pháp tương ứng — nội dung này là bước tiếp theo sau phần giới thiệu hiện tại. [2]\n\n---\n\nTất cả nội dung trên được trích trực tiếp từ các đoạn của video (các chunk) theo timestamps tương ứng: [1], [2], …, [19].",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 13,
          "end_time": 61,
          "text": "Trong bài hôm nay thì chúng ta sẽ đến với một cái môi học sâu đầu tiên rất là phổ biến và nổi tiếng trong lĩnh vực xử lý ngôn ngữ tự nhiên đó chính là mạng Recurrent Neural Network hay còn gọi là tên viết tắt là RNN. Trong giai đoạn đầu của Deep Learning trong lĩnh vực xử lý ngôn ngữ tự nhiên thì RNN là gần như là một trong những kiến trúc mà được sử dụng rất là phổ biến và gần như tất cả các bài báo đều xoay xung quanh các biến thể của mạng RNN này. Và nội dung chính của ngày hôm nay chúng ta sẽ cùng giới thiệu qua loại dữ liệu dạng chuỗi. Đối với những loại dữ liệu dạng chuỗi nó sẽ có những tính chất gì, nó có gì khác so với lại những loại dữ liệu khác."
        },
        {
          "index": 2,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 47,
          "end_time": 113,
          "text": "chúng ta sẽ cùng giới thiệu qua loại dữ liệu dạng chuỗi. Đối với những loại dữ liệu dạng chuỗi nó sẽ có những tính chất gì, nó có gì khác so với lại những loại dữ liệu khác. Và từ đó thì chúng ta sẽ biết rằng là liệu có thể sử dụng được các kiến trúc mạng trước đây, ví dụ như là neural network bình thường cho loại dữ liệu dạng chuỗi này hay không. Trong phần thứ hai thì chúng ta sẽ cùng tìm hiểu sâu hơn về kiến trúc mạng recurrent neural network, xem các cấu phần của nó. Các cấu phần của mạng recurrent neural network là cái gì và cách thức tính toán như thế nào. Và cuối cùng đó là chúng ta sẽ cùng tìm hiểu về một số cái vấn đề của mạng RNN hiện nay nó đang gặp phải. Và các giải pháp để giúp cho chúng ta giải quyết những cái vấn đề đó như thế nào. Đầu tiên đó là loại dữ liệu dạng chuỗi. Thì dữ liệu dạng chuỗi nó xuất hiện ở trong một số cái hình thức."
        },
        {
          "index": 3,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 99,
          "end_time": 168,
          "text": "Và các giải pháp để giúp cho chúng ta giải quyết những cái vấn đề đó như thế nào. Đầu tiên đó là loại dữ liệu dạng chuỗi. Thì dữ liệu dạng chuỗi nó xuất hiện ở trong một số cái hình thức. Ví dụ như là loại dữ liệu văn bản, loại dữ liệu âm thanh hoặc là dữ liệu giá chứng khoá. Thế thì thế nào gọi là dữ liệu dạng chuỗi. Dữ liệu dạng chuỗi nó sẽ được mô hình hóa dưới dạng là xt, xt cộng 1, v.v. Thì cái đầu ra, xin lỗi, cái giá trị tiếp theo nó sẽ đi phụ thuộc vào cái giá trị ở phía trước. Thông thường trong các cái nội dung của mình, không phải các cái từ xt, xt cộng 1, v.v. Cái từ xt và xt cộng 1 nó độc lập nhau mà nó có cái sự phụ thuộc lẫn nhau. Cái từ thứ t cộng 1 nó sẽ có cái mối quan hệ phụ thuộc với lại cái từ thứ t. Và trong tổng thể một cái câu hoặc là một cái đoạn âm thanh hoặc là giá chứng khoán, v.v. thì tùy vào cái trình tự xuất hiện của các cái giá trị vào mà mình sẽ có các cái ý nghĩa nó khác nhau hoàn toàn."
        },
        {
          "index": 4,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 143,
          "end_time": 207,
          "text": "Cái từ xt và xt cộng 1 nó độc lập nhau mà nó có cái sự phụ thuộc lẫn nhau. Cái từ thứ t cộng 1 nó sẽ có cái mối quan hệ phụ thuộc với lại cái từ thứ t. Và trong tổng thể một cái câu hoặc là một cái đoạn âm thanh hoặc là giá chứng khoán, v.v. thì tùy vào cái trình tự xuất hiện của các cái giá trị vào mà mình sẽ có các cái ý nghĩa nó khác nhau hoàn toàn. Ví dụ đối với văn bản thì chúng ta hay có cái câu đó là. Ví dụ từ tiếng Anh đi là mình sẽ dễ minh hoại nhất là do you understand? Chắc là bạn có hiểu không? Thì cái từ do này á, nó đặt ở phía trước nên ở đây chính là cái câu hỏi. Nhưng cũng 3 cái từ này nếu như chúng ta đặt ở cái trình tự khác, ví dụ như là you do understand? Ví dụ như là you do understand? Ví dụ như là you do understand? Thì nó lại ra một cái câu khẳng định là bạn hiểu rồi đó."
        },
        {
          "index": 5,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 202,
          "end_time": 259,
          "text": "Ví dụ như là you do understand? Ví dụ như là you do understand? Thì nó lại ra một cái câu khẳng định là bạn hiểu rồi đó. Còn ở trên đó là bạn có hiểu không? Đó thì cái trình tự xuất hiện của các cái từ x t và x t cộng 1 nó rất là quan trọng. Do đó cái dữ liệu dạng chuỗi chúng ta cần phải chú ý đến cái yếu tố này. Đó là trình tự. Và chúng ta sẽ so sánh một số cái loại dữ liệu với nhau để xem coi cái sự khác biệt của nó là gì. Đối với lại dữ liệu văn bản, à xin lỗi đối với lại dữ liệu chuỗi và cụ thể ở đây chúng ta sẽ lấy một cái ví dụ đó là dữ liệu văn bản. Thì cái ví dụ để minh họa cho cái dữ liệu này đó chính là một cái câu, một cái đoạn văn. Ví dụ như là bầu trời xanh và bãi biển ống ánh. Thì cái dữ liệu hình ảnh chúng ta sẽ có cái ví dụ đó là một cái tấm hình như thế này."
        },
        {
          "index": 6,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 247,
          "end_time": 313,
          "text": "Ví dụ như là bầu trời xanh và bãi biển ống ánh. Thì cái dữ liệu hình ảnh chúng ta sẽ có cái ví dụ đó là một cái tấm hình như thế này. Đó. Rồi.  Đối với dữ liệu mà dạng đặc trưng. Ví dụ như các cái thuộc tính của một cái học sinh. Chúng ta có các cái thuộc tính ví dụ như là thuộc tính đầu tiên là lớp 7. Thuộc tính thứ hai 15 tuổi. Thuộc tính thứ ba là điểm toán. Thuộc tính thứ tư là điểm văn. Thuộc tính thứ năm đó là điểm trung bình. Ví dụ vậy. Thì về các biểu diễn thông thường. Các biểu diễn thông thường. Của cái loại dữ liệu dạng chuỗi. Đó chính là chúng ta sẽ sử dụng dạng là danh sách các cái từ. Hay còn gọi là string. Trong cái lập trình của mình thì gọi là string. Hoặc là mảng các cái ký tự. Đó. Rồi. Ở trong loại dữ liệu là hình ảnh. Thì chúng ta sẽ có cái cách biểu diễn phổ biến."
        },
        {
          "index": 7,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 298,
          "end_time": 363,
          "text": "Trong cái lập trình của mình thì gọi là string. Hoặc là mảng các cái ký tự. Đó. Rồi. Ở trong loại dữ liệu là hình ảnh. Thì chúng ta sẽ có cái cách biểu diễn phổ biến. Đó chính là dữ liệu mạng ma trận 2 chiều. Đối với lại những cái ảnh mà không có màu. Hay còn gọi là ảnh. Ảnh mức sám. Ảnh grayscale. Và tensor 3 chiều. Đối với cả ảnh màu. Và ảnh màu này thì có 3 canh màu thông thường. Là red, green, blue. Là đỏ, xanh lá và xanh dương. Còn để biểu diễn cho cái dữ liệu mà dưới dạng là đặc trưng của một cái đối tượng. Thì người ta thường hay sử dụng đó là vector. Biểu diễn dưới dạng vector. Rồi. Và tiếp theo. Đó là về hệ thống ký hiệu. Thì đối với dữ liệu dạng chuỗi. Chúng ta sẽ hay ký hiệu đó là w1, w2 cho đến wt. Trong đó t chính là cái số từ trong một cái câu."
        },
        {
          "index": 8,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 348,
          "end_time": 412,
          "text": "Đó là về hệ thống ký hiệu. Thì đối với dữ liệu dạng chuỗi. Chúng ta sẽ hay ký hiệu đó là w1, w2 cho đến wt. Trong đó t chính là cái số từ trong một cái câu. Hoặc là cái độ dài. Và chúng ta có một cái lưu ý đó là độ dài của cái văn bản t này nè. Là có thể thay đổi. T này có thể là rất là ít. Ví dụ như là chỉ là bằng 1. Nhưng nó cũng có thể rất là nhiều. Ví dụ như có thể lên đến hàng ngàn. Còn đối với lại dữ liệu hình ảnh. Thì chúng ta sẽ ký hiệu nó dưới dạng là ma trận. Ví dụ trong trường hợp này. Chúng ta sử dụng ma trận 2 chiều. Còn đối với tensor 3 chiều. Thì nó sẽ phức tạp hơn một chút. Đối với ma trận 2 chiều. Thì chúng ta sẽ có 2 cái thông số. Đó là bề ngang. Và bề cao. Là để thể hiện cái kích thước của cái hình ảnh của mình. Bề ngang và bề cao 3. Và chúng ta cũng lưu ý. Đó là cái bề ngang và bề cao."
        },
        {
          "index": 9,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 400,
          "end_time": 460,
          "text": "Và bề cao. Là để thể hiện cái kích thước của cái hình ảnh của mình. Bề ngang và bề cao 3. Và chúng ta cũng lưu ý. Đó là cái bề ngang và bề cao. Bề rộng và bề dài. Trong cái này dùng từ bề rộng và bề dài. Thì hoàn toàn có thể thay đổi được. Có thể thay đổi. Thì chúng ta thấy là. Các cái ảnh của mình. Nó có thể có những cái độ phân giải. Khác nhau. Có những ảnh rất là nhỏ. Nhưng mà có những cái ảnh rất là to. Còn khi. Biểu diễn cho đặc trưng. Của một cái đối tượng. Thì. Thông thường chúng ta sẽ phải biểu diễn dưới dạng là một cái vector. Với. N phần tử. Và n này phải là cố định. N này sẽ là không thay đổi. Rồi. Và tính chất. Của các cái phần tử. Trong cái dữ liệu này của mình. Đó là. Đầu tiên. Đối với lại dữ liệu dạng chuỗi á. Thì hồi nãy chúng ta đã có trình bày rồi."
        },
        {
          "index": 10,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 449,
          "end_time": 510,
          "text": "Và tính chất. Của các cái phần tử. Trong cái dữ liệu này của mình. Đó là. Đầu tiên. Đối với lại dữ liệu dạng chuỗi á. Thì hồi nãy chúng ta đã có trình bày rồi. Là cái tính trình tự. Nó rất là quan trọng. Cái từ thứ 2. Mà đứng sau từ thứ 3. Thì nó sẽ có một cái ý nghĩa. Nhưng mà đứng trước từ thứ W2. Mà đứng trước từ W1. Thì nó lại có một cái nghĩa khác. Giống như ví dụ ở trên. Do đó thì. Ở đây chúng ta sẽ có cái mối quan hệ đó là. Các cái phần tử. Trong dữ liệu. Nó sẽ phụ thuộc theo. Một chiều thời gian. Thì tại sao ở đây mình lại dùng cái từ là thời gian. Tại vì. Nguồn gốc của ngôn ngữ. Nó xuất phát là từ. Giọng nói. Từ tiếng nói. Thì khi cái tiếng nói của mình. Mà mình cất ra đó. Thì nó đi theo cái chuỗi là cái chuỗi thời gian. Đúng không? Lúc mà nó đưa vô bên trong. Qua. Cái đường là. Thính giác. Thì nó sẽ là đi theo cái chuỗi thời gian. Thì đó là. Tại sao mình lại dùng cái từ đó là phụ thuộc theo."
        },
        {
          "index": 11,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 497,
          "end_time": 560,
          "text": "Thì nó đi theo cái chuỗi là cái chuỗi thời gian. Đúng không? Lúc mà nó đưa vô bên trong. Qua. Cái đường là. Thính giác. Thì nó sẽ là đi theo cái chuỗi thời gian. Thì đó là. Tại sao mình lại dùng cái từ đó là phụ thuộc theo. Theo chiều thời gian. Tương tự như vậy cho dữ liệu âm thanh. Mà chứng khoán. Đúng không? Thì cái T này. Hàm ý đó là thời gian. Đối với dữ liệu hình ảnh. Thì. Cái sự phụ thuộc này. Là nó sẽ phụ thuộc. Ở 2 chiều. Nó phụ thuộc ở cả 2 chiều. Và 2 chiều này. Nó gọi là chiều không gian. Bề ngang. Bề cao. Nó gọi là chiều không gian. Trong khi đó. Dữ liệu đặc trưng. Thì. Các cái phần tử này. Nó độc lập nhau. Nghĩa là sao. Nếu như chúng ta. Quy ước. Là. Thành phần đầu tiên. Là lớp. Thành phần thứ 2 là tuổi. Thành phần thứ 3 là điểm toán. Thành phần thứ 4 là điểm văn. Thành phần thứ 5 là điểm trung bình. Đúng không? Thì nếu như chúng ta đổi lại. Cái trình tượng này. Ví dụ. Chúng ta đưa điểm toán lên trước. Sau đó sẽ đến điểm văn."
        },
        {
          "index": 12,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 549,
          "end_time": 610,
          "text": "Thành phần thứ 4 là điểm văn. Thành phần thứ 5 là điểm trung bình. Đúng không? Thì nếu như chúng ta đổi lại. Cái trình tượng này. Ví dụ. Chúng ta đưa điểm toán lên trước. Sau đó sẽ đến điểm văn. Sau đó đến điểm trung bình. Rồi sau đó là lớp. Và sau đó là tuổi. Thì cái thông tin của cái đặc trưng này. Nó vẫn bảo toàn. Nó không hề thay đổi cái nội dung. Nó không thay đổi cái nội dung của cái. Cái đặc trưng. Trong khi đó. Cũng là cái từ. Du từ Ananda.  Nhưng nếu chúng ta đảo lại. Thứ tự cho nhau. Du lên trước. Du ra sau. Thì đó là khẳng định. Nhưng mà du lên trước. Du ra sau. Thì đó là câu hỏi. Thì tự nhiên cái. Ý nghĩa của cái. Đoạn văn. Của cái văn bản đó. Là bị thay đổi hoàn toàn. Cũng tương tự như vậy cho hình ảnh. Nếu như. Chúng ta đưa các cái đối tượng hình ảnh này. Lên trên các cái vị trí khác nhau. Ví dụ như đưa đám mây xuống dưới. Đưa mặt trời lên trên."
        },
        {
          "index": 13,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 600,
          "end_time": 659,
          "text": "Cũng tương tự như vậy cho hình ảnh. Nếu như. Chúng ta đưa các cái đối tượng hình ảnh này. Lên trên các cái vị trí khác nhau. Ví dụ như đưa đám mây xuống dưới. Đưa mặt trời lên trên. Thì tự nhiên nó sẽ tạo ra một cái tấm hình. Có cái. Kích thước. Xin lỗi. Nó có cái ý nghĩa. Khác nhau hoàn toàn. Do đó thì. Ở đây. Hai cái loại dữ liệu chuỗi và hình ảnh. Thì nó bị phù thuộc lẫn nhau. Nó bị ràng buộc lẫn nhau. Trong khi đó. Dữ liệu vector. Thì nó sẽ. Độc lập. Và. Ý tưởng. Để áp dụng. Cho cái loại dữ liệu văn bản. Lên. Lên trên các cái mô hình máy học. Đúng không? Thì đó. Chính là chúng ta. Có những cái ý tưởng đầu tiên. Để mà. Hiệp. Để mà. Và kế thừa những cái thành tựu. Của mạng Neural Network trước đây. Thế thì. Cái khó khăn đầu tiên. Mà chúng ta khi áp dụng. Cái dữ liệu dạng chuỗi. Vào một cái mạng Neural Network. Đó chính là chúng ta có một cái nhận xét như sau."
        },
        {
          "index": 14,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 649,
          "end_time": 710,
          "text": "Thế thì. Cái khó khăn đầu tiên. Mà chúng ta khi áp dụng. Cái dữ liệu dạng chuỗi. Vào một cái mạng Neural Network. Đó chính là chúng ta có một cái nhận xét như sau. Văn bản. Thì có cái độ dài là không cố định. Ví dụ. Đối với cái câu này. Thì ở đây. Độ dài của cái văn bản này là 2. Nhưng. Ở cái câu sau. Bầu trời xanh và biển mạng ông ánh. Ví dụ vậy. Thì. Cái độ dài của mình. Nó có thể lên đến là. 10 chữ. Trong khi đó. Cái mạng Neural Network của mình. Cái đầu vào của mình. Nó lại cố định. Thì chúng ta đã học cái mạng Neural Network rồi. Đầu vào của mình. Nếu như nó chỉ có 4 neuron. Thì. Xuyên suốt. Từ cái quá trình huấn luyện. Cho đến. Quá trình mà. Dự đoán. Nó cũng hoàn toàn có thể là. Là. Nó. Để cho đến. Cái độ dài của mình. Nó sẽ dựa nguyên. Là 4 neuron. Các bạn sẽ hỏi là. Tại sao. Ở trong cái mạng CNN. Đúng không. Thì."
        },
        {
          "index": 15,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 700,
          "end_time": 761,
          "text": "Để cho đến. Cái độ dài của mình. Nó sẽ dựa nguyên. Là 4 neuron. Các bạn sẽ hỏi là. Tại sao. Ở trong cái mạng CNN. Đúng không. Thì. Các cái ảnh của mình. Khi chúng ta đưa vào một cái mạng CNN. Đưa vào một cái mạng CNN. Thì nó sẽ làm một cái thao tác. Đó là. Scale. Mình sẽ đưa một cái ảnh rất là to. Scale về đúng cái tỷ lệ. Scale về đúng cái tỷ lệ. Mà cái mạng CNN.      Nó. Dẫn làm đầu vào. Đúng không. Thì. Đối với ảnh. Nó lại là một cái loại dữ liệu đặc biệt. Các cái trình tự. Các cái điểm ảnh. Nó phụ thuộc theo trình tự. Không gian. Đúng không. Nhưng mà một cái ảnh to. Một cái ảnh to. Đó. Ví dụ như chúng ta có một cái đối tượng ở đây. Khi chúng ta. Thu nhỏ nó lại. Để tạo ra thành một cái. Ảnh nhỏ. Đáp ứng được cái yêu cầu đầu vào. Của cái mạng CNN. Thì về mặt ngũ nghĩa. Là chúng ta nhìn vô tấm ảnh này."
        },
        {
          "index": 16,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 749,
          "end_time": 810,
          "text": "Khi chúng ta. Thu nhỏ nó lại. Để tạo ra thành một cái. Ảnh nhỏ. Đáp ứng được cái yêu cầu đầu vào. Của cái mạng CNN. Thì về mặt ngũ nghĩa. Là chúng ta nhìn vô tấm ảnh này. Chúng ta vẫn biết được cái ý nghĩa của nó. Hay nói cách khác là ý nghĩa nó không thay đổi. Còn. Ở đây. Cái đoạn văn của mình. Mình sẽ không có cái cách nào. Để mà mình nén. Mình nén cái. Đoạn văn này. Về cái dạng. Là một cái vector 4 chiều. Cố định về số chiều. Có bạn sẽ nói. Tôi dùng giải pháp. Là. Bên cạnh.           Mình sẽ đưa về cái. Mấy câu quật có được hay không. Tức là. Mọi cái câu trong văn bản. Đúng không. Hoặc là mọi từ. Trong văn bản. Sẽ đưa về cái dạng là. Một cái vector. Cố định số chiều. Ví dụ. Cái từ tuyệt. Đúng không. Nó tương ứng sẽ là. 0 1 0 0. Rồi. Cái từ quá."
        },
        {
          "index": 17,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 799,
          "end_time": 860,
          "text": "Cố định số chiều. Ví dụ. Cái từ tuyệt. Đúng không. Nó tương ứng sẽ là. 0 1 0 0. Rồi. Cái từ quá. Đúng không. Nó sẽ là. Đây là từ tuyệt ha. Từ quá. Nó sẽ là. 0 0 1 0.     Rồi. Và khi chúng ta. Sử dụng cái mô hình. Bên cạnh. Một cái vector. Thì chúng ta sẽ trộn. Hai cái vector này. Lại với nhau. Để tạo thành. Một cái vector. Đó là. 0 1. 1 0. Đúng không. Để ra cái vector biểu diễn. Cho từ quá. Và. Số từ. Của cái vector biểu diễn này. Nó sẽ đều là. Cố định là. V. Tức là. Số từ.      Số phần tử trong cái tập dictionary. Trong cái tập tiểu điển. Tương tự như vậy. Cho cái câu. Bầu trời xanh. Và. Biển. Vàng ống ánh. Ví dụ vậy. Thì nó cũng sẽ biểu diễn. Với một cái vector."
        },
        {
          "index": 18,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 849,
          "end_time": 910,
          "text": "Số phần tử trong cái tập dictionary. Trong cái tập tiểu điển. Tương tự như vậy. Cho cái câu. Bầu trời xanh. Và. Biển. Vàng ống ánh. Ví dụ vậy. Thì nó cũng sẽ biểu diễn. Với một cái vector. Có số chiều là b. Tại như vậy. Thì nó cố định. Số chiều. Đó. Thì. Với giải pháp này. Nó sẽ bị một vấn đề. Đó là. Nó không đảm bảo được. Cái yếu tố. Về mặt trình tự. Tại sao. Tại vì cái câu. Do you. Understand. Với cái câu là. You do understand. Sẽ có cùng. Cái vector biểu diễn. Nó sẽ có cùng một cái vector biểu diễn. Ví dụ như là. Một. Không. Không. Không. Một. Không. Ví dụ vậy. Cả hai từ này. Đều có cùng cách biểu diễn. Thì như vậy là. Chúng ta có thể.             Để tính đảm bảo. Của mạng. Neural Network. Là. Cho cái phần."
        },
        {
          "index": 19,
          "video_id": "Chương 7__KvZN8-SyvQ",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 1： Giới thiệu mạng RNN - Xử lý dữ liệu dạng chuỗi",
          "video_url": "https://youtu.be/_KvZN8-SyvQ",
          "start_time": 899,
          "end_time": 929,
          "text": "Cả hai từ này. Đều có cùng cách biểu diễn. Thì như vậy là. Chúng ta có thể.             Để tính đảm bảo. Của mạng. Neural Network. Là. Cho cái phần. Mà tính thứ tự. Của văn bản. Là không đảm bảo. Và đó chính là. Những cái rào cản. Để cho chúng ta không thể sử dụng. Cái mạng. Neural Network. Một cách. Trực tiếp. Với cái loại dữ liệu. Là văn bản. Hoặc là cho các cái loại. Dữ liệu dạng chuỗi khác. Các bạn có thể."
        }
      ]
    },
    {
      "video_id": "Chương 7_TqKBlC-zyKY",
      "summary": "## 1. Giới thiệu (Introduction)\n- Mục tiêu chính của bài giảng: Giới thiệu kiến trúc Recurrent Neural Network (RNN) và cách triển khai RNN để mã hóa thông tin trình tự (sequence) trong dữ liệu ngôn ngữ, đồng thời trình bày công thức toán học cơ bản và một bài tập về kích thước ma trận tham số. [1][2][6][8][12]  \n- Các khái niệm sẽ được đề cập: ý tưởng *recurrent* (hồi quy) để giữ thông tin quá khứ, trạng thái ẩn (hidden state), chia sẻ tham số qua các time step (U, V, W dùng chung), hàm kích hoạt (sigmoid/tanh), hàm softmax cho đầu ra, và một ví dụ tính kích thước ma trận U, W, V cho dữ liệu cụ thể (one-hot / embedding). [1][2][6][9][10][11][12]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Tại sao cần RNN — vấn đề của MLP với dữ liệu theo trình tự\n- Mạng Neural Network kiểu feedforward không thể hiện được thứ tự của các token khi ta biểu diễn từ bằng one-hot vectors; nó không biết từ nào xuất hiện trước hay sau. [1]  \n- RNN thêm yếu tố *recurrent* (hồi quy) để mã hóa yếu tố trình tự bằng cách truyền thông tin trạng thái từ thời điểm trước sang thời điểm hiện tại. [2][3]\n\n### 2.2. Ý tưởng cơ bản về truyền trạng thái (state propagation)\n- Khi xử lý token tại thời điểm t-1, mạng tạo ra một trạng thái ẩn (quá khứ) và truyền trạng thái này đến nút xử lý token ở thời điểm t; tại thời điểm t, trạng thái quá khứ kết hợp với đầu vào hiện tại để tổng hợp thông tin và tạo ra dự đoán ŷ_t. Quá trình này lặp đi lặp lại theo thời gian. [2][3][4]  \n- Trạng thái ẩn St thể hiện thông tin tổng hợp của quá khứ và hiện tại, và St của thời điểm t tiếp tục là \"quá khứ\" cho thời điểm t+1. [3][4]\n\n### 2.3. Ký hiệu chuẩn và chia sẻ tham số\n- Ký hiệu hay dùng: x_t là input tại time t, s_t là trạng thái ẩn (hidden state) tại time t, ŷ_t là đầu ra dự đoán tại time t. Ma trận tham số chính: U, W, V. [6][7]  \n- Một điểm rất quan trọng: các bộ tham số U, V, W được *dùng chung* cho mọi bước tính toán (shared across time steps). Điều này cho phép mô hình xử lý chuỗi có độ dài thay đổi. [6][7]\n\n### 2.4. Công thức toán học cho RNN đơn giản (Elman / vanilla RNN)\n- Công thức cập nhật trạng thái ẩn:\n  s_t = activation( U x_t + W s_{t-1} )  \n  với activation là hàm phi tuyến như sigmoid hoặc tanh. [9][10]  \n- Công thức cho đầu ra (trước softmax) và dự đoán:\n  o_t = V s_t  \n  ŷ_t = softmax( o_t ) = softmax( V s_t ). [10][11]  \n- Ghi chú về hàm phi tuyến (sigmoid/tanh): các hàm này hoạt động phần tử trên vector đầu vào nên không thay đổi kích thước vector; softmax cũng giữ nguyên kích thước đầu vào. [9][14][18]\n\n### 2.5. Giải thích vai trò các ma trận U, W, V\n- U: ánh xạ input x_t vào không gian của trạng thái ẩn; W: ánh xạ trạng thái ẩn trước đó s_{t-1} vào cùng không gian để cộng hợp với thông tin hiện tại; V: ánh xạ trạng thái ẩn sang không gian đầu ra (logits trước softmax). [10][11]  \n- Kích thước và chiều của các ma trận tuân theo quy tắc nhân ma trận (số cột của ma trận trái phải bằng số hàng của ma trận phải). Trong tài liệu có hai cách ký hiệu transpose ở các nguồn khác nhau, nhưng ý nghĩa cuối cùng không khác nhau về bản chất — chỉ khác cách viết kích thước. [11][17]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n### 3.1. Ví dụ bài tập kích thước ma trận (từ video)\n- Dữ liệu cho trước trong bài tập:\n  - x_t là vector one-hot có 8000 phần tử (vocabulary size V = 8000). [12][13]  \n  - embedding / word vector (hay trạng thái ẩn) được cho là 100 chiều (hidden size = 100). [12][13]  \n  - ŷ_t là vector 8000 chiều (logits cho mỗi từ trong từ điển). [12]  \n- Mục tiêu: xác định kích thước của U, W, V. [12]\n\n- Lý luận và nghiệm:\n  - s_t được cho là vector 100 x 1 (hidden size = 100). Vì activation là phần tử-wise nên kích thước không thay đổi. [14][15]  \n  - Xác định U: để U x_t cho ra vector 100 x 1 khi x_t là 8000 x 1, U phải có kích thước 100 x 8000. (Vì (100 x 8000) * (8000 x 1) = 100 x 1). [15][16]  \n  - Xác định W: để W s_{t-1} (s_{t-1} là 100 x 1) cho ra 100 x 1, W phải có kích thước 100 x 100. (Vì (100 x 100) * (100 x 1) = 100 x 1). [16][17]  \n  - Xác định V: để V s_t (s_t là 100 x 1) cho ra vector y kích thước 8000 x 1 trước softmax, V phải có kích thước 8000 x 100. (Vì (8000 x 100) * (100 x 1) = 8000 x 1). [18][19]  \n- Kết quả cuối cùng: U ∈ R^{100 x 8000}, W ∈ R^{100 x 100}, V ∈ R^{8000 x 100}. [16][17][19]\n\n### 3.2. Ứng dụng thực tế / trường hợp sử dụng (những gì đề cập trong video)\n- RNN được dùng để mã hóa thứ tự các từ trong văn bản, giúp mô hình có cơ sở để dự đoán token tiếp theo dựa trên lịch sử (sequence modeling). Video nhấn mạnh khả năng *encode thứ tự* thông qua cơ chế hồi quy. [1][2][7]\n\n## 4. Kết luận (Conclusion)\n- Tóm tắt các ý chính:\n  - RNN bổ sung cơ chế hồi quy (recurrent) để giữ và truyền thông tin quá khứ, giải quyết hạn chế của mạng feedforward với dữ liệu theo trình tự. [1][2][3]  \n  - Mạng RNN cơ bản dùng công thức s_t = activation(U x_t + W s_{t-1}) và ŷ_t = softmax(V s_t), với U, W, V là tham số được chia sẻ qua các bước thời gian. [9][10][6]  \n  - Trong ví dụ số, khi x_t là one-hot 8000-dim, hidden size = 100 và output logits 8000-dim, ta có U: 100x8000, W: 100x100, V: 8000x100. [12][16][17][19]\n- Tầm quan trọng: Hiểu cấu trúc và kích thước tham số của RNN là nền tảng để triển khai các mô hình chuỗi (sequence models) và để mở rộng sang các kiến trúc phức tạp hơn (ví dụ: LSTM/GRU hay Transformer — mặc dù không được thảo luận chi tiết trong video này). [1][2][6]  \n- Liên hệ với bài giảng khác: video nhắc lại khái niệm word vector/embedding đã được học trong bài trước (word vector = embedding), và bài tập kết nối trực tiếp embedding với kích thước ma trận tham số trong RNN. [8][12]\n\n---  \nGhi chú: Các citation [1], [2], … tương ứng với các đoạn (chunk) trong video ở timestamps đã cho; click vào [N] sẽ dẫn tới thời điểm tương ứng trong video.",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 0,
          "end_time": 58,
          "text": "Trong phần tiếp theo thì chúng ta sẽ cùng tìm hiểu về kiến trúc mạng Recurrent Neural Network Đầu tiên đó là chúng ta sẽ phải xem cái dạng triển khai của mạng Recurrent Network Thứ nhất đó là chúng ta xem đối với cái mạng Neural Network Thì điểm yếu của nó đó là chúng ta không thể mẽ hóa được cái yếu tố về mặt trình tự của các cái từ Ví dụ như cái từ thứ xt-1, xt, rồi xt-1 Chúng ta đưa nó về một cái dạng vector one hot như thế này Thì rõ ràng là nó sẽ không biết cái từ nào là từ xuất hiện trước, từ nào là xuất hiện sau Thế thì đối với mạng Recurrent Neural Network thì cái yếu tố là Recurrent Dịch tiếng Việt đó là hồi quy Thế thì đối với mạng Recurrent Neural Network thì cái yếu tố là Recurrent"
        },
        {
          "index": 2,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 49,
          "end_time": 114,
          "text": "Thế thì đối với mạng Recurrent Neural Network thì cái yếu tố là Recurrent Dịch tiếng Việt đó là hồi quy Thế thì đối với mạng Recurrent Neural Network thì cái yếu tố là Recurrent Thì hồi quy chính là cái cơ chế để giúp cho mình mẽ hóa cái yếu tố về mặt trình tự Nó chính là mạng hóa cái yếu tố về mặt trình tự Thì cái cách thức mà mình mẽ hóa nó là như thế nào Khi chúng ta gặp cái từ thứ xt-1 đúng không Chúng ta đưa vào và bây giờ tạm thời chúng ta sẽ chưa cần biết là cái mạng này nó tính toán như thế nào không ha Chúng ta đi tính cái giá trị thứ xt-1 Rồi sau đó chúng ta đi tính cái giá trị output Và khi chúng ta tính được cái từ thứ xt-1 xong Chúng ta lan truyền cái thông tin này đến cái nốt tiếp theo Và chúng ta lại nhận cái thông tin tại thời điểm thứ xt Và tại thời điểm thứ xt này á Thì chúng ta sẽ kết hợp cả cái thông tin của quá khứ"
        },
        {
          "index": 3,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 99,
          "end_time": 161,
          "text": "Chúng ta lan truyền cái thông tin này đến cái nốt tiếp theo Và chúng ta lại nhận cái thông tin tại thời điểm thứ xt Và tại thời điểm thứ xt này á Thì chúng ta sẽ kết hợp cả cái thông tin của quá khứ Tức là thông tin của cái xt-1 Khi đã xử lý cái từ xt-1 rồi đúng không Nó tạo ra cái thông tin là xt-1 Thì đây chính là quá khứ Và cái quá khứ này nó sẽ kết hợp với thông tin của thời điểm hiện tại Để tổng hợp thông tin Thì như vậy là xt là nó mang cái tính chất gọi là tổng hợp Tổng hợp thông tin Đó Thì khi đó cái việc đưa ra cái dự đoán giá trị it Y-t Nó sẽ mang đầy đủ thông tin của những từ trước đó Và những cái từ và của cái từ hiện tại Và đồng thời là cái từ trước đó"
        },
        {
          "index": 4,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 148,
          "end_time": 210,
          "text": "Thì khi đó cái việc đưa ra cái dự đoán giá trị it Y-t Nó sẽ mang đầy đủ thông tin của những từ trước đó Và những cái từ và của cái từ hiện tại Và đồng thời là cái từ trước đó Thì nó sẽ có trước đúng không Rồi nó sẽ kết hợp với từ hiện tại Thì nó sẽ giúp cho cái việc phán đoán này Nó sẽ toàn nhiệm hơn Nó sẽ toàn nhiệm hơn Và như vậy thì cái yếu tố hồi quy Yếu tố hồi quy Nó thể hiện ở chỗ đó là yếu tố hồi quy Và cái quá trình này được lặp đi lặp lại Cái xt này Nó sẽ lại tiếp tục lan truyền đến cho cái thời điểm thứ t cộng 1 Thì nó sẽ là quá khứ của cái xt cộng 1 Rồi xt cộng 1 Là hiện tại mới Kết hợp với cái quá khứ trước đó Để tổng hợp thông tin Và lại đưa ra cái output tiếp theo Thì đây là cái dạng triển khai Tức là cách thức mà chúng ta triển khai Các cái từ Đầu vào cho một cái mô hình"
        },
        {
          "index": 5,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 198,
          "end_time": 260,
          "text": "Và lại đưa ra cái output tiếp theo Thì đây là cái dạng triển khai Tức là cách thức mà chúng ta triển khai Các cái từ Đầu vào cho một cái mô hình Recurrent Neural Network Và viết như vậy thì nó cũng sẽ hơi Tắt quá Thì chút nữa chúng ta sẽ có cái dạng Gọi là dạng rút gọt Là thay vì Chúng ta đưa vào x1, x2 trên xt Thì ở đây chúng ta chỉ cần cái hiệu là x thôi Và đầu ra sẽ là giá trị y ngã Và ở đây chúng ta sẽ vẽ Một cái vòng hồi quy S sẽ được đưa trở lại Trở lại cho cái nốt s này Và ở đây chúng ta sẽ có một cái vòng hồi quy Thống nhất với nhau về mặt cái hiệu Đối với cái dữ kiện đầu vào Xt Thì cái xt này thì t Là có thể thay đổi Cái độ dài Tức là t sẽ di chuyển từ T có thể giao động từ Từ 1 cho đến t lớn T sẽ thay đổi Chiều dài của mình từ 1 cho đến t lớn"
        },
        {
          "index": 6,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 249,
          "end_time": 310,
          "text": "Tức là t sẽ di chuyển từ T có thể giao động từ Từ 1 cho đến t lớn T sẽ thay đổi Chiều dài của mình từ 1 cho đến t lớn Và tại một cái thời điểm Thời hiện tại là xt Chúng ta sẽ đi tính cái giá trị dự đoán Chúng ta sẽ đi tính cái giá trị dự đoán Là cái hiệu bằng y ngã t Và ở đây có một cái lưu ý cực kỳ quan trọng Đó là Các cái bộ tham số U, V và W này Là chúng ta sẽ dùng chung Dùng chung cho mỗi bước tính toán Cho ví dụ chúng ta tính với xt trừ 1 Hay tính với xt hay tính với xt cộng 1 Chúng ta đều sử dụng chung Các cái bộ Trọng số này Và xt này Thì được gọi là trạng thái ổn Đây là cái Ký hiệu Và quý ước về cái cách đặt tên Cho cái mạng Neuron Network này sau Như vậy đây sẽ là input X sẽ là input"
        },
        {
          "index": 7,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 299,
          "end_time": 357,
          "text": "Ký hiệu Và quý ước về cái cách đặt tên Cho cái mạng Neuron Network này sau Như vậy đây sẽ là input X sẽ là input Đây sẽ là dự đoán Và S là trạng thái ổn Đây sẽ là trạng thái ổn của mô hình Và các cái bộ Ma trận U, V và W Chính là các cái tham số Của mô hình Và như vậy thì ANEN Đã có thể Encode có thể mã hóa được cái thứ tự Trình tự của các cái từ Trong một văn bản Thông qua cái cơ chế là cơ chế hồi quy Thông qua cơ chế hồi quy Rồi Và bây giờ chúng ta sẽ đến Với cái Các cái bước để xây dựng một cái mô hình Mà dưới dạng công thức ha Đầu tiên Đó là bước số 1 Là thiết kế của mô hình"
        },
        {
          "index": 8,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 349,
          "end_time": 410,
          "text": "Với cái Các cái bước để xây dựng một cái mô hình Mà dưới dạng công thức ha Đầu tiên Đó là bước số 1 Là thiết kế của mô hình Thì cho trước các cái vector Cho trước các cái word vector là W1 X1 Xt-1 Xt, Xt-1 Xt, Xt-1  Xt, Xt-1   Thì ở đây chúng ta lưu ý Đây là word vector Cái khái niệm word vector thì chúng ta đã học ở trong bài trước rồi Đó chính là cái vector embedding Hay là cái vector Biểu diễn Biểu diễn Biểu diễn Biểu diễn   Của cái từ W1 Xt-1 này Sẽ là cái vector biểu diễn của cái từ W-T Rồi Và tại mỗi thời điểm Hay còn gọi là time step Tức là tại mỗi thời điểm T Thì Chúng ta sẽ có Nhận cái dữ kiện đầu vào"
        },
        {
          "index": 9,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 400,
          "end_time": 456,
          "text": "Rồi Và tại mỗi thời điểm Hay còn gọi là time step Tức là tại mỗi thời điểm T Thì Chúng ta sẽ có Nhận cái dữ kiện đầu vào Là Xt Và chúng ta sẽ tính toán Tính toán cái giá trị trạng thái ẩn St Dựa trên cái công thức này Dựa trên công thức này Thì cái St St sẽ có công thức như sau Là bằng Hàm kích hoạt Hàm kích hoạt Xigmoid hoặc là hàm tanh Hàm này có thể là hàm xigmoid hoặc là hàm tanh Hoặc là hàm tanh Rồi Và nó sẽ phối hợp Cái thông tin của Quá khứ Đây là quá khứ Và đây là hiện tại Rồi Còn đây là hiện tại"
        },
        {
          "index": 10,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 455,
          "end_time": 510,
          "text": "Còn đây là hiện tại Và hai cái ma trận U và W ở đây Nó sẽ giúp cho chúng ta Ánh xạ hai cái vector Là Xt và St Về cùng một cái không gian Và sau đó nó sẽ tổng hợp thông tin lại với nhau Tổng hợp thông tin lại Rồi từ đó qua cái hàm kích hoạt Để ra cái trạng thái ẩn Trạng thái ẩn St Như vậy St nó đã chứa đầy đủ thông tin Chứa đầy đủ thông tin để giúp cho mình Đưa ra cái giá trị dự đoán St là Đủ thông tin Để Mình dự đoán Cái kết quả Và để dự đoán kết quả thì chúng ta sẽ nhân với lại cái W nhân với ma trận V Để ra cái Và qua cái hàm Softmax để ra cái I ngã T"
        },
        {
          "index": 11,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 500,
          "end_time": 559,
          "text": "Cái kết quả Và để dự đoán kết quả thì chúng ta sẽ nhân với lại cái W nhân với ma trận V Để ra cái Và qua cái hàm Softmax để ra cái I ngã T Thì trong một số tài liệu Người ta sẽ ký hiệu là Tất cả cái ma trận Tham số người ta sẽ để là U chuyển vị Hoặc là W chuyển vị Rồi V chuyển vị Thì trong tài liệu này thì chúng ta Sẽ lựa chọn cái cách thức ký hiệu UVW sao cho nó gọn nhất Thực ra là cả hai cách thì đều Giống nhau thôi ha đều được hết Nó chỉ là U Trong cái U chuyển vị trong các tài liệu trước đó là U và V Thì nó chính là cái Chuyển vị cho Tức là nó sẽ có cái kích thước Nó khác so với lại cái U Của cái Hệ thống bài giảng ở đây Ví dụ như nếu U Ở cái hệ thống trước Ở trong các bài giảng trước Hoặc các bài hình giảng khác Nó có kích thước là V Nhân với lại N Trong đó V là số tập Số từ trong từ điển N là cái"
        },
        {
          "index": 12,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 550,
          "end_time": 610,
          "text": "Ở trong các bài giảng trước Hoặc các bài hình giảng khác Nó có kích thước là V Nhân với lại N Trong đó V là số tập Số từ trong từ điển N là cái Một cái Gọi là cái Chiều dài của cái vector biểu diễn ví dụ vậy Thì ở bên đây U nó sẽ có cái kích thước là N Nhân V Nó sẽ ngược lại một chút Nhưng cái đó thì nó không quá quan trọng Và ý nghĩa của nó thì vẫn giống nhau Và Bài tập cho chúng ta đó chính là Cho trước các cái thông tin Về cái độ dài Của cái XT XT Và Y ngã T Là như sau XT Là vector Có 8000 chữ Có 8000 phần tử XT Là một cái vector 100 chiều Và Y ngã T Là một cái vector có 8000 chiều hay 8000 phần tử Thì câu hỏi đặt ra đó là Kích thước Của cái tham số"
        },
        {
          "index": 13,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 599,
          "end_time": 660,
          "text": "Là một cái vector 100 chiều Và Y ngã T Là một cái vector có 8000 chiều hay 8000 phần tử Thì câu hỏi đặt ra đó là Kích thước Của cái tham số U V và W Trong trường hợp này Sẽ là bao nhiêu Hay nói cách khác Đó là U Sẽ thuộc một cái R Bao nhiêu Nhân với lại bao nhiêu Thì chúng ta sẽ cùng Làm một cái thông tin này Hãy làm thử cái bài tập như sau Đầu tiên Đó là chúng ta sẽ phải bám vào Hai cái công thức này Đây là hai cái công thức Để giúp cho chúng ta Xác định được cái độ dài của U V Và W Thì Hàm activation Ở đây giả sử như chúng ta gọi là hàm sigmoid luôn đi ha Thì đây là một cái hàm Mà theo kiểu là Thực hiện trên từng phần tử Hay còn gọi là 11 11 Y Nó sẽ tính trên phần tử Từng phần tử Do đó"
        },
        {
          "index": 14,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 649,
          "end_time": 710,
          "text": "Mà theo kiểu là Thực hiện trên từng phần tử Hay còn gọi là 11 11 Y Nó sẽ tính trên phần tử Từng phần tử Do đó Qua cái hàm sigmoid Nó sẽ không Giúp Nó sẽ không làm thay đổi cái kích thước của cái vector của mình Ví dụ Đầu vào của cái hàm sigmoid này Nó là một cái ma trận hoặc một vector nào đấy Thì qua cái hàm sigmoid nó sẽ không làm thay đổi Như vậy Chúng ta đã biết ST Là một cái ma trận Kích thước, xin lỗi là một cái vector có 100 phần tử Hay biết dưới dạng ma trận Thì nó sẽ là 100 phần tử Nhân 1 Như vậy thì Toàn bộ Cái phép cộng này Nó sẽ là 100 Nhân 1 Mà cái phép cộng này Thì nó cũng là 11 Y Tức là tính trên từng phần tử Do đó thì Hai cái Này U XT Và W XT Trừ 1 Nó cũng là"
        },
        {
          "index": 15,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 699,
          "end_time": 759,
          "text": "Thì nó cũng là 11 Y Tức là tính trên từng phần tử Do đó thì Hai cái Này U XT Và W XT Trừ 1 Nó cũng là Kết quả của nó cũng là Cái vector có kích thước là 100 nhân 1 Đó Như vậy thì chúng ta sẽ Bám vào cái Nhận xét này Để Dự đoán Để xác định Xem UV W Là kích thước bao nhiêu Thì Chúng ta sẽ lấy ra cái U nhân XT trước Thì U của mình Nó sẽ là Kích thước bao nhiêu Nhân bao nhiêu mình chưa biết XT của mình XT là 8000 Nhân với 1 Như vậy thì XT Sẽ là 8000 Nhân với 1 Nhân với 1 Và ở đây sẽ là bao nhiêu Mình không biết Và đầu ra Thì nó sẽ tạo ra là một cái vector Có kích thước là 100 Nhân 1"
        },
        {
          "index": 16,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 748,
          "end_time": 809,
          "text": "Và ở đây sẽ là bao nhiêu Mình không biết Và đầu ra Thì nó sẽ tạo ra là một cái vector Có kích thước là 100 Nhân 1 Rồi Thì để cái U và XT này có thể nhân được với nhau Thì cái giá trị ở đây Nó phải khớp Số cột của U Sẽ tương ứng với số dòng Của X Do đó thì ở đây nó sẽ là Đáp số của mình nó sẽ là 8000 Rồi Và khi nhân 2 cái Giá trị 2 cái cái ma trận này với nhau Thì cái 100 nó sẽ tạo ra vector là 100 x 1 Như vậy thì ở đây Số của mình nó sẽ là 100 Như vậy U của mình Sẽ có kích thước đó là 100 Nhân cho 8000 Rồi tương tự như vậy chúng ta sẽ thực hiện Cái thao tác Cho cái phép biến đổi đó là 100 x 8000   W x ST-1"
        },
        {
          "index": 17,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 801,
          "end_time": 859,
          "text": "Rồi tương tự như vậy chúng ta sẽ thực hiện Cái thao tác Cho cái phép biến đổi đó là 100 x 8000   W x ST-1 Thì W Của mình Là nó sẽ là bằng bao nhiêu x bao nhiêu mình chưa biết ST-1 ST nó cũng tương tự như ST-1 Như vậy đó là một cái vector có kích thước là 100 x 1 Và Thằng này output của nó sẽ ra là Một cái vector cũng là 100 x 1 luôn Rồi Như vậy thì chúng ta sẽ dùng các quy tắc Về số chiều của nhân 2 ma trận Để 2 ma trận W và ST có thể nhân được với nhau Thì ở đây Số này phải giống với số này Nó là 100 Số cột của W sẽ giống với lại số hàng của ST Vậy là 100 sẽ khớp với 100 Và Ở đây Sẽ là 100 luôn Như vậy W của mình sẽ là cái vector"
        },
        {
          "index": 18,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 848,
          "end_time": 910,
          "text": "Số cột của W sẽ giống với lại số hàng của ST Vậy là 100 sẽ khớp với 100 Và Ở đây Sẽ là 100 luôn Như vậy W của mình sẽ là cái vector W của mình sẽ là một cái ma trận Kích thước là 100 x 100   Nhân quay lại 100 Rồi thời tháng xuống lên Thì chúng ta sẽ Tính xem Cái V ma trận W Sẽ là bao nhiêu Thì tương tự như sigmoid softmax Cũng là một cái hàm Nó Đảm bảo là giữ nguyên cái số chiều khi chúng ta Biến đổi Như vậy Y của mình Nó sẽ là một cái ma trận Kích thước là 8000 x 1 Thì V của mình Sẽ là kích thước là bao nhiêu Mình không biết Đúng không? Mình sẽ để ở đây ST Sẽ là 100 Nhân 1 Như vậy ở đây chúng ta muốn thực hiện được cái phép nhân này"
        },
        {
          "index": 19,
          "video_id": "Chương 7_TqKBlC-zyKY",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_1： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/TqKBlC-zyKY",
          "start_time": 898,
          "end_time": 939,
          "text": "Sẽ là kích thước là bao nhiêu Mình không biết Đúng không? Mình sẽ để ở đây ST Sẽ là 100 Nhân 1 Như vậy ở đây chúng ta muốn thực hiện được cái phép nhân này Thì Số cột Của V Sẽ phải là 100 Và output của mình Là ra một cái vector Là một cái ma trận kích thước là 8000 x 1 Như vậy ở đây nó sẽ phải là 8000 Tóm lại V Sẽ là một cái ma trận Kích thước là 8000 x 100 Rồi Như vậy thì chúng ta đã có được 3 cái đáp án Chúng ta đã có được cái đáp án cho cái bài tập ở đây"
        }
      ]
    },
    {
      "video_id": "Chương 7_ptwSPTt2XnM",
      "summary": "## 1. Giới thiệu (Introduction)\n- Mục tiêu chính của bài giảng: Giải thích cách thiết kế hàm loss cho mô hình tuần tự (RNN/ANN cho NLP) và phân loại các kiểu bài toán tuần tự (1-to-1, 1-to-many, many-to-one, many-to-many dạng 1 và dạng 2) cùng các ngữ cảnh ứng dụng đi kèm. [1][4][5]  \n- Các khái niệm sẽ được đề cập: hàm loss theo thời gian (time-step loss), cross-entropy cho từng bước thời gian, tổng/mean loss trên chuỗi, và các kiểu kiến trúc I/O của mạng tuần tự (1→1, 1→many, many→1, many→many dạng 1 và dạng 2) cùng ví dụ ứng dụng. [1][2][3][4][5][6][7][8][9]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Thiết kế hàm loss cho bài toán tuần tự\n- Ý tưởng chung: tại mỗi time step t, ta có giá trị thực (ground truth) và giá trị dự đoán; ta muốn hai giá trị này xấp xỉ nhau, nên cần định nghĩa hàm loss tại từng thời điểm và tổng hợp lại cho toàn chuỗi. Ví dụ giảng viên nhắc tới tính loss tại thời điểm thứ 3 như một bước trong quá trình thiết kế. [1]  \n- Sử dụng cross-entropy cho từng time step: hàm loss tại time step t được chọn giống công thức cross-entropy đã học trước đó, tức là dạng tổng trên tất cả các từ trong từ điển của chỉ số ground-truth nhân log của xác suất dự đoán cho từng phần tử. [1][2][3]  \n- Ký hiệu và chú ý về V: trong bài ví dụ, V là kích thước tập từ điển (|V|), vì output y tại mỗi bước là một vector one-hot/đa chiều có độ dài |V| (không nhầm lẫn với một ma trận ký hiệu V khác). [2][3]  \n- Công thức (như trình bày trong video):  \n  - Loss thành phần tại time step t: L_t = - ∑_{i=1}^{|V|} I_t^{(i)} * log(ŷ_t^{(i)})  (I_t^{(i)} là chỉ số 1-hot cho nhãn thực tại bước t; ŷ_t^{(i)} là xác suất dự đoán cho từ i tại bước t). [2][3]  \n  - Loss tổng cho toàn chuỗi: trung bình (hoặc tổng) các L_t với t chạy từ 1 đến T, tức L = (1/T) ∑_{t=1}^T L_t (giảng viên nhấn là tính trung bình các loss thành phần để được loss chung). [3][4]\n\n### 2.2. Diễn giải chi tiết các ký hiệu trong công thức\n- Vector output y: biểu diễn một output phân bố trên từ điển, có độ dài bằng số từ trong tập từ điển |V|; mỗi phần tử tương ứng xác suất cho một từ cụ thể. [2][3]  \n- Thành phần tính trên từng phần tử: tại thời điểm t, ta nhân chỉ báo ground-truth (I_t^{(i)}) với log của xác suất dự đoán (log(ŷ_t^{(i)})) rồi tổng lên các i trong từ điển để có L_t. [3]\n\n### 2.3. Tổng hợp và mục đích của hàm loss\n- Mục đích: đưa ra một chỉ số đo lường tổng thể cho sai số mô hình trên toàn bộ chuỗi đầu vào X_1..X_T bằng cách kết hợp các loss từng bước (tính trung bình). Việc này cho phép huấn luyện mạng tuần tự bằng backpropagation through time (BPTT) — (lưu ý: video chỉ trình bày phần thiết kế loss và tổng hợp, không đi sâu vào thuật toán tối ưu). [3][4]\n\n### 2.4. Phân loại các dạng I/O trong bài toán tuần tự (các kiểu ứng dụng của RNN/ANN)\nGiảng viên liệt kê và giải thích các tình huống sử dụng (dạng cấu hình input/output) của mạng tuần tự:\n\n- 1 to 1: đầu vào X1, đầu ra Ŷ1 — chỉ dự đoán một phần tử duy nhất (ví dụ dịch một từ sang một từ). [5]  \n- 1 to many: một đầu vào tạo ra một chuỗi đầu ra (ví dụ: cho một chủ đề như “biển” đầu vào, mạng sinh ra một bài thơ về biển). Ứng dụng: sinh văn bản từ một chủ đề. [6]  \n- many to one: nhiều từ đầu vào, chỉ một đầu ra (ví dụ: phân loại một đoạn comment thành thể loại/cảm xúc: positive/negative/neutral; hoặc spam detection cho email). [6][7]  \n- many to many — có hai dạng được phân biệt:  \n  - many to many (dạng 1): phải đọc toàn bộ nội dung đầu vào xong rồi mới bắt đầu tạo toàn bộ chuỗi đầu ra (ví dụ: dịch máy theo kiểu cần xem toàn câu/đoạn trước khi dịch; tóm tắt văn bản: cần đọc hết rồi mới tóm tắt). Đây là dạng mà toàn bộ input được xem trước khi bắt đầu sinh output. [7][8]  \n  - many to many (dạng 2): tạo output dần theo từng bước khi đọc input (cứ đọc đến đâu thì dự đoán output đến đó). Ví dụ ứng dụng: part-of-speech tagging (gán nhãn từ loại cho từng từ ngay khi nhận từ đó) hoặc các bài toán sequence labeling khác. [7][9]\n\n(Ở phần trên, giảng viên sử dụng từ “ANN” để chỉ mạng tuần tự/ mạng thần kinh áp dụng cho NLP.) [4][5]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n- Ví dụ minh họa hàm loss: cho chuỗi X1..XT, tại mỗi bước t tính L_t = -∑ I_t^{(i)} log(ŷ_t^{(i)}), sau đó lấy trung bình các L_t để có loss chung dùng cho huấn luyện. Đây là cách đơn giản và phổ biến sử dụng cross-entropy cho các bài toán dự đoán từ/ngôn ngữ. [1][2][3][4]  \n- Ứng dụng theo từng cấu hình I/O:\n  - 1-to-1: dịch một từ sang một từ (ví dụ từ tiếng Anh sang tiếng Việt). [5]  \n  - 1-to-many: sinh một chuỗi từ một chỉ dẫn/ngữ cảnh (ví dụ cho chủ đề “biển” sinh ra một bài thơ về biển). [6]  \n  - many-to-one: phân loại đoạn văn hoặc xác định cảm xúc/spam từ một đoạn text (ví dụ đưa vào comment, output là label sentiment hoặc spam/not spam). [6][7]  \n  - many-to-many (dạng 1): dịch máy truyền thống hoặc tóm tắt văn bản — cần đọc hết câu/đoạn trước khi bắt đầu tạo output. [8]  \n  - many-to-many (dạng 2): sequence labeling như POS tagging — cho từ vào đến đâu, tạo nhãn đến đó. [9]\n\n- Trường hợp sử dụng tổng quát: giảng viên nhấn mạnh mạng ANN/RNN có thể áp dụng cho nhiều bài toán NLP bằng cách chọn cấu hình input/output phù hợp và thiết kế hàm loss cộng dồn trên time steps. [4][5]\n\n## 4. Kết luận (Conclusion)\n- Tóm tắt các ý chính:  \n  - Thiết kế hàm loss cho bài toán tuần tự thường dùng cross-entropy trên từng time step và sau đó kết hợp (ví dụ lấy trung bình) toàn bộ loss thành phần để có loss chung cho chuỗi; các ký hiệu chú ý |V| là kích thước từ điển, y là vector phân bố trên từ điển, và L_t = -∑ I_t^{(i)} log(ŷ_t^{(i)}). [1][2][3][4]  \n  - Có nhiều cấu hình input/output cho mạng tuần tự: 1→1, 1→many, many→1, many→many dạng 1 (đọc toàn bộ input trước khi sinh output) và many→many dạng 2 (sinh output song hành với input). Mỗi dạng ứng dụng vào các bài toán NLP khác nhau như dịch máy, tóm tắt, sinh văn bản, phân loại cảm xúc, POS tagging, spam detection. [5][6][7][8][9]\n- Tầm quan trọng: Việc hiểu rõ hàm loss theo từng time-step và phân loại dạng I/O giúp thiết kế mô hình và pipeline huấn luyện thích hợp cho các bài toán tuần tự trong NLP. [3][4][5]  \n- Liên hệ với các bài giảng khác: video trình bày tiếp nối phần kiến thức về cross-entropy đã học trước đây (giảng viên trực tiếp tham chiếu công thức cross-entropy nền tảng khi xây hàm loss cho từng time step). [1][2]\n\nNếu bạn muốn, tôi có thể:\n- Trích xuất riêng ví dụ công thức và viết lại dưới dạng LaTeX rõ ràng.  \n- Vẽ sơ đồ minh họa các cấu hình I/O (1→1, 1→many, many→1, many→many loại 1/2).  \n\n(Tất cả nội dung trên trích trực tiếp từ các đoạn video: [1] [2] [3] [4] [5] [6] [7] [8] [9].)",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 7_ptwSPTt2XnM",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_2： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/ptwSPTt2XnM",
          "start_time": 0,
          "end_time": 61,
          "text": "xem bước thứ 2 bước thứ 2 của cái quá trình xây dựng một cái mô hình máy học đó chính là chúng ta thiết kế cái hàm độ lỗi thì ở đây chúng ta sẽ có các cái giá trị dự đoán và ở phía trên chúng ta sẽ có là các cái giá trị thực tế là chúng ta ký hiệu là IT IT trừ 1, IT cộng 1 và chúng ta luôn mong muốn là 2 cái giá trị này nó sắp xỉ với nhau thì chúng ta sẽ có cái hàm loss tại cái thời điểm thứ 3 tức là chúng ta sẽ tính tại đây trước chúng ta sẽ tính tại đây trước và chi phí, cái hàm chi phí, hàm loss của mình sẽ được tính bằng công thức như sau đó là hàm loss khi tại thời điểm thứ 3 theo theta thì nó sẽ là bằng cái công thức giống như công thức gross entropy mà chúng ta đã học trước đây và công thức của nó sẽ là tổng với chi chạy từ 1 và cho đến v trong đó v"
        },
        {
          "index": 2,
          "video_id": "Chương 7_ptwSPTt2XnM",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_2： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/ptwSPTt2XnM",
          "start_time": 47,
          "end_time": 110,
          "text": "thì nó sẽ là bằng cái công thức giống như công thức gross entropy mà chúng ta đã học trước đây và công thức của nó sẽ là tổng với chi chạy từ 1 và cho đến v trong đó v lưu ý là trong cái bài này trong cái ví dụ này thì v của mình là cái tập từ điển của mình nó bị trùng một chút xíu đây chính là cái tập từ điển của mình tại vì sao tại vì cái y này nó sẽ là một cái vector nó sẽ là một cái vector nó sẽ là một cái vector để cho biết là cái giá trị này  là cái giá trị output của mình đó là gì? nếu như đây là một cái bài toán đoán từ nếu như đây là một cái bài toán đoán từ thì đây sẽ là một cái vector có độ dài là số từ trong tập từ điển và chúng ta kí hiệu là trị tuyệt đối của V lưu ý cái V này là V từ điển nó không phải là cái ma trận V ở đây nó không phải là ma trận V ở đây"
        },
        {
          "index": 3,
          "video_id": "Chương 7_ptwSPTt2XnM",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_2： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/ptwSPTt2XnM",
          "start_time": 97,
          "end_time": 159,
          "text": "thì đây sẽ là một cái vector có độ dài là số từ trong tập từ điển và chúng ta kí hiệu là trị tuyệt đối của V lưu ý cái V này là V từ điển nó không phải là cái ma trận V ở đây nó không phải là ma trận V ở đây và ở đây sẽ là số từ trong tập từ điển của mình rồi nó sẽ tính trên từng cái phần tử chia ra 1 phần tử trên từ điển này tại thời điểm thứ T rồi nó sẽ là ITG nhân cho lốc của Y ngã TG và như vậy thì chúng ta có cái chuỗi với tất cả là T bước đúng không? cái chuỗi của chúng ta là X1 cho đến XT thì chúng ta sẽ phải tính tổng tất cả các cái sai số cho các cái time step T như vậy thì chúng ta sẽ có là loss tổng thể sẽ là bằng trung bình cộng của các cái loss thành phần trong đó cái loss thành phần thì nó sẽ có công thức là trừ của"
        },
        {
          "index": 4,
          "video_id": "Chương 7_ptwSPTt2XnM",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_2： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/ptwSPTt2XnM",
          "start_time": 148,
          "end_time": 211,
          "text": "tổng thể sẽ là bằng trung bình cộng của các cái loss thành phần trong đó cái loss thành phần thì nó sẽ có công thức là trừ của tổng ITG nhân cho lốc của Y ngã TG và chúng ta sẽ tính trên tất cả các cái time step tính với T chạy từ 1 cho đến T T lớn rồi như vậy thì chúng ta đã thiết kế được cái hàm loss thì cái cách thích thiết kế hàm loss này cũng rất là đơn giản chúng ta sẽ sử dụng cái độ đo gross entropy cho từng cái loss thành phần để tính ra được cái LT và tổng tất cả các LT tính trung bình cộng lại lại thì chúng ta sẽ có cái hàm loss chung thì đó là thiết kế cho cái hàm đồ lỗi của cái việc dự đoán và sau đây thì chúng ta sẽ tính ra được cái hàm đồ lỗi của cái việc dự đoán thì chúng ta sẽ có thể thể hiện một số cái tình huống sử dụng của cái mạng ANN"
        },
        {
          "index": 5,
          "video_id": "Chương 7_ptwSPTt2XnM",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_2： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/ptwSPTt2XnM",
          "start_time": 198,
          "end_time": 261,
          "text": "thì đó là thiết kế cho cái hàm đồ lỗi của cái việc dự đoán và sau đây thì chúng ta sẽ tính ra được cái hàm đồ lỗi của cái việc dự đoán thì chúng ta sẽ có thể thể hiện một số cái tình huống sử dụng của cái mạng ANN tình huống sử dụng nghĩa là sao mạng ANN nó có thể áp dụng cho rất nhiều những cái bài toán của NLP ví dụ như trong cái tình huống đầu tiên đó là 1 to 1 tức là đầu vào của mình sẽ có X1 và đầu ra của mình sẽ có là Y ngã 1 thì ở đây là chúng ta chỉ dự đoán trên một phần tử thôi thì nó có thể là ý nghĩa của nó có thể là cho cái bài toán là dịch dịch một cái từ nào đó chúng ta có thể là dịch từ đầu vào là một cái từ tiếng anh và đầu ra sẽ là một cái từ tiếng Việt đối với bài toán 1 to many thì đầu vào của mình sẽ là một từ và đầu ra của mình sẽ là nhiều từ thì ở đây cái ngữ cảnh của mình nó có thể là mình cho đầu vào là một cái từ"
        },
        {
          "index": 6,
          "video_id": "Chương 7_ptwSPTt2XnM",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_2： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/ptwSPTt2XnM",
          "start_time": 248,
          "end_time": 308,
          "text": "đối với bài toán 1 to many thì đầu vào của mình sẽ là một từ và đầu ra của mình sẽ là nhiều từ thì ở đây cái ngữ cảnh của mình nó có thể là mình cho đầu vào là một cái từ của một cái chủ đề ví dụ như mình có chủ đề là về biển và đầu ra của mình sẽ là một cái bài thơ một cái bài thơ về về biển thì đây là một cái ngữ cảnh một cái tình huống sử dụng của ANN cho cái dạng là 1 to many đối với cái dạng many to one thì đầu vào của mình sẽ là rất nhiều từ và đầu ra thì chúng ta chỉ có duy nhất một cái đầu ra thôi và ngữ cảnh cho tình huống này đó là chúng ta có thể có một cái đoạn comment trên một cái mạng xã hội và đầu ra của mình có thể là cho biết là đó là thể loại của mình là thể loại gì hoặc là cái cảm xúc của mình đó là positive negative hay là neutral"
        },
        {
          "index": 7,
          "video_id": "Chương 7_ptwSPTt2XnM",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_2： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/ptwSPTt2XnM",
          "start_time": 297,
          "end_time": 359,
          "text": "và đầu ra của mình có thể là cho biết là đó là thể loại của mình là thể loại gì hoặc là cái cảm xúc của mình đó là positive negative hay là neutral hoặc là có thể là cho cái bài toán spam detection đầu vào của mình sẽ là email nội dung của một cái đoạn email và đầu ra thì cho biết đó là spam hay không phải là not spam thì đó là cho cái dạng many to many à xin lỗi cho many to one rồi đối với cái many to many thì chúng ta sẽ có hai dạng dạng đầu tiên đó là many to many dạng 1 và many to many bên đây là many to many dạng 2 thì many to many dạng 1 nó khác gì so với many to many dạng 2 many to many dạng 2    là chúng ta sẽ phải đọc xong hết toàn bộ cái nội dung này"
        },
        {
          "index": 8,
          "video_id": "Chương 7_ptwSPTt2XnM",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_2： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/ptwSPTt2XnM",
          "start_time": 349,
          "end_time": 411,
          "text": "nó khác gì so với many to many dạng 2 many to many dạng 2    là chúng ta sẽ phải đọc xong hết toàn bộ cái nội dung này rồi sau đó chúng ta mới đi ra mới đưa ra cái phán đoán còn many to many dạng 2 là chúng ta đưa cái từ nào đến đâu thì chúng ta sẽ tính ra cái output đến đó đưa đến đâu ra đến đó đưa đến đâu ra đến đó do đó thì ở đây chúng ta sẽ có một cái ngữ cảnh cho cái bài toán cho cái dạng là many to many dạng 1  đó là bài toán dịch máy rõ ràng là chúng ta sẽ phải đọc hết toàn bộ cái nội dung của một cái đoạn văn của một câu xong rồi chúng ta mới có thể bắt đầu dịch được đúng không hoặc là bài toán tóm tắt văn bản đầu vào là chúng ta sẽ nhận một cái văn bản rất là dài và sau khi đọc xong hết thì chúng ta mới đưa ra cái bản tóm tắt thì đó là cho ứng dụng ngữ cảnh ứng dụng"
        },
        {
          "index": 9,
          "video_id": "Chương 7_ptwSPTt2XnM",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 2_2： Kiến trúc mạng RNN",
          "video_url": "https://youtu.be/ptwSPTt2XnM",
          "start_time": 399,
          "end_time": 446,
          "text": "hoặc là bài toán tóm tắt văn bản đầu vào là chúng ta sẽ nhận một cái văn bản rất là dài và sau khi đọc xong hết thì chúng ta mới đưa ra cái bản tóm tắt thì đó là cho ứng dụng ngữ cảnh ứng dụng cho cái many to many dạng 1 đối với cái many to many dạng 2 thì chúng ta sẽ đưa đến đâu chúng ta đưa ra cái phán đoán đến đó thì ở đây nó có thể là cho cái bài toán là post stacking tức là đưa vô một cái từ chúng ta sẽ cho biết từ đó là chủ từ đưa vô một cái từ tiếp theo đó sẽ là động từ rồi đưa vô cái từ tiếp theo nó sẽ là vị ngữ thì đây là một cái dạng dịch máy dạng 1 và đây là một cái ngữ cảnh ứng dụng cho cái bài toán many to many dạng 2"
        }
      ]
    },
    {
      "video_id": "Chương 7_8-3xv_NElG0",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: phân tích *một số vấn đề* gặp phải khi huấn luyện mạng tuần tự (ANN/RNN) và chuẩn bị cho một số giải pháp khắc phục tạm thời (giới thiệu vấn đề về tính toán loss và đạo hàm theo thời gian). [1]  \n- Các khái niệm sẽ được đề cập: triển khai mạng theo chuỗi thời gian (unfold), hàm loss tổng theo các bước thời gian LT, cách tính đạo hàm (backpropagation qua thời gian / chain rule), và hiện tượng *vanishing gradient* (đạo hàm tiến về 0 khi chuỗi dài). [1][2][3]\n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Mô hình triển khai theo chuỗi và ký hiệu\n- Khi triển khai mạng theo thời gian, ta xử lý từng input theo thứ tự thời điểm t = 1 ... T; tại mỗi thời điểm t có một loss thành phần LT; tổng loss là tổng các LT. [1][2]  \n- Ký hiệu K được dùng để biểu diễn độ dài của vector output (chiều của vector dự đoán). [1][2]\n\n### 2.2. Công thức đạo hàm tổng theo các tham số (gradient of total loss)\n- Đạo hàm của tổng loss theo ma trận U là tổng các đạo hàm thành phần:\n  dL/dU = sum_t dL_t/dU. (tổng các đạo hàm thành phần) [2]  \n- Tương tự với các ma trận V và W: dL/dV = sum_t dL_t/dV, dL/dW = sum_t dL_t/dW. [2]\n\n### 2.3. Chain rule khi tính đạo hàm theo W (backpropagation qua thời gian)\n- Mỗi LT là một hàm hợp của nhiều hàm trung gian F1, F2, ..., Fn (tương ứng các phép tính qua các bước thời gian/phép biến đổi khác nhau). Đạo hàm theo W được khai triển theo quy tắc dây chuyền (chain rule) dạng tích các đạo hàm từng bước:\n  dL/dW = (∂F_n/∂F_{n-1}) · (∂F_{n-1}/∂F_{n-2}) · ... · (∂F_1/∂W). [3][4]  \n- Quá trình lan truyền đạo hàm thực hiện từ bước cuối (Fn) ngược về các bước trước đó, cập nhật các đạo hàm thành phần để tính gradient cho W. [4]\n\n### 2.4. Nguyên nhân dẫn đến vanishing gradients (đạo hàm tiến về 0)\n- Nhận xét quan trọng: đa số các đạo hàm thành phần (những ∂F_i/∂F_{i-1}) có trị tuyệt đối nhỏ hơn 1 (|∂F_i/∂F_{i-1}| < 1). [5]  \n- Ví dụ cụ thể với hàm trạng thái St = sigmoid(U·Xt + W·St-1): đạo hàm của sigmoid là σ(x)(1−σ(x)) và có giá trị nằm trong khoảng (0, 0.25] (nói chung từ 0 đến 1), tức là < 1; hệ số ma trận W khi khởi tạo thường nhỏ (ví dụ rời rạc xung quanh phân phối chuẩn), nên các thành phần nhân cũng thường < 1. Do đó các nhân liên tiếp của những yếu tố < 1 dẫn đến tích nhỏ dần. [6][7]  \n- Kết luận: vì ∂F thành phần < 1 và ta nhân nhiều yếu tố như vậy theo chiều dài chuỗi, tích các yếu tố này có xu hướng → 0, gây *vanishing gradient*. [5][6][7][11]\n\n### 2.5. Ảnh hưởng của độ dài chuỗi T\n- Khi xử lý dữ liệu tuần tự (văn bản, tóm tắt sách, dịch máy), T có thể rất lớn (vài chục đến hàng ngàn từ). Khi T lớn thì số lượng hàm hợp (số nhân trong chain rule, gọi là N) tăng nhiều, làm cho tích các đạo hàm thành phần càng có xu hướng tiến về 0. [8][9]  \n- So sánh trường hợp T nhỏ và T lớn: với T nhỏ, đường lan truyền ngược ngắn hơn (ít nhân hơn) nên gradient ít bị suy yếu; với T lớn, chuỗi lan truyền đạo hàm dài (n lớn), dẫn tới gradient gần như bằng 0 cho các tham số liên quan đến thông tin xa trong quá khứ. [9][10]  \n- Minh họa toán học: nếu 0 < a < 1 thì lim_{n→∞} a^n = 0; ví dụ thực nghiệm đơn giản: 0.9^20 là một số rất nhỏ — trực quan cho hiện tượng này. [11][12]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa trong video: khi tóm tắt nguyên một cuốn truyện (summarization) hoặc dịch máy với nhiều từ, T có thể lên đến vài chục hoặc hàng ngàn, do đó vanishing gradient gây khó khăn trong việc học các phụ thuộc dài hạn. [8][9]  \n- Ví dụ số học trực quan: lấy hệ số nhân gần 1 (ví dụ 0.9) và nâng lên lũy thừa N ≈ 20 sẽ cho giá trị rất nhỏ, chứng minh trực quan lý do gradient mất đi khi nhân nhiều yếu tố nhỏ. [11][12]  \n- Trường hợp sử dụng có ít ảnh hưởng: các dependency ngắn hạn (T nhỏ) — ở đây gradient lan truyền ngược qua ít bước nên không bị vanishing nhiều, nên các mô hình có thể học tốt các quan hệ cục bộ. [9][10]\n\n---\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính: bài giảng trình bày cách tổng hóa loss theo các bước thời gian và cách tính gradient bằng chain rule qua nhiều phép biến đổi; chỉ ra rằng đa số các đạo hàm thành phần có trị tuyệt đối < 1 (ví dụ đạo hàm của sigmoid và trọng số khởi tạo nhỏ) nên khi nhân nhiều yếu tố này theo chiều dài chuỗi sẽ dẫn đến *vanishing gradient* — gradient tiến về 0 đặc biệt khi T lớn. [1][2][3][4][5][6][7][8][9][11]  \n- Tầm quan trọng: hiểu rõ cơ chế vanishing gradient rất quan trọng vì nó giải thích vì sao RNN/ANN gặp khó khăn khi học phụ thuộc dài hạn trong dữ liệu tuần tự (ví dụ tóm tắt sách, dịch máy nhiều từ). [8][9]  \n- Liên hệ với các bài giảng khác: bài giảng nhắc rằng trước đó có slide đã trình bày một số công thức liên quan và phần tiếp theo sẽ xem các *vấn đề và một số giải pháp tạm thời* (như đã nêu ở phần đầu bài giảng), nên nội dung này là nền tảng để đi tới các kỹ thuật khắc phục trong phần tiếp theo. [1][2]\n\nNếu bạn muốn, tôi có thể:\n- rút ngắn thành \"cheatsheet\" 1 trang chỉ với các công thức và nhận xét chính, hoặc  \n- chuẩn bị sơ đồ tuần tự minh hoạ quá trình lan truyền đạo hàm (backprop through time) với các bước và phép nhân làm rõ vanishing gradient.",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 7_8-3xv_NElG0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_1： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/8-3xv_NElG0",
          "start_time": 0,
          "end_time": 63,
          "text": "Trong phần tiếp theo thì chúng ta sẽ tìm hiểu về một số cái vấn đề đối với cái mạng ANN và một số cái giải pháp để tạm thời khắc phục những cái vấn đề đó Đầu tiên đó là chúng ta sẽ cùng khảo sát lại cái sơ đồ của cái thuật vạn ANN này Từ trái sang phải chúng ta sẽ lần lượt fix các cái giá trị trong cái chuỗi vào các cái từ từ thứ T chừng 1 cho đến T cho đến T cộng 1 vào và tại thời điểm thứ T thì chúng ta sẽ có một cái giá trị loss thành phần đó là LT Thì trong cái slide trước thì ở đây chúng ta dùng cái hệ thống ký hiệu là trị tiệt đối V nhưng mà do nó sẽ dễ nhầm lẫn với cái V này Nên ở đây chúng ta sẽ ký hiệu nó bằng một cái từ tổng quát hơn đó là từ K Trong đó K là cái độ dài của cái vector của mình K của mình là Đó chính là cái độ dài của cái vector output hoặc là cái vector dự đoán của mình"
        },
        {
          "index": 2,
          "video_id": "Chương 7_8-3xv_NElG0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_1： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/8-3xv_NElG0",
          "start_time": 46,
          "end_time": 114,
          "text": "Nên ở đây chúng ta sẽ ký hiệu nó bằng một cái từ tổng quát hơn đó là từ K Trong đó K là cái độ dài của cái vector của mình K của mình là Đó chính là cái độ dài của cái vector output hoặc là cái vector dự đoán của mình Rồi và chúng ta sẽ có cái hàm loss của mô hình sẽ là công thức như sau Thì cái này là chúng ta đã được trình bày trong cái slide trước Như vậy thì đến cái giai đoạn số 3, cái bước số 3 của cái quá trình mà xây dựng một cái mô hình máy học Đó là chúng ta sẽ phải đi tính đạo hàm Và có được đạo hàm rồi thì chúng ta sẽ đi cập nhật các cái tham số Cho cái việc tính đạo hàm thì chúng ta sẽ có cái công thức như ở dưới đây Đạo hàm của cái hàm loss theo cái ma trận U Thì nó sẽ là bằng tổng của các cái đạo hàm thành phần Tức là tổng của cái đạo hàm của LT theo ma trận U Như vậy đạo hàm của tổng bằng tổng các cái đạo hàm Và tương tự như vậy cho các cái ma trận V và ma trận W về sau"
        },
        {
          "index": 3,
          "video_id": "Chương 7_8-3xv_NElG0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_1： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/8-3xv_NElG0",
          "start_time": 98,
          "end_time": 161,
          "text": "Thì nó sẽ là bằng tổng của các cái đạo hàm thành phần Tức là tổng của cái đạo hàm của LT theo ma trận U Như vậy đạo hàm của tổng bằng tổng các cái đạo hàm Và tương tự như vậy cho các cái ma trận V và ma trận W về sau Và bây giờ chúng ta sẽ xem Chi tiết hơn Giả sử như cái hàm loss của mình Giả sử như cái hàm loss thành phần của mình là LT này ha Viết gọn lại thay vì là LT Thế ta thì chúng ta sẽ để lại LT Là một cái hàm hợp Bao gồm hàm F1, Fn-1, Fn-2 Cho đến F2, F1 và W Và ở đây chúng ta đang xem xét Cái việc tính đạo hàm này theo biến W Tương tự, một cách tương tự cho Cái việc chúng ta tính đạo hàm theo O và V Bây giờ chúng ta sẽ tính theo W trước Rồi, thì khi đó tiết công thức Đạo hàm hàm hợp Chain rule Là đạo hàm của LT"
        },
        {
          "index": 4,
          "video_id": "Chương 7_8-3xv_NElG0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_1： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/8-3xv_NElG0",
          "start_time": 149,
          "end_time": 210,
          "text": "O và V Bây giờ chúng ta sẽ tính theo W trước Rồi, thì khi đó tiết công thức Đạo hàm hàm hợp Chain rule Là đạo hàm của LT Theo biến W Theo ma trận W Thì sẽ là bằng đạo hàm Của hàm thức Fn Theo Fn-1 Đạo hàm của Fn-1 Theo Fn-2 Đạo hàm của F2 theo F1 Đạo hàm của F1 theo W Đạo hàm của F1 theo W Đạo hàm của F2 theo W Đạo hàm của F3 theo W Đạo hàm của F3 theo W Đạo hàm của F3 theo W   Và khi chúng ta thực hiện lan truyền Thì chúng ta sẽ đi thực hiện Từ cái bước cuối cùng trước Ví dụ như Fn và Fn-1 Thì bản chất nó chính là những cái thao tác Ở cái bước cuối Ví dụ như là Chúng ta tính cái hàm loss Đó là Sai số giữa cái y ngã Tức là cái giá trị dự đoán Và cái giá trị thực tế Đến cái Fn-1 Trừ 2"
        },
        {
          "index": 5,
          "video_id": "Chương 7_8-3xv_NElG0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_1： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/8-3xv_NElG0",
          "start_time": 199,
          "end_time": 260,
          "text": "Ví dụ như là Chúng ta tính cái hàm loss Đó là Sai số giữa cái y ngã Tức là cái giá trị dự đoán Và cái giá trị thực tế Đến cái Fn-1 Trừ 2 Xin lỗi Fn Đạo hàm của Fn-1 Theo Fn-2 Thì đó sẽ là cái thao tác Mà Softmax Và Tổng hợp thông tin y ngã T Từ Xt Cứ như vậy thì Nó sẽ lan truyền cho đến những cái thao tác Tính đầu tiên Cho đến những cái thao tác tính đầu tiên Lan truyền Lan truyền Nó sẽ lan truyền ngược về Và Cập nhật các cái đạo hàm Để từ đó sẽ tính cho Các cái thăm Các cái thăm Ma trận W Và ở đây chúng ta sẽ có cái nhận xét Đầu tiên Đó chính là Đa số Các cái đạo hàm thành phần Đều có cái giá trị tiệt đối nhỏ hơn 1 Đều có cái giá trị tiệt đối nhỏ hơn 1 Hay cái đói cách khác Là cái công thức là Trị tiệt đối của đạo hàm Ft"
        },
        {
          "index": 6,
          "video_id": "Chương 7_8-3xv_NElG0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_1： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/8-3xv_NElG0",
          "start_time": 249,
          "end_time": 308,
          "text": "Đều có cái giá trị tiệt đối nhỏ hơn 1 Đều có cái giá trị tiệt đối nhỏ hơn 1 Hay cái đói cách khác Là cái công thức là Trị tiệt đối của đạo hàm Ft Ft-1 Là Bn-1 Tức là các cái thành phần này nè Đa số của nó Nó sẽ là Bn-1 Thì điều này là tại sao Chúng ta sẽ lấy cái ví dụ sau Chúng ta có cái công thức tính Cho cái St Các cái trạng thái ở St Là bằng sigmoid Của U-St Cộng cho W-St-1 Và khi chúng ta triển khai cái đạo hàm này Khi chúng ta triển khai cái đạo hàm này Thì nó sẽ là bằng Đạo hàm của sigmoid Thì sẽ là bằng sigmoid Nhân cho 1 Trừ 1  Trừ cho sigmoid Mà chúng ta biết rằng là Sigmoid của mình Là một cái hàm Mà giải giá trị của mình là từ"
        },
        {
          "index": 7,
          "video_id": "Chương 7_8-3xv_NElG0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_1： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/8-3xv_NElG0",
          "start_time": 299,
          "end_time": 359,
          "text": "Trừ cho sigmoid Mà chúng ta biết rằng là Sigmoid của mình Là một cái hàm Mà giải giá trị của mình là từ 0 cho đến 1 Từ 0 cho đến 1 Do đó cái giá trị này sẽ là từ 0 cho đến 1 Giá trị này sẽ là từ 0 cho đến 1 Và 1 trừ cho một cái giá trị Từ 0 cho đến 1 Thì nguyên cái này cũng sẽ là Từ 0 cho đến 1 Như vậy tích của 2 cái giá trị này Sẽ là từ 0 cho đến 1 Má trận W-St-1 Má trận W của mình cũng vậy Má trận W cũng sẽ là các cái giá trị random ban đầu Là được Lấy xung quanh cái phân bối chuẩn Do đó nó cũng sẽ từ 0 cho đến 1 Như vậy Tổng thể thì Tất cả các cái giá trị thành phần ở đây Đều là các cái giá trị từ 0 cho đến 1 Rồi Và từ cái nhận xét số 1 đó Nó sẽ nảy sinh ra cho Chúng ta đến cái nhận xét số 2"
        },
        {
          "index": 8,
          "video_id": "Chương 7_8-3xv_NElG0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_1： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/8-3xv_NElG0",
          "start_time": 351,
          "end_time": 407,
          "text": "Rồi Và từ cái nhận xét số 1 đó Nó sẽ nảy sinh ra cho Chúng ta đến cái nhận xét số 2 Khi chúng ta xử lý văn bản Đúng không? Thì cái T này thường là những cái con số rất là lớn Có những cái đoạn văn bản dài lên đến hàng ngàn Hàng ngàn chữ Ví dụ như cho bài toán tấm tắt văn bản Chúng ta sẽ đọc nguyên một cái cuốn truyện Và Chúng ta sẽ phải tổng hợp để tấm tắt lại Cái nội dung chính của Cái cuốn truyện đó là gì Hoặc là trong một số cái bài toán dịch máy Thì cái cụm từ của mình Nó có thể lên đến vài chục chữ Vào vài chục chữ này  Và với vài chục từ này Thì nó cũng được xem là lớn rồi Nó cũng đã xem được là lớn rồi Và khi T này lớn Thì N sẽ càng lớn Và khi T càng lớn Tức là sao? Đối với cái bàn bản của mình Cái T này mà càng lớn"
        },
        {
          "index": 9,
          "video_id": "Chương 7_8-3xv_NElG0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_1： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/8-3xv_NElG0",
          "start_time": 397,
          "end_time": 460,
          "text": "Thì N sẽ càng lớn Và khi T càng lớn Tức là sao? Đối với cái bàn bản của mình Cái T này mà càng lớn Thì cái đạo hàm thành phần này Cũng sẽ càng lớn Tại vì cứ một cái Một cái từ Chúng ta sẽ phải thực hiện tổ hợp rất nhiều thao tác Tổng hợp Thông tin từ quá khứ hiện tại Qua hàm sốp max Nó sẽ còn nhiều cái hàm con Như vậy T mà càng lớn Tức là số từ mà càng lớn Thì cái số lượng cái hàm F Của cái hàm hợp này nè Sẽ càng lớn Mà nó càng lớn Thì sẽ dẫn đến là cái đạo hàm này Sẽ càng tiến về 0 Rồi Và Ở đây Chúng ta có một cái để ý đó là Với cái lót tính này Số 2 Tức là T nhỏ Với cái T nhỏ Thì Đạo hàm Cái hàm hợp Của cái hàm L2 này Nó sẽ ngắn hơn"
        },
        {
          "index": 10,
          "video_id": "Chương 7_8-3xv_NElG0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_1： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/8-3xv_NElG0",
          "start_time": 449,
          "end_time": 510,
          "text": "Số 2 Tức là T nhỏ Với cái T nhỏ Thì Đạo hàm Cái hàm hợp Của cái hàm L2 này Nó sẽ ngắn hơn So với lại cái hàm LP Tại vì sao Từ L2 Chúng ta lan truyền ngược lại Thì ở đây chúng ta chỉ mất có 3 bước Trong khi đó Ở đây là T Khi chúng ta lan truyền ngược Chúng ta sẽ phải thực hiện cái hàm này Hàm hợp này rất là nhiều lần Do đó thì N của cái LT này nè Sẽ là rất là lớn Thì cụ thể trong trường hợp này Nó sẽ phải là đến T cộng 1 bước Đúng không? 1 bước 2 bước cho đến đây Là T cộng 1 bước Rồi Và tại sao nó lại có cái việc là Đạo hàm của mình Tiến đến 0 Thì như cái nhận xét số 1 Chúng ta đã Đã đã thấy Là các cái giá trị đạo hàm thành phần này Đại đa số là những con số Bé hơn 1 Là những con số bé hơn 1 Và các cái giá trị đạo hàm thành phần này Các cái con số mà bé hơn 1"
        },
        {
          "index": 11,
          "video_id": "Chương 7_8-3xv_NElG0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_1： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/8-3xv_NElG0",
          "start_time": 499,
          "end_time": 559,
          "text": "Là các cái giá trị đạo hàm thành phần này Đại đa số là những con số Bé hơn 1 Là những con số bé hơn 1 Và các cái giá trị đạo hàm thành phần này Các cái con số mà bé hơn 1 Mà nhân với nhau thì nó sẽ có xu hướng Là tiến đến Tiến đến 0 Chúng ta từng Ở trong cái chương trình cấp 3 Chúng ta đã từng học một cái kiến thức đó là Lim của N Tiến đến vô cùng Của A mũi N Với A là một cái con số Với A là con số bé hơn 1 Lớn vô 0 Thì lim của nó sẽ là bằng 0 Còn nếu như chúng ta muốn kiểm chứng Bằng một cách gọi là Trực quan và có thể tính toán được ngay Thì chúng ta sẽ lấy một con số rất gần số 1 thôi Là 0.9 Và chúng ta thử Lý thừa N Với N ở đây chúng ta có thể cho là khoảng 20 đi Thì chúng ta có thể thấy cái con số này là con số rất là bé Nó sẽ tiến về 0"
        },
        {
          "index": 12,
          "video_id": "Chương 7_8-3xv_NElG0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_1： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/8-3xv_NElG0",
          "start_time": 549,
          "end_time": 561,
          "text": "Lý thừa N Với N ở đây chúng ta có thể cho là khoảng 20 đi Thì chúng ta có thể thấy cái con số này là con số rất là bé Nó sẽ tiến về 0 Rồi"
        }
      ]
    },
    {
      "video_id": "Chương 7_IKD0O35NOUI",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Phân tích một số vấn đề cơ bản của mạng RNN (Recurrent Neural Network) khi làm việc với dữ liệu tuần tự — đặc biệt là *long-term dependency* (phụ thuộc dài) và các hiện tượng liên quan đến gradient (vanishing / exploding), đồng thời trình bày các chiến lược khắc phục ngắn gọn. [1][7][13]\n\n- Các khái niệm sẽ được đề cập: Long-term dependency (phụ thuộc dài), cách phân tích thông qua hàm lỗi và đạo hàm (loss & gradients), *vanishing gradients* / *exploding gradients*, giải pháp gồm thay đổi activation (sigmoid/tanh → ReLU), khởi tạo ma trận W (identity init), gradient clipping, và thay cell RNN bằng các cổng như LSTM để điều tiết nhớ/quên. [1][3][9][10][11][12][13][14]\n\n- Phần này dựa trên các nhận xét và công thức đạo hàm đã trình bày ở các slide trước của bài giảng. [7]\n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 Vấn đề phụ thuộc dài (Long-Term Dependency)\n- Mô tả vấn đề: Khi xử lý ngôn ngữ, từ cần dự đoán có thể phụ thuộc vào một từ xuất hiện rất xa về phía trước trong chuỗi. Ví dụ: câu \"In France, I had a great time and I learned some of the ___\" — để điền chỗ trống cần biết thông tin liên quan đến \"France\" (→ \"French\"), mà từ \"France\" nằm rất xa vị trí cần dự đoán. RNN đơn giản không có cơ chế lưu giữ thông tin từ các vị trí rất xa này. [1][2][3]\n\n### 2.2 Phân tích bằng hàm lỗi và đạo hàm\n- Ý tưởng phân tích: Ta xét hàm lỗi theo từng thời điểm (ví dụ các thành phần loss L2 và LT) và đạo hàm theo tham số (ví dụ W). Khi tính đạo hàm tổng (hàm lỗi tổng = trung bình cộng các hàm thành phần), các thành phần loss gần thời điểm hiện tại có đóng góp đạo hàm lớn hơn so với các thành phần ở xa (các t = T xa). Điều này dẫn đến mất cân xứng trong đóng góp gradient giữa các từ gần và từ xa. [3][4][6]\n\n- Công thức tổng quát (ý nghĩa được nêu trong video): hàm lỗi tổng được tính là trung bình cộng của các loss thành phần, do đó khi lấy đạo hàm từng phần, thành phần ở gần (ví dụ L2) có tác động lớn hơn thành phần ở xa (ví dụ LT). (Video nêu xét đạo hàm theo W; tương tự cho V, U). [3][4]\n\n### 2.3 Nguyên nhân kỹ thuật dẫn đến vanishing gradients\n- Khi tính chuỗi đạo hàm qua nhiều bước thời gian, ta nhân nhiều hệ số có giá trị tuyệt đối nhỏ hơn 1 (do đạo hàm của activation và các trọng số khởi tạo), nên tích các hệ số này có xu hướng tiến về 0 khi chuỗi dài → *vanishing gradient*. Ví dụ minh họa: tích lặp các số như 0.9 × 0.9 × ... càng nhiều càng tiến về 0. [5][7][8]\n\n- Cụ thể, đạo hàm của trạng thái S_t theo S_{t-1} chứa các nhân tố (ví dụ đạo hàm của sigmoid và các trọng số W) có giá trị trong khoảng (0,1), và W thường được khởi tạo nhỏ (random với mean ~0 và std ~1 nhỏ), nên khi nhân dồn qua nhiều bước sẽ thu nhỏ đạo hàm rất nhanh. [7][8]\n\n### 2.4 Các kỹ thuật chống vanishing gradients qua activation và khởi tạo\n- Thay activation sigmoid/tanh bằng ReLU:\n  - Sigmoid/tanh có đạo hàm giới hạn (|derivative| ≤ 1); tanh có giá trị trong [-1,1] nên vẫn gây vanishing. [9]\n  - ReLU: f(x) = max(0, x) có đạo hàm bằng 1 khi x > 0, do đó giúp ngăn ngừa sự tiêu biến quá nhanh của gradient qua lớp/hoặc bước thời gian. Đây là một trong những lý do các mạng sâu chuyển sang ReLU (và biến thể) kể từ sau cuộc thi lớn năm 2012. [10][11]\n\n- Khởi tạo ma trận trọng số W bằng ma trận đơn vị (identity initialization):\n  - Nếu khởi tạo W ≈ I thì khi nhân chuỗi ma trận tuyến tính sẽ giữ được thông tin (I × A = A), giúp ngăn giảm giá trị quá mức do nhân các ma trận nhỏ. Đây là một chiến lược để giảm thiểu vanishing gradient trong thành phần lặp. [11][12]\n\n- Tóm tắt: hai giải pháp chính nêu trong video để chống vanishing là (1) đổi activation sang ReLU (hoặc biến thể) và (2) khởi tạo W là ma trận đơn vị. [11][12]\n\n### 2.5 Exploding gradients và cách khắc phục\n- Exploding gradients là hiện tượng ngược lại: khi các hệ số (đạo hàm, trọng số) lớn hơn 1 dẫn đến tích dần tăng rất lớn (→ +∞). [13]\n\n- Giải pháp được nêu: sử dụng *gradient clipping* — nếu gradient quá lớn thì cắt (clamp) xuống một mức trần nhất định để tránh đạo hàm explosion. [13]\n\n### 2.6 Thay đổi cấu trúc cell: dùng cổng (gated cells) như LSTM\n- Thay vì dùng một cell RNN đơn giản (single simple cell), có thể thay bằng các cell có cổng (gated cells) như LSTM để kiểm soát thông tin nhớ/ quên. [13][14]\n\n- Lý do: các hàm sigmoid trong cell đơn giản dễ gây tiêu biến gradient và khiến thông tin bị mất dần; LSTM Cell có các cơ chế cổng giúp điều tiết (retain/forget) thông tin cần nhớ hoặc quên, từ đó giảm vấn đề mất thông tin dài hạn. [14]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa trong video: bài toán dự đoán từ thiếu trong câu tiếng Anh:\n  - \"In France, I had a great time and I learned some of the ___\" — từ cần điền nhiều khả năng là \"French\", nhưng thông tin liên quan nằm ở từ \"France\" xuất hiện rất xa vị trí cần dự đoán → minh họa cho vấn đề long-term dependency trong language modeling. [1][2][3]\n\n- Ứng dụng của các giải pháp:\n  - Thay activation bằng ReLU áp dụng rộng rãi cho mạng sâu để giảm vanishing gradients (được áp dụng đại trà sau khoảng 2012). [10][11]\n  - Khởi tạo W bằng ma trận đơn vị có thể được sử dụng trong RNN để giữ ổn định khi lan truyền qua nhiều bước thời gian. [11][12]\n  - Gradient clipping được dùng trong huấn luyện RNN để ngăn exploding gradients. [13]\n  - Sử dụng gated RNN (LSTM) cho các tác vụ ngôn ngữ/sequence modeling khi cần lưu giữ thông tin dài hạn (language modeling, machine translation, v.v.) — LSTM cho phép *remember* những thông tin quan trọng và *forget* thông tin không cần thiết. [14]\n\n- Trường hợp sử dụng: mọi bài toán chuỗi thời gian và ngôn ngữ tự nhiên nơi thông tin quan trọng có thể nằm rất xa vị trí dự đoán (ví dụ language modeling, sequence prediction) sẽ hưởng lợi từ các chiến lược nêu trên. (Ý này được suy ra từ ví dụ minh họa và các vấn đề/giải pháp đã nêu trong video). [1][14]\n\n---\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - RNN truyền thống gặp khó khăn với *long-term dependency* vì gradient từ các vị trí xa bị nhân nhiều hệ số nhỏ và “biến mất” khi lan truyền ngược qua thời gian. [1][3][5][7]\n  - Nguyên nhân kỹ thuật chính liên quan đến đạo hàm của activation (sigmoid/tanh có đạo hàm ≤ 1) và khởi tạo trọng số nhỏ → dẫn đến vanishing; ngược lại, trọng số lớn có thể gây exploding. [7][8][9][5]\n  - Các chiến lược khắc phục được đề xuất trong video: dùng ReLU (hoặc biến thể) thay cho sigmoid/tanh, khởi tạo W bằng ma trận đơn vị, áp dụng gradient clipping cho exploding gradients, và thay cell RNN bằng các gated cell như LSTM để điều tiết nhớ/quên. [10][11][12][13][14]\n\n- Tầm quan trọng: Hiểu và xử lý các vấn đề này là then chốt để huấn luyện RNN sâu ổn định và để mô hình có khả năng nắm bắt phụ thuộc dài trong dữ liệu chuỗi, đặc biệt trong các nhiệm vụ NLP. [1][14]\n\n- Liên hệ với các bài giảng/slide khác: Phân tích dựa trên các công thức đạo hàm và hàm hợp đã trình bày ở các slide trước (được nhắc lại trong bài), do đó nội dung này là phần tiếp nối trực tiếp của các nhận xét và công thức đã thảo luận trước đó. [7]\n\n---\n\nChú thích: các trích dẫn [1]...[14] tương ứng với các đoạn (chunks) trong video ở timestamps đã cung cấp; mỗi ý trong tóm tắt được dẫn nguồn ngay sau nội dung tương ứng.",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 7_IKD0O35NOUI",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_2： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/IKD0O35NOUI",
          "start_time": 0,
          "end_time": 59,
          "text": "Và vấn đề đầu tiên mà chúng ta sẽ nói sẽ bàn về mạng ANN Trong 2 slide trước chúng ta đưa ra những nhận xét Những nhận xét đó sẽ là tiền đề để giải thích cho các vấn đề của mạng ANN Vấn đề đầu tiên đó chính là sự phụ thuật dài Vấn đề đầu tiên đó là sự phụ thuật dài hay còn gọi là Long Term Dependency Chúng ta lấy một ví dụ sau Một ví dụ tiếng Anh sau In France, I had a great time and I learned some of the Chúng ta sẽ để trống Language Và nhiệm vụ của chúng ta cần phải đoán xem là trong cái chỗ trống này đó là gì Đúng không? Thì chúng ta thấy là cái mô hình ANN Nó không có cái cơ chế để cho phép chúng ta nắm bắt cái sự phụ thuật dài của từ Tức là nó chỉ truyền XT sang XT1 XT1 sang XT2 XT2"
        },
        {
          "index": 2,
          "video_id": "Chương 7_IKD0O35NOUI",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_2： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/IKD0O35NOUI",
          "start_time": 47,
          "end_time": 112,
          "text": "Nó không có cái cơ chế để cho phép chúng ta nắm bắt cái sự phụ thuật dài của từ Tức là nó chỉ truyền XT sang XT1 XT1 sang XT2 XT2 Nhưng mà nó không có cái cơ chế để cho chúng ta có thể lưu cái thông tin Đối với những cái từ mà ở từ rất xa trước đó Nó không có cái cơ chế nào để mà ý thức được cái việc là Những cái từ ở đằng xa Đúng không? Ví dụ như cái từ France Nó sẽ có khoảng cách xa Đến cái chỗ mà chúng ta cần dự đoán hơn rất nhiều So với những từ như là từ xâm, từ ốc, từ đợt Thì nó không có cái cơ chế đó Và chúng ta sẽ có thể tìm ra những cái từ xa trước đó là gì? Và trong khi cái từ mà chúng ta cần dự đoán Thì nó lại phụ thuộc vào những cái từ đó Thì ở đây chúng ta đang nói về language Như vậy thì khả năng cao là cái từ này sẽ là Từ French, từ tiếng Pháp Nhưng mà muốn biết được cái từ này là từ French Nó phải bám vào cái từ Ở cách nó rất xa Đúng không? Xa so với những cái từ order, xâm, learn"
        },
        {
          "index": 3,
          "video_id": "Chương 7_IKD0O35NOUI",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_2： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/IKD0O35NOUI",
          "start_time": 97,
          "end_time": 160,
          "text": "Như vậy thì khả năng cao là cái từ này sẽ là Từ French, từ tiếng Pháp Nhưng mà muốn biết được cái từ này là từ French Nó phải bám vào cái từ Ở cách nó rất xa Đúng không? Xa so với những cái từ order, xâm, learn Đó chính là cái từ French này Thì cái từ cần dự đoán nó phụ thuộc vào cái từ này Rất là xa Và anên Nó không có cái cơ chế để cho mình nắm mắt cái sự phụ thuộc dài này Và Đối với cái Giải thích cho cái Cái việc này á Đó là chúng ta sẽ dựa trên cái công thức của cái hàm độ lỗi Chúng ta sẽ dựa trên cái công thức của cái hàm độ lỗi Là chúng ta sẽ xét cái 2 cái từ Xin lỗi chúng ta sẽ xét 2 cái loss là L2 Và LT Thì chúng ta sẽ có 2 cái đạo hàm thành phần Và ở đây chúng ta cũng giả sử là chúng ta chỉ Xét với lại cái biến W nha Còn tương tự, hoàn toàn tương tự cho 2 cái biến là V và U"
        },
        {
          "index": 4,
          "video_id": "Chương 7_IKD0O35NOUI",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_2： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/IKD0O35NOUI",
          "start_time": 149,
          "end_time": 211,
          "text": "Thì chúng ta sẽ có 2 cái đạo hàm thành phần Và ở đây chúng ta cũng giả sử là chúng ta chỉ Xét với lại cái biến W nha Còn tương tự, hoàn toàn tương tự cho 2 cái biến là V và U Thì cái nhận xét đó là cái thành phần L2 Nó sẽ đóng vai trò quan trọng hơn So với lại cái thành phần LT Trong cái công thức của cái hàm độ lỗi này nè Nó là bằng trung tâm của cái hàm độ lỗi này nè Nó là trung bình cộng Trung bình cộng Của các cái hàm thành phần Nhưng khi tính đạo hàm Thì cái thành phần L2 Nó lại đóng vai trò quan trọng hơn Thì điều này là tại sao Và cái việc này thì nó dẫn đến là cái từ thứ 2 Là cái từ gần nó sẽ có cái ảnh hưởng hơn So với lại cái từ thứ T Thì cái điều này nó cũng chính là Ý nghĩa cho cái việc là cụ thuật dài đó Thì điều này giải thích tại sao Tại vì cái hàm L2 Nó sẽ có cái ảnh hưởng hơn   Nó gần hơn"
        },
        {
          "index": 5,
          "video_id": "Chương 7_IKD0O35NOUI",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_2： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/IKD0O35NOUI",
          "start_time": 199,
          "end_time": 260,
          "text": "Thì cái điều này nó cũng chính là Ý nghĩa cho cái việc là cụ thuật dài đó Thì điều này giải thích tại sao Tại vì cái hàm L2 Nó sẽ có cái ảnh hưởng hơn   Nó gần hơn Nên cái hàm này Cái hàm hợp của nó Nó sẽ ít phép biến đổi hơn Cái hàm LT thì nó ở xa hơn Nên nó sẽ nhiều phép biến đổi hơn Mà cái hàm hợp nào mà càng nhiều phép biến đổi Thì các cái con số Ví dụ 0.9 x 0.9 Cái chuỗi này mà càng dài Thì nó sẽ càng tiến đến 0 Vậy thì vấn đề đặt ra đó là Khi cái đạo hàm này tiến đến 0 Đúng không? Khi cái đạo hàm này tiến đến 0 Tức là nó đóng góp vào bên trong cái Cái công thức của cái hàm tổng của chúng ta Tức là cái thằng này nó sẽ đóng góp ít Trong khi đó L2 thì đóng góp vào cái hàm tổng này là đóng góp nhiều"
        },
        {
          "index": 6,
          "video_id": "Chương 7_IKD0O35NOUI",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_2： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/IKD0O35NOUI",
          "start_time": 246,
          "end_time": 310,
          "text": "Cái công thức của cái hàm tổng của chúng ta Tức là cái thằng này nó sẽ đóng góp ít Trong khi đó L2 thì đóng góp vào cái hàm tổng này là đóng góp nhiều Mà cái hàm này thì nó lại là bằng trung bình cộng Nó lại bằng trung bình cộng Của cái tổng Đó thì dẫn đến là Cái thành phần mà đóng góp nhiều Nhưng mà hệ số của nó vẫn tương đương với lại cái hệ số của cái thằng đóng góp ít Thì đó chính là cái vấn đề Và nó gây ra cái sự dài Có những cái từ T càng dài Đúng không? Cái T mà càng dài Thì cái đóng góp cho cái công thức đảng đạo hàm lại càng ít Trong khi đó những cái từ rất là ngắn Những cái từ rất là ngắn Những cái từ rất là ngắn  Những cái từ rất là ngắn Thì những cái từ ở đầu tiên Thì lại đóng góp nhiều hơn Nó tạo ra cái sự mất cân xứng Trong nguyên một cái câu của mình Thì rõ ràng là câu nào từ nào Cũng tại những cái vị trí nào Nó cũng đều có những cái giá trị nhất định"
        },
        {
          "index": 7,
          "video_id": "Chương 7_IKD0O35NOUI",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_2： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/IKD0O35NOUI",
          "start_time": 299,
          "end_time": 360,
          "text": "Thì lại đóng góp nhiều hơn Nó tạo ra cái sự mất cân xứng Trong nguyên một cái câu của mình Thì rõ ràng là câu nào từ nào Cũng tại những cái vị trí nào Nó cũng đều có những cái giá trị nhất định Và hiện tượng thứ hai Đó chính là hiện tượng Vanishing Radian Hoặc là Exploding Radian Thì cũng dựa trên cái công thức Của hàm hợp Ở các cái slide trước Chúng ta có cái công thức đạo hàm Của hàm hợp Như sau Và nhận xét đó là khi cái văn bản của mình Mà càng dài tức là T này Có thể tiến đến là từ trừ Vài trăm cho đến vài ngàn Thì cái đạo hàm này Đạo hàm tại cái LT này Sẽ tiến đến không Thì điều này Như chúng ta đã từng đề cập trước đó Cái công thức này Chúng ta có công thức đạo hàm Của một cái hàm Là sigmoid của O X T C Cộng cho V S T Trừ một"
        },
        {
          "index": 8,
          "video_id": "Chương 7_IKD0O35NOUI",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_2： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/IKD0O35NOUI",
          "start_time": 349,
          "end_time": 410,
          "text": "Chúng ta có công thức đạo hàm Của một cái hàm Là sigmoid của O X T C Cộng cho V S T Trừ một Công thức này là công thức của S T Thì khi chúng ta tính đạo hàm Chúng ta tính đạo hàm của nó Thì nó sẽ ra cái công thức này Nó sẽ ra công thức này Và thật ra thì Nếu mà chỉ tính S T theo S T trừ một Thì Các cái thành phần này Nó đều là các cái con số Nhỏ hơn không À xin lỗi là từ không cho đến một Con số này Là từ không Cho đến một W của mình Ban đầu Nó cũng sẽ khởi tạo Bởi một cái giá trị random Nó là một cái ma trợ Bởi các giá trị random Trong Min là bằng không Và standard deviation là bằng một Như vậy nó cũng là các cái con số Rất là nhỏ Và các cái con số mà nhỏ"
        },
        {
          "index": 9,
          "video_id": "Chương 7_IKD0O35NOUI",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_2： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/IKD0O35NOUI",
          "start_time": 400,
          "end_time": 459,
          "text": "Min là bằng không Và standard deviation là bằng một Như vậy nó cũng là các cái con số Rất là nhỏ Và các cái con số mà nhỏ Thì khi nhân với nhau Nó sẽ Nảy sinh ra cái vấn đề đó Do đó thì ở đây Chúng ta sẽ có cái giải pháp Cái giải pháp Giải pháp để giải quyết Cái vấn đề về vanishing Vấn đề về Tiêu biến các cái radian Đó là thay vì chúng ta Sử dụng các cái hàm Sigmoid Thay vì là Này nè Chúng ta sử dụng hàm sigmoid Thì chúng ta sẽ sử dụng hàm khác Có thể là sử dụng hàm Ví dụ như là hàm tanh Nhưng mà lưu ý Là với hàm tanh Thì cái giải giá trị của mình Nó thay vì là từ không Đến một Thì nó sẽ là từ trừ một Cho đến một Thì suy cho cùng Nó cũng là những cái con số Có giá trị tuyệt đối Mấy hôm một Như vậy thì sigmoid và tanh Không giúp cho mình Giảm bớt cái hiện tượng vanishing này"
        },
        {
          "index": 10,
          "video_id": "Chương 7_IKD0O35NOUI",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_2： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/IKD0O35NOUI",
          "start_time": 450,
          "end_time": 511,
          "text": "Nó cũng là những cái con số Có giá trị tuyệt đối Mấy hôm một Như vậy thì sigmoid và tanh Không giúp cho mình Giảm bớt cái hiện tượng vanishing này Mà chúng ta sẽ sử dụng Cái hàm Là hàm relu Tại vì sao Hàm relu Là Có cái công thức như sao Là bằng max Của không Và x Như vậy thì Hàm relu Nó sẽ có cái đạo hàm Nó sẽ có cái đạo hàm Với x mà lớn hơn không Thì đạo hàm của nó sẽ là bằng một Đạo hàm của nó sẽ là bằng một Như vậy nó sẽ ngăn Nó sẽ giúp cho mình ngăn ngừa Nó sẽ giúp cho mình ngăn ngừa Cái đạo hàm của mình Đạo hàm Fn Fn trừ một Nó sẽ ngăn cho cái đạo hàm của mình Bị tiêu biến dần Cái radian Thì Đây cũng là một cái lý do Tại sao Từ năm 2012 Sau cái Cuộc thi MNS Thì tất cả các cái Tất cả gần như tất cả các cái mô hình học sâu Đều chuyển từ sigmoid"
        },
        {
          "index": 11,
          "video_id": "Chương 7_IKD0O35NOUI",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_2： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/IKD0O35NOUI",
          "start_time": 499,
          "end_time": 560,
          "text": "Đây cũng là một cái lý do Tại sao Từ năm 2012 Sau cái Cuộc thi MNS Thì tất cả các cái Tất cả gần như tất cả các cái mô hình học sâu Đều chuyển từ sigmoid Sang sử dụng các cái hàm relu Hoặc cái các biến thể của relu Và tiếp theo Thì chúng ta sẽ Giải quyết cái vấn đề liên quan đến ma trận W Sigmoid thì chúng ta đã giải quyết rồi Do cái mì giá trị sigmoid là từ 0 cho đến 1 Do đó chúng ta thay thế Thay thế bằng relu Bây giờ đối với W Thì ban đầu Là chúng ta dùng Cái phân bố Là 0,1 Đúng không Thì các cái giá trị random của mình Nó sẽ thường là sẽ Nhỏ hơn 1 và lớn hơn 0 Thì bây giờ W của mình Mình sẽ cố định nó luôn Là bằng một cái ma trận đơn vị Thì cái ma trận đơn vị này Thì khi nhân Ma trận đơn vị Khi nhân với lại một cái ma trận khác Thì nó sẽ ra Ngăn ngừa cho cái việc là Thay đổi cái giá trị Của cái ma trận"
        },
        {
          "index": 12,
          "video_id": "Chương 7_IKD0O35NOUI",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_2： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/IKD0O35NOUI",
          "start_time": 549,
          "end_time": 611,
          "text": "Thì cái ma trận đơn vị này Thì khi nhân Ma trận đơn vị Khi nhân với lại một cái ma trận khác Thì nó sẽ ra Ngăn ngừa cho cái việc là Thay đổi cái giá trị Của cái ma trận Ví dụ IN Nhân với A Thì nó sẽ bằng chính là A Đúng không Và Nó sẽ làm cho mình ngăn Làm giảm Cái giá trị Và Cụ thể là giá trị Của cái bé này Nó ngăn Làm giảm cái giá trị này xuống Giá trị theo kiểu Tuyệt tối Như vậy thì Ở đây chúng ta sẽ có Hai giải pháp Giải pháp đầu tiên Là thay thế Relu Thay thế cái hàm sigmoid Hoặc là hàm tanh Bằng relu Và giải pháp thứ 2 Đó là cái giá trị W Chúng ta sẽ khởi tạo Nó là bằng một cái ma trận đơn vị và đây là 2 cách để giúp cho chúng ta chống lại cái hiện tượng Vanishing Radian và ngoài ra thì chúng ta sẽ còn một số cái vấn đề khác ví dụ như cái vấn đề về"
        },
        {
          "index": 13,
          "video_id": "Chương 7_IKD0O35NOUI",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_2： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/IKD0O35NOUI",
          "start_time": 598,
          "end_time": 661,
          "text": "và đây là 2 cách để giúp cho chúng ta chống lại cái hiện tượng Vanishing Radian và ngoài ra thì chúng ta sẽ còn một số cái vấn đề khác ví dụ như cái vấn đề về Exploding Radian thì Exploding Radian nó là ngược của Vanishing Radian nếu như cái đạo hàm của mình mà lớn quá, các cái con số mà lớn thì khi nhân với nhau nó cũng sẽ có xu hướng là tiến đến cộng vô cùng nhưng vậy thì ở đây người ta sẽ có một cái kỹ thuật để chống cái loại hiện tượng Exploding Radian đó là mình sẽ sử dụng Clipping sẽ chạc tức là Radian mà quá lớn thì mình sẽ lấy nó làm một cái mức trần nào đó thôi bên cạnh các cái giải pháp về Radian thì người ta có một số cái phương pháp khác đó là chúng ta thay các cái nốt trong cái mạng Recurrent Neural Network thay vì chúng ta sử dụng một cái cell ở dạng đơn giản"
        },
        {
          "index": 14,
          "video_id": "Chương 7_IKD0O35NOUI",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 7] Part 3_2： Một số vấn đề của mạng RNN",
          "video_url": "https://youtu.be/IKD0O35NOUI",
          "start_time": 650,
          "end_time": 698,
          "text": "đó là chúng ta thay các cái nốt trong cái mạng Recurrent Neural Network thay vì chúng ta sử dụng một cái cell ở dạng đơn giản thì chúng ta có thể thay thế bằng các cái cổng chúng ta sẽ thay thế bằng các cái cổng để kiểm soát thông tin ví dụ đối với cái cell này đối với cái cell này thì các cái hàm sigmoid của mình xử lỗi các hàm tăng hoặc hàm sigmoid của mình khi chúng ta thực hiện thì nó sẽ dễ tiêu biến và dễ tiêu biến thì có khả năng là nó làm cho cho cái thông tin của mình bị mắc mát đi do đó thì chúng ta sẽ sử dụng cái LSTM Cell chúng ta sử dụng LSTM Cell để mà điều tiết cái thông tin nhớ cái cần nhớ và quên cái cần quên"
        }
      ]
    },
    {
      "video_id": "Chương 7_qJj_LY1r91U",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giới thiệu và phân tích một số biến thể của mạng Recurrent Neural Network (RNN), tập trung vào LSTM nhằm giải quyết các vấn đề của RNN truyền thống. [1][2][6]  \n- Các khái niệm sẽ được đề cập: Long Term Dependency (phụ thuộc dài hạn), Vanishing Gradient (biến mất gradient), cấu trúc và cơ chế của LSTM (Cell / Context Cell, Input Gate, Forget Gate, Output Gate), các kiểu ứng dụng của RNN (1-to-1, 1-to-many, many-to-one, many-to-many), và đề cập sơ lược tới các phần tiếp theo như Bi-directional RNN và Deep Stacked RNN. [1][2][3][4][5][6]\n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Vấn đề của RNN truyền thống\n- Long Term Dependency: Một token ở cuối chuỗi có thể phụ thuộc vào một token ở đầu chuỗi — tức là phụ thuộc ở khoảng cách rất xa trong thời gian. [1]  \n- Vanishing Gradient: Khi huấn luyện các mạng sâu (kể cả RNN) với nhiều phép biến đổi, đạo hàm qua các hàm hợp tích lũy có thể làm cho gradient thu nhỏ dần về 0, dẫn tới khó học các phụ thuộc dài hạn. Đây là một vấn đề kinh điển của Deep Learning. [2][3]\n\n### 2.2. Ôn lại hoạt động cơ bản của RNN\n- Ở mỗi bước thời gian t, RNN tính trạng thái ẩn s_t dựa trên trạng thái quá khứ và input hiện tại, sau đó dùng trạng thái này để dự đoán y_t. (ký hiệu chung: s_t là hidden state, y_t là output tại thời điểm t). [3][4]  \n- Ví dụ công thức đầu ra: output y_t thường được tính dựa trên một phép biến đổi theo dạng softmax trên vector đầu ra o_t (ghi chú: bài giảng đề cập công thức liên quan đến softmax của o_t). [4]\n\n### 2.3. Các tình huống sử dụng (Io/Io patterns) của RNN\n- 1-to-1: một input → một output (ví dụ: bài toán đơn giản không tuần tự). [4]  \n- 1-to-many: một input → một chuỗi output (ví dụ: sinh một bài thơ từ một chủ đề cho trước). [4]  \n- Many-to-one: một chuỗi input → một output (ví dụ: Sentiment Analysis, phân loại văn bản). [4][5]  \n- Many-to-many: hai dạng  \n  - dạng 1: input và output là chuỗi (ví dụ: dịch máy, tóm tắt văn bản). [5]  \n  - dạng 2: tại mỗi thời điểm nhận một từ vào và dự đoán nhãn tương ứng ngay thời điểm đó (ví dụ: POS-tagging — gán nhãn từ loại). [5]\n\n### 2.4. Giới thiệu LSTM (Long Short-Term Memory)\n- Lịch sử/độ phổ biến: LSTM là một biến thể của RNN được phát triển từ thập niên 1990 và được sử dụng rộng rãi đến khoảng 2015–2016, trước khi Transformer/Attention trở nên phổ biến. [6]  \n- Mục tiêu: LSTM được thiết kế để giải quyết vấn đề Vanishing Gradient và giúp mô hình *nhớ* những thông tin quan trọng lâu hơn và *quên* thông tin không cần thiết. [9][10]\n\n### 2.5. Cấu trúc và cơ chế của LSTM\n- Thành phần chính: LSTM cell (hay *Context Cell*) gồm 4 thành phần chính: Context Cell (cell chứa ngữ cảnh), Input Gate, Output Gate, Forget Gate. Mỗi cell được xử lý tuần tự theo thời gian. [7][8][9]  \n  - Context Cell (Cell): nơi lưu trữ thông tin ngữ cảnh của toàn bộ nội dung đã đọc. [7]  \n  - Input Gate: quyết định có nên chấp nhận (bổ sung) thông tin mới vào Context Cell hay không. [7]  \n  - Output Gate: quyết định có lấy thông tin từ Context Cell để cung cấp ra ngoài hay không. [7]  \n  - Forget Gate: quyết định có nên quên (loại bỏ) một số thông tin trong Context Cell hay không, tránh tích lũy thông tin thừa suốt chuỗi. [7][8]  \n- Vai trò của các cổng: Ba cổng trên (Input/Forget/Output) điều hướng luồng thông tin vào/ra/giữ trong Context Cell, cho phép LSTM chọn lọc thông tin cần nhớ và cần quên. [8]  \n- Tính tuần tự: Mỗi LSTM cell xử lý dữ liệu tuần tự giống như RNN nhưng với cơ chế gating để quản lý trạng thái dài hạn. [9]\n\n### 2.6. Tại sao LSTM giúp giảm Vanishing Gradient\n- Nhờ cơ chế *nhớ/điều chỉnh/quên* (remember/forget) qua các gates, LSTM duy trì thông tin quan trọng trên nhiều bước thời gian, từ đó làm cho gradient truyền ngược qua nhiều bước thời gian ổn định hơn so với RNN đơn giản — giúp phần nào giảm hiện tượng vanishing gradient khi huấn luyện. [9][10]\n\n### 2.7. Hạn chế của RNN truyền thống (tái khẳng định)\n- Trong RNN truyền thống (ví dụ dùng hàm tanh cho trạng thái), mọi thông tin khi truyền qua trạng thái s_t đều bị đẩy vào và không có cơ chế lọc, dẫn đến việc thông tin ở đầu chuỗi dễ bị “pha loãng” hay bị mất khi truyền về cuối chuỗi. Điều này làm cho RNN khó nhớ các thông tin rất xa. [10][11]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- 1-to-many: Sinh chuỗi từ một input — ví dụ: tạo bài thơ từ một chủ đề cho trước. [4]  \n- Many-to-one: Phân loại chuỗi thành một nhãn — ví dụ: Sentiment Analysis (phân tích cảm xúc) hoặc phân loại văn bản. [4][5]  \n- Many-to-many (dạng 1): Dịch máy, tóm tắt văn bản — đầu vào và đầu ra đều là chuỗi. [5]  \n- Many-to-many (dạng 2): Nhận từng từ vào và dự đoán nhãn ngay thời điểm đó — ví dụ: POS-tagging (gán nhãn từ loại). [5]  \n- Ứng dụng chung của LSTM: nhiệm vụ phân loại / xử lý dữ liệu tuần tự, nơi phụ thuộc sau phụ thuộc trước (temporal dependencies) quan trọng — LSTM thường được dùng rộng rãi cho các bài toán chuỗi trước khi Transformer xuất hiện. [9][6]\n\n---\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính: Bài giảng nêu rõ hai vấn đề lớn của RNN truyền thống là Long Term Dependency và Vanishing Gradient, ôn lại cơ chế hoạt động cơ bản của RNN, phân loại các kiểu input-output của bài toán tuần tự, sau đó giới thiệu LSTM — một biến thể RNN với Context Cell và ba gates (Input, Forget, Output) — giúp lựa chọn thông tin cần nhớ/quên và giảm thiểu vấn đề vanishing gradient, làm cho LSTM phù hợp cho nhiều tác vụ xử lý tuần tự như dịch máy, tóm tắt, phân loại cảm xúc, POS-tagging. [1][2][3][4][5][6][7][8][9][10][11]  \n- Tầm quan trọng: LSTM là một kiến trúc then chốt trong lịch sử xử lý chuỗi và Deep Learning tuần tự, đóng vai trò lớn từ những năm 1990 đến giữa thập niên 2010. [6][9]  \n- Liên hệ với các bài giảng khác: Video này là phần đầu của chương (chỉ ra rằng trong phần sau sẽ bàn về Bi-directional RNN (ANN 2 chiều) và Deep Stacked RNN), tức phần 2 và phần 3 sẽ mở rộng thêm biến thể và kiến trúc sâu hơn. [6]\n\n---\n\nGhi chú: Tất cả nội dung trên được tóm lược trực tiếp từ các đoạn trích của video (xem các chunk đã cho — [1] đến [11]).",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 7_qJj_LY1r91U",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_1： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/qJj_LY1r91U",
          "start_time": 13,
          "end_time": 64,
          "text": "Trong bài hôm nay thì chúng ta sẽ tiến hành tìm hiểu một số cái biến thể của mạng Recurrent Neural Network thì như trong cái bài trước chúng ta đã chỉ ra mà ai nên có một số cái vấn đề cần phải giải quyết Cái vấn đề đầu tiên đó chính là vấn đề về Long Term Dependency Tức là một cái từ ở một cái vị trí ở cuối câu thì có khả năng phụ thuộc vào một cái từ ở vị trí đầu câu Như vậy là nó có cái sự phụ thuộc rất là xa Và cái thứ hai đó chính là vấn đề về Vanishing Gradient Đây là vấn đề kinh điển của lĩnh vực học sâu"
        },
        {
          "index": 2,
          "video_id": "Chương 7_qJj_LY1r91U",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_1： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/qJj_LY1r91U",
          "start_time": 47,
          "end_time": 112,
          "text": "Như vậy là nó có cái sự phụ thuộc rất là xa Và cái thứ hai đó chính là vấn đề về Vanishing Gradient Đây là vấn đề kinh điển của lĩnh vực học sâu Tại vì các cái mô hình học sâu như là ANN nói riêng cũng như là các cái mạng CNN khác nói chung Thì các cái kiến trúc của mình nó sẽ bao gồm rất nhiều các cái thao tác biến đổi Và do có rất nhiều các cái thao tác biến đổi như vậy sẽ dẫn đến là cái hàm của mình khi tính đạo hàm theo hàm hợp Thì sẽ là bao gồm tích của các cái hàm hợp thành phần Với mỗi cái hàm hợp thành phần nếu như nhận các cái giá trị gradient nhỏ dành Và nó có giá trị là từ 0 cho đến 1 Thì nó sẽ làm cho cái giá trị gradient của mình có xu hướng là thu hẹp lại Và tiến về 0 Thì đây là cái vấn đề cố ủ của Deep Learning Nếu như không có những cái giải pháp để giải quyết"
        },
        {
          "index": 3,
          "video_id": "Chương 7_qJj_LY1r91U",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_1： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/qJj_LY1r91U",
          "start_time": 97,
          "end_time": 160,
          "text": "Thì nó sẽ làm cho cái giá trị gradient của mình có xu hướng là thu hẹp lại Và tiến về 0 Thì đây là cái vấn đề cố ủ của Deep Learning Nếu như không có những cái giải pháp để giải quyết Thế thì các cái biến thể hôm nay mà cũng là các cái biến thể của các cái biến thể này Chúng ta cùng tìm hiểu thì để giúp cho giải quyết cái vấn đề này Đầu tiên đó là chúng ta sẽ ôn lại một số cái kiến thức cơ bản về mạng ANEN Trong mạng ANEN thì chúng ta sẽ tính toán 2 bước tại một thời điểm T Tại một cái thời điểm T thì chúng ta sẽ tính cái ST đầu tiên ST là cái trạng thái ẩn Và trạng thái ẩn này thì được tính từ cái giá trị quá khứ Và kết hợp với lại cái thông tin của hiện tại Sau khi đã tổng hợp được thông tin rồi Thì chúng ta sẽ tiến hành đưa ra cái giá trị dự đoán là Y-T"
        },
        {
          "index": 4,
          "video_id": "Chương 7_qJj_LY1r91U",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_1： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/qJj_LY1r91U",
          "start_time": 153,
          "end_time": 210,
          "text": "Sau khi đã tổng hợp được thông tin rồi Thì chúng ta sẽ tiến hành đưa ra cái giá trị dự đoán là Y-T Dựa trên cái công thức đó là SOPMASK của OV-T Và một số cái tình huống sử dụng của mạng ANEN Bao gồm là tình huống 1-to-1 Tức là biến từ 1-to-1 Từ đầu vào và tạo ra một cái giá trị output 1-to-many Tức là từ một cái đầu vào chúng ta sẽ tạo ra một cái chuỗi output Lấy ví dụ như bài toán tạo ra một cái bài thơ từ một cái chủ đề cho trước Many-to-one Là đầu vào sẽ là một chuỗi Và đầu ra sẽ là một giá trị Thì ví dụ cho cái tình huống sử dụng này Đó là bài toán Sentiment Analysis Hoặc là bài toán phân loại văn bản Dạng 1-to-1 Thì chúng ta phải đọc hết toàn bộ chuỗi"
        },
        {
          "index": 5,
          "video_id": "Chương 7_qJj_LY1r91U",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_1： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/qJj_LY1r91U",
          "start_time": 199,
          "end_time": 262,
          "text": "Đó là bài toán Sentiment Analysis Hoặc là bài toán phân loại văn bản Dạng 1-to-1 Thì chúng ta phải đọc hết toàn bộ chuỗi Rồi sau đó mới tính toán ra cái giá trị chuỗi output Thì cái ví dụ minh họa cho cái Many-to-many dạng 1 Chính là bài toán dịch máy Hoặc là bài toán tấm tắt văn bản Many-to-many dạng 2 Thì đầu vào là chúng ta sẽ nhận vào từng từ Và chúng ta sẽ đưa ra cái giá trị dự đoán Ngay tại thời điểm đó Thì chúng ta sẽ đưa ra cái giá trị dự đoán Thì cái ví dụ cho cái bài toán Many-to-many Cho cái dạng tức Many-to-many dạng 2 này Chính là bài toán Post-Tagging Tức là gán nhãn từ loại Và nội dung của ngày hôm nay Thì chúng ta sẽ bao gồm 3 phần chính Phần đầu tiên đó chính là Long Short Term Memory Tức là đây là một trong những cái kiến trúc Mà được sử dụng rất phổ biến Cho đến vào cái giai đoạn là những năm 2016"
        },
        {
          "index": 6,
          "video_id": "Chương 7_qJj_LY1r91U",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_1： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/qJj_LY1r91U",
          "start_time": 249,
          "end_time": 310,
          "text": "Phần đầu tiên đó chính là Long Short Term Memory Tức là đây là một trong những cái kiến trúc Mà được sử dụng rất phổ biến Cho đến vào cái giai đoạn là những năm 2016 Cái LSTM thì nó có từ những năm 2016 đến năm 2016 Từ những năm 1990 rồi Tức là nó có những năm 90 Nhưng mà nó đã được sử dụng Cho đến tận những năm 2015-2016 Cho đến khi có sự ra đời của Transformer Và Attention Trong phần thứ 2 Thì chúng ta sẽ tìm hiểu về cái biến thể B-Direct Hanno Tức là ANN 2 chiều Và ở cái phần số 3 Phần cuối cùng Đó là chúng ta sẽ tìm hiểu về DeepStack Asian ANN Thì giới thiệu về LSTM Thì LSTM Là một trong những cái biến thể của ANN Bao gồm 4 cái thành phần chính Đầu tiên đó là cái thành phần về Contact Cell Thì đúng như cái tên gọi của Cục nó Contact Cell tức là cái Cell"
        },
        {
          "index": 7,
          "video_id": "Chương 7_qJj_LY1r91U",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_1： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/qJj_LY1r91U",
          "start_time": 299,
          "end_time": 361,
          "text": "Bao gồm 4 cái thành phần chính Đầu tiên đó là cái thành phần về Contact Cell Thì đúng như cái tên gọi của Cục nó Contact Cell tức là cái Cell Để chứa cái thông tin về mặt ngữ cảnh Để chứa cái thông tin về mặt ngữ cảnh Của toàn bộ cái nội dung Văn bản mà chúng ta Đọc được Cái Input Gate Tức là cái cổng Input Là cái nơi để cho chúng ta biết Là chúng ta sẽ Nhận cái thông tin đó Hay không Chúng ta sẽ xử lý cái thông tin đó Đưa vào bên trong cái Contact Cell này hay không Output Gate Là để cho biết chúng ta có Lấy cái thông tin đó Và Lấy cái thông tin từ Contact Cell ra ngoài hay không Và Forget Gate Là cái cổng Input Gate Thì là cái cổng Input Gate Thông tin Để cho chúng ta biết là Có nên quên hết cái thông tin Ở bên trong Cái Contact Cell này hay không"
        },
        {
          "index": 8,
          "video_id": "Chương 7_qJj_LY1r91U",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_1： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/qJj_LY1r91U",
          "start_time": 349,
          "end_time": 410,
          "text": "Contact Cell ra ngoài hay không Và Forget Gate Là cái cổng Input Gate Thì là cái cổng Input Gate Thông tin Để cho chúng ta biết là Có nên quên hết cái thông tin Ở bên trong Cái Contact Cell này hay không Có nên quên cái thông tin này hay không Tại vì nếu như chúng ta Cho cái Contact Cell mà cứ Đưa hết thông tin vào bên trong đó Và truyền đến Cuối Của cái văn bản Thì Nó dẫn đến có rất nhiều thông tin thừa Thì Forget là nó sẽ giúp cho mình Quên đi những cái thông tin không có Còn quan trọng nữa Và Ba cái cổng này Thì nó còn có một cái cổng này Cái cách gọi khác Đó chính là nó giúp cho chúng ta điều hướng Điều hướng cái luồng thông tin Ra vào và ra khỏi Cái Contact Cell này Rồi Và mỗi Cell thì Trong cái mạng LSTM thì sẽ được Sử lý tuần tự nó cũng tương tự như cái Cell Của ANN Nó sẽ phải sử lý tuần tự Thì ở đây chúng ta sẽ ký hiệu là LSTM Cell"
        },
        {
          "index": 9,
          "video_id": "Chương 7_qJj_LY1r91U",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_1： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/qJj_LY1r91U",
          "start_time": 398,
          "end_time": 461,
          "text": "Trong cái mạng LSTM thì sẽ được Sử lý tuần tự nó cũng tương tự như cái Cell Của ANN Nó sẽ phải sử lý tuần tự Thì ở đây chúng ta sẽ ký hiệu là LSTM Cell Và bên trong cái LSTM Cell này Thì nó sẽ bao gồm 4 cái thành phần Đã nói Và LSTM thì rất thích hợp Cho các cái nhiệm vụ phân loại Với các cái dữ liệu tuần tự Thì chúng ta một lần nữa khẳng định Đó là LSTM nó chỉ là một biến thể Của ANN Và nó phù hợp cho những cái dữ liệu mà Cái giá trị trao, giá trị sau Phụ thuộc vào giá trị trước LSTM thì nó cũng góp phần Cho chúng ta giải quyết cái vấn đề về Vanishing Radian Khi huấn luyện cái mạng ANN Chính nhờ cái cơ chế là nhớ cái cần nhớ Cần quên, nó sẽ giúp cho chúng ta Tạo ra các cái Radian Cách hiệu quả hơn Rồi Đối với cái mạng ANN truyền thống Thì chúng ta sẽ thấy là cái hiện tượng Mà rất là hay mắc phải Đó chính là hiện tượng Vanishing Radian"
        },
        {
          "index": 10,
          "video_id": "Chương 7_qJj_LY1r91U",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_1： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/qJj_LY1r91U",
          "start_time": 449,
          "end_time": 510,
          "text": "Cách hiệu quả hơn Rồi Đối với cái mạng ANN truyền thống Thì chúng ta sẽ thấy là cái hiện tượng Mà rất là hay mắc phải Đó chính là hiện tượng Vanishing Radian Và Thứ 2 đó là chúng ta không nhớ được Những cái thông tin đủ dài Tức là có những cái từ Ở đầu câu Nhưng mà đến cuối câu thì nó quên mất Thì cái việc mà Một cái từ ở đầu câu Mà đến cuối câu nó quên mất Đó là vì trong cái quá trình mà Thông tin nó truyền xuyên suốt Cái trục Hittinstein Nó truyền xuyên suốt trục S Thì thông tin nào nó cũng nạp vào Thông tin nào nó cũng nạp vào Cái trạng thái ẩn này Dẫn đến là những cái từ ở đầu Nó sẽ bị file thông tin đi Còn những cái từ ở giữa Hoặc là những cái từ gần cuối Thì thông tin rất là dày đặc Và đầy đủ Và Đó là vì cái Mô đun là hàm tanh này nè Gặp bất cứ cái Thông tin nào"
        },
        {
          "index": 11,
          "video_id": "Chương 7_qJj_LY1r91U",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_1： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/qJj_LY1r91U",
          "start_time": 498,
          "end_time": 522,
          "text": "Hoặc là những cái từ gần cuối Thì thông tin rất là dày đặc Và đầy đủ Và Đó là vì cái Mô đun là hàm tanh này nè Gặp bất cứ cái Thông tin nào Của cái ST Khi chúng ta đưa vào Thì cũng đẩy vào bên trong cái ST Tức là thông tin nào nó cũng sẽ Sử dụng cái ST này hết Nó không có cái tính chất gọi là Chắc lọc thông tin"
        }
      ]
    },
    {
      "video_id": "Chương 7__Km_A2iRUds",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giải thích chi tiết kiến trúc LSTM (Long Short-Term Memory) — các thành phần chính (cell state/context cell, forget gate, input gate, output gate, module tạo candidate) và cách chúng điều hướng thông tin để giải quyết vấn đề ghi nhớ/quên trong chuỗi (sequence) và giảm bớt hiện tượng vanishing gradient. [1][13][14]\n\n- Các khái niệm sẽ được đề cập: *context cell* (ký hiệu C — cell state), **forget gate**, **input gate**, **output gate**, hàm kích hoạt *sigmoid* và *tanh*, phép toán cập nhật cell (nhân + cộng), module tạo thông tin (candidate) tương tự ANN, và cách tính giá trị output cuối cùng (nhân trạng thái ẩn với vector V và qua hàm xuất). [1][2][4][7][8][10][11][12]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 Tổng quan về *context cell* (C) và mục đích của LSTM\n- LSTM giữ một trục xuyên suốt chuỗi gọi là *context cell* (ký hiệu C) để truyền thông tin qua các bước thời gian và quyết định gì cần giữ/quen. [1]\n\n### 2.2 Forget gate (cổng quên)\n- Vai trò: quyết định phần thông tin của quá khứ (C_{t-1}) sẽ bị quên bao nhiêu. [1][3]  \n- Cách hoạt động: forget gate dùng hàm *sigmoid*, xuất ra giá trị trong đoạn [0, 1]; nếu giá trị = 0 thì nhân với C_{t-1} tương đương *quên hoàn toàn*; nếu giá trị ≈ 1 thì giữ gần như toàn bộ thông tin quá khứ. [2][3]  \n- Quyết định của forget gate dựa trên thông tin của trạng thái ẩn trước đó và đầu vào hiện tại x_t. [3]\n\n### 2.3 Input gate và module tạo (candidate)\n- Vai trò của input gate: quyết định có nên nạp thông tin của đầu vào hiện tại x_t vào *context cell* hay không, và với mức độ bao nhiêu. [4]  \n- Cách hoạt động: input gate sử dụng *sigmoid* (giá trị trong [0,1]). Nếu output gần 0 → không nạp; nếu gần 1 → nạp nhiều. [4][5]  \n- Module tạo thông tin (candidate): tương tự một module ANN/tầng xử lý, rút trích thông tin cần thiết từ trạng thái/đầu vào hiện tại để làm phần thông tin mới đưa vào cell. Việc *rút trích* (tạo candidate) và *có đưa vào cell hay không* là hai bước khác nhau — đưa vào hay không phụ thuộc input gate. [7][8][12]  \n- Ví dụ minh họa: những từ không quan trọng (ví dụ mạo từ, giới từ như \"in\", \"on\"...) có thể bị input gate lọc bỏ, không nạp vào cell. [5]\n\n### 2.4 Cập nhật *context cell* (C_t)\n- Quy trình cập nhật: thông tin quá khứ (đã bị modulated bởi forget gate) và thông tin hiện tại (candidate, bị lọc bởi input gate) được **cộng** lại để tạo C_t mới. (Phép cộng ở đây là phép tổng hợp thông tin tại thời điểm hiện tại). [8][9]  \n- Công thức (diễn đạt theo nội dung video):  \n  C_t = f_t * C_{t-1} + i_t * g_t  \n  - f_t: giá trị từ forget gate (sigmoid) ∈ [0,1] (quyết định quên/giữ C_{t-1}). [2][3][8]  \n  - i_t: giá trị từ input gate (sigmoid) ∈ [0,1] (quyết định lượng thông tin hiện tại được nạp vào). [4][5][8]  \n  - g_t (candidate): thông tin được trích từ đầu vào hiện tại qua module tương tự ANN (dịch sang tiếng video: \"hàm chế\"/module trích xuất). [7][8]\n\n### 2.5 Output gate và trạng thái ẩn (S_t / h_t)\n- Vai trò của output gate: quyết định có lấy (xuất) thông tin từ *context cell* ra để tạo trạng thái ẩn tại thời điểm hiện tại hay không, và với mức độ bao nhiêu. [6]  \n- Cách tính trạng thái ẩn:  \n  S_t = O_t * tanh(C_t)  \n  (O_t là output gate, tanh biến đổi C_t trước khi cho ra S_t). [10][11]  \n- Nếu O_t ≈ 0 → khóa không cho CT đi ra; nếu O_t ≈ 1 → cho phép lấy thông tin nhiều. [6][10]\n\n### 2.6 Tính toán giá trị output/prediction\n- Sau khi có trạng thái ẩn S_t, phần dự đoán (output) được tính tương tự ANN: nhân trạng thái ẩn với vector V và đưa qua hàm (hàm xuất) để ra giá trị output. Video nhắc rằng thao tác này giống như ANN truyền thống: nhân với vector V rồi qua hàm số để tính output. [11][12]\n\n### 2.7 So sánh ngắn gọn với phiên bản ANN/RNN đơn giản\n- Phiên bản ANN/RNN truyền thống chỉ dùng một hàm tanh để tổng hợp thông tin hiện tại và quá khứ; trong khi LSTM mở rộng bằng cách thêm 3 cổng (forget, input, output) và một *context cell*. Những cổng này giúp điều hướng thông tin (khi nào nhớ, khi nào quên, khi nào xuất), từ đó làm cho việc truyền gradient hiệu quả hơn khi huấn luyện. [12][13]  \n- Nhờ ba cổng điều hướng thông tin, LSTM phần nào giúp giải quyết hiện tượng vanishing gradient (tiêu biến gradient). [13][14]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa trong video: Khi xử lý câu, một số từ (ví dụ mạo từ, giới từ như \"in\", \"on\"...) không quan trọng, input gate có xu hướng lọc bỏ chúng và không đưa vào *context cell*. [5]\n\n- Ứng dụng thực tế / trường hợp sử dụng (theo ngữ cảnh bài giảng): LSTM dùng cho dữ liệu tuần tự như chuỗi ký tự/chuỗi câu — tức là những bài toán cần lưu thông tin dài hạn qua nhiều bước thời gian; kiến trúc này giúp lưu giữ thông tin quan trọng và quên bớt thông tin không cần thiết trong chuỗi. [1][5]\n\n- Lợi ích huấn luyện: bằng cách điều hướng thông tin qua các cổng, LSTM làm cho gradient trong quá trình lan truyền ngược trở nên hiệu quả hơn, giúp giảm bớt vấn đề *vanishing gradient* so với mô hình đơn giản. [13][14]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính: LSTM bổ sung một *context cell* và ba cổng (forget, input, output) dùng hàm *sigmoid* để điều hướng thông tin (nhớ/quên/nạp/xuất). Cập nhật cell được thực hiện bằng cách nhân các phần thông tin với các giá trị cổng tương ứng rồi cộng lại; trạng thái ẩn được lấy bằng O_t * tanh(C_t); output cuối cùng tính bằng nhân trạng thái ẩn với vector V và qua hàm xuất. Những cơ chế này giúp LSTM giữ được thông tin dài hạn và giảm phần nào vấn đề vanishing gradient. [1][2][3][4][7][8][9][10][11][12][13][14]\n\n- Tầm quan trọng: Việc tách biệt rõ ràng giữa *rút trích thông tin* (module candidate) và *quyết định nạp/không nạp* (input gate), cùng với khả năng quên có kiểm soát (forget gate) và xuất có kiểm soát (output gate), là yếu tố then chốt khiến LSTM phù hợp cho các nhiệm vụ sequence modeling lâu phụ thuộc. [7][8][12][13]\n\n- Liên hệ với các bài giảng khác: Video so sánh sơ lược với phiên bản ANN/RNN (phiên bản đơn giản chỉ có tanh) và nhấn mạnh LSTM là một biến thể của RNN có thêm các cổng và *context cell* để điều hướng thông tin tốt hơn. [12][13]\n\n---\n\nGhi chú: các trích dẫn [1]–[14] tương ứng với các đoạn (chunk) trong video với timestamps như đã cung cấp; bạn có thể click vào từng citation để nhảy tới đoạn tương ứng trong video.",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 7__Km_A2iRUds",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_2： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/_Km_A2iRUds",
          "start_time": 0,
          "end_time": 58,
          "text": "Và để giải quyết vấn đề này thì chúng ta sẽ đi qua các cái từng cái thành phần của cái kiến trúc LSTM Đầu tiên đó là cái hàm FT Cái hàm FT này mục đích của nó chính là quyết định xem cái gì là cần nhớ Hay là cần giữ lại Cái gì thì cần quên Với những cái thông tin của quá khứ Thì ở đây chúng ta sẽ có một cái trục xuyên suốt toàn bộ cái Có một cái trục để đi xuyên suốt Cái chuỗi ký tự của mình Cái chuỗi câu của mình Đó là trục C C là viết tắt của chữ là contact Contact cell Và ở đây chúng ta sẽ thấy là có cái cổng đầu tiên Ở đây nó sẽ có cái cổng đầu tiên Thì cái cổng này Nó gọi là forget"
        },
        {
          "index": 2,
          "video_id": "Chương 7__Km_A2iRUds",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_2： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/_Km_A2iRUds",
          "start_time": 49,
          "end_time": 108,
          "text": "Và ở đây chúng ta sẽ thấy là có cái cổng đầu tiên Ở đây nó sẽ có cái cổng đầu tiên Thì cái cổng này Nó gọi là forget Forget gate Và chúng ta để ý là cái cổng forget gate này Thì có cái hàm Có sử dụng một cái hàm Là hàm sigmoid Thì trong cái hàm sigmoid Thì cái miền giá trị của nó là từ 0 cho đến 1 Thế thì với cái hàm sigmoid này Nó sẽ giúp cho chúng ta điều hướng thông tin Ví dụ nếu cái kết quả trả ra Cho cái FT này nè Tức là cái kết quả trả ra tại đây nè Kết quả trả ra tại đây Mà bằng 0 Nếu kết quả này mà bằng 0 Thì điều gì sẽ xảy ra Cái giá trị 0 này Nhân với lại cái contact cell Là CT triệu 1 Tức là nó đang thực hiện cái việc là Quên đi cái thông tin của quá khứ Trong cái thằng CT triệu 1 Nếu cái giá trị này là bằng 1"
        },
        {
          "index": 3,
          "video_id": "Chương 7__Km_A2iRUds",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_2： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/_Km_A2iRUds",
          "start_time": 98,
          "end_time": 159,
          "text": "Tức là nó đang thực hiện cái việc là Quên đi cái thông tin của quá khứ Trong cái thằng CT triệu 1 Nếu cái giá trị này là bằng 1 Thì Hoặc là giá trị gần bằng 1 Thì nó sẽ giữ lại Nó sẽ giữ lại gần như toàn bộ cái thông tin Của quá khứ Và nó truyền tới tiếp theo Thì đó chính là Cái ý đồ của cái forget gate Tức là nó sẽ biết Có nên nhớ hay là quên Cái thông tin của quá khứ hay không Thông qua cái việc sử dụng cái hàm sigmoid Và để đưa ra được cái quyết định Là có quên hay không Thì nó phải dựa vào cái thông tin Của cái trạng thái ẩn Trước đó là CT triệu 1 Và cái thông tin của Cái từ X hiện tại Của cái đầu vào hiện tại Là XT Mô đun thứ 2 Đó chính là Cái cổng thông tin input gate Nó gọi là input gate"
        },
        {
          "index": 4,
          "video_id": "Chương 7__Km_A2iRUds",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_2： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/_Km_A2iRUds",
          "start_time": 152,
          "end_time": 209,
          "text": "Mô đun thứ 2 Đó chính là Cái cổng thông tin input gate Nó gọi là input gate Rồi cái input gate này Nó sẽ quyết định xem Là cái thông tin XT này nè Chúng ta có đưa nó Vào bên trong Đưa nó vào bên trong Cái context Cell này hay không Ở đây chúng ta thấy có một cái mũi tên Tức là sau khi chúng ta tính cái này xong Chúng ta sẽ nhân với lại cái thông tin Đi qua cái cổng này Rồi sau đó chúng ta sẽ cộng nó vào Cái context cell Thì cái giá trị ở đây Chúng ta sẽ sử dụng một cái hàm sigmoid Và tương tự như vậy Thì cái sigmoid này nó sẽ nhận Cái giá trị là từ 0 cho đến 1 Nếu như cái hàm sigmoid này trả ra Cái giá trị mà gần bằng 0 Tức là Nó nói rằng là Chúng ta sẽ không có Không cần nạp cái thông tin của XT Vào bên trong cái cổng này"
        },
        {
          "index": 5,
          "video_id": "Chương 7__Km_A2iRUds",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_2： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/_Km_A2iRUds",
          "start_time": 199,
          "end_time": 260,
          "text": "Nếu như cái hàm sigmoid này trả ra Cái giá trị mà gần bằng 0 Tức là Nó nói rằng là Chúng ta sẽ không có Không cần nạp cái thông tin của XT Vào bên trong cái cổng này Thì thực tế chúng ta thấy là có những cái từ Mà không quá quan trọng Trong một cái câu Ví dụ như là những cái Trong tiếng Anh thì chúng ta sẽ có những cái mạo từ Phần Hoặc là những cái giới từ In, on, off, on, on Thì đó là những cái từ kém quan trọng Do đó thì cái cổng này nó sẽ có xu hướng là Lọc bỏ những cái thông tin không quan trọng Để không đưa vào bên trong cái context cell Thì cái IT này Chỉ là cái ký hiệu của cái chữ input Tiếp theo Đó là cái cổng output Cái cổng này thì chúng ta Chút nữa chúng ta sẽ nói sau Cái cổng output Ở đây là ký hiệu bằng chữ O Thì Nó sẽ quyết định xem Là chúng ta có lấy Chúng ta có lấy cái thông tin"
        },
        {
          "index": 6,
          "video_id": "Chương 7__Km_A2iRUds",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_2： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/_Km_A2iRUds",
          "start_time": 249,
          "end_time": 307,
          "text": "Ở đây là ký hiệu bằng chữ O Thì Nó sẽ quyết định xem Là chúng ta có lấy Chúng ta có lấy cái thông tin Từ cái CT này Chúng ta có lấy cái thông tin Từ cái CT này Từ Xin lỗi Đến đây Đến cái thời điểm này Thì nó đã tính ra cái CT rồi Tính ra cái CT rồi Thì chúng ta có lấy cái thông tin Của cái trục context cell Đi ra Để Thực hiện cái tính toán Cái giá trị output này không Thì output này sẽ là quyết định xem là có lấy hay không Nếu Qua cái hàm sigmoid này Mà nó nhận cái giá trị là 0 Hoặc là gồng bằng 0 Thì khi không nhân với giá trị này Tức là nó đang khóa Nó khóa cái thông tin này lại Không cho Cái thông tin từ cái CT này Đi ra Cái ST Còn nếu như Giá trị này nó sắp xỉ lọ Nó tiến về 1 Tức là nó sẽ cho phép lấy cái thông tin của CT đi ra Để tính toán Cho cái giá trị output"
        },
        {
          "index": 7,
          "video_id": "Chương 7__Km_A2iRUds",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_2： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/_Km_A2iRUds",
          "start_time": 298,
          "end_time": 360,
          "text": "Giá trị này nó sắp xỉ lọ Nó tiến về 1 Tức là nó sẽ cho phép lấy cái thông tin của CT đi ra Để tính toán Cho cái giá trị output Và C ngã T C ngã T Tức là cái thông tin Sau khi chúng ta đã Xào nấu Thông tin sau khi chúng ta đã xào nấu Giữa Cái quá khứ Là ST Và cái ST-1 Và cái thông tin đầu vào của mình Đó là cái hiện tại Chúng ta trộn 2 cái thông tin này lại với nhau Để tạo ra 1 cái thông tin tổng hợp Và cái thông tin tổng hợp này Thì nó cứ tính toán Nhưng mà cái việc Cái C ngã T này Có đưa vào bên trong cái trục Nó có đưa vào bên trong cái trục Cần tắt sale hay không Đó là phụ thuộc vào cái input gate này Cái việc là có đưa nó vào hay không Đó là do hàng này Còn cái hàm chế này là do hàng này này   Cái hàm chế này là do hàng này này Là nó sẽ tổng hợp thông tin của quá khứ và hiện tại Thì cái module này"
        },
        {
          "index": 8,
          "video_id": "Chương 7__Km_A2iRUds",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_2： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/_Km_A2iRUds",
          "start_time": 349,
          "end_time": 410,
          "text": "Đó là phụ thuộc vào cái input gate này Cái việc là có đưa nó vào hay không Đó là do hàng này Còn cái hàm chế này là do hàng này này   Cái hàm chế này là do hàng này này Là nó sẽ tổng hợp thông tin của quá khứ và hiện tại Thì cái module này Nó cũng tương tự như cái Nó cũng tương tự như cái ANN sale Nó cũng tương tự như cái ANN sale Phiên bản gọi là phiên bản đầu tiên của mình Thì nhiệm vụ của nó là để rút Trích thông tin cần thiết Của cái sale hiện tại Để đưa vào contact specter Nhưng mà lưu ý là rút trích thông tin cần thiết thôi Còn có đưa vào hay không Nó sẽ phụ thuộc vào cái input gate này Rồi và cuối cùng là Ở cái công thức này Công thức tính CT này Tức là nó sẽ được cập nhật tại đây Nó sẽ cập nhật CT mặc dù nó ghi là ở đây Nhưng mà chúng ta phải hiểu là cái gồm thông tin Là nó đã được thay đổi Tại cái vị trí này Thì ở đây là chúng ta dùng cái toán tử cộng Toán tử cộng Nghĩa là gì? Đây là cái thông tin tổng hợp Đây là cái thông tin tổng hợp Tại thời điểm hiện tại"
        },
        {
          "index": 9,
          "video_id": "Chương 7__Km_A2iRUds",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_2： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/_Km_A2iRUds",
          "start_time": 399,
          "end_time": 459,
          "text": "Là nó đã được thay đổi Tại cái vị trí này Thì ở đây là chúng ta dùng cái toán tử cộng Toán tử cộng Nghĩa là gì? Đây là cái thông tin tổng hợp Đây là cái thông tin tổng hợp Tại thời điểm hiện tại Còn đây là cái thông tin Tổng hợp Của quá khứ Nhưng mà lưu ý Đó là cái quá khứ này Nó có chứa thông tin nhiều hay không Thì nó nằm Ở cái phần quyết định là do cái forget gate Ví dụ đến đây forget gate Là bằng sắp xịn bằng không Tức là nó đã quên sạch thông tin rồi Như vậy đến đây thì cái lượng thông tin Đi tiếp nó gần như là không còn Còn hiện tại Cũng tương tự như vậy do cái cổng input Nó sẽ quyết định xem là Cái hàm lượng thông tin của cái CT Khi đưa vào Cái contact cell này Khi đưa vào cái contact cell này Là nhiều hay ít Đó là do cái cổng này Còn ở đây là cái sự tổng hợp thông tin Của quá khứ và hiện tại Cuối cùng"
        },
        {
          "index": 10,
          "video_id": "Chương 7__Km_A2iRUds",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_2： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/_Km_A2iRUds",
          "start_time": 449,
          "end_time": 511,
          "text": "Khi đưa vào cái contact cell này Là nhiều hay ít Đó là do cái cổng này Còn ở đây là cái sự tổng hợp thông tin Của quá khứ và hiện tại Cuối cùng Đó là ST ST thì Ở đây chúng ta sẽ là Hàm tanh của CT Ở đằng trước Thì Sau đó thì chúng ta sẽ tính cái ST thôi Thì công thức này nó cũng rất là đơn giản Nó sẽ là bằng OT Nhân hàm tanh của CT Rồi Thì ở đây chúng ta sẽ có một cái Một cái Nhầm lẫn trong công thức một chút xíu Ở đây là hàm tanh này Là hàm tanh của C ngã T Hàm tanh này là của C ngã T À xin lỗi Đây là hàm tanh của CT đúng rồi Rồi Như vậy thì Ở đây là cái thông tin CT nè"
        },
        {
          "index": 11,
          "video_id": "Chương 7__Km_A2iRUds",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_2： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/_Km_A2iRUds",
          "start_time": 501,
          "end_time": 560,
          "text": "À xin lỗi Đây là hàm tanh của CT đúng rồi Rồi Như vậy thì Ở đây là cái thông tin CT nè Nó truyền qua hàm tanh Truyền qua hàm tanh Và đến đây Thì CT ở đây là cái thông tin Contact cell Và cái việc mà quyết định xem có lấy cái thông tin Của CT này ra hay không Có lấy cái thông tin của CT không Thì nó sẽ phụ thuộc vào Cái giá trị OT là Đến từ cái cổng output Output gate này sẽ quyết định xem là Có lấy hay không Rồi Và sau khi chúng ta đã có được cái CT này rồi Thì chúng ta sẽ thực hiện cái việc dự đoán Và cái việc dự đoán này thì cũng tương tự Chúng ta sẽ thực hiện tương tự Như cái ANN Bình thường Tương tự như cái phiên bản ANN bình thường Đó là có cái trạng thái ẩn Chúng ta sẽ nhân với vector V"
        },
        {
          "index": 12,
          "video_id": "Chương 7__Km_A2iRUds",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_2： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/_Km_A2iRUds",
          "start_time": 550,
          "end_time": 610,
          "text": "Như cái ANN Bình thường Tương tự như cái phiên bản ANN bình thường Đó là có cái trạng thái ẩn Chúng ta sẽ nhân với vector V Để Nhân với lại cái vector V Để qua hàm số 3 Để qua hàm số 3 Để tính cái giá trị output Và như vậy thì Chúng ta thấy Với cái phiên bản của ANN và LSTM ANN Thì chúng ta chỉ có duy nhất Một cái cổng là tanh Là để tổng hợp thông tin Của ST Và đưa vào bên trong cái Tính toán cái giá trị ST tiếp theo Và nó tương ứng Chính là cái module này Về mặt ý nghĩa Đó là nó tương ứng với module này Nó tổng hợp thông tin Của trạng thái hiện tại Và quá khứ Của trạng thái hiện tại và quá khứ Tuy nhiên Trong cái phiên bản LSTM Nó có thêm 3 cái cổng khác Và cộng thêm một cái module nữa Đó là cái context Thì cái cổng này Sẽ giúp cho chúng ta"
        },
        {
          "index": 13,
          "video_id": "Chương 7__Km_A2iRUds",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_2： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/_Km_A2iRUds",
          "start_time": 598,
          "end_time": 657,
          "text": "Của trạng thái hiện tại và quá khứ Tuy nhiên Trong cái phiên bản LSTM Nó có thêm 3 cái cổng khác Và cộng thêm một cái module nữa Đó là cái context Thì cái cổng này Sẽ giúp cho chúng ta Có nên quên thông tin của quá khứ hay không Cổng này Sẽ giúp cho chúng ta xác định xem Có nên đưa cái thông tin Của trạng thái hiện tại Vào cái cổng CT Vào cái context cell hay không Và cái cổng này Thì sẽ giúp cho chúng ta Xác định xem có nên đưa cái thông tin Xác định xem là Cái lượng thông tin Mà chúng ta lấy ra từ Cái CT Tức là cái context cell này Là nhiều hay ít Thì nó có thêm 3 cái cổng này Và nhờ 3 cái cổng này Thì nó sẽ giúp cho chúng ta Điều hướng được cái thông tin Để từ đó Nó sẽ giúp cho cái gradient của mình Trong quá trình tính toán Nó sẽ được trở nên hiệu quả hơn Và đó chính là cái Việc mà LSTM Nó có thể giúp cho chúng ta Phần nào giải quyết được cái hiện tượng Là vanishing gradient Hiện tượng tiêu biến gradient Điều hướng được cái thông tin"
        },
        {
          "index": 14,
          "video_id": "Chương 7__Km_A2iRUds",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 1_2： Một số biến thể của RNN： LSTM",
          "video_url": "https://youtu.be/_Km_A2iRUds",
          "start_time": 650,
          "end_time": 657,
          "text": "Phần nào giải quyết được cái hiện tượng Là vanishing gradient Hiện tượng tiêu biến gradient Điều hướng được cái thông tin"
        }
      ]
    },
    {
      "video_id": "Chương 7__Cu7kGoRaE0",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- **Mục tiêu chính của bài giảng**: Giới thiệu và giải thích biến thể *Bidirectional RNN* (mạng RNN hai chiều), động cơ ra đời và cách nó cải thiện biểu diễn ngữ cảnh so với RNN một chiều, qua ví dụ bài toán Sentiment Analysis. [1][6]  \n- **Các khái niệm sẽ được đề cập**: mô tả trạng thái ẩn dưới dạng vector, cơ chế tổng hợp (element-wise mean / max), ý tưởng chạy song song hai chiều (left→right và right→left), phép nối (concatenate) hai trạng thái ẩn để tạo biểu diễn đầy đủ ngữ cảnh, và phạm vi áp dụng/không áp dụng của Bidirectional RNN. [2][3][8][10][11]  \n- **Nguồn minh họa**: Ví dụ câu “The movie was terribly exciting” với các trạng thái ẩn S1…S5 được dùng để minh họa vì từ “terribly” cần ngữ cảnh phía sau (“exciting”) để hiểu đúng ý. [1][4][5]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Biểu diễn trạng thái ẩn dưới dạng vector và tổng hợp (aggregation)\n- Thay vì vẽ dạng nút, trạng thái ẩn được biểu diễn thành các *vector* (S1…S5), thuận tiện cho việc hiểu và minh họa hoạt động của mô hình. [3]  \n- Các vector trạng thái có thể được *tổng hợp* (aggregation) bằng các phép trên từng phần tử như element-wise mean hoặc max để tạo ra vector đại diện tổng hợp dùng cho dự đoán (ví dụ: positive / negative / neutral). [2][3]\n\n### 2.2. Động cơ cho kiến trúc Bidirectional\n- Vấn đề: Trong câu “The movie was terribly exciting”, từ “terribly” khi chỉ nhìn về phía trước (quá khứ) dễ hiểu sai vì tính chất thực sự của nó phụ thuộc vào từ phía sau “exciting”. RNN chỉ chạy theo một hướng (thường trái→phải) nên tại thời điểm xử lý “terribly” sẽ không thấy ngữ cảnh phía sau. [4][5][6]  \n- Giải pháp: Cần tổng hợp ngữ cảnh từ cả hai phía (trái→phải và phải→trái) để có hiểu biết đầy đủ về từng token. Đó là ý tưởng cơ bản của *Bidirectional RNN*. [6]\n\n### 2.3. Kiến trúc và công thức cơ bản của Bidirectional RNN\n- Kiến trúc gồm hai RNN độc lập: một chạy *forward* (từ trái sang phải) và một chạy *backward* (từ phải sang trái). Kết quả là tại mỗi time-step t ta có hai trạng thái ẩn:\n  - *Forward hidden state*: \\overrightarrow{s_t} = f(\\overrightarrow{s_{t-1}}, x_t) — tổng hợp thông tin quá khứ theo chiều trái→phải. [9]  \n  - *Backward hidden state*: \\overleftarrow{s_t} = b(\\overleftarrow{s_{t+1}}, x_t) — tổng hợp thông tin “quá khứ” theo chiều phải→trái (tức là nhìn về phía phải trong chuỗi gốc). [10]\n- Tại mỗi time-step, hai vector này được *nối* (concatenate) để tạo vector trạng thái cuối cùng:\n  - s_t = concat( \\overrightarrow{s_t}, \\overleftarrow{s_t} ) — vector này chứa ngữ cảnh từ cả hai phía và được dùng cho dự đoán. [8][10]\n- Công thức/luồng tổng quát: tính \\overrightarrow{s_t} bằng RNN forward, tính \\overleftarrow{s_t} bằng RNN backward, rồi nối để có s_t; s_t dùng cho layer dự đoán cuối cùng. [9][10]\n\n### 2.4. Cách hoạt động minh hoạ (flow)\n- Khi thực hiện pass backward (phải→trái), token “exciting” được xử lý và thông tin tích cực của nó lan truyền ngược về token “terribly”, do đó tại vị trí “terribly” ta có biểu diễn đầy đủ hơn (kết hợp cả left và right contexts). [7][8]  \n- Quá trình này thực hiện cho mọi time-step, nên mỗi s_t sau khi concat có thông tin từ cả hai chiều. [7][8][9][10]\n\n### 2.5. Nhận xét về tính phù hợp và giới hạn\n- *Điểm mạnh*: Bidirectional phù hợp với các bài toán mà toàn bộ input sequence có thể được truy cập trước khi đưa ra dự đoán (tức là cho phép “đọc hết” X1..Xt trước khi dự đoán). [11]  \n- *Hạn chế*: Không phù hợp cho các bài toán language modeling thuần túy kiểu dự đoán từ tiếp theo khi chỉ có quyền nhìn về phía trước (left→right), vì đó là mô hình yêu cầu tính tuần tự dự đoán tương lai từ quá khứ. [11][12]  \n- Mô hình BERT là một ứng dụng nổi bật của ý tưởng *bidirectional encoder* (Bidirectional Encoder Representations), thể hiện sức mạnh của biểu diễn hai chiều trong nhiều nhiệm vụ NLP. [12]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n### Ví dụ minh họa từ video\n- Ví dụ chi tiết: câu “The movie was terribly exciting”. Các trạng thái ẩn S1…S5 được tính theo từng token; nếu chỉ dùng RNN một chiều thì tại token “terribly” (S4) ta chỉ có thông tin từ S1..S3, thiếu ngữ cảnh từ phía sau (“exciting”), dẫn tới hiểu nhầm. Bidirectional RNN cho phép token “exciting” (được tính trong pass backward) truyền thông tin về “terribly”, giúp s_terribly chứa đủ ngữ cảnh hai chiều. [1][4][5][6][7][8]  \n- Minh họa xử lý vector: các vector trạng thái được biểu diễn bằng màu sắc trên slide, sau khi concat tạo vector “màu xanh động” mang thông tin đầy đủ hơn phục vụ dự đoán. [3][7][8]\n\n### Ứng dụng thực tế / Trường hợp sử dụng\n- Các bài toán phù hợp: dịch máy (machine translation), tóm tắt văn bản (text summarization), các bài toán phân loại/nhận diện trên chuỗi khi toàn bộ input có thể truy cập trước (cho phép đọc hết input trước khi dự đoán). [11][13]  \n- Ví dụ mô hình: BERT (Bidirectional Encoder Representations for Transformers) là một mô hình nổi bật tận dụng biểu diễn hai chiều và được sử dụng rộng rãi; ở Việt Nam có đề cập đến một mô hình tương tự tên chơi chữ là “phở bệt” dựa trên ý tưởng bidirectional. [12][13]  \n- Lưu ý: với các bài toán phải dự đoán tuần tự theo thời gian (language modeling dự đoán từ kế tiếp), bidirectional không phù hợp. [11][12]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính: Bidirectional RNN xử lý sequence bằng cách chạy hai RNN theo hai chiều và nối hai trạng thái ẩn để tạo biểu diễn chứa ngữ cảnh từ cả bên trái và bên phải, từ đó cải thiện khả năng hiểu nghĩa của các token phụ thuộc ngữ cảnh hai phía (ví dụ “terribly exciting”). [6][8][9][10]  \n- Tầm quan trọng: Kiến trúc này rất hữu ích cho nhiều nhiệm vụ NLP khi toàn bộ input có thể truy cập trước, và là nền tảng ý tưởng cho các mô hình hiện đại như BERT. [11][12]  \n- Liên hệ với các bài giảng khác: Bài giảng tiếp nối từ phần giới thiệu RNN/ANN và trạng thái ẩn (ANEN/ANN) đã học trước đó, mở rộng bằng cách biểu diễn trạng thái dưới dạng vector và giới thiệu kiến trúc hai chiều; cũng liên quan tới phần giới thiệu mô hình Transformer/BERT được đề cập như một ứng dụng của ý tưởng bidirectional. [1][3][12]\n\n(Trích dẫn các đoạn theo thứ tự trong video: [1] … [13] như đưa ra trong nguồn.)",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 7__Cu7kGoRaE0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 2： Một số biến thể của RNN： Bidirectional RNN",
          "video_url": "https://youtu.be/_Cu7kGoRaE0",
          "start_time": 1,
          "end_time": 60,
          "text": "Trong phần tiếp theo thì chúng ta sẽ cùng tìm hiểu về một trong những cái biến thể rất là mạnh và hiệu quả Đó chính là Bidirectional Anand hay còn gọi là Anand 2 chiều Thì chúng ta sẽ xem xét cái bài toán đơn giản trước đó là bài toán Sentiment Analysis Và ở trong cái mô hình Anand mà chúng ta đã được tìm hiểu trước đây Thì chúng ta sẽ ký hiệu bằng cái hệ thống các cái nốt như thế này Mỗi cái nốt này nó sẽ có các cái thao tác xử lý Và chúng ta lấy ví dụ như chúng ta đưa vào một cái câu comment, một cái câu bình luận về một cái bộ phim Là The movie was terribly exciting Thì cứ khi đưa vô một cái từ chúng ta sẽ tính toán các cái giá trị ẩn Ví dụ như ở đây là S1, S2, S3, cho đến S5 Và các cái giá trị ẩn là các cái bài toán đơn giản Và các cái bài toán này nó sẽ được tổng hợp thông tin lại"
        },
        {
          "index": 2,
          "video_id": "Chương 7__Cu7kGoRaE0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 2： Một số biến thể của RNN： Bidirectional RNN",
          "video_url": "https://youtu.be/_Cu7kGoRaE0",
          "start_time": 48,
          "end_time": 110,
          "text": "Ví dụ như ở đây là S1, S2, S3, cho đến S5 Và các cái giá trị ẩn là các cái bài toán đơn giản Và các cái bài toán này nó sẽ được tổng hợp thông tin lại Tổng hợp thông tin lại để tính ra cái nốt ở trên cùng Và đây là cái nốt output Và cái việc tổng hợp thông tin này thì sẽ được thực hiện là element-wise, min hoặc là max Chúng ta sẽ tính toán trên cấp độ đó là từng phần tử giữa các cái giá trị S1, S2, S3 Nó sẽ tính trên từng cái phần tử của cái vector này Và sau khi tổng hợp xong thì chúng ta sẽ đưa ra cái dự đoán đó là positive hay là negative hay là neutral Thì ở đây chúng ta sẽ biểu diễn nó dưới dạng một cái dạng thứ hai Đó là dạng vector Thì khi đưa về cái dạng biểu diễn vector của ANEN thì nó sẽ giúp cho chúng ta hình dung được"
        },
        {
          "index": 3,
          "video_id": "Chương 7__Cu7kGoRaE0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 2： Một số biến thể của RNN： Bidirectional RNN",
          "video_url": "https://youtu.be/_Cu7kGoRaE0",
          "start_time": 97,
          "end_time": 164,
          "text": "Thì ở đây chúng ta sẽ biểu diễn nó dưới dạng một cái dạng thứ hai Đó là dạng vector Thì khi đưa về cái dạng biểu diễn vector của ANEN thì nó sẽ giúp cho chúng ta hình dung được là cái giá trị của cái trạng thái ẩn nó sẽ là một cái vector Nó sẽ là một cái vector Thì nó sẽ mang tính chất đại diện nhiều hơn Và dựa trên màu sắc của các cái vector này ở trong những slide sau nó sẽ giúp cho chúng ta thuận tiện trong cái việc hiểu cái cách thức hoạt động của mô hình hơn thay vì là chúng ta dùng cái node như thế này Như vậy thì ở đây là ở dạng biểu diễn vector và tất cả các cái vector mà ở trạng thái ẩn S1 cho đến S5 ở đây Chúng ta sẽ được thực hiện thao tác trung bình hoặc là thao tác max trên từng cái phần tử giữa các cái vector này từng cái phần tử để ra một cái vector tổng hợp Và vector này thì chúng ta sẽ thấy là nó có màu xanh động Và do nó đã tổng hợp thông tin của các cái trạng thái ẩn trước đó Bây giờ chúng ta sẽ qua cái động cơ tại sao lại có cái kiến trúc mạng bidirectional ANN"
        },
        {
          "index": 4,
          "video_id": "Chương 7__Cu7kGoRaE0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 2： Một số biến thể của RNN： Bidirectional RNN",
          "video_url": "https://youtu.be/_Cu7kGoRaE0",
          "start_time": 149,
          "end_time": 215,
          "text": "Và vector này thì chúng ta sẽ thấy là nó có màu xanh động Và do nó đã tổng hợp thông tin của các cái trạng thái ẩn trước đó Bây giờ chúng ta sẽ qua cái động cơ tại sao lại có cái kiến trúc mạng bidirectional ANN Thì chúng ta sẽ để ý rằng là ở cái từ terribly ở đây thì từ terribly này khi kết hợp với những cái thông tin ngữ cảnh tại thông tin bất kể của ANN thì chúng ta có thể được nhận ra những cái biểu diễn bằng bất kì hình thức hoạt động của mô hình trong đây từ thời điểm trước đó đó là từ World từ Movie và từ đờ thì cái cái từ này nó không mang tính chất thể hiện cái trạng thái cảm xúc các cái từ đờ movie World thì nó đều là những cái từ trung tính đó nhưng mà khi chúng ta mắc gặp đến cái từ terribly thì cái từ này nếu mà nguyên bản của nó là cái từ terrible thì đây chính là một cái từ thể hiện cái tính chất tiêu cực nó thể hiện cái tính chất tiêu cực rồi và nhưng rõ ràng là cái từ ngay phía sau đó là cái từ exciting"
        },
        {
          "index": 5,
          "video_id": "Chương 7__Cu7kGoRaE0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 2： Một số biến thể của RNN： Bidirectional RNN",
          "video_url": "https://youtu.be/_Cu7kGoRaE0",
          "start_time": 199,
          "end_time": 267,
          "text": "nó là cái từ terrible thì đây chính là một cái từ thể hiện cái tính chất tiêu cực nó thể hiện cái tính chất tiêu cực rồi và nhưng rõ ràng là cái từ ngay phía sau đó là cái từ exciting cái từ exciting này mang cái tính chất là tích cực thì khi đó cái từ terribly này chỉ mang tính chất là thể hiện cái mức độ cái cái cái mức độ của cái việc exciting thôi là cực kỳ tiêu cực gọi là thú vị cực kỳ là tích cực như vậy thì khi chúng ta xử lý đến cái từ thứ S4 chúng ta chỉ nhận được các thông tin của quá khứ tức là những cái từ S1 S2 S3 truyền đến cho S4 mà chúng ta không được thấy cái từ ở phía sau đó chính là cái từ exciting và phải nhờ có cái từ exciting này thì nó mới giúp cho chúng ta hoàn thiện cái ý nghĩa của cái từ terribly này hơn và chúng ta cũng không thể nhận được các cái từ exciting này thì nó mới giúp cho chúng ta hoàn thiện cái ý nghĩa của cái từ terribly này hơn do đó chúng ta cần phải có cái ngữ cảnh này của các cái từ bên tay phải nữa chứ không phải là chỉ có những cái từ bên tay trái"
        },
        {
          "index": 6,
          "video_id": "Chương 7__Cu7kGoRaE0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 2： Một số biến thể của RNN： Bidirectional RNN",
          "video_url": "https://youtu.be/_Cu7kGoRaE0",
          "start_time": 246,
          "end_time": 308,
          "text": "mà chúng ta không được thấy cái từ ở phía sau đó chính là cái từ exciting và phải nhờ có cái từ exciting này thì nó mới giúp cho chúng ta hoàn thiện cái ý nghĩa của cái từ terribly này hơn và chúng ta cũng không thể nhận được các cái từ exciting này thì nó mới giúp cho chúng ta hoàn thiện cái ý nghĩa của cái từ terribly này hơn do đó chúng ta cần phải có cái ngữ cảnh này của các cái từ bên tay phải nữa chứ không phải là chỉ có những cái từ bên tay trái thì đó chính là cái động cơ của cái bidirectional anem như vậy thì chúng ta sẽ phải có một cái kiến trúc như thế nào đó để có thể duyệt được các cái câu của mình theo chiều ngược lại nữa thì như vậy nó mới hoàn thiện và đó chính là ý tưởng của biến thể bidirectional anem ý tưởng của đó nó có thể nói tóm gặp lại đó chính là tổng hợp thông tin ngữ cảnh từ cả hai phía và ở đây thì chúng ta sẽ thấy là cái màu sắc của chúng ta là màu xanh đúng không là tương ứng với lại các cái vector trạng thái ẩn ở theo cái chiều là từ trái sang phải"
        },
        {
          "index": 7,
          "video_id": "Chương 7__Cu7kGoRaE0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 2： Một số biến thể của RNN： Bidirectional RNN",
          "video_url": "https://youtu.be/_Cu7kGoRaE0",
          "start_time": 296,
          "end_time": 362,
          "text": "và ở đây thì chúng ta sẽ thấy là cái màu sắc của chúng ta là màu xanh đúng không là tương ứng với lại các cái vector trạng thái ẩn ở theo cái chiều là từ trái sang phải sau đó khi đến được cái từ cuối cùng thì chúng ta có thể thực hiện cái thao tác ngược lại và biểu diễn bằng cái vector và từ exciting này sẽ được duyệt ở đây sau đó sẽ lan truyền tổng hợp thông tin với cái từ terribly thì tại đây khi cái từ exciting này mang cái nghĩa tích cực kết hợp với cái từ terribly thì nó sẽ tạo ra cho chúng ta cái nghĩa tích cực rồi và cứ như vậy chúng ta lan truyền đến đầu và tại cái vị trí số 1 tại cái time step t là bằng 1 nè thì chúng ta thấy cái vector màu xanh nó đã được kết nối nó là concate concate tên này tức là nối với lại cái vector ẩn mà đã tổng hợp"
        },
        {
          "index": 8,
          "video_id": "Chương 7__Cu7kGoRaE0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 2： Một số biến thể của RNN： Bidirectional RNN",
          "video_url": "https://youtu.be/_Cu7kGoRaE0",
          "start_time": 350,
          "end_time": 412,
          "text": "thì chúng ta thấy cái vector màu xanh nó đã được kết nối nó là concate concate tên này tức là nối với lại cái vector ẩn mà đã tổng hợp theo cái chiều từ phải sang trái và như vậy thì cái vector tại đây nó đã có đầy đủ thông tin hơn đầy đủ thông tin ngữ cảnh từ phía bên tay phải và phía tay trái truyền về rồi và cứ như vậy chúng ta sẽ thực hiện cho tất cả những cái vector cho những cái time step còn lại và nếu xét về cái công thức biểu diễn cho cái kiến trúc mạng bidirectional này thì chúng ta sẽ có cái công thức sau ở đây thì chúng ta có một cái lưu ý đó là với cái vector này là tại cái vị trí của cái terribly này thì thông tin ngữ cảnh của cái từ terribly đã được tổng hợp toàn diện hơn từ cả hai phía thì đầu tiên đó là tại một cái time step t chúng ta sẽ có cái s mà với một cái mũi tên"
        },
        {
          "index": 9,
          "video_id": "Chương 7__Cu7kGoRaE0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 2： Một số biến thể của RNN： Bidirectional RNN",
          "video_url": "https://youtu.be/_Cu7kGoRaE0",
          "start_time": 399,
          "end_time": 461,
          "text": "của cái terribly này thì thông tin ngữ cảnh của cái từ terribly đã được tổng hợp toàn diện hơn từ cả hai phía thì đầu tiên đó là tại một cái time step t chúng ta sẽ có cái s mà với một cái mũi tên là từ trái sang phải thì chúng ta sẽ có cái công thức là I need feed forward của cái thông tin của quá khứ và lưu ý là cái thông tin của quá khứ này thì cũng lấy theo cái chiều là từ trái sang phải sau đó chúng ta kết hợp với thông tin của hiện tại thì nó sẽ ra được cái trạng thái ẩn của cái nó sẽ ra được cái vạn trạng thái ẩn tương tự như vậy thì chúng ta sẽ có cái vector trạng thái ẩn theo cái chiều từ phải sang trái là st này thì công thức cho nó sẽ là I need backward backward tức là đi theo cái chiều ngược thì chúng ta sẽ tổng hợp thông tin của cái s"
        },
        {
          "index": 10,
          "video_id": "Chương 7__Cu7kGoRaE0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 2： Một số biến thể của RNN： Bidirectional RNN",
          "video_url": "https://youtu.be/_Cu7kGoRaE0",
          "start_time": 446,
          "end_time": 511,
          "text": "trạng thái ẩn theo cái chiều từ phải sang trái là st này thì công thức cho nó sẽ là I need backward backward tức là đi theo cái chiều ngược thì chúng ta sẽ tổng hợp thông tin của cái s t trừ 1 và st trừ 1 này là cái dấu mũi tên từ phải sang trái và tương tự như đây cũng là quá khứ nhưng mà lưu ý là quá khứ cho cái đường backward kết hợp với thông tin hiện tại thì chúng ta sẽ có được cái st theo cái chiều backward và cuối cùng đó là chúng ta sẽ tổng hợp thông tin st bằng cách đó là chúng ta thực hiện cái phép con cát nối thì cái dấu chấm phải này ở đây đó chính là phép nối và chúng ta sẽ tổng hợp thông tin st bằng cách đó là chúng ta thực hiện cái phép con cát nối  nối 2 cái vector st theo chiều forward và st theo chiều backward để tạo thành một cái vector trạng thái ẩn và dựa trên cái thông tin của cái trạng thái ẩn này st này nó sẽ giúp cho chúng ta đưa ra cái giá trị dự đoán một cách thông tin có chứa đầy đủ toàn diện hơn từ hai phía"
        },
        {
          "index": 11,
          "video_id": "Chương 7__Cu7kGoRaE0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 2： Một số biến thể của RNN： Bidirectional RNN",
          "video_url": "https://youtu.be/_Cu7kGoRaE0",
          "start_time": 498,
          "end_time": 561,
          "text": "để tạo thành một cái vector trạng thái ẩn và dựa trên cái thông tin của cái trạng thái ẩn này st này nó sẽ giúp cho chúng ta đưa ra cái giá trị dự đoán một cách thông tin có chứa đầy đủ toàn diện hơn từ hai phía rồi và như vậy thì chúng ta sẽ có một số cái nhận xét sau thứ nhất đó là bidirectional thì nó sẽ phù hợp đối với những cái bài toán và chúng ta có khả năng tiếp cận được thông tin tiếp cận được toàn bộ cái nội dung của dữ liệu đầu vào tức là sao nếu như chúng ta có được cái input và chúng ta có thể đọc được hết nội dung của toàn bộ chúng ta đọc được hết nội dung toàn bộ của cái x1 x2 cho đến xt chúng ta đọc được hết này và mới đưa ra được cái dự đoán thì đó là những cái bài toán mà cho phép có khả năng sử dụng được bidirectional và nếu như vậy thì rõ ràng cái language model là nó không phù hợp tại vì"
        },
        {
          "index": 12,
          "video_id": "Chương 7__Cu7kGoRaE0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 2： Một số biến thể của RNN： Bidirectional RNN",
          "video_url": "https://youtu.be/_Cu7kGoRaE0",
          "start_time": 550,
          "end_time": 610,
          "text": "thì đó là những cái bài toán mà cho phép có khả năng sử dụng được bidirectional và nếu như vậy thì rõ ràng cái language model là nó không phù hợp tại vì language model là chỉ cho phép là nhìn từ trái sang phải đúng không và dự đoán cái từ tiếp theo thôi do đó thì chúng ta không có phù hợp sử dụng bidirectional cho cái gọi là sử dụng cái bidirectional rn cho cái language model và và BERT mô hình BERT là bidirectional encoder representation for transformer thì đây là một trong những cái mô hình mà một cái biến thể của bidirectional nó thể hiện ở trong cái chữ này và vô cùng hiệu quả và rất là được sử dụng rất là phổ biến hiện nay thì hiện nay ở Việt Nam chúng ta có một cái mô hình đó là phở bệt là cũng dựa trên cái cái cái mô hình à của bidirectional này"
        },
        {
          "index": 13,
          "video_id": "Chương 7__Cu7kGoRaE0",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 2： Một số biến thể của RNN： Bidirectional RNN",
          "video_url": "https://youtu.be/_Cu7kGoRaE0",
          "start_time": 599,
          "end_time": 646,
          "text": "thì hiện nay ở Việt Nam chúng ta có một cái mô hình đó là phở bệt là cũng dựa trên cái cái cái mô hình à của bidirectional này và từ nay về sau thì chúng ta sẽ có cái mẹo đó là bất cứ cái bài toán nào mà chúng ta được phép khả năng tiếp cận toàn bộ nội dung của dữ liệu đầu vào được phép tiếp cận toàn bộ nội dung dữ liệu đầu vào ví dụ bài toán dịch máy bài toán tấm tắt văn bản thì chúng ta được phép đọc hết cái nội dung của cái đầu vào này của mình trước khi dịch trước khi tấm tắt thì đó là những cái bài toán chúng ta có thể sử dụng bidirectional được và bidirectional thì thông thường luôn chỉ có thể làm cho cái kết quả của cái mô hình của mình nó chỉ có thể là tốt hơn mà thôi"
        }
      ]
    },
    {
      "video_id": "Chương 7_KjPEqyGCtUs",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giải thích biến thể **Deep Stacked RNN (DeepStack RNN / Multi-layer RNN)** — tại sao cần nó, cấu trúc và cách hoạt động, các biến thể kết hợp (ví dụ với Bidirectional) và mẹo thực hành khi thiết kế mô hình RNN nhiều tầng. [1][2][10][20]\n\n- Các khái niệm sẽ được đề cập:\n  - Sự khác biệt giữa *độ sâu theo chiều thời gian* (horizontal/depth across time) và *độ sâu theo chiều dọc* (vertical/depth across layers) trong RNN. [1][2]\n  - Ý tưởng của *DeepStack / Multi-Layer RNN* — trồng các tầng RNN để có feature ở nhiều mức (low → mid → high). [2][3][5]\n  - Công thức tổng quát cho trạng thái ẩn theo tầng và theo thời điểm. [6][7][8][9]\n  - Kết hợp **DeepStack** với **Bidirectional RNN** và lợi ích của sự kết hợp này. [10][11][21]\n  - Các mẹo thực hành: khi dùng Bidirectional, số tầng hợp lý (encoder/decoder), và *skip connections* (ResNet-style) để giảm vanishing gradient. [12][13][14][15][16][17][18][19]\n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Vấn đề: RNN “chỉ sâu” theo chiều thời gian\n- Một RNN truyền thống chủ yếu sâu theo **chiều thời gian** (những phép tính lặp lại khi chuỗi dài), nhưng với mỗi đặc trưng tại một thời điểm tính toán thì vẫn chỉ được biến đổi ở một cấp độ (cấp thấp). [1][2]\n\n### 2.2. Ý tưởng của Deep Stacked RNN (độ sâu theo chiều dọc)\n- *DeepStack / Multi-Layer RNN* là cách \"trồng\" nhiều tầng RNN chồng lên nhau để biến đổi đặc trưng ở nhiều mức: từ *low-level* → *mid-level* → *high-level*, tương tự như nhiều lớp trong CNN. [2][3][5]\n- Minh hoạ bằng ví dụ ngôn ngữ (rút gọn): với input embedding của từ \"Movie\", nếu chỉ 1 layer thì s_t chỉ chứa đặc trưng cấp thấp; thêm tầng thứ 2, tầng thứ 3 thì ta tổng hợp các mid / high-level features. [4][5]\n\n### 2.3. Luồng thông tin và công thức tổng quát cho DeepStack RNN\n- Thứ tự tính toán: tính tầng 1 trước (layer 1), rồi truyền kết quả lên tầng 2, rồi lên tầng 3... — tức là xử lý theo chiều dọc cho mỗi thời điểm t. [6]\n- Công thức dạng tổng quát (ký hiệu theo video):\n  - Với tầng 1 tại thời điểm t: S1_t = f_layer1(x_t, S1_{t-1}) — (tổng hợp input hiện tại và trạng thái quá khứ ở cùng tầng). [6][7]\n  - Tầng 2 tại thời điểm t: S2_t = f_layer2(S1_t, S2_{t-1}) — (tổng hợp thông tin từ tầng dưới S1_t và quá khứ cùng tầng S2_{t-1}). [7][8]\n  - Tương tự cho tầng 3: S3_t = f_layer3(S2_t, S3_{t-1}). [8][9]\n- Ý chính: mỗi S_{l,t} phụ thuộc vào đầu vào từ tầng l-1 tại cùng thời điểm (t) và trạng thái quá khứ của tầng l (t-1). [7][8][9]\n\n### 2.4. Kết hợp DeepStack với Bidirectional\n- **Bidirectional RNN**: xử lý chuỗi theo hai chiều (forward từ trái→phải và backward từ phải→trái) để thu thập ngữ cảnh đầy đủ tại mỗi thời điểm. [10][20]\n- **DeepStack + Bidirectional**: kết hợp độ sâu theo tầng với xử lý hai chiều sẽ cho kiến trúc mạnh hơn — mỗi tầng có thể thu thập thông tin hai chiều và học features ở nhiều cấp độ; sơ đồ minh hoạ có nét liền cho forward và nét đứt cho backward. [10][11][21]\n- Lưu ý: Bidirectional không luôn có thể dùng (ví dụ *language modeling* không dùng được vì không được phép dùng thông tin tương lai). Do đó, khuyến nghị là *nên sử dụng Bidirectional khi có thể*, không phải luôn luôn. [12]\n\n### 2.5. Ưu/nhược điểm và các trade-offs khi tăng số tầng\n- Lợi ích: DeepStack giúp mô hình học đặc trưng ở nhiều mức (tương tự CNN), thường cải thiện hiệu năng nếu dữ liệu đủ lớn. [2][3][13]\n- Hạn chế:\n  - Tăng số tầng → tăng chi phí tính toán và thời gian (do các tầng thực hiện tuần tự cho mỗi time-step). [15][16]\n  - Tăng nguy cơ *vanishing gradient* và *overfitting* nếu không có biện pháp khắc phục hoặc khi dữ liệu ít. [15][16]\n- Kinh nghiệm từ video:\n  - Với encoder (trong kiến trúc encoder-decoder) thì tăng từ 2 lên 3-4 tầng có cải thiện nhưng hiệu suất gia tăng giảm dần; cần trade-off giữa hiệu năng và chi phí. [14][15]\n  - Với decoder, theo kinh nghiệm tổng hợp, 4 lớp thường cho kết quả tốt (nhưng phụ thuộc dữ liệu và bài toán). [16]\n\n### 2.6. Skip connections (ResNet-style) để giảm vanishing gradient\n- Giải pháp: áp dụng *skip connections* (residual connections) như trong ResNet để giảm hiện tượng vanishing gradient và cho phép tăng độ sâu lên (ví dụ có thể lên ~8 lớp trong ngữ cảnh video). [17][18]\n- Công thức residual (ví dụ trong video): y = F(x) + x, trong đó F(x) là hàm biến đổi (layer) và x là input (skip). Đây là trick để giữ gradient truyền tốt hơn. [18][19]\n\n### 2.7. Tổng hợp các biến thể RNN đã học (nhắc lại)\n- Video nhắc lại các biến thể kinh điển đã học:\n  - **LSTM**: dùng các cổng (forget, input, output) và cell state để *\"ghi nhớ cái cần nhớ, quên cái cần quên\"*, giúp giảm vanishing gradient. [19]\n  - **Bidirectional RNN**: tổng hợp thông tin hai chiều cho ngữ cảnh đầy đủ. [10][20]\n  - **DeepStack RNN**: tăng độ sâu theo tầng để học features nhiều cấp. [20][21]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh hoạ ngắn: câu \"The movie was terribly exciting\" được dùng để minh họa input embedding và cách một layer duy nhất chỉ thu được đặc trưng cấp thấp cho từ \"movie\"; thêm tầng thì cho ra mid/high-level features. [4][5]\n\n- Ứng dụng / trường hợp sử dụng nêu trong video:\n  - **Encoder-Decoder (Many-to-Many dạng 2)**: kiến trúc encoder đọc toàn bộ input, decoder sinh output; trong encoder-decoder, việc sử dụng nhiều tầng (encoder 2–4 tầng) có thể cải thiện, nhưng tối ưu còn phụ thuộc dữ liệu và bài toán. [14]\n  - **Language Modeling**: lưu ý không dùng Bidirectional vì không được phép tham chiếu thông tin tương lai. (Do đó chỉ *nên* dùng Bidirectional khi bài toán cho phép). [12][14]\n  - Các bài toán chuỗi khác (dịch máy, tagging, v.v.) có thể hưởng lợi khi kết hợp DeepStack và Bidirectional để vừa có đặc trưng nhiều cấp vừa có ngữ cảnh hai chiều. [10][11][21]\n\n- Mẹo thực hành tóm tắt:\n  - M1: *Nên* dùng Bidirectional khi có thể (không dùng với LM). [12]\n  - M2: DeepStack thường cải thiện kết quả giống như CNN (học features nhiều lớp), nhưng cần cân nhắc số tầng, dữ liệu, chi phí. [13]\n  - Với Decoder, kinh nghiệm cho thấy ~4 lớp thường là hợp lý (tùy dữ liệu). [16]\n  - Sử dụng skip connections để cho phép tăng chiều sâu mà không gặp quá nhiều vấn đề vanishing gradient (ví dụ có thể lên ~8 lớp với skip). [17][18][19]\n\n---\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - RNN truyền thống chủ yếu sâu theo chiều thời gian; để có *độ sâu theo chiều dọc* (multiple representation levels) ta dùng **Deep Stacked RNN / Multi-Layer RNN**. [1][2][5]\n  - Cấu trúc DeepStack: mỗi tầng l tại thời điểm t kết hợp thông tin từ tầng l-1 tại thời điểm t và trạng thái quá khứ S_{l,t-1} (công thức tổng quát S1_t, S2_t, S3_t như đã trình bày). [6][7][8][9]\n  - Kết hợp với **Bidirectional RNN** cho ngữ cảnh hai chiều sẽ càng làm kiến trúc mạnh hơn khi bài toán cho phép. [10][11][20][21]\n  - Khi tăng số tầng cần cân bằng lợi ích (feature levels, hiệu năng) với chi phí tính toán và nguy cơ vanishing/overfitting; *skip connections* là một giải pháp hiệu quả để khắc phục vanishing gradient. [15][17][18][19]\n\n- Tầm quan trọng:\n  - DeepStack RNN là kỹ thuật cơ bản để nâng cao khả năng biểu diễn của mô hình chuỗi, cho phép học đặc trưng có tính ngữ nghĩa cao hơn ở các tầng sâu hơn — tương tự lợi ích của nhiều tầng trong CNN. [2][3][13]\n\n- Liên hệ với các bài giảng khác (được nhắc trong video):\n  - Liên kết chặt với các biến thể đã học trước đó: **LSTM** (giải quyết vanishing bằng cơ chế cell/gates) và **Bidirectional RNN** (ngữ cảnh hai chiều); DeepStack có thể kết hợp với cả hai để thu được lợi ích cộng hưởng. [19][10][20][21]\n\n---\n\nGhi chú: Tóm tắt trên dựa hoàn toàn trên nội dung các đoạn trích đã cho và giữ nguyên quan điểm, mẹo kinh nghiệm như trong video. Các citation [1]...[21] tương ứng trực tiếp với các chunk/timestamp được cung cấp.",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 1,
          "end_time": 62,
          "text": "Trong phần tiếp theo thì chúng ta sẽ cùng tìm hiểu về biến thể DeepStack ANEN Thì đây là một trong những cái biến thể mà giúp cho cái mạng học sâu của mình được hoàn thiện hơn Cụ thể đó là đối với cái kiến trúc ANEN truyền thống Thì mô hình của mình thật sự là nó đã sâu hay chưa Thì mô hình này nó mới chỉ sâu Nó mới chỉ sâu theo cái chiều thời gian thôi Nghĩa là theo chiều ngang Khi cái văn bản của mình rất là dài Có thể lên đến hàng chục, hàng trăm, thậm chí là hàng ngàn chữ Thì rõ ràng là cái số thao tác xử lý này sẽ được lập đi lập lại lập đi lập lại Và nó sẽ tiến tới sâu theo chiều ngang Nhưng với mỗi một cái đặc trưng Với mỗi một cái đặc trưng tại một cái thời điểm tính toán Thì nó đã thật sự sâu hay chưa Thì câu trả lời là chưa Và nó đang thiếu Nó đang thiếu một cái sự độ sâu theo cái chiều dọc"
        },
        {
          "index": 2,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 49,
          "end_time": 111,
          "text": "Với mỗi một cái đặc trưng tại một cái thời điểm tính toán Thì nó đã thật sự sâu hay chưa Thì câu trả lời là chưa Và nó đang thiếu Nó đang thiếu một cái sự độ sâu theo cái chiều dọc Nó mới chỉ sâu theo chiều ngang thôi Còn sâu theo chiều dọc là chưa có Do đó thì ta có thể làm cho mô hình sâu hơn theo chiều dọc Và cái chiều này là được hiểu theo chiều của từng đặc trưng Với mỗi cái đặc trưng ST Thì đây là một đặc trưng cấp thấp Chúng ta sẽ làm cho nó nâng lên thành một cái đặc trưng cấp trung Mid level Sau đó chúng ta lại nâng lên thành một cái đặc trưng cấp cao hơn Thì đó là sâu theo chiều dọc Và nó sẽ cho phép mô hình của mình Nó biểu diễn được các cái đặc trưng ở nhiều cái cấp độ Nó sẽ biểu diễn được đặc trưng ở nhiều cấp độ Và giống như trong mạng CNN Chúng ta thấy Ở trong mạng CNN Thì ở những cái layer đầu tiên Những cái layer đầu tiên Thì cái feature map của mình"
        },
        {
          "index": 3,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 99,
          "end_time": 160,
          "text": "Nó sẽ biểu diễn được đặc trưng ở nhiều cấp độ Và giống như trong mạng CNN Chúng ta thấy Ở trong mạng CNN Thì ở những cái layer đầu tiên Những cái layer đầu tiên Thì cái feature map của mình Là những cái đặc trưng cấp thấp Sau đó chúng ta biến đổi Thành các cái feature map Càng về lớp cuối Thì chúng ta thấy là cái feature map của mình Cái tính đặc trưng Cái tính ngữ nghĩa của nó càng lúc càng cao Thì ở đây sẽ là Hai level feature Trong khi đó Đối với cái mạng ANN Thì truyền thống Thì chúng ta thấy là Với cái đặc trưng đầu vào XT Chúng ta chỉ mới thực hiện biến đổi Trên một cấp độ thôi Trên một tầng thôi Thì cái đặc trưng này Nó vẫn còn mang tính chất Nó là cấp thấp Nó sẽ không thể nào giúp cho chúng ta Giải quyết được các cái bài toán"
        },
        {
          "index": 4,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 149,
          "end_time": 210,
          "text": "Thì cái đặc trưng này Nó vẫn còn mang tính chất Nó là cấp thấp Nó sẽ không thể nào giúp cho chúng ta Giải quyết được các cái bài toán Phức tạp hơn Khó hơn Và như vậy thì Chúng ta sẽ có một cái phiên bản Đó chính là DeepStack ANN Cái từ stack này có nghĩa là trồng Thì chắc stack này có nghĩa là trồng Và một cái tên gọi khác Đó là Multi Layer ANN Tức là Layer có Là mạng ANN có nhiều tầng Rồi Thì đây là cái sơ đồ Cho cái mạng ANN Mà với một layer Với một layer Chúng ta sử dụng lại cái ví dụ là Cũ là The Movie was terribly exciting Và lưu ý là Đây là chúng ta đang làm gọn Chứ hàm ý Cái đầu vào ở đây Nó phải là cái emitting của cái từ Movie Và nếu như chỉ có một layer Nó phải là cái emitting của cái từ Movie Thì cái ST Cái ST tại đây"
        },
        {
          "index": 5,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 199,
          "end_time": 260,
          "text": "Đây là chúng ta đang làm gọn Chứ hàm ý Cái đầu vào ở đây Nó phải là cái emitting của cái từ Movie Và nếu như chỉ có một layer Nó phải là cái emitting của cái từ Movie Thì cái ST Cái ST tại đây Là chỉ chứa được cái đặc trưng cấp thấp Của cái từ này Do đó chúng ta cần phải tổng hợp thêm Các cái thông tin của mid level Và high level feature Muốn vậy thì chúng ta cần phải Trồng các cái layer biến đổi Thì mỗi cái đường màu cam này nè Đó chính là một cái layer Hay gọi là một cái layer biến đổi Và chúng ta sẽ trồng lên Cái layer thứ 2 Rồi sau đó trồng lên layer số 3 Như vậy Và cái đường đi di chuyển của thông tin Nó sẽ được biểu diễn bởi các cái vector Bằng cái mũi tên Thể hiện cái hướng đi của cái dữ liệu của mình Rồi Sau đó thì trạng thái ẩn Ở layer thứ Y Là ST"
        },
        {
          "index": 6,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 250,
          "end_time": 310,
          "text": "Thể hiện cái hướng đi của cái dữ liệu của mình Rồi Sau đó thì trạng thái ẩn Ở layer thứ Y Là ST Là đầu vào cho cái layer Ở layer thứ Y cộng 1 Như vậy là cái kết quả của layer thứ Y Sẽ là đầu vào cho cái layer thứ Y cộng 1 Chính là ST cộng 1 Và cái cách mà chúng ta Luôn chuyển cái thông tin giữa các layer đó là Chúng ta sẽ tính toán trên layer số 1 trước Rồi sau đó chúng ta truyền thông tin lên Cho cái layer số 2 Rồi sau đó chúng ta truyền thông tin lên Cho cái layer số 3 Thì đây là cái animation Để minh họa cho cái cách thức mà chúng ta Chuyển dữ liệu từ giữa các cái tầng Các cái layer này    Để chúng ta có thể tính toán với nhau Rồi Và để cụ thể hơn dưới dạng công thức Thì chúng ta sẽ có Các cái công thức như sau Đầu tiên đó là ST1 Tức là"
        },
        {
          "index": 7,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 301,
          "end_time": 359,
          "text": "Rồi Và để cụ thể hơn dưới dạng công thức Thì chúng ta sẽ có Các cái công thức như sau Đầu tiên đó là ST1 Tức là Số 1 ở đây ám chỉ đó là layer số 1 Ở đây sẽ là layer số 2 Đây là layer số 3 Và đây chính là cái layer số 1 Tại cái thời điểm thứ T Tại thời điểm thứ T Là ST mà thứ 1 Layer số 1 Nó sẽ được tính thông tin Nó sẽ được tổng hợp thông tin Từ cái ST Tức là cái này ST cộng với lại thông tin của quá khứ Nhưng ở cùng tầng Đây là số 1 đúng không Thì đây là cùng tầng Thông tin của quá khứ Ở cùng tầng tức là S1 Nhưng mà T chữ 1 Rồi sau đó lên cái tầng thứ 2 Lên cái tầng thứ 2 Thì ST2"
        },
        {
          "index": 8,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 349,
          "end_time": 410,
          "text": "S1 Nhưng mà T chữ 1 Rồi sau đó lên cái tầng thứ 2 Lên cái tầng thứ 2 Thì ST2 Nó sẽ được tổng hợp thông tin Từ cái tầng trước đó Nếu như trước đây cái S1T Nó tổng hợp thông tin từ ST Thì ở đây Cái S2T Nó sẽ được tổng hợp thông tin từ S1T Tức là từ cái tầng Thấp hơn chuyển lên Thì đây chính là cái thông tin Của cái tầng thấp hơn chuyển lên Đây chính là thông tin Của Cái tầng Hoặc là cái layer Trước đó Rồi và nó sẽ Không quen là tổng hợp thông tin Với cái quá khứ Của cái tầng Tức là cái thông tin quá khứ Ở trên cái tầng hiện đại"
        },
        {
          "index": 9,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 400,
          "end_time": 461,
          "text": "Rồi và nó sẽ Không quen là tổng hợp thông tin Với cái quá khứ Của cái tầng Tức là cái thông tin quá khứ Ở trên cái tầng hiện đại Chính là S2T chữ 1 S2T chữ 1 Thì đây chính là cái thông tin Quá khứ Thông tin quá khứ Nhưng mà mở ngoặt cùng tầng Và tương tự như vậy Cho cái S3 Tương tự như vậy cho cái S3T Chúng ta cũng sẽ tổng hợp thông tin từ S2T Kết hợp với lại cái thông tin quá khứ Cùng tầng Đó là S3T chữ 1 Thì đây chính là Một loại hình dạng công thức biến đổi Của DeepStack ANN Và Cũng không thể nào Quên không nhắc đến Cái phi biến thể Có cái sự kết hợp của DeepStack Và Bidirectional"
        },
        {
          "index": 10,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 449,
          "end_time": 510,
          "text": "Và Cũng không thể nào Quên không nhắc đến Cái phi biến thể Có cái sự kết hợp của DeepStack Và Bidirectional Bidirectional nhắc lại Đó chính là một cái biến thể giúp cho chúng ta Tổng hợp được thông tin Ngẫu cảnh Theo chiều từ trái sang phải Và từ phải sang trái Đó sẽ giúp cho chúng ta hoàn thiện hơn Thông tin về mặt ngũ cảnh Còn DeepStack Là nó sẽ giúp cho mình Cho các đặc trưng tại từng tầng Nó sẽ học được các cấp Của đặc trưng Từ cấp thấp cho đến cấp giữa Cho đến cấp cao Như vậy 2 cái DeepStack và Bidirectional Nó thực hiện 2 cái nhiệm vụ độc lập nhau Và nếu như chúng ta bổ trợ cho nhau Thì rõ ràng là kiến trúc mạng của mình Nó sẽ càng hoàn thiện hơn Và hoàn hảo hơn Thì DeepStack Bidirectional Nếu mà vẽ gọn lại Thì chúng ta sẽ dùng cái sơ đồ này"
        },
        {
          "index": 11,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 498,
          "end_time": 559,
          "text": "Thì rõ ràng là kiến trúc mạng của mình Nó sẽ càng hoàn thiện hơn Và hoàn hảo hơn Thì DeepStack Bidirectional Nếu mà vẽ gọn lại Thì chúng ta sẽ dùng cái sơ đồ này Ở đây chúng ta sẽ thấy Có những cái nét liền Chính là Cho cái chiều thuận Forward Forward Pass Còn cái nét đứt Là để thể hiện cho các cái đường Theo chiều Chiều Backward Rồi Và ở đây thì chúng ta sẽ tổng hợp thông tin Cho 1 tầng Và với cái tầng này Thì chúng ta lại đẩy lên tiếp Chúng ta sẽ tắt thêm Chúng ta sẽ start thêm 1 cái tầng mới Rồi chúng ta lại trồng lên 1 cái tầng mới Đó thì cái ví dụ này Là chúng ta đang minh họa cho cái DeepStack Bidirectional Là vừa có cái sự kết hợp Của Sử lý theo 2 chiều"
        },
        {
          "index": 12,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 549,
          "end_time": 610,
          "text": "Đó thì cái ví dụ này Là chúng ta đang minh họa cho cái DeepStack Bidirectional Là vừa có cái sự kết hợp Của Sử lý theo 2 chiều Sử lý thông tin ngữ cảnh 2 chiều Mà vừa có cái sự trồng ra Vừa có cái sự trồng ra Vừa có cái sự trồng ra Rồi Cuối cùng Đó chính là cái mẹo thực hành Cho cái bài học ngày hôm nay Đầu tiên đó là chúng ta Cái mẹo số 1 M1 là nên sử dụng Bidirectional Khi có thể Tại sao chúng ta không dùng từ là luôn luôn Mà dùng từ là có thể Tại vì có 1 số bài toán Ví dụ như Language Model Thì chúng ta không được phép Thông tin của những cái từ phía sau Do đó Language Model Là không có sử dụng Bidirectional được Nên ở đây chúng ta chỉ nói là Nên sử dụng Bidirectional ANN Khi có thể thôi Và thứ 2 Đó là Mẹo thứ 2 M2"
        },
        {
          "index": 13,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 599,
          "end_time": 658,
          "text": "Nên ở đây chúng ta chỉ nói là Nên sử dụng Bidirectional ANN Khi có thể thôi Và thứ 2 Đó là Mẹo thứ 2 M2 Là DeepStack ANN Thì cho cái kết quả tốt hơn Cho cái kết quả tốt hơn Đó cũng tương tự như cái mạng CNN Nó sẽ giúp cho cái kiến trúc của mình Mình có thể học được các cái đặc trưng Theo nhiều lớp khác nhau Theo nhiều cái mức độ khác nhau Từ cấp thấp lên cấp cao Và ở đây thì chúng ta sẽ có thêm 1 số Cái kinh nghiệm khác Đó là đối với cái quá trình Encoder Trong cái mạng ANN Thì chúng ta biết rồi nó sẽ có 1 số cái biến thể là Encode Và Decode Encoder và Decode Encoder là sẽ giúp cho chúng ta đọc hết toàn bộ Cái nội dung đầu vào Đọc hết cái input Và Decoder Là giúp cho chúng ta Tạo sinh ra kết quả"
        },
        {
          "index": 14,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 649,
          "end_time": 709,
          "text": "Cái nội dung đầu vào Đọc hết cái input Và Decoder Là giúp cho chúng ta Tạo sinh ra kết quả Thì ví dụ như cái biến thể Many to many dạng 2 Là 1 cái ví dụ như vậy Many to many dạng 2 Chính là 1 cái kiểu là Encode Decode Thì Encode Encoder mà từ 2 cho đến 4 lớp Thì cái lớp thứ 2 Cái lớp thứ 2 hay cái tầng thứ 2 Nó giúp cho chúng ta cải thiện nhiều Nhưng mà theo kinh nghiệm Của những cái người đi trước Thì đến cái lớp thứ 3 thứ 4 Thì cái sự hiệu quả của nó nó ít hơn Tức là nó có hiệu quả hơn Nhưng mà nó hiệu quả ít Như vậy thì ở đây chúng ta cần phải có cái sự Đánh đổi Đó là nếu như chúng ta thêm Cái tầng thứ 3 thứ 4 Thì điều gì sẽ ra Nó sẽ phát sinh thêm điều gì"
        },
        {
          "index": 15,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 698,
          "end_time": 761,
          "text": "Như vậy thì ở đây chúng ta cần phải có cái sự Đánh đổi Đó là nếu như chúng ta thêm Cái tầng thứ 3 thứ 4 Thì điều gì sẽ ra Nó sẽ phát sinh thêm điều gì Ok nó sẽ phát sinh thêm cái Chi phí tính toán Đó là điều chắc chắn Và đồng thời Nó sẽ làm cho cái mô hình của mình phức tạp hơn Như vậy nó sẽ có thể phát sinh là Hiện tượng Vanishing Nó sẽ làm phát sinh thêm cái hiện tượng Vanishing Radian Như vậy thì chúng ta phải cân đối Đó là Ok nếu như chúng ta quá cần Cái yếu tố về mặt đuổi chúng ta Chúng ta phải đúng kiến sát Và chúng ta không quan tâm lắm Về yếu tố chi phí tính toán Hoặc thời gian tính toán Thì chúng ta có thể thêm cái lớp số 3 số 4 Nhưng mà Do là cái tầng thứ 3 và thứ 4 này Nó được thực hiện một cách tuần tự Nó cũng không thể giúp chúng ta Thực hiện song song được Nên chi phí tính toán và thời gian nó sẽ lâu"
        },
        {
          "index": 16,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 750,
          "end_time": 807,
          "text": "Nhưng mà Do là cái tầng thứ 3 và thứ 4 này Nó được thực hiện một cách tuần tự Nó cũng không thể giúp chúng ta Thực hiện song song được Nên chi phí tính toán và thời gian nó sẽ lâu Đối với Decoder Thì theo kinh nghiệm đó là 4 lớp là cho cái kết quả tốt nhất Nhưng mà lưu ý Đây là những cái kinh nghiệm cá nhân Của những cái bài báo khoa học Họ tổng hợp Còn thực tế nó cũng rất phụ thuộc vào Khối lượng dữ liệu nó phụ thuộc vào cái bài toán của mình Nếu như dữ liệu của mình ít Thì có khi càng thêm lớp nó lại càng tệ hơn Tại vì nó phát sinh thêm Cái trọng số Hoặc là phát sinh thêm cái chi phí tính toán Làm cho phức tạp mô hình hơn Dẫn đến hiện tượng vanishing gradient Hoặc là overfitting Do đó thì thêm không chắc là tốt nhưng mà đối với trường hợp này Mà dữ liệu của mình đủ nhiều Và bài toán đủ đơn giản Thì chúng ta hoàn toàn có thể áp dụng Là đối với cái decoder Thì chúng ta sẽ có 4 lớp"
        },
        {
          "index": 17,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 800,
          "end_time": 861,
          "text": "Và bài toán đủ đơn giản Thì chúng ta hoàn toàn có thể áp dụng Là đối với cái decoder Thì chúng ta sẽ có 4 lớp Rồi và một trong những cái mẹo cuối Nhưng mà nó không có được nhắc đến Trong cái môn này Trong cái bài này Đó chính là skip connection Thì các bạn Quay lại cái bài về CNN Và cụ thể Đó là cái biến thể Cụ thể là cái biến thể ResNet Thì chúng ta thấy là Cái skip connection Nó sẽ giúp cho chúng ta Giải quyết được cái hiện tượng Là vanishing gradient Nó sẽ giúp cho chúng ta giải quyết được hiện tượng vanishing gradient    Của cái bài này Nó sẽ giúp cho chúng ta giải quyết được hiện tượng vanishing gradient Và điều đó Đã giúp cho chúng ta có thể Tăng cái độ sâu của bài này Tăng cái độ sâu của cái mạng của mình lên"
        },
        {
          "index": 18,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 849,
          "end_time": 911,
          "text": "Nó sẽ giúp cho chúng ta giải quyết được hiện tượng vanishing gradient Và điều đó Đã giúp cho chúng ta có thể Tăng cái độ sâu của bài này Tăng cái độ sâu của cái mạng của mình lên Có thể lên đến là 8 lớp Có thể lên đến là 8 lớp Thì như hồi nãy chúng ta nói Nếu như bình thường Chúng ta không có phải chịu cái sự ảnh hưởng Chúng ta không có phải chịu cái sự ảnh hưởng Của vấn đề về chi phí tính toán Thì chúng ta có thể thêm 3 đến 4 lớp Nhưng mà khi thêm vô mà không có một cái cơ chế nào khác Nhưng mà khi thêm vô mà không có một cái cơ chế nào khác Thì nó sẽ rất dễ xảy ra cái hiện tượng là vanishing gradient Và để khắc chế được cái chuyện này Khắc chế được cái vấn đề về vanishing gradient Thì chúng ta sẽ sử dụng các cái skip connection Thì chúng ta sẽ sử dụng các cái skip connection Đó sẽ giúp cho chúng ta giải quyết được cái vấn đề về vanishing Đó sẽ giúp cho chúng ta giải quyết được cái vấn đề về vanishing Và cái công thức của cái biến thể ResNet Đó chính là FX Là bằng cái hàm biến đổi của mình Là bằng cái hàm biến đổi của mình Ví dụ như là INS Mình để là INS Cộng cho X Thì đây chính là cái trick"
        },
        {
          "index": 19,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 899,
          "end_time": 959,
          "text": "Là bằng cái hàm biến đổi của mình Là bằng cái hàm biến đổi của mình Ví dụ như là INS Mình để là INS Cộng cho X Thì đây chính là cái trick Thì đây chính là cái trick Một cái mẹo để giúp cho chúng ta giải quyết Với vấn đề về vanishing gradient Với vấn đề về vanishing gradient Như vậy thì trong cái bài học ngày hôm nay Như vậy thì trong cái bài học ngày hôm nay Thì chúng ta đã lần lượt đi qua các cái module Các cái biến thể của ANEN Và các biến thể này là những cái biến thể kinh điển Đó là LSTM Cơ chế của LSTM Đó là nhớ cái cần nhớ Và quên cái cần quên Thông qua các cái cổng là FORGET Cổng INPUT Cổng OUTPUT Cổng OUTPUT Và đồng thời chúng ta sẽ có những cái bài học Đồng thời nó sẽ kết hợp với một cái contact Một cái contact cell Để lưu truyền Thông tin từ quá khứ cho đến hiện tại Thì đây chính là cái ý tưởng của LSTM LSTM sẽ giúp cho mình Giải quyết được cái vấn đề về vanishing gradient"
        },
        {
          "index": 20,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 950,
          "end_time": 1010,
          "text": "Để lưu truyền Thông tin từ quá khứ cho đến hiện tại Thì đây chính là cái ý tưởng của LSTM LSTM sẽ giúp cho mình Giải quyết được cái vấn đề về vanishing gradient Do cái sự điều phối thông tin Dẫn đến là cái gradient của mình nó sẽ được tính toán Một cách hiệu quả Biến thể thứ hai Đó chính là Bidirectional Bidirectional ANEN Bidirectional ANEN Bidirectional ANEN nó sẽ giúp cho chúng ta tổng hợp với bài học ngày hôm nay Bidirectional ANEN nó sẽ giúp cho chúng ta tổng hợp với bài học ngày hôm nay Thông tin từ hai chiều Theo chiều từ forward Từ trái sang phải Và theo chiều từ phải qua trái Thì sẽ giúp cho chúng ta có được cái thông tin đầy đủ Và toàn diện hơn Và cuối cùng đó chính là biến thể Deep Stuck Deep Stuck ANEN Deep Stuck ANEN Thì nó sẽ giúp cho chúng ta Tăng cái độ sâu của mô hình Thay vì là chúng ta đi theo chiều ngang Thì sẽ giúp cho chúng ta tăng theo chiều ngang chiều sâu và giúp cho các cái đặc trưng có thể học được từ cấp"
        },
        {
          "index": 21,
          "video_id": "Chương 7_KjPEqyGCtUs",
          "chapter": "Chương 7",
          "video_title": "[CS431 - Chương 8] Part 3： Một số biến thể của RNN： Deep Stacked RNN",
          "video_url": "https://youtu.be/KjPEqyGCtUs",
          "start_time": 998,
          "end_time": 1020,
          "text": "Tăng cái độ sâu của mô hình Thay vì là chúng ta đi theo chiều ngang Thì sẽ giúp cho chúng ta tăng theo chiều ngang chiều sâu và giúp cho các cái đặc trưng có thể học được từ cấp thấp, cấp giữa và trở nên cấp cao và đương nhiên là kết hợp bidirectional dip stack thì chúng ta sẽ có là dip stack bidirectional anem, đây là một biến thể phổ hợp"
        }
      ]
    },
    {
      "video_id": "Chương 8_0DGe4fjr1aw",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Hướng dẫn cài đặt mạng Recurrent Neural Network (RNN) và biến thể Long Short-Term Memory (LSTM) với Keras, kèm theo xử lý dữ liệu chuỗi (tokenization, padding, embedding) cho bài toán phân loại cảm xúc trên dataset IMDB. [1][2][3]  \n- Các khái niệm sẽ được đề cập: các lớp Keras (Embedding, Dense, SimpleRNN / RNN cell, LSTM cell), đối tượng Input/Model, tiện ích Sequence (padding/truncation), dataset IMDB (binary sentiment), cách chuẩn bị dữ liệu (vocab size, token indices, max length), sử dụng embedding (trainable vs. pre-trained) và thao tác inspect dữ liệu / tạo model / load model / predict. [1][2][3][14][16]\n\n## 2. Các điểm chính (Main Points)\n\n### A. Thư viện và các thành phần Keras chính\n- Sử dụng Keras với các module Layer: Embedding (lớp nhúng từ), Dense (fully connected), RNN/SimpleRNN (cell xử lý hidden state & input → output), và LSTM cell cho biến thể LSTM. [1]  \n- Các đối tượng hỗ trợ: Input (quy định kích thước dữ liệu đầu vào), Model (đóng gói Input và Output), Sequence (tiện ích xử lý chuỗi như padding), cùng các hàm load model / summary / predict tương tự trong mô hình CNN trước đó. [2][16]\n\n### B. Dataset IMDB và token hóa\n- Dataset: IMDB chứa các review phim với nhãn nhị phân (positive / negative). [3]  \n- Khi load dataset có thể chỉ định vocabulary size để giới hạn số từ dùng cho thí nghiệm (ví dụ giới hạn xuống 5.000 từ để giảm chi phí tính toán). [3][4]  \n- Các review được mã hóa thành chuỗi chỉ số (token indices): mỗi từ được thay bằng chỉ số vị trí của nó trong từ điển (ví dụ \"this\"→10, \"movie\"→20, \"is\"→23, \"exciting\"→90). Dữ liệu lưu dưới dạng chỉ số, không lưu text thô. [7]\n\n### C. Quy định vocab size và token lạ\n- Khi giới hạn vocab (ví dụ 5.000), những từ ngoài vocab sẽ được ánh xạ vào một chỉ số đặc biệt (ví dụ 0) để đánh dấu token không nằm trong từ điển. [4]\n\n### D. Chuẩn hóa chiều dài chuỗi: max length, truncation, padding\n- Quy ước max review length trong ví dụ: 500 token (max_review_length = 500). Những review dài hơn sẽ bị cắt ngắn — theo video là **lấy những từ cuối cùng** (truncate to last tokens). [6][8]  \n- Nếu review ngắn hơn 500, thực hiện padding bằng cách chèn các số 0. Trong ví dụ này, padding được chèn ở phía trước (pre-padding — tức hàng loạt 0 ở đầu chuỗi, phần cuối chứa các token thực). Ví dụ sau padding: 0,0,...,0,10,20,23,90. [8][13][14]  \n- Lý do chọn pre-padding (padding ở đầu) theo bài giảng: đưa các giá trị 0 lên trước để khi mạng xử lý theo thứ tự, các token thực (phần cuối chuỗi) sẽ được tính toán ở bước sau cùng và không bị “loãng” do các 0 nằm ở phía sau; (giảng viên trình bày lập luận về ảnh hưởng của vị trí padding đến việc lan truyền thông tin). [9][10][14]  \n\n### E. Cấu trúc dữ liệu trả về khi load IMDB\n- Khi load dataset IMDB sẽ trả về (X_train, y_train) và (X_test, y_test). X_train chứa các chuỗi chỉ số token, y_train chứa nhãn (0 = negative, 1 = positive). Kích thước mẫu trong ví dụ: 25.000 mẫu train và 25.000 mẫu test (tức tỷ lệ train/test 50/50). [5][11][12]\n\n### F. Embedding layer — trainable vs. pre-trained\n- Embedding layer có thể được xem như một biến đổi tuyến tính ánh xạ chỉ số token sang vector nhúng. Embedding có thể là *trainable* (được huấn luyện cùng mô hình) hoặc *static* (dùng trọng số tải sẵn). [14]  \n- Trong ví dụ, tác giả có dùng embedding pre-trained (tải mô hình embedding có sẵn — tác giả đề cập tới việc tải một mô hình \"work to back / FastTech\" (ý chỉ các embedding như word2vec / FastText dưới dạng file đã tải sẵn) và giải nén trước khi dùng). [15]  \n- Thời gian load/prepare embedding có thể đáng kể (tác giả ước lượng một số bước có thể tốn vài phút đến ~16 phút tùy trường hợp). [15]\n\n### G. Xây dựng và thao tác với model RNN/LSTM\n- Sau khi chuẩn bị dữ liệu và embedding, tiến hành xây dựng model (kết hợp Input → Embedding → RNN/LSTM → Dense → Output). Tác giả đề cập tới thao tác tạo model, lưu/load architecture và sử dụng các phương thức như load_model / summary / predict giống cách làm với CNN đã học trước đó. [14][16]\n\n### H. Quan sát mẫu dữ liệu (inspect)\n- Ví dụ quan sát một mẫu: in ra X_train[i] sẽ thấy một dãy các chỉ số token (sau padding sẽ có nhiều 0 ở đầu), và y_train[i] là 0 hoặc 1. Video chỉ ra thao tác quan sát 10 mẫu đầu và một mẫu thứ 100 cụ thể. [11][12]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa preprocessing:\n  - Token hóa câu \"this movie is exciting\" → [10, 20, 23, 90] (chỉ số là vị trí từ trong từ điển). Sau pre-padding về độ dài 500 → [0,0,...,0,10,20,23,90]. [7][8][13]  \n  - Nếu chuỗi dài > 500 → cắt ngắn, giữ 500 token cuối cùng; nếu < 500 → chèn 0 ở đầu để đạt đúng 500. [6][8][13]\n\n- Ví dụ quan sát dataset:\n  - Kích thước train = 25.000, test = 25.000; in ra vài mẫu thấy y ∈ {0,1} tương ứng negative/positive. [11][12]\n\n- Ứng dụng thực tế:\n  - Bài toán sentiment analysis (phân loại cảm xúc của review phim) sử dụng RNN/LSTM + embedding là ví dụ điển hình. [3]  \n  - Embedding pre-trained (như word2vec / FastText — được nhắc trong video dưới dạng mô hình embedding đã tải) có thể giúp tận dụng kiến thức ngôn ngữ đã học sẵn, giảm thời gian huấn luyện hoặc cải thiện chất lượng biểu diễn từ. [15][14]\n\n- Trường hợp sử dụng khác:\n  - Bất kỳ bài toán xử lý chuỗi token (text classification, sequence labeling...) đều cần các bước tương tự: giới hạn vocab, ánh xạ token → chỉ số, quy định max length, padding/truncation, dùng embedding (trainable hoặc pre-trained), và chọn cell RNN hay LSTM tùy yêu cầu dài hạn/short-term dependencies. (Ý này được gợi ý qua việc triển khai RNN/LSTM và cách chuẩn bị dữ liệu). [1][2][6][14]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Bài giảng hướng dẫn từ bước khai báo module Keras, chuẩn bị dataset IMDB (giới hạn vocab = 5.000 trong ví dụ), token hóa thành chỉ số, quy định max_review_length = 500 với truncation (giữ phần cuối) và pre-padding (chèn 0 ở đầu), tới việc sử dụng Embedding (trainable hoặc tải embedding có sẵn) và xây dựng model RNN / LSTM. [1][2][3][4][6][8][13][14][15]  \n  - Tác giả cũng minh họa kiểm tra dữ liệu (in mẫu), nhắc đến thời gian tải embedding pre-trained và nhấn mạnh các phương thức quản lý model (load, summary, predict) tương tự như với CNN. [11][12][15][16]\n\n- Tầm quan trọng:\n  - Quy trình chuẩn hóa dữ liệu chuỗi (vocab limit, token index, truncation, padding), lựa chọn embedding và cell (RNN vs LSTM) là các bước thiết yếu để triển khai các mô hình xử lý ngôn ngữ tự nhiên cơ bản như sentiment analysis. [3][6][14]  \n\n- Liên hệ với các bài giảng khác:\n  - Các thao tác tạo/kiểm tra/restore model (load_model, summary, predict) được thực hiện tương tự như trong các bài học triển khai CNN trước đó, do đó kiến thức về quản lý model là có tính tái sử dụng giữa các chương. [16]\n\nGhi chú: Nội dung trên tóm tắt trực tiếp từ các đoạn trong video theo thứ tự thời gian; các ví dụ, tham số (vocab = 5000, max_review_length = 500, train/test = 25.000) và minh họa token indices (10,20,23,90) nêu đúng theo transcript gốc. [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16]",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 8_0DGe4fjr1aw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_1： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/0DGe4fjr1aw",
          "start_time": 13,
          "end_time": 61,
          "text": "Trong phần này thì chúng ta sẽ cùng tiến hành cài đặt mạng Recurrent Neural Network và một cái biến thể của nó đó chính là Long Short Term Memory LSTM thì ở phần đầu tiên chúng ta sẽ tiến hành khai báo các thư viện mà chúng ta có sử dụng trong bài hướng dẫn này Đầu tiên đó là thư viện Keras với các module như là Layer thì trong Layer thì nó sẽ có chứa các module như là Embedding Layer rồi Dense tức là các lớp fully connected hoặc là RNN Simple tức là cái module RNN với lại cái Cell của mình là gồm các thao tác tính toán trên Hidden State và Input State như là Output State"
        },
        {
          "index": 2,
          "video_id": "Chương 8_0DGe4fjr1aw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_1： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/0DGe4fjr1aw",
          "start_time": 47,
          "end_time": 111,
          "text": "hoặc là RNN Simple tức là cái module RNN với lại cái Cell của mình là gồm các thao tác tính toán trên Hidden State và Input State như là Output State và đồng thời trong Layer này thì cũng có LSTM Cell thì trong cái lớp Layer này thì nó sẽ có một cái module có một cái lớp đối tượng đặc biệt đó chính là Input Input sẽ quy định cho chúng ta biết là cái dữ liệu đầu vào của mình nó sẽ có kích thước là bao nhiêu và có thể có các cái hành module vụ trợ khác như là LoadModuleLens LoadModule đã được huấn luyện trước đó Model giúp cho chúng ta đóng gói cái Input và cái Output vào một cái đối tượng tên là Model rồi chuỗi Sequence Sequence thì giúp cho chúng ta có thể chuyển đổi từ có thể làm một số cái thao tác trên các cái chuỗi ví dụ như là Padding tức là cho một cái đoạn chuỗi ngắn dài khác nhau chúng ta sẽ đưa về cùng một cái chuỗi có cùng một cái kích thước"
        },
        {
          "index": 3,
          "video_id": "Chương 8_0DGe4fjr1aw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_1： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/0DGe4fjr1aw",
          "start_time": 98,
          "end_time": 160,
          "text": "Sequence thì giúp cho chúng ta có thể chuyển đổi từ có thể làm một số cái thao tác trên các cái chuỗi ví dụ như là Padding tức là cho một cái đoạn chuỗi ngắn dài khác nhau chúng ta sẽ đưa về cùng một cái chuỗi có cùng một cái kích thước thì chút nữa trong cái phần ví dụ chúng ta sẽ được quan sát rõ hơn Và Keras Dataset IMDB thì trong cái tutorial này thì chúng ta sẽ sử dụng một cái bộ Dataset IMDB đây là một cái bộ Dataset chứa các cái review các cái bình luận về các cái bộ phim và ở đây thì các cái trạng thái bình luận của mình thì có sẽ có hai trạng thái một đó là Posit tích cực và hai đó là Negative tiêm cực thì ở bước đầu tiên đó là chúng ta sẽ loát cái bộ dữ liệu IMDB này lên thì ở đây chúng ta có thể cho biết là cái vocabulary size tức là cho biết kích thước của từ điển này của mình thì nếu bình thường thì cái số từ của mình nó sẽ rất là lớn, nó có thể lên đến hàng trăm ngàn, thậm chí là hàng triệu từ tuy nhiên để cho cái thí nghiệm này của chúng ta"
        },
        {
          "index": 4,
          "video_id": "Chương 8_0DGe4fjr1aw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_1： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/0DGe4fjr1aw",
          "start_time": 147,
          "end_time": 209,
          "text": "tức là cho biết kích thước của từ điển này của mình thì nếu bình thường thì cái số từ của mình nó sẽ rất là lớn, nó có thể lên đến hàng trăm ngàn, thậm chí là hàng triệu từ tuy nhiên để cho cái thí nghiệm này của chúng ta có thể thực hiện được trong thời gian ngắn thì chúng ta sẽ giới hạn cái từ điển của mình là 5 ngàn từ thôi và chúng ta hoàn toàn có thể thay tức là các bạn hoàn toàn có thể thay cái con số 5 ngàn này bằng một con số khác thế thì nếu như chúng ta có nhiều hơn 5 ngàn từ nhưng mà chúng ta lại chỉ lấy có 5 ngàn từ thì các từ còn lại nó bỏ đâu nó sẽ lưu trong một số đặc biệt, ví dụ như là số 0 và 5 ngàn từ này là 5 ngàn từ nào thì nó sẽ lấy tốt những cái từ xuất hiện thường xuyên nhất trong cái từ điển của mình và rồi thì như vậy thao tác đầu tiên của mình đó là chúng ta sẽ tiến hành đặt cái bộ dữ liệu này chúng ta sẽ tiến hành đặt cái bộ dữ liệu và khi kết quả trả về đúng không thì chúng ta sẽ trả về hai biến là Xtrend"
        },
        {
          "index": 5,
          "video_id": "Chương 8_0DGe4fjr1aw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_1： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/0DGe4fjr1aw",
          "start_time": 199,
          "end_time": 260,
          "text": "đặt cái bộ dữ liệu này chúng ta sẽ tiến hành đặt cái bộ dữ liệu và khi kết quả trả về đúng không thì chúng ta sẽ trả về hai biến là Xtrend Etrend rồi Xtest và Etest trong đó thì Xtrend nó sẽ chứa chuỗi các cái giá trị nó sẽ chứa chuỗi các cái giá trị tương ứng với lại các cái từ trong một cái câu review và Etrend thì nó sẽ chứa cái nhãn kết quả nể ra mình sẽ trả về cái trạng thái của mình nó là positive tích cực hay là negative tiêu cực tương tự như vậy cho Xtest và Etest rồi thì ở đây chúng ta sẽ có một cái chú ý đó là cái câu bình luận của mình nó có thể dài và ngắn khác nhau thường thì bình luận của mình là ngắn đúng không tuy nhiên nó cũng không loại trừ có những cái câu mình sẽ trả về giá trị thơm đơn giản Rất is呢 ạ bình luận của mình là dài có thể là trên 500 chữ"
        },
        {
          "index": 6,
          "video_id": "Chương 8_0DGe4fjr1aw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_1： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/0DGe4fjr1aw",
          "start_time": 247,
          "end_time": 312,
          "text": "nó có thể dài và ngắn khác nhau thường thì bình luận của mình là ngắn đúng không tuy nhiên nó cũng không loại trừ có những cái câu mình sẽ trả về giá trị thơm đơn giản Rất is呢 ạ bình luận của mình là dài có thể là trên 500 chữ thế thì đối với những cái câu mà quá dài thì chúng ta sẽ cắt ngắn chúng ta sẽ lấy những cái từ cuối cùng để làm sao cho nó đủ cho nó chẳng 500 chữ ngược lại nếu như cái câu comment của mình nó ít hơn 500 chữ thì khi đó chúng ta có thể làm cái thao tác và padding chúng ta sẽ chèn thêm vô ví dụ giả sử như cái x đầu vào của mình nó là bao gồm các cái chuỗi ký tự ví dụ như là 10 20 rồi 23 90 lưu ý là cái số 10 20 23 rồi 90 này"
        },
        {
          "index": 7,
          "video_id": "Chương 8_0DGe4fjr1aw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_1： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/0DGe4fjr1aw",
          "start_time": 296,
          "end_time": 364,
          "text": "ví dụ như là 10 20 rồi 23 90 lưu ý là cái số 10 20 23 rồi 90 này đó chính là cái thứ tự của cái từ đó trong từ điển tức là nó sẽ không lưu ví dụ như nó sẽ không lưu cái dạng thô ví dụ như là this movie is exciting nó không nói như vậy mà cái từ this này nó sẽ được mã hóa bằng con số 10 tức là cái vị trí của từ this trong từ điển là vị trí thứ 10 movie thì tương ứng vị trí của nó trong từ điển là 20 is vị trí trong từ điển của nó là 23 và exciting tức là tương ứng trong từ điển của mình là vị trí thứ 90 thì nó sẽ không lưu dữ liệu thô như thế này mà nó sẽ lưu cái dữ liệu chỉ số cái vị trí của nó trong cái tập từ điển của mình"
        },
        {
          "index": 8,
          "video_id": "Chương 8_0DGe4fjr1aw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_1： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/0DGe4fjr1aw",
          "start_time": 347,
          "end_time": 416,
          "text": "is vị trí trong từ điển của nó là 23 và exciting tức là tương ứng trong từ điển của mình là vị trí thứ 90 thì nó sẽ không lưu dữ liệu thô như thế này mà nó sẽ lưu cái dữ liệu chỉ số cái vị trí của nó trong cái tập từ điển của mình đó là ý thứ nhất ý thứ hai đó là cái phần về padding sequence padding thì ở đây chúng ta sẽ có cái kích thước tối đa của mình chính là max review length max review length của mình trong trường hợp này là bằng 500 và chúng ta hoàn toàn có thể thay thế cái con số 500 này bằng một cái con số khác rồi thì bây giờ nếu như chúng ta thực hiện cái thao tác padding này thì is is path của mình nó sẽ là bằng lúc này nó sẽ là bằng 0, 0 cho đến 0 và cái phần cuối của mình đó chính là 10, 20, 23 và 90"
        },
        {
          "index": 9,
          "video_id": "Chương 8_0DGe4fjr1aw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_1： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/0DGe4fjr1aw",
          "start_time": 399,
          "end_time": 460,
          "text": "lúc này nó sẽ là bằng 0, 0 cho đến 0 và cái phần cuối của mình đó chính là 10, 20, 23 và 90 thế thì câu hỏi đặt ra là tại sao chúng ta không phải để padding ở đầu mà chúng ta lại để cái padding ở phần cuối? à xin lỗi tại sao chúng ta không phải để padding ở đầu và chúng ta lại để cái padding ở phần cuối?  mà chúng ta lại để cái padding ở phần cuối? mà chúng ta lại để phần đầu như thế này thì rõ ràng khi chúng ta lần lượt thực hiện cái việc mà đưa các cái giá trị vào để xử lý trong mạng ANN thì nó sẽ xử lý số 10 trước xử lý số 20 23 và 90 tức là tương ứng là các cái từ this, movie, exciting sau đó nó sẽ đưa vào hàng loạt các cái con số 0 thì rõ ràng là những cái con số 0 vào sau nó sẽ được xử lý cuối cùng và nó sẽ làm loãng đi cái thông tin"
        },
        {
          "index": 10,
          "video_id": "Chương 8_0DGe4fjr1aw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_1： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/0DGe4fjr1aw",
          "start_time": 445,
          "end_time": 510,
          "text": "và 90 tức là tương ứng là các cái từ this, movie, exciting sau đó nó sẽ đưa vào hàng loạt các cái con số 0 thì rõ ràng là những cái con số 0 vào sau nó sẽ được xử lý cuối cùng và nó sẽ làm loãng đi cái thông tin của những cái từ đầu những cái từ mà quan trọng nhất của mình do đó thì cái padding của mình mình sẽ để nó là padding ở phần đầu tại vì khi chúng ta tính toán trên những con số 0 nó sẽ không ra những cái thông tin gì hết và chúng ta sẽ thật sự tính toán trên các cái dữ liệu của mình ở những cái con số cuối cùng này thôi thì đó là tại sao chúng ta phải padding ở đằng sau rồi bây giờ chúng ta sẽ cùng mình cuối cùng là lát cái thứ bị của mình rồi thì ở đây mình có thực hiện trước cái thao tác đó là chúng ta là lát cái dữ liệu của chúng ta đau lát cái gì này ở đây chúng ta sẽ can xô rồi chạy nè"
        },
        {
          "index": 11,
          "video_id": "Chương 8_0DGe4fjr1aw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_1： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/0DGe4fjr1aw",
          "start_time": 488,
          "end_time": 560,
          "text": "mình cuối cùng là lát cái thứ bị của mình rồi thì ở đây mình có thực hiện trước cái thao tác đó là chúng ta là lát cái dữ liệu của chúng ta đau lát cái gì này ở đây chúng ta sẽ can xô rồi chạy nè rồi thì bây giờ chúng ta sẽ cùng quan sát thử ha có x-treme của mình nó là cái gì đầu tiên chúng ta sẽ xem thử lại cấm sát 25.000 tương tự như vậy y cấm sát cũng là 25.000 như vậy ý nghĩa đó là gì đây chính là số mẫu dữ liệu trend còn số mẫu dữ liệu test của mình đó là cũng 25.000 luôn như vậy là tỷ lệ trend và test là 50 50 rồi bây giờ chúng ta sẽ quan sát thử cái x đầu vào nó là cái gì ok tìm→t Max"
        },
        {
          "index": 12,
          "video_id": "Chương 8_0DGe4fjr1aw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_1： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/0DGe4fjr1aw",
          "start_time": 542,
          "end_time": 616,
          "text": "còn số mẫu dữ liệu test của mình đó là cũng 25.000 luôn như vậy là tỷ lệ trend và test là 50 50 rồi bây giờ chúng ta sẽ quan sát thử cái x đầu vào nó là cái gì ok tìm→t Max thì chúng ta sẽ lái cái mẫu dữ liệu đầu tiên đuôi mặt đây là cái con số tương ứng như là cái chỉ số của cái từ trong tự nhiệm của mình như đã đề cập ở bên dưới à như chúng ta đã liên kết yêu buckle tay hơn thì cắt các con số mình nói chín là cái thứ tự của thần trong từ đi cũng bạc ạ và chúng ta sẽ lấy thử mẫu thứ 100 Ừ rồi bây giờ chúng ta sẽ cùng quan sát thao tác y, y thứ 100 đó là số 0, y 0 là số 1. Chúng ta sẽ cùng quan sát 10 mẫu dữ liệu đầu tiên rồi thì 0101 tức là không có thể là negative và 1 tức là positive thì đây chính là cái ý nghĩa của cái x-trend và y-trend rồi bây giờ chúng ta sẽ qua cái thao tác là padding như đã đề cập"
        },
        {
          "index": 13,
          "video_id": "Chương 8_0DGe4fjr1aw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_1： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/0DGe4fjr1aw",
          "start_time": 601,
          "end_time": 658,
          "text": "rồi thì 0101 tức là không có thể là negative và 1 tức là positive thì đây chính là cái ý nghĩa của cái x-trend và y-trend rồi bây giờ chúng ta sẽ qua cái thao tác là padding như đã đề cập ở trong hồi nãy chúng ta sẽ chèn cái số 0 vào đằng trước và sau khi chèn xong thì chúng ta sẽ quan sát thử x-trend của mình với cái mẫu dữ liệu đầu tiên nó sẽ là gì? Nó sẽ có chứa một loạt các con số 0 và những cái con số cuối chính là cái nội dung chính của cái review của mình nó không chèn vào cuối tại vì nó chèn vào cuối thì những cái thông tin chính của mình nó sẽ được tính toán sau cùng. Được từng cái đầu tiên và khi chúng ta loan truyền đến những cái con số không ở phía sau thì cái thông tin của những cáiamam thông tin của mình sẽ đã bị mat 1 thông tin đi rồi."
        },
        {
          "index": 14,
          "video_id": "Chương 8_0DGe4fjr1aw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_1： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/0DGe4fjr1aw",
          "start_time": 646,
          "end_time": 712,
          "text": "Được từng cái đầu tiên và khi chúng ta loan truyền đến những cái con số không ở phía sau thì cái thông tin của những cáiamam thông tin của mình sẽ đã bị mat 1 thông tin đi rồi. Ở bây giờ chúng ta sẽ vào cái phần chính của mình đó chính là cài đặt cái mã Create Państ sau link can be saved via déplacke ^^ Đây anh em thì ở đây SHENTA sẽ có moraleis trênWidetail E sia displaced Zel extract well thank you I've been there này đó chính là cái môismade to the vai và một loạt mắt thông tin các型 với các bạn кноп nh sortir của mình để khi goughtア be anythingS Amaque và một chν İ te freyd commission, Thêm một cách tổn quát. cái embedding layer này chúng ta cũng có thể là một cái phép biến đổi tuyến tính, nó có thể là một cái phép biến đổi tuyến tính và chúng ta sẽ so sánh chúng ta sẽ hữu luyện cái embedding layer này tức là chúng ta hoàn toàn có thể can thiệp để cho cái embedding layer này có thể hữu luyện được thay vì nó là một cái layer tỉnh, thì trong cái ví dụ này chúng ta sẽ đang xem xét là chúng ta sử dụng layer tỉnh với cái môn work to back đã được hữu luyện trước đó ở đây thì vocab tức là cái từ điển của mình sẽ có là 5.000 từ và cái chuỗi"
        },
        {
          "index": 15,
          "video_id": "Chương 8_0DGe4fjr1aw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_1： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/0DGe4fjr1aw",
          "start_time": 697,
          "end_time": 760,
          "text": "ví dụ này chúng ta sẽ đang xem xét là chúng ta sử dụng layer tỉnh với cái môn work to back đã được hữu luyện trước đó ở đây thì vocab tức là cái từ điển của mình sẽ có là 5.000 từ và cái chuỗi của cái review của mình tối đa sẽ là 500 chữ như đã đề cập trước đó rồi bây giờ chúng ta sẽ tải về cái môn work to back của FastTech rồi sau đó chúng ta sẽ giải nén ok thì ở đây là nó đã được giải nén лег trước đó rồi do đó thì chúng ta sẽ không có làm gì tiếp theo chúng ta sẽ không làm gì rồi juntos queria chúng ta sẽ Więc 3k môn củawno past thì cái việc nguyện ngày cũng tương tự như trong cái bài work to end thì nó có thể tốn của chúng ta phải 16 phút"
        },
        {
          "index": 16,
          "video_id": "Chương 8_0DGe4fjr1aw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_1： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/0DGe4fjr1aw",
          "start_time": 751,
          "end_time": 798,
          "text": "3k môn củawno past thì cái việc nguyện ngày cũng tương tự như trong cái bài work to end thì nó có thể tốn của chúng ta phải 16 phút ta sẽ gọi cho nó thể tốn của chúng ta khoảng 3 cho đến 4 phút rồi thì tranh thủ trong cái quá trình mà mô hình work to back cái embedding layer nó load lên thì chúng ta sẽ cùng bàn về cái lớp R&N cái lớp R&N này thì các cái phương thức như là load mô hình sell summary và predict nó cũng tương tự như cái lớp CNN mà chúng ta đã học trong những bài trước đây là những cái phương thức để load những cái mô hình mô tả cái kiến trúc của mô hình và dự đoán trên cái mẫu lĩnh liệu instead mới"
        }
      ]
    },
    {
      "video_id": "Chương 8_wKMBVF_bJdw",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Hướng dẫn lập trình một mô hình mạng hồi tiếp (RNN biến thể: simple ANN cell và LSTM) để phân loại nhị phân trên dữ liệu review, bao gồm xây dựng kiến trúc (phương thức `build`) và huấn luyện (`trend` / train) mô hình, kèm theo việc sử dụng embedding (word2vec) tiền huấn luyện hoặc cho mô hình tự học embedding. [1][5][19]\n\n- Các khái niệm sẽ được đề cập:\n  - Input layer với độ dài cố định (max review length) và cách biểu diễn chỉ số từ (token index). [1]\n  - Embedding layer (sử dụng ma trận trọng số word2vec hoặc trainable embedding), gồm tham số dictionary length (vocab size) và embedding length (chiều embedding). [2][3][4]\n  - Mô-đun xử lý chuỗi (ANN cell / LSTM) để sinh trạng thái ẩn st và lớp dense đầu ra cho phân loại nhị phân (sigmoid + binary cross-entropy). [6][7]\n  - Các bước huấn luyện, lưu lịch sử (history), kiểm tra trọng số và đánh giá trên tập test (threshold 0.5 → nhãn). [8][11][14]\n\n(Thông tin trên trích trực tiếp từ video theo các đoạn tương ứng.) [1][2][3][4][5][6][7][8][11][14][19]\n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 Input layer: biểu diễn chuỗi đầu vào\n- Input chứa chỉ số (index) của các từ trong câu review — không chứa dữ liệu thô. [1]\n- Mỗi review được biểu diễn thành vector độ dài cố định bằng tham số max review length (ví dụ ở bài là 500), tức mọi review sẽ có 500 phần tử (padding/truncation được giả định). [1]\n\n[Citation: 00:00–01:02] [1]\n\n### 2.2 Embedding layer: cấu hình và lựa chọn\n- Embedding layer có hai tham số chính:\n  - dictionary length (vocab size — tổng số từ trong từ điển). [2]\n  - embedding length (chiều vector biểu diễn mỗi từ). [2]\n- Trong ví dụ, sử dụng ma trận embedding từ word2vec đã được huấn luyện sẵn (`word2vec.vector`) với kích thước khoảng dictionary ~ 900k (gần 1 triệu) và embedding length = 300. [3][9]\n- Embedding có thể được:\n  - Khóa (fixed): sử dụng ma trận pretrained và không train lại (trainable = False). [3][5]\n  - Hoặc trainable: để mô hình tự học embedding trong quá trình huấn luyện, điều này có thể cải thiện hiệu năng. [3][19]\n- Cài đặt thêm: có thể cung cấp `embedding_initializer` và `regularizer` (cấu hình khởi tạo và regularization). [4]\n\n[Citation: 00:47–03:31] [2][3][4][5][9][19]\n\n### 2.3 Kiến trúc xử lý chuỗi: ANN cell (simple) và LSTM\n- Sau embedding (ở ví dụ này embedding output là vector 300 chiều vì dùng embedding length = 300), dữ liệu được đưa vào cell xử lý chuỗi (ANN cell đơn giản hoặc LSTM). [6]\n- ANN cell tính ra trạng thái ẩn st (hidden state vector). Kích thước của st có thể do người thiết kế chọn (trong ví dụ chọn 64). [6][7][14]\n- Sau cell, dùng một lớp Dense (fully connected) để map st → output. Vì là bài toán phân loại nhị phân nên output là 1 node với activation = sigmoid, và loss = binary cross-entropy. [7]\n- Optimizer được dùng: Adam; metric: accuracy. [8]\n\n[Citation: 04:10–06:04] [6][7][8]\n\n### 2.4 Cách cài đặt / các bước trong mã\n- Hai phương thức chính trong đối tượng mô hình: `build` (xây dựng kiến trúc) và `trend` (huấn luyện). [1]\n- Trình tự xây dựng: tạo input layer → embedding layer (gắn pretrained weights nếu có) → ANN/LSTM cell → dense → compile (loss, optimizer, metrics) → fit. [1][5][11]\n- Truy cập trọng số: model.layers theo chỉ số (layer 0 = input, layer 1 = embedding, layer 2 = ANN/LSTM là nơi chứa phần lớn tham số huấn luyện trong ví dụ). [11][12]\n\n[Citation: 00:00–01:02] [5][11][12]\n\n### 2.5 Vấn đề hiệu năng và thực thi\n- Tập huấn (fit) được cấu hình ngắn gọn trong demo: 3 epochs để tiết kiệm thời gian. [8]\n- Tốc độ huấn luyện trong ví dụ khá chậm vì mạng ANN (ở dạng tuần tự trong cài đặt) không tận dụng được tính toán song song, do đó mỗi bước thực hiện tương tự và mất thời gian. [8][10]\n\n[Citation: 05:49–08:32] [8][10]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n### 3.1 Ví dụ minh họa thực thi trên dataset review\n- Thông số trong demo:\n  - max review length = 500 → input vector 500 phần tử. [1]\n  - embedding: pretrained word2vec với dictionary length ≈ 900,000 và embedding length = 300. [3][9]\n  - hidden st size (ANN/LSTM) = 64. [7][14]\n  - loss = binary cross-entropy, optimizer = Adam, metric = accuracy, epochs = 3. [7][8]\n\n[Citation: 00:00–07:44] [1][3][7][8][9]\n\n### 3.2 Kết quả thực nghiệm (demo)\n- Trong quá trình huấn luyện ban đầu:\n  - Quan sát loss ≈ 0.7 và accuracy khoảng 51–52% sau epoch đầu. [10]\n  - Lịch sử huấn luyện (history) được lưu, có thể in ra để phân tích loss theo epoch. [10][11]\n  - Biểu đồ loss có giai đoạn giảm rồi tăng nhẹ (overfitting/biến thiên nhỏ trong demo). [13]\n- Sau khi dự đoán trên tập test và chuyển probability → label bằng threshold = 0.5, tính accuracy = số dự đoán đúng / tổng mẫu:\n  - Với biến thể simple ANN, accuracy thu được khoảng 68.8% (kết quả trên tập test trong demo). [14][15][16]\n- Khi thay `simple ANN` bằng `LSTM` (giữ hidden = 64) và train lại từ đầu, **LSTM cho kết quả chính xác hơn** so với simple ANN trong demo. [17][18]\n\n[Citation: 07:27–15:13] [10][11][13][14][15][16][17][18]\n\n### 3.3 Ứng dụng / Trường hợp sử dụng\n- Bài toán minh họa là phân loại sentiment/label nhị phân từ review/comment (thuật toán phổ biến trong NLP). Các thành phần (fixed pretrained embedding vs trainable embedding, ANN vs LSTM) là các lựa chọn thường gặp khi triển khai mô hình cho dữ liệu ngôn ngữ tự nhiên. [1][2][3][19]\n\n[Citation: 00:00–03:31][19]\n\n---\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Thiết kế mô hình gồm input (fixed-length token indices) → embedding (pretrained hoặc trainable) → cell xử lý chuỗi (ANN/LSTM) → dense sigmoid cho phân loại nhị phân. [1][2][6][7]\n  - Sử dụng pretrained word2vec làm embedding không train (trainable=False) trong demo; tuy nhiên cho phép embedding trainable thường đem lại hiệu năng tốt hơn. [3][5][19]\n  - LSTM (với cùng số chiều trạng thái ẩn) cho kết quả tốt hơn simple ANN trong ví dụ minh họa. [17][18]\n  - Các bước thực thi: build kiến trúc → compile (binary cross-entropy, Adam, accuracy) → fit (history lưu) → predict (threshold 0.5) → đánh giá accuracy. [7][8][10][14][11]\n\n[Citation: 00:00–15:44] [1][2][3][5][6][7][8][10][11][14][17][19]\n\n- Tầm quan trọng:\n  - Bài giảng nhấn mạnh hai quyết định thiết kế quan trọng trong mô hình NLP thực tế: (1) dùng embedding pretrained cố định hay cho học lại, (2) chọn cell xử lý chuỗi thích hợp (simple ANN vs LSTM). Những lựa chọn này ảnh hưởng trực tiếp đến hiệu năng và thời gian huấn luyện. [3][19][17][18]\n\n[Citation: 01:35–03:31][14][17][19]\n\n- Liên hệ với bài giảng khác:\n  - Video tham chiếu khái niệm word2vec và việc sử dụng ma trận trọng số pretrained (được học trước trong các bài/chuỗi bài liên quan), do đó liên kết trực tiếp tới phần học về word2vec trước đó. [2][4]\n\n[Citation: 00:47–03:31][4]\n\n---\n\nGhi chú: Các con số, tham số và kết quả (ví dụ: dictionary length ≈ 900k, embedding=300, hidden st=64, accuracy ≈ 68.8% cho ANN, LSTM cho kết quả tốt hơn) đều được trích từ nội dung video (các đoạn được trích dẫn). [3][9][14][15][17][18]",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 0,
          "end_time": 62,
          "text": "hai cái phương thức chính của mình đó chính là phương thức build và trend trong đó build thì sẽ tiến hành định nghĩa cái kiến trúc của mạng ANN thì đây chúng ta sẽ cùng nhìn lại cái kiến trúc của mình đầu tiên đó chính là cái emitting layer à xin lỗi đầu tiên của mình đó chính là cái input cái input này nó sẽ chứa các cái chỉ số index của các cái từ trong cái câu comment trong cái câu review của mình nó chỉ chứa cái chỉ số chứ nó không có lưu cái dữ liệu thô ha dữ liệu gốc ban đầu rồi và lớp input này thì nó sẽ có đầu vào của mình họ làm max review length tức là ở đây là 500 nó sẽ là 500 như vậy ở đây sẽ là một cái vector 500 chiều 500 phần tử input của mình sẽ là một cái vector có 500 phần tử cho dù review ngắn hay review dài thì cũng sẽ đều có 500 phần tử"
        },
        {
          "index": 2,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 47,
          "end_time": 110,
          "text": "nó sẽ là 500 như vậy ở đây sẽ là một cái vector 500 chiều 500 phần tử input của mình sẽ là một cái vector có 500 phần tử cho dù review ngắn hay review dài thì cũng sẽ đều có 500 phần tử rồi lớp tiếp theo đó chính là lớp embedding thì cái embedding này nó sẽ có các cái thông tin đó là vocab size tức là cái số lượng từ của mình thì ở đây chúng ta sẽ cùng xem xét cái dictionary ở đây thì do cái mô hình của mình nó chưa được lát lên ha nhưng mà khi chúng ta lát lên sau thì chúng ta sẽ có cái mô hình của mình nó chưa được lát lên ha  thì cái word2vec.vector này nó chính là cái ma trận trọng số đã được huấn luyện của mình thì nó sẽ có hai cái thông số về kích thước đó là dictionary length tức là tổng số từ tổng số từ trong tập từ điển của mình và emitting length tức là cái chiều của cái vector mà mình dự kiến mình sẽ biểu diễn"
        },
        {
          "index": 3,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 95,
          "end_time": 157,
          "text": "thì nó sẽ có hai cái thông số về kích thước đó là dictionary length tức là tổng số từ tổng số từ trong tập từ điển của mình và emitting length tức là cái chiều của cái vector mà mình dự kiến mình sẽ biểu diễn thì ở đây là khoảng một triệu dictionary nó khoảng là gần một triệu và emitting length nó là 300 triệu do ở đây mình sử dụng là 300 một triệu rồi do đó thì ở đây chúng ta sẽ để là emitting dictionary length tức là cái số từ tiễn của mình số từ trong từ điển của mình ở đây sẽ là emitting length và như đề cập hồi nãy á tức là cái thông số emitting length này chúng ta hoàn toàn có thể thay đổi và cho cái mô hình của mình nó học cái emitting layer này luôn thay vì làm một cái layer tỉnh nhưng mà trong cái ví dụ này thì chúng ta đang xem xét nó làm một cái layer tỉnh"
        },
        {
          "index": 4,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 146,
          "end_time": 211,
          "text": "và cho cái mô hình của mình nó học cái emitting layer này luôn thay vì làm một cái layer tỉnh nhưng mà trong cái ví dụ này thì chúng ta đang xem xét nó làm một cái layer tỉnh rồi emitting initializer rồi emitting initializer thì ở đây chúng ta sẽ để mặt đệnh luôn emitting layer thì nó sẽ có các cái regularizer ở phía sau thì chúng ta đã có một cái bộ code tương ứng cho cái mstm thì ở đây là nó sẽ có thêm hai cái thông số nữa nó sẽ có thêm hai thông số nữa rồi thông số đầu tiên á đó chính là way way chính là cái mô hình mà word2vec mà chúng ta đã học trước đây và ở đây chúng ta sẽ sử dụng nhưng không hề khuấn luyện lại"
        },
        {
          "index": 5,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 198,
          "end_time": 261,
          "text": "rồi thông số đầu tiên á đó chính là way way chính là cái mô hình mà word2vec mà chúng ta đã học trước đây và ở đây chúng ta sẽ sử dụng nhưng không hề khuấn luyện lại chúng ta sẽ sử dụng không khuấn luyện lại do đó thì cái trainable sẽ là bằng phone tức là cái emitting này có được train lại hay không đúng không thì ở đây là không chúng ta sẽ không train lại mà chúng ta sẽ tái sử dụng luôn cho cái chúng ta sẽ tái sử dụng luôn cho cái bộ trọng số của mô hình của word2vec rồi và ở đây thì chúng ta sẽ cùng truyền vào cái input layer thì ở đây nó sẽ có cái thông tin đó là input input này là cái kết quả của cái lớp biến đổi trước đó cái lớp tiếp theo đó chính là cái lớp về ANN đó chính là cái lớp ANN và cái lớp ANN này thì nó sẽ cho chúng ta biết rằng là cái kích thước của hidden layer của mình là bao nhiêu"
        },
        {
          "index": 6,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 250,
          "end_time": 312,
          "text": "đó chính là cái lớp về ANN đó chính là cái lớp ANN và cái lớp ANN này thì nó sẽ cho chúng ta biết rằng là cái kích thước của hidden layer của mình là bao nhiêu cái vector hidden cái vector của cái layer này là bao nhiêu cái quá trình mà biến đổi là st của mình là bao nhiêu rồi rồi rồi rồi rồi thì sau khi chúng ta thực hiện cái emitting layer thì ở đây lưu ý chúng ta ở đây là nó không phải là vector 32 chiều mà nó sẽ là 300 chiều nha trong trường hợp tổng quát thì cái output này nó có thể cái số chiều của cái st này có thể là một con số bất kỳ do chúng ta định nghĩa và mô hình của mình nó sẽ học cái emitting layer còn trong trường hợp này emitting layer của mình là tỉnh thì ở đây sẽ là 300 và qua đây thì chúng ta sẽ qua cái ANN cell thì chúng ta sẽ tính ra cái st st này chính là cái vector của cái trạng thái ẩn ở đây"
        },
        {
          "index": 7,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 299,
          "end_time": 364,
          "text": "còn trong trường hợp này emitting layer của mình là tỉnh thì ở đây sẽ là 300 và qua đây thì chúng ta sẽ qua cái ANN cell thì chúng ta sẽ tính ra cái st st này chính là cái vector của cái trạng thái ẩn ở đây và nó có thể là 64 chiều ở đây chúng ta để 64 chiều rồi sau đó chúng ta sẽ thực hiện cái phép biến đổi là dense tức là kết nối đầy đủ để từ cái st này biến thành cái output và ở đây chúng ta phân loại nhị phân chúng ta phân loại nhị phân nên ở đây sẽ là một cái hàm activation sẽ là sigmoid đầu ra của mình sẽ là một node và activation sẽ là sigmoid rồi thì cái đầu vào cho cái dense này nó chính là cái hidden hidden này chính là cái kết quả của cái layer trước đó là simple ANN thì ở đây chúng ta có thể để là 64 theo như cái sơ đồ nó sẽ là 64 rồi và cái hàm loss của mình chúng ta sẽ sử dụng là binary cross entropy"
        },
        {
          "index": 8,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 349,
          "end_time": 409,
          "text": "là simple ANN thì ở đây chúng ta có thể để là 64 theo như cái sơ đồ nó sẽ là 64 rồi và cái hàm loss của mình chúng ta sẽ sử dụng là binary cross entropy sử dụng adam và độ đo đánh giá của mình sẽ là accuracy rồi chúng ta sẽ fit thì ở đây là để đơn giản thì chúng ta sẽ fit trong 3 epoch thôi do chúng ta không có nhiều thời gian rồi thì cái moving work to back cũng đã được drag lên rồi và bây giờ chúng ta sẽ cùng xem là cái chích thước của cái dictionary length là bao nhiêu và cái embedding length sẽ là bao nhiêu thì ở đây chúng ta đã đoát muộn rồi chúng ta đã đoát muộn rồi nên mình sẽ không phải đoán lại nữa tại vì nó sẽ tốn hết 3 phút rồi"
        },
        {
          "index": 9,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 399,
          "end_time": 464,
          "text": "chúng ta đã đoát muộn rồi nên mình sẽ không phải đoán lại nữa tại vì nó sẽ tốn hết 3 phút rồi  ở đây thì dictionary length là bằng 900 ngàn và embedding của mình sẽ là 300 tức là cái chích thước của tập tựa điểm của mình sẽ là gần 1 triệu rồi bây giờ chúng ta sẽ chạy cái ANN rồi chúng ta sẽ khởi tạo một cái lớp đối tượng là ANN và gọi cái hàm build build này nó sẽ dựng lên cái kiến trúc của on-map ANN của mình rồi sau đó chúng ta sẽ tiến hành drag thì cái việc drag này nó cũng tốn của chúng ta khoảng 3-4 phút do là cái mạng ANN thì nó không có thực hiện tính toán song song được các cái bước của mình nó đều thực hiện tương tự nên cái tốc độ tính toán của mình nó sẽ rất là chậm rồi thì chúng ta quan sát ở đây là cái loss của mình là đang 0.7"
        },
        {
          "index": 10,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 447,
          "end_time": 512,
          "text": "do là cái mạng ANN thì nó không có thực hiện tính toán song song được các cái bước của mình nó đều thực hiện tương tự nên cái tốc độ tính toán của mình nó sẽ rất là chậm rồi thì chúng ta quan sát ở đây là cái loss của mình là đang 0.7 và accuracy của mình đang là khoảng 51-52% loss của mình nó đang có xu hướng giảm xuống đây là 1 trên 3 epoch rồi thì ở đây nếu như cái chương trình này chạy sao thì nó sẽ trả ra cái history trong cái history này nó sẽ lưu cái loss của cái quá trình huấn luyện của mình chúng ta sẽ in ra đây để quan sát cái trọng số của cái mô hình của mình"
        },
        {
          "index": 11,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 499,
          "end_time": 589,
          "text": "thì nó sẽ trả ra cái history trong cái history này nó sẽ lưu cái loss của cái quá trình huấn luyện của mình chúng ta sẽ in ra đây để quan sát cái trọng số của cái mô hình của mình thì chúng ta sẽ dùng là ANN.modal.layer chúng ta có thể viết một cái phương thức đó là lấy cái trọng số nhưng mà ở đây cho nhanh thì chúng ta có thể để là ANN.modal.layer và chúng ta sẽ lấy cái layer số 2 tại sao? tại vì đây là layer số 1 nè à, sẽ gọi layer, đây là layer số 0 nè là input layer nè sau đó sẽ là layer số 1 là embedding layer thì 2 cái này là không có cái tham số huấn luyện nào chủ yếu cái tham số huấn luyện của mình nó sẽ nằm ở cái lớp ANN này nằm ở cái lớp ANN này do đó thì nó sẽ nằm ở layer số 2 0, 1, 2 ok ok ok"
        },
        {
          "index": 12,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 549,
          "end_time": 619,
          "text": "do đó thì nó sẽ nằm ở layer số 2 0, 1, 2 ok ok ok"
        },
        {
          "index": 13,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 589,
          "end_time": 660,
          "text": "asına ok ärke ok ok ok ok rồi và đây là cái biểu đồ của ông đã giảm xuống và đến đây là nó bắt đầu nó tăng lên rồi chọn số"
        },
        {
          "index": 14,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 652,
          "end_time": 711,
          "text": "rồi và đây là cái biểu đồ của ông đã giảm xuống và đến đây là nó bắt đầu nó tăng lên rồi chọn số thì đây nó sẽ là 64 không Tại vì trong cái sơ đồ này ST này của mình là có 64 nên cái cái cái cái cái cái trọng số của mình ở đây nó không còn là 32 mà nó là 64 rồi bây giờ chúng ta sẽ tiến hành giữ dự đoán trên cái dữ liệu test và xem xem là cái mô đồ của mình nó có cái độ chính xác là bao nhiêu phần trăm rồi thì trong ký phần về đánh giá thì chúng ta sẽ kiểm tra xem nếu y mà lớn hơn hoặc bằng lớn hơn 0.5 thì nó tư ứng là cái nhãn 1 còn ngược lại thì nó sẽ là bằng 0"
        },
        {
          "index": 15,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 689,
          "end_time": 761,
          "text": "nhiêu phần trăm rồi thì trong ký phần về đánh giá thì chúng ta sẽ kiểm tra xem nếu y mà lớn hơn hoặc bằng lớn hơn 0.5 thì nó tư ứng là cái nhãn 1 còn ngược lại thì nó sẽ là bằng 0 và chúng ta sẽ kiểm đưa nó về cái vector sau đó chúng ta sẽ kiểm tra xem cái y predict này có khớp với lại cái y test hay không nó có bằng bằng y test hay không sau đó chúng ta sẽ tính tổng tất cả những cái mẫu mà cho cái kết quả là bằng nhau rồi bằng 1 rồi chia cho tổng số mẫu của mình tức là chi cho y test thì khi đó chúng ta sẽ có được cái độ chính xác rồi và chúng ta sẽ cùng quan sát xem preparation game có rând trong các phần của mọi trường hợp history mình các một phần bờ nội real nhưng mà các ch則 tính được những cái mẫu này mình phải có được cái tầng trong các something khác danh cáiались ground carte ở trong này chúng ta sẽ phải ở trên triangle ra lại làm vậy các các sản phẩm này các bài переж får bài kết bạn nên do thì chúng ta tuyên nên được một viên bài phần chính xác bằng nhé kì đây là down a gee nè rồi và chúng ta sẽ cùng quan sát"
        },
        {
          "index": 16,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 748,
          "end_time": 808,
          "text": "sẽ cùng quan sát xem preparation game có rând trong các phần của mọi trường hợp history mình các một phần bờ nội real nhưng mà các ch則 tính được những cái mẫu này mình phải có được cái tầng trong các something khác danh cáiались ground carte ở trong này chúng ta sẽ phải ở trên triangle ra lại làm vậy các các sản phẩm này các bài переж får bài kết bạn nên do thì chúng ta tuyên nên được một viên bài phần chính xác bằng nhé kì đây là down a gee nè rồi và chúng ta sẽ cùng quan sát vào rồi và chúng ta sẽ cùng quan sát kết quả của mình khi mà chúng ta dự đoán trên cái tập dữ liệu XTEC là bằng bao nhiêu rồi như vậy thì cái độ chính xác của mình đó là khoảng 68% 68,8% bây giờ chúng ta sẽ cùng cài đặt cho cái biến thể ANEN thì chúng ta sẽ để ý là biến thể ANEN gần như y chang gần như y chang so với lại cái lớp này cái lớp của ANEN nó chỉ khác duy nhất đó là chúng ta sẽ thay ANEN ở đây chúng ta thay vì là dùng simple ANEN thì chúng ta sẽ thay là bằng LSTM"
        },
        {
          "index": 17,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 798,
          "end_time": 862,
          "text": "ở đây chúng ta thay vì là dùng simple ANEN thì chúng ta sẽ thay là bằng LSTM rồi chúng ta sẽ thay bằng LSTM ở đây chúng ta sẽ để là 64 rồi đây sẽ để là MEDDING rồi cho nó dễ hiểu ra rồi đây sẽ là HIDDEN rồi và bây giờ thì chúng ta tạo một cái lớp đối tượng tên là LSTM thì chúng ta sẽ TREND lại từ đầu rồi thì ở đây nó sẽ bắt đầu cái quá trình TREND của mình chúng ta sẽ naturally     cà phê đống"
        },
        {
          "index": 18,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 855,
          "end_time": 913,
          "text": "chúng ta sẽ naturally     cà phê đống HARD あ không thấy giống l discharge thì chúngielle em không biết cáiера tôi không thấy क activation range và số hidden và cái số chiều của cái vector trạng thái ẩn của mình là 64 chiều thì ở đây chúng ta thấy LSTM cho cái kết quả chính xác hơn. Như vậy thì qua cái tutorial này thì chúng ta đã cùng thực hiện cài đặt cái mạng ANN biến thể và biến thể của nó là LSTM và một cách tổng quát thì chúng ta cũng có thể không sử dụng cái emitting layer như là"
        },
        {
          "index": 19,
          "video_id": "Chương 8_wKMBVF_bJdw",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 8] Part 4_2： Hướng dẫn lập trình với kiến trúc mạng RNN và LSTM",
          "video_url": "https://youtu.be/wKMBVF_bJdw",
          "start_time": 898,
          "end_time": 944,
          "text": "đã cùng thực hiện cài đặt cái mạng ANN biến thể và biến thể của nó là LSTM và một cách tổng quát thì chúng ta cũng có thể không sử dụng cái emitting layer như là một cái ma trận cố định này mà chúng ta có thể cho cái mô hình của mình có thể học tự học cái emitting này thay vì sử dụng một cái ma trận chỉnh. Và cái kết quả thì khi chúng ta sử dụng cái ma trận emitting, xin gọi cái lớp layer cái lớp emitting này mà được huấn luyện thì nó cho kết quả khỏi lụt. Thì chúng ta sẽ có thể làm tốt hơn nữa so với nãy cái việc sử dụng một cái work emitting cố định."
        }
      ]
    },
    {
      "video_id": "Chương 8_4EdX3Ga9YoM",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giới thiệu bài toán dịch máy (machine translation) dưới góc nhìn *Neural Machine Translation*, hiểu kiến trúc *sequence-to-sequence* (còn gọi là \"six to six\" trong bài) và làm quen với **cơ chế attention** cùng một số biến thể của nó. [1][2]\n\n- Các khái niệm sẽ được đề cập:\n  - Mạng ANN kiểu tuần tự (RNN) gồm hai bước: tính trạng thái ẩn (hidden state) tổng hợp thông tin quá khứ và hiện tại, rồi từ đó tính output. [1]\n  - Các biến thể của RNN đã học trước: LSTM, bidirectional RNN, ANN 2 chiều và deep stacked RNN. [1][2]\n  - Khái niệm *Neural Machine Translation* (end-to-end RNN) và kiến trúc *sequence-to-sequence* (encoder–decoder). [5][6]\n  - Vai trò của token đặc biệt (start / end) trong quá trình decode và nguyên tắc inference (argmax / feed-back). [8][10]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Tại sao chọn bài toán dịch máy\n- Dịch máy có độ khó cao vì phải làm việc trên hai domain khác nhau — hai ngôn ngữ nguồn và đích. [3]  \n- Tính tổng quát: dịch máy là bài toán biến đổi từ một chuỗi sang một chuỗi khác; cấu trúc này xuất hiện trong nhiều bài toán NLP khác (tóm tắt, paraphrase, phân loại khi đầu ra là một giá trị, v.v.). [3][4]\n\n### 2.2. Neural Machine Translation (NMT) — End-to-end với RNN\n- *Neural Machine Translation* ở đây là cách tiếp cận sử dụng mạng Neural (RNN) từ đầu đến cuối (end-to-end): một mô hình duy nhất học trực tiếp ánh xạ input → output mà không cần các bước trung gian thủ công. [6]  \n- Kiến trúc cơ bản là *sequence-to-sequence* (seq2seq / \"six to six\"), gồm hai thành phần chính: **encoder** và **decoder**. [6][7]\n\n### 2.3. Encoder — tạo vector biểu diễn của chuỗi nguồn\n- Encoder (RNN) đọc toàn bộ câu nguồn và tạo ra một vector (hidden state) biểu diễn tích hợp toàn bộ thông tin của input. Vector này đóng vai trò là mã (code) của câu nguồn. [7][13]  \n- Kết quả của quá trình encode là một trạng thái ẩn (một hoặc một tập vector phụ thuộc thiết kế) được truyền sang decoder. [7][12]\n\n### 2.4. Decoder — tạo sinh chuỗi đích (như một language model)\n- Decoder hoạt động như một *language model* để sinh ra văn bản đích, sử dụng thông tin đã được tổng hợp từ encoder cùng với lịch sử token đã sinh. [12]  \n- Quá trình decode bắt đầu bằng một token đặc biệt (start), và kết thúc khi decoder sinh token đặc biệt (end). Việc sử dụng token đặc biệt phải thống nhất trong toàn bộ dữ liệu, huấn luyện và inference. [8][9][11]\n\n### 2.5. Cơ chế sinh token và inference\n- Ở mỗi bước decode, mô hình xuất ra một vector phân phối trên từ vựng; để chọn từ sinh, thường dùng phép argmax trên vectơ này (hoặc các chiến lược khác trong thực tế như sampling/beam search, nhưng trong video nhấn mạnh argmax). [10]  \n- Token được chọn tại bước t được đưa lại làm input cho bước t+1 cùng với trạng thái ẩn trước đó, tạo nên chuỗi các token đầu ra từng bước một. [10][11]\n\n### 2.6. Tính linh hoạt của kiến trúc seq2seq\n- Mô hình seq2seq không chỉ hiệu quả cho dịch máy mà còn áp dụng cho nhiều bài toán khác khi đầu vào/đầu ra là chuỗi: tóm tắt văn bản (summarization), hội thoại (dialog/chatbot), phân tích cú pháp (parsing), sinh mã nguồn (code generation), v.v. [14][15][16]  \n- Tính tổng quát nằm ở việc một mạng neural nhận chuỗi input và tạo vector biểu diễn; mạng khác dùng vector đó để sinh chuỗi output — tức là quá trình encode → decode có thể áp dụng cho nhiều tác vụ. [13][12]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa trong bài:\n  - Input: một câu tiếng Anh \"I'm not sure\" được encoder đọc và mã hóa thành trạng thái ẩn (hidden state). [7][8]  \n  - Quá trình decode: bắt đầu bằng token đặc biệt *start*, decoder sinh chuỗi token đích từng bước. Video mô tả chuỗi token dự đoán như GER → NE → SUI → PA → ... và cuối cùng token kết thúc *N*, mỗi token được chọn bằng argmax trên vectơ output và sau đó được cấp lại cho decoder cho bước tiếp theo. (lưu ý: tên token trong ví dụ là theo transcript trong video). [10][11]  \n  - Yêu cầu kỹ thuật: các token đặc biệt (start/end) phải được thống nhất giữa dữ liệu huấn luyện và lúc inference/triển khai. [9]\n\n- Ứng dụng thực tế / trường hợp sử dụng:\n  - Dịch giữa bất kỳ cặp ngôn ngữ nào (English ↔ French, English ↔ Vietnamese, v.v.), và mục tiêu dài hạn có thể là dịch đa ngôn ngữ (multilingual translation). [5]  \n  - Tóm tắt văn bản: input là văn bản dài, output là đoạn tóm tắt ngắn gọn. [14]  \n  - Hội thoại / Chatbot: input là lời thoại trước, output là phản hồi tiếp theo. [15]  \n  - Phân tích cú pháp (parsing): input là đoạn văn, output là cây cú pháp (dưới dạng chuỗi). [15]  \n  - Sinh mã nguồn (code generation): input là mô tả chức năng, output là mã nguồn (ví dụ Python). [15][16]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Bài giảng giới thiệu bài toán dịch máy dưới dạng Neural Machine Translation sử dụng kiến trúc seq2seq (encoder–decoder) với RNN; encoder tổng hợp toàn bộ câu nguồn thành vector biểu diễn, decoder hoạt động như language model để sinh chuỗi đích từng bước, dùng token start/end và chọn token bằng argmax trên output vector. [6][7][13][10][11]  \n  - Bài toán dịch máy được chọn vì tính khó (làm việc trên hai ngôn ngữ khác nhau) và tính tổng quát (mô hình chuỗi→chuỗi có thể áp dụng cho nhiều bài toán NLP khác). [3][4]  \n  - Kiến thức về seq2seq và các biến thể RNN (LSTM, bidirectional, deep stacked) là nền tảng để tiếp tục nghiên cứu **cơ chế attention** và các biến thể của attention, nội dung sẽ được trình bày tiếp theo. [1][2]\n\n- Tầm quan trọng: Nắm vững seq2seq và cơ chế encode–decode là bước cơ bản để hiểu các mô hình dịch máy hiện đại và để mở rộng sang các nhiệm vụ chuỗi→chuỗi khác như summarization, dialog, parsing, code generation. [12][14][15][16]\n\n- Liên hệ với các bài giảng khác: Bài học dựa trên các biến thể mạng ANN đã học trước (LSTM, bidirectional, 2D, deep stack) và sẽ tiếp tục sang phần tìm hiểu attention và các biến thể của attention trong các bài tiếp theo. [1][2]\n\n(Phần tóm tắt trên sử dụng toàn bộ các đoạn trích đã cung cấp trong video và trích dẫn tương ứng theo timestamp.)",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 8_4EdX3Ga9YoM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_1： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/4EdX3Ga9YoM",
          "start_time": 13,
          "end_time": 65,
          "text": "Trong bài hôm nay thì chúng ta sẽ cùng tìm hiểu về bài toán dịch máy mô hình 6 to 6, sequence to sequence và cơ chế attention. Bài này chúng ta sẽ dựa trên ý tưởng của mạng ANN trước đây. Mạng ANN sẽ có bao gồm 2 bước biến đổi. Bước số 1 là chúng ta sẽ đi tính trạng thái ẩn. Trạng thái ẩn sẽ tổng hợp thông tin của quá khứ và thông tin của hiện tại. Sau đó từ trạng thái ẩn này chúng ta sẽ đi tính toán ra giá trị output. Và một số biến thể của mạng ANN mà chúng ta đã tìm hiểu là đã được thảo luận trước đây đó là biến thể về long software memory LSTM, biến thể về bidirectional ANN, ANN 2 chiều và deep stack ANN."
        },
        {
          "index": 2,
          "video_id": "Chương 8_4EdX3Ga9YoM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_1： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/4EdX3Ga9YoM",
          "start_time": 50,
          "end_time": 111,
          "text": "Và một số biến thể của mạng ANN mà chúng ta đã tìm hiểu là đã được thảo luận trước đây đó là biến thể về long software memory LSTM, biến thể về bidirectional ANN, ANN 2 chiều và deep stack ANN. Thì trong cái nội dung đầu tiên mà chúng ta sẽ học bài hôm nay đó chính là tìm hiểu về bài toán dịch máy machine translation. Tiếp theo đó là chúng ta sẽ tìm hiểu về cơ chế attention trong cái việc giải quyết cái bài toán dịch máy này. Và một số cái biến thể của attention. Và tại sao thì chúng ta lại tìm hiểu về dịch máy mà không phải là các cái bài toán khác. Ví dụ như bài toán tóm tắt văn bản, bài toán phân loại văn bản hoặc là bài toán post tagging, gán nhãn từ loại. Bài toán dịch máy nó có một cái vai trò rất là quan trọng. Đầu tiên đó là nó có cái độ khó."
        },
        {
          "index": 3,
          "video_id": "Chương 8_4EdX3Ga9YoM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_1： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/4EdX3Ga9YoM",
          "start_time": 92,
          "end_time": 163,
          "text": "Ví dụ như bài toán tóm tắt văn bản, bài toán phân loại văn bản hoặc là bài toán post tagging, gán nhãn từ loại. Bài toán dịch máy nó có một cái vai trò rất là quan trọng. Đầu tiên đó là nó có cái độ khó. Cái độ khó của nó nó thể hiện ở chỗ là dịch máy chúng ta phải làm việc trên hai cái domain, hai cái không gian khác nhau. Đó chính là hai cái ngôn ngữ mà chúng ta cần phải dịch. Cái thứ hai đó là dịch máy là một cái bài toán mà nó tổng quát. Cái kiểu tổng quát của dịch máy đó chính là nó biến đổi từ một cái chuỗi về một cái chuỗi khác. Và từ cái dạng chuỗi sang cái chuỗi này là một cái chuỗi khác.  Cái dạng chuỗi này thì nó cũng có thể tương tự để giải quyết cho các cái bài toán như là bài toán về tấm tắt văn bản, bài toán về chuyển đổi lại, tức là paraphrase một cái văn bản. Rồi bài toán thậm chí cả cả bài toán phân loại văn bản. Thì cái chuỗi đầu ra của mình nó có thể hiểu là một cái giá trị."
        },
        {
          "index": 4,
          "video_id": "Chương 8_4EdX3Ga9YoM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_1： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/4EdX3Ga9YoM",
          "start_time": 148,
          "end_time": 209,
          "text": "bài toán về tấm tắt văn bản, bài toán về chuyển đổi lại, tức là paraphrase một cái văn bản. Rồi bài toán thậm chí cả cả bài toán phân loại văn bản. Thì cái chuỗi đầu ra của mình nó có thể hiểu là một cái giá trị. Cái chuỗi này có độ dài là một. Như vậy thì cái tính tổng quát của nó nó là cao. Và đó là lý do mà tại sao chúng ta nghiên cứu về cái bài toán này. Bài toán dịch máy và dùng nó như là một cái kiến thức tổng quát để có thể sau này áp dụng những cái kiến thức về extension vào cho các cái bài toán khác. Thì định nghĩa bài toán dịch máy đó là một cái bài toán cho phép chuyển đổi từ một cái câu từ ngôn ngữ nguồn sang một cái ngôn ngữ khác. Ví dụ như ở đây chúng ta có đầu vào. Đầu vào sẽ là một cái câu thuộc cái văn bản là một cái ngôn ngữ đó là tiếng Anh. Và đầu ra của mình là một cái ngôn ngữ đó là tiếng Anh.  Đây chính là cái giá trị Routes. Thì nó sẽ ra là một cái văn bản tiếng Pháp."
        },
        {
          "index": 5,
          "video_id": "Chương 8_4EdX3Ga9YoM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_1： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/4EdX3Ga9YoM",
          "start_time": 196,
          "end_time": 259,
          "text": "Đầu vào sẽ là một cái câu thuộc cái văn bản là một cái ngôn ngữ đó là tiếng Anh. Và đầu ra của mình là một cái ngôn ngữ đó là tiếng Anh.  Đây chính là cái giá trị Routes. Thì nó sẽ ra là một cái văn bản tiếng Pháp. Thì cái hệ thống dịch máy là làm sao có thể thực hiện được cái việc chuyển đổi một cái câu từ cái ngôn ngữ tiếng Anh sang tiếng Pháp. Đây là một cái ví dụ ngôn ngữ. Nó còn hoàn toàn có thể chuyển đổi qua lại giữa tiếng Anh, tiếng Việt, tiếng Pháp, tiếng Tây Ban Nha, v.v. Và thậm chí là các cái hệ thống sau này có khả năng là dịch đa ngôn ngữ. Tức là chúng ta có thể từ một cái ngôn ngữ bất kỳ, có thể chuyển sang một cái ngôn ngữ bất kỳ khác. Thì đó là cái tầm nhìn về thiết kế các cái mô hình để cho phép các mô hình máy học để có thể dịch được rất nhiều cái ngôn ngữ qua lại với nhau. Và tiếp theo thì chúng ta sẽ nói về cái hướng tiếp cận Neural Machine Translation. Thì Neural tức là cái hướng tiếp cận sử dụng RNN là Recurrent Neural Network."
        },
        {
          "index": 6,
          "video_id": "Chương 8_4EdX3Ga9YoM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_1： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/4EdX3Ga9YoM",
          "start_time": 247,
          "end_time": 312,
          "text": "Và tiếp theo thì chúng ta sẽ nói về cái hướng tiếp cận Neural Machine Translation. Thì Neural tức là cái hướng tiếp cận sử dụng RNN là Recurrent Neural Network. Và Machine Translation là machine translation. Chính là cái tên của cái bài toán của mình. Cái tên bài toán cần giải. Thì đây là cái phương pháp sử dụng một cái mạng Neural Network, chính là RNN của mình. Từ đầu đến cuối, tức là end to end. Nghĩa là sao? Chúng ta sẽ fit, chúng ta sẽ đưa vào cái mạng Neural Network này, cái mạng RNN này, và nó sẽ tạo ra các cái giá trị output của mình, mà không có thực hiện những cái thao tác thực hiện trung gian. Qua những cái mô hình, những cái loại mô hình khác. Mà tất cả mọi thứ đều chỉ được thực hiện bởi duy nhất một cái mô hình đó là Neural Network, a Recurrent Neural Network. Và cái kiến trúc sử dụng ở đây, chính là cái kiến trúc là sequence to sequence, hay còn gọi là, gọi tắt là six to six."
        },
        {
          "index": 7,
          "video_id": "Chương 8_4EdX3Ga9YoM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_1： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/4EdX3Ga9YoM",
          "start_time": 295,
          "end_time": 363,
          "text": "Mà tất cả mọi thứ đều chỉ được thực hiện bởi duy nhất một cái mô hình đó là Neural Network, a Recurrent Neural Network. Và cái kiến trúc sử dụng ở đây, chính là cái kiến trúc là sequence to sequence, hay còn gọi là, gọi tắt là six to six. Thì đây sẽ là bao gồm hai cái thành phần, hai cái vai trò là encoder và decoder. Trong đó encoder nó thực hiện cái việc đọc và hiểu cái thông tin input đầu vào. Decoder là tạo sinh ra cái kết quả trả về. Tạo sinh ra cái kết quả. Và chúng ta sẽ đến với cái mô hình six to six theo cái animation như sau. Ở bên trái, nó sẽ là cái ký hiệu encoder RNN, tức là win những cái mô hình.  Mình sẽ nhấn vào cái cái phần màu xanh ở đây, chính là cái hidden state, cái trại thấy ẩn mà mình sẽ encode cái câu đầu vào của mình. Ví dụ như ở đây chúng ta có đưa một cái câu của một cái văn bản nguồn là I'm not sure."
        },
        {
          "index": 8,
          "video_id": "Chương 8_4EdX3Ga9YoM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_1： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/4EdX3Ga9YoM",
          "start_time": 348,
          "end_time": 410,
          "text": "Mình sẽ nhấn vào cái cái phần màu xanh ở đây, chính là cái hidden state, cái trại thấy ẩn mà mình sẽ encode cái câu đầu vào của mình. Ví dụ như ở đây chúng ta có đưa một cái câu của một cái văn bản nguồn là I'm not sure. Thì tại cái mũi tên này đó là cung cấp, nó là cái tổng hợp thông tin của toàn bộ nội dung của cái câu đầu vào. Và nó sẽ bắt đầu, nó sẽ bắt đầu, tiến hành cái quá trình decode, tức là giải mã. Rồi, nó sẽ phải truyền vào một cái ký tự đặc biệt. Nó sẽ truyền vào một ký tự đặc biệt. Thì ở đây mình dùng là từ start. Start là để cho hàm ý là có cái cái ý nghĩa của nó là bắt đầu thôi. Còn trong thực tế, lúc thực hành, chúng ta có thể sử dụng những cái ký tự đặc biệt. Ví dụ như là ký tự amok, ký tự amok hoặc là các ký tự đô la, yếu thăng. Ví dụ, đô la, rồi yếu thăng."
        },
        {
          "index": 9,
          "video_id": "Chương 8_4EdX3Ga9YoM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_1： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/4EdX3Ga9YoM",
          "start_time": 396,
          "end_time": 460,
          "text": "chúng ta có thể sử dụng những cái ký tự đặc biệt. Ví dụ như là ký tự amok, ký tự amok hoặc là các ký tự đô la, yếu thăng. Ví dụ, đô la, rồi yếu thăng. Nhưng mà lưu ý, đó là phải có cái sự đồng nhất. Ví dụ như trong toàn bộ những cái văn bản, trong cái cặp dữ liệu XI của mình, và trong cái quá trình huấn luyện, thậm chí là trong cái quá trình mà mình inference, tức là mình thực hiện, mình test cái mô hình, hoặc là triển khai thực tế đó, thì chúng ta đều phải thống nhất sử dụng chung một cái hệ thống ký hiệu này. Chứ không phải là lúc thực hiện, thì chúng ta sử dụng là alpha amok, lúc thì chúng ta sử dụng là đô la, thì không được. Rồi, thì khi chúng ta truyền vô một cái ký tự đặc biệt để đánh dấu, cái này là đánh dấu cái quá trình, đánh dấu cái quá trình decode. Đánh dấu cái quá trình decode, và nó sẽ tạo ra cái dự đoán,"
        },
        {
          "index": 10,
          "video_id": "Chương 8_4EdX3Ga9YoM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_1： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/4EdX3Ga9YoM",
          "start_time": 450,
          "end_time": 511,
          "text": "cái quá trình decode. Đánh dấu cái quá trình decode, và nó sẽ tạo ra cái dự đoán, cái dự đoán y ngã, đó chính là từ GER. Và ở đây chúng ta sẽ dùng cái hàm argument max, argument max, tại vì cái đầu ra của mình nó sẽ ra là một cái vector, nó sẽ ra một cái vector, rồi sau đó mình sẽ tìm coi là cái trọng số nào là cái trọng số lớn nhất, tương ứng với lại cái từ trong tiếng Pháp của mình, trong cái em lý tiếng tiếng Pháp của mình. Rồi, sau khi chúng ta có được cái từ GER, thì chúng ta sẽ truyền, nối tiếp, chúng ta sẽ đưa cái từ GER này như là một cái đầu vào cho cái bước tiếp theo, cái bước decode tiếp theo, truyền cái từ GER này, và đương nhiên là sử dụng cái thông tin của quá khứ để mà thực hiện cái hàm tính giá trị output, nó sẽ tạo ra cái từ là từ NE."
        },
        {
          "index": 11,
          "video_id": "Chương 8_4EdX3Ga9YoM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_1： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/4EdX3Ga9YoM",
          "start_time": 499,
          "end_time": 561,
          "text": "và đương nhiên là sử dụng cái thông tin của quá khứ để mà thực hiện cái hàm tính giá trị output, nó sẽ tạo ra cái từ là từ NE. Rồi cái từ NE này, nó sẽ là cái đầu vào cho cái mạng của mình, và nó sẽ tạo ra từ SUI, từ SUI này sẽ truyền vào là nó sẽ tạo ra từ PA, cứ như vậy. Và đến cái từ kết thúc cái quá trình decode, thì cái hệ thống này nó sẽ phải trả ra một cái từ đặc biệt, đó là N, và cũng tương tự như start, thì N này là để đánh dấu là chúng ta kết thúc kết thúc cái quá trình decode. Và chúng ta sẽ phải sử dụng một cái từ đặc biệt, ví dụ như là nếu ở đây đã sử dụng AMOC rồi, thì chúng ta có thể sử dụng là ví dụ là dấu than. Và phải có cái sự đồng nhất từ đầu đến cuối. Thì ở bên tay trái,"
        },
        {
          "index": 12,
          "video_id": "Chương 8_4EdX3Ga9YoM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_1： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/4EdX3Ga9YoM",
          "start_time": 550,
          "end_time": 610,
          "text": "thì chúng ta có thể sử dụng là ví dụ là dấu than. Và phải có cái sự đồng nhất từ đầu đến cuối. Thì ở bên tay trái, encoder thực hiện cái công việc đó là tổng hợp thông tin của toàn bộ cái code văn ngữ của mình. Còn ở phía bên tay phải là decoder lúc này nó đóng vai trò như là một cái mô hình ngôn ngữ, là một cái language model để tạo ra cái văn bản đích, tạo ra cái code văn đích, và dựa trên cái decoder này nó thực hiện được là dựa trên cái thông tin đã được tổng hợp từ cái code văn ngữ. Rồi, như vậy thì cái tính linh hoạt của C2C, nó sẽ thể hiện ở những cái, ví dụ sau. Đầu tiên, đó là bất cứ cái văn bản ở dạng chuỗi, cái input nào mà ở dạng chuỗi, và cái output nào ở dạng chuỗi, thì chúng ta đều có thể sử dụng được cái C2C này."
        },
        {
          "index": 13,
          "video_id": "Chương 8_4EdX3Ga9YoM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_1： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/4EdX3Ga9YoM",
          "start_time": 598,
          "end_time": 659,
          "text": "Đầu tiên, đó là bất cứ cái văn bản ở dạng chuỗi, cái input nào mà ở dạng chuỗi, và cái output nào ở dạng chuỗi, thì chúng ta đều có thể sử dụng được cái C2C này. Thì cái ý đầu tiên này nó minh họa cho cái việc là cái tính linh hoạt của C2C. Và do đó, thì một cái mạng neural network mà nhận cái input và tạo ra một cái vector biểu diễn. Một cái mạng neural network thì nhận cái input và tạo ra một cái vector biểu diễn. Rồi mạng neural network khác nó sẽ sinh ra cái output và từ cái vector biểu diễn trên. Thì thật ra đây chính là cái quá trình encode. Quá trình encode. Và đây là quá trình decode. Mạng neural sẽ nhận toàn bộ cái nội dung đầu vào, nội dung input. Và sau khi đã đọc hết toàn bộ thông tin đó, thì nó sẽ tạo ra một cái vector. Và cái vector này nó sẽ tích hợp toàn bộ thông tin của cái input. Toàn bộ thông tin của cái input."
        },
        {
          "index": 14,
          "video_id": "Chương 8_4EdX3Ga9YoM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_1： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/4EdX3Ga9YoM",
          "start_time": 648,
          "end_time": 710,
          "text": "Và sau khi đã đọc hết toàn bộ thông tin đó, thì nó sẽ tạo ra một cái vector. Và cái vector này nó sẽ tích hợp toàn bộ thông tin của cái input. Toàn bộ thông tin của cái input. Và với cái thông tin của cái input này, đó, sẽ sinh ra cái output từ cái vector biểu diễn trên. Tức là cái vector này. Nó sẽ sinh ra cái output. Và đây chính là cái quá trình decode. Và Cic2Cic thì không chỉ hiệu quả cho bài toán dịch máy, mà nó còn hiệu quả cho cả những cái bài toán khác. Ví dụ như là bài toán tóm tắt văn bản. Summarization. Bài toán tóm tắt văn bản là gì? Đầu vào của mình cũng sẽ là một chuỗi, một cái đoạn văn rất là dài. Và đầu ra của mình sẽ là một cái đoạn văn ngắn. Mô tả lại toàn bộ cái nội dung của cái đoạn văn dài. Tóm tắt lại cái nội dung chính của cái đoạn văn dài này. Cho bài toán hội thoại hay là dialog. Là input của mình"
        },
        {
          "index": 15,
          "video_id": "Chương 8_4EdX3Ga9YoM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_1： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/4EdX3Ga9YoM",
          "start_time": 697,
          "end_time": 760,
          "text": "Mô tả lại toàn bộ cái nội dung của cái đoạn văn dài. Tóm tắt lại cái nội dung chính của cái đoạn văn dài này. Cho bài toán hội thoại hay là dialog. Là input của mình sẽ là một cái lời thoại. Một cái lời thoại trước. Và output của mình sẽ là cái lời thoại sau. Giống như là khi chúng ta đặt cái mình sẽ chat với lại một cái combo. Thì mình sẽ cung cấp cho nó một cái lời thoại trước. Và nó sẽ trả lời mình. Nó sẽ trò chuyện với mình. Thì đây chính là cái lời thoại. Thì cái này phục vụ cho bài toán là chatbot. Bài toán phân tích cú pháp. Thì đầu vào của mình sẽ là một cái chuỗi. Đoạn văn. Một cái code văn. Và đầu ra của mình cũng sẽ là một cái chuỗi. Để mô tả cái cây cú pháp. Tương ứng với cái đoạn văn trên. Rồi bài toán sync code. Code generation. Thì đầu vào của mình sẽ là mô tả cái chức năng của một cái chương trình. Hoặc là chức năng của một thực toán mà mình đang muốn cài đặt. Và bài toán sinh code. Và đầu ra là nó sẽ tạo ra một cái mã nguồn."
        },
        {
          "index": 16,
          "video_id": "Chương 8_4EdX3Ga9YoM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_1： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/4EdX3Ga9YoM",
          "start_time": 750,
          "end_time": 782,
          "text": "Thì đầu vào của mình sẽ là mô tả cái chức năng của một cái chương trình. Hoặc là chức năng của một thực toán mà mình đang muốn cài đặt. Và bài toán sinh code. Và đầu ra là nó sẽ tạo ra một cái mã nguồn. Theo một cái ngôn ngữ nào đó. Mà chúng ta đã được chọn được. Ví dụ như mã nguồn cho ngôn ngữ python. Ví dụ như mã nguồn cho ngôn ngữ python. Như vậy thì toàn bộ nội dung của slide này. Nó thể hiện là cái tính linh hoạt. Nó thể hiện là cái tính linh hoạt. Của sys2s, sys2s. Có thể xử lý được bất cứ cái bài toán nào.  Mà đầu vào của mình là ở dạng chuỗi. Và đầu ra của mình cũng ở dạng chuỗi."
        }
      ]
    },
    {
      "video_id": "Chương 8_--JpgsDEL40",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giới thiệu bài toán Neural Machine Translation (dịch máy sử dụng ANN/ARN), quy trình huấn luyện, hàm loss, kiến trúc Deep Stack Encoder (mô hình nhiều tầng), và cách đánh giá chất lượng dịch máy (độ đo BLEU — được ghi là \"blur\"/bilingual evaluation understudy) cùng lịch sử chuyển dịch từ phương pháp thống kê sang neural. [1][4][5][6][8]\n\n- Các khái niệm sẽ được đề cập: \n  - Quy trình dự đoán từng từ và tính loss theo từng time-step (cross-entropy loss) trong NMT. [1][2][3]  \n  - Lan truyền lỗi ngược (backpropagation) qua các bước thời gian và cập nhật ma trận trọng số U, V, W. [3][4]  \n  - Kiến trúc DeepStack Encoder (stacking các layer encoder). [4][5]  \n  - Sự chuyển đổi công nghiệp sang Neural Machine Translation (các mốc lịch sử, ví dụ Google Translate, Bing). [5][6][7]  \n  - Độ đo BLEU (ghi là blur trong video), cách tính dựa trên n-gram precision trung bình điều hòa và hạn chế của nó. [8][9][10]\n\n(Trích dẫn tương ứng: [1], [2], [3], [4], [5], [6], [7], [8], [9], [10])\n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Quy trình huấn luyện NMT: dự đoán theo time-step và tính loss\n- Trong quá trình huấn luyện NMT, khi đưa vào một câu nguồn (ví dụ \"I'm not sure\"), mô hình dự đoán từng từ xuất ra theo chuỗi; với mỗi time-step mô hình sinh ra một dự đoán cho từ tiếp theo và ta tính một loss tương ứng cho từ đó. [1]  \n- Các loss thành phần này sử dụng **Cross-Entropy Loss** (được nói là \"Gross Entropy Loss\") giữa vector dự đoán và vector biểu diễn của từ mục tiêu. [2]  \n- Nếu dự đoán khớp với từ đúng (ví dụ từ Pa trong tiếng Pháp) thì loss thấp; ngược lại loss cao; tương tự với ký tự kết thúc End. [2][3]\n\n- Công thức tổng quát hàm loss (theo mô tả trong video):  \n  L = (1/T) * sum_{t=1..T} CE(y_t, ŷ_t)  \n  (tức trung bình cộng của các loss thành phần tại mỗi time-step, trong đó CE là cross-entropy giữa từ mục tiêu y_t và dự đoán ŷ_t). [3]\n\n(Citations: [1][2][3])\n\n### 2.2. Backpropagation qua thời gian và cập nhật trọng số\n- Mỗi loss thành phần sẽ thực hiện lan truyền ngược (backpropagation); loss thứ 1 lan truyền ngược, loss thứ 2 lan truyền ngược, ... và toàn bộ các loss được cộng lại để cập nhật các ma trận trọng số của mạng (ví dụ U, V, W là các ma trận trọng số được cập nhật). [3][4]  \n- Quá trình này cho phép cập nhật các tham số của ANN sao cho mô hình học được các đặc trưng cần thiết cho dịch máy. [3][4]\n\n(Citations: [3][4])\n\n### 2.3. Kiến trúc DeepStack Encoder (mạng nhiều tầng)\n- Để mô hình học được đặc trưng ở nhiều mức (cấp thấp, trung, cao), người ta sử dụng kiến trúc *DeepStack Encoder* — các layer xếp chồng: đầu ra của layer y làm đầu vào cho layer y+1 (layer 1 → layer 2 → layer 3, ...). [4][5]  \n- DeepStack Encoder là một biến thể của mô hình \"6 to 6\" được đề xuất (đề cập trong video), giúp giải quyết bài toán dịch máy bằng cách cho phép học đặc trưng sâu hơn. [5]\n\n(Citations: [4][5])\n\n### 2.4. Lịch sử, thành tựu và ứng dụng công nghiệp của NMT\n- Thành tựu: Sau đề xuất mô hình \"6 to 6\" (năm 2014, tác giả ghi là \"Schick Cover\" trong video), trong vòng khoảng 2 năm Google Translate chuyển sang dùng Neural Machine Translation và nhiều công ty (ví dụ Bing Translate của Microsoft) cũng chuyển đổi sang mô hình NMT. [5][6]  \n- Đến ~2018, hầu như tất cả các dịch vụ dịch thuật lớn đã chuyển sang sử dụng NMT, cho thấy bước đột phá cả về thuật toán và ứng dụng công nghiệp. Video dùng thuật ngữ \"Cytosix\" để chỉ mô hình này và nhấn mạnh khả năng mở rộng với nhiều ngôn ngữ và cập nhật khi có từ khóa mới. [6][7]\n\n(Citations: [5][6][7])\n\n### 2.5. Đánh giá chất lượng dịch máy: độ đo \"blur\" (BLEU)\n- Đánh giá dịch máy là vấn đề khó vì một câu nguồn có nhiều cách dịch đúng khác nhau (phong cách chuyên môn, độ tuổi người dịch, v.v.). [7][8]  \n- Một phương pháp phổ biến được trình bày là độ đo *blur* (viết tắt của \"bilingual evaluation understudy\" trong video), so sánh bản dịch máy với một hoặc nhiều bản dịch tham chiếu của chuyên gia để tăng tính khách quan. [8][9]  \n- Cách tính: sử dụng trung bình điều hòa của các n-gram precision (n-gram precision cho n = 1..N), không chỉ so sánh từng từ mà so sánh các n-gram (ví dụ unigram, bigram, 4-gram, ...), và có thể lấy trung bình qua nhiều bản dịch tham chiếu. [9][11]  \n- Hạn chế: BLEU/“blur” không hoàn hảo — có bias do phụ thuộc bản dịch tham chiếu của chuyên gia; một câu có nhiều cách dịch tốt nhưng ít trùng n-gram với tham chiếu có thể bị cho điểm thấp. Tuy nhiên hiện tại BLEU vẫn là độ đo tin cậy để so sánh tương đối giữa các phương pháp (phương pháp A tốt hơn B hay không). [10][11]\n\n(Citations: [7][8][9][10][11])\n\n### 2.6. Xu hướng kết quả thực nghiệm theo thời gian\n- Dữ liệu lịch sử cho thấy sau 2014 (mốc ra đời \"syscochecker\"/mô hình nêu) từ 2015 trở về sau, hướng tiếp cận NMT dựa trên ANN đã cho tốc độ tăng trưởng đáng kể về độ chính xác (đường tăng dốc mạnh), trong khi các phương pháp thống kê (phrase-based, syntax-based) vẫn có tăng nhưng nhẹ hơn; từ ~2017 trở đi hầu như các nghiên cứu không còn theo hướng phrase/syntax-based mà chuyển sang NMT. [12][13][14][15]\n\n(Citations: [12][13][14][15])\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa tính n-gram khi tính BLEU:\n  - Video đưa ví dụ so sánh một bản dịch máy với bản dịch người (tham chiếu). Ta thấy các n-gram trùng khớp như \"after the\" (bigram), các unigram như \"attack\", và với bản dịch khác có cụm \"international airport and is\" là một n-gram dài (4-gram) — từ đó tính n-gram precision và trung bình điều hòa qua các bản dịch tham chiếu. [11][12]  \n- Ứng dụng thực tế:\n  - Google Translate chuyển sang NMT ~2 năm sau đề xuất ban đầu; Bing Translate (Microsoft) cũng chuyển sang NMT; đến ~2018 hầu hết dịch vụ dịch thuật công nghiệp áp dụng NMT. Điều này thể hiện NMT đã được ứng dụng rộng rãi trong công nghiệp do hiệu quả và khả năng mở rộng. [5][6][7]  \n- Trường hợp sử dụng:\n  - Dịch máy cho nhiều ngôn ngữ khác nhau, cập nhật khi xuất hiện từ khóa mới, áp dụng trong dịch vụ trực tuyến (Google Translate, Bing). [6][7]\n\n(Citations: [11][12][5][6][7])\n\n---\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Huấn luyện NMT là quá trình dự đoán từng từ theo time-step, tính cross-entropy loss trên từng bước, lấy trung bình các loss thành phần làm hàm loss tổng và dùng backpropagation để cập nhật trọng số U, V, W. [1][2][3][4]  \n  - DeepStack Encoder (mạng nhiều tầng) giúp học đặc trưng từ thấp đến cao; biến thể \"6 to 6\" là một mốc quan trọng. [4][5]  \n  - NMT đã tạo ra bước nhảy vọt so với phương pháp thống kê, được triển khai vào Google Translate, Bing và rộng rãi trong công nghiệp ~2015–2018. [5][6][7]  \n  - Đánh giá dịch dùng độ đo BLEU (ghi là blur trong video) dựa trên n-gram precision trung bình điều hòa; BLEU hữu dụng để so sánh tương đối nhưng không hoàn hảo do phụ thuộc bản dịch tham chiếu. [8][9][10][11]\n\n- Tầm quan trọng của nội dung: Bài giảng cho thấy cơ chế huấn luyện và đánh giá của NMT, lý do NMT đã thay thế các phương pháp thống kê trong thực tế, và các giới hạn hiện tại của độ đo đánh giá (BLEU). Hiểu rõ những điểm này giúp nắm bản chất kỹ thuật và ứng dụng của phương pháp dịch máy hiện đại. [1][4][5][6][8][10]\n\n- Liên hệ với các bài giảng khác: Video đề cập mốc lịch sử và sự chuyển đổi phương pháp (từ thống kê sang neural) — thông tin này liên quan đến các bài giảng về kiến trúc mô hình (encoder-decoder, RNN/ARN), kỹ thuật huấn luyện sâu (stacking layers, backpropagation), và các lecture về đánh giá mô hình (độ đo như BLEU). [3][4][5][8][9]\n\n(Trích dẫn tổng hợp: [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15])\n\n---\n\nGhi chú: Tóm tắt trên chỉ sử dụng thông tin có trong các đoạn trích được cung cấp (với các thuật ngữ và tên gọi xuất hiện trong video). Các chỗ dùng thuật ngữ như \"Gross Entropy Loss\", \"blur\", \"6 to 6\", \"Schick Cover\", \"Cytosix\"/\"syscochecker\" được giữ nguyên theo lời giảng trong video.",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 8_--JpgsDEL40",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_2： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/--JpgsDEL40",
          "start_time": 1,
          "end_time": 60,
          "text": "và cái quá trình huấn luyện một cái mô hình là Neural Machine Translation tức là cái bài toán về dịch máy mà có sử dụng ARN đó là khi chúng ta đưa vào các cái giá trị input ví dụ như là đưa vào là I'm not sure thì bắt đầu cái quá trình tính toán nó sẽ tạo ra cái giá trị dự đoán và dựa trên cái route dựa trên cái route thì chúng ta sẽ tính ra được cái loss chúng ta sẽ tính ra được cái loss cho cái từ đầu tiên của cái đoạn văn rồi sau đó đến cái từ thứ 2 chúng ta sẽ có cái loss thứ 2 từ thứ 3 sẽ có cái loss thứ 3 đến từ thứ 6 chúng ta sẽ ra được cái loss thứ 6 và tổng hợp và lưu ý đó là các cái loss thành phần này thì nó được sử dụng là Gross Entropy Loss của cái từ G dựa dựa    tức là đối với cái văn bản nít"
        },
        {
          "index": 2,
          "video_id": "Chương 8_--JpgsDEL40",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_2： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/--JpgsDEL40",
          "start_time": 49,
          "end_time": 112,
          "text": "và tổng hợp và lưu ý đó là các cái loss thành phần này thì nó được sử dụng là Gross Entropy Loss của cái từ G dựa dựa    tức là đối với cái văn bản nít là I ở đây thì lấy ra là chúng ta phải trả ra từ G đây là route thì ở đây cái dự đoán I này nó sẽ đi so với lại cái vector biểu diện của từ G để xem xem là 2 cái vector đó nó có tương đồng với nhau hay không và để tính được cái size số đó thì chúng ta sẽ sử dụng bộ đo là Gross Entropy Loss và trong trường hợp này là Gross Entropy Loss cho cái từ G rồi tương tự như vậy đến cái thứ 4 thì chúng ta sẽ là phải đưa ra được cái giá trị dự đoán và nếu như cái giá trị dự đoán này nó khớp với lại cái từ Pa trong tiếng Pháp đúng không thì cái loss của mình sẽ rất là thấp nhưng mà nếu mà nó không khớp thì cái loss của mình nó sẽ là rất là cao rồi tương tự như vậy đến cái từ cuối cùng lẽ ra mình sẽ phải trả về cái ký tự kết thúc"
        },
        {
          "index": 3,
          "video_id": "Chương 8_--JpgsDEL40",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_2： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/--JpgsDEL40",
          "start_time": 99,
          "end_time": 160,
          "text": "nhưng mà nếu mà nó không khớp thì cái loss của mình nó sẽ là rất là cao rồi tương tự như vậy đến cái từ cuối cùng lẽ ra mình sẽ phải trả về cái ký tự kết thúc đó là End đúng không nhưng mà mình lại trả ra một cái từ khác thì rõ ràng là cái độ cái cái cái loss của mình sẽ là cao đó thì tổng hợp toàn bộ các cái loss cho tương ứng với cái cái time step các cái thời điểm thì mình sẽ ra được là hàm loss như sau đây là trung bình cộng của các cái loss thành phần và trong quá trình huấn luyện thì từng cái loss thành phần này sẽ thực hiện thuật toán loan truyền ngược để cập nhật các cái trọng số ở trên cái mô hình ANN đó với loss số 1 nó sẽ lan truyền cái độ lỗi lan truyền cái độ lỗi ngược loss số 2 sẽ lan truyền và toàn bộ các cái loss này sẽ được đưa lên"
        },
        {
          "index": 4,
          "video_id": "Chương 8_--JpgsDEL40",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_2： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/--JpgsDEL40",
          "start_time": 149,
          "end_time": 212,
          "text": "nó sẽ lan truyền cái độ lỗi lan truyền cái độ lỗi ngược loss số 2 sẽ lan truyền và toàn bộ các cái loss này sẽ được đưa lên lan truyền trên suyên suốt toàn bộ cái mạng của mình và nó sẽ cập nhật các cái ma trọng UVW ví dụ như đây là V đây là W đây là U output của mình thì nó sẽ là V nó sẽ cập nhật các cái ma trọng UVW  và các cái ma trọng trọng số này và để cho cái bài toán để cho cái mô hình này của mình có khả năng học được những cái đặc trưng cấp cao hơn thì chúng ta sẽ sử dụng cái kiến trúc đó là DeepStack Encoder thì cái kiến trúc DeepStack Encoder này thì đầu ra của cái layer thứ y sẽ là đầu vào của cái layer thứ y cộng 1 tức là đây là cái layer số 1 tức là đây là cái layer số 1 nó sẽ là đầu vào cho cái layer số 2"
        },
        {
          "index": 5,
          "video_id": "Chương 8_--JpgsDEL40",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_2： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/--JpgsDEL40",
          "start_time": 199,
          "end_time": 261,
          "text": "sẽ là đầu vào của cái layer thứ y cộng 1 tức là đây là cái layer số 1 tức là đây là cái layer số 1 nó sẽ là đầu vào cho cái layer số 2 layer số 2 sẽ là đầu vào cho cái layer số 3 thì đây là một cái biến thể của 6 to 6 nhằm giúp cho chúng ta giải quyết được bài toán dịch máy mà với các cái đặc trưng có thể học được qua các cái tầng từ tầng cấp thấp cho đến tầng cấp giữa cho đến tầng cấp cao và thành tựu của Neural Machine Translation đó là nếu như năm 2014 Schick Cover và các công sự đã đề xuất ra cái 6 to 6 thì ngay sau đó chỉ 2 năm tức là với cái sự phát triển rất là nhanh thì chỉ sau 2 năm là Google Translate đã sử dụng và đã chuyển toàn bộ các cái mô hình dịch máy theo cái hướng tiếp cận truyền thống"
        },
        {
          "index": 6,
          "video_id": "Chương 8_--JpgsDEL40",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_2： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/--JpgsDEL40",
          "start_time": 250,
          "end_time": 312,
          "text": "tức là với cái sự phát triển rất là nhanh thì chỉ sau 2 năm là Google Translate đã sử dụng và đã chuyển toàn bộ các cái mô hình dịch máy theo cái hướng tiếp cận truyền thống đó là học thống kê sang cái hướng đó là Neural Machine Translation sang cái dự án đó là Neural Machine Translation cái dự án đó là dùng ANN và sau đó cũng 2 năm là đến năm 2018 thì gần như tất cả các cái công ty nào mà có sử dụng các cái dịch vụ dịch thuật thì đều chuyển đổi sử dụng sang cái mô hình này ví dụ như là Bing Translate của Microsoft cũng vậy cũng đã chuyển sang là sử dụng các cái mô hình về Neural Machine Translation như vậy thì điều đó có thể nói là trong một cái thời gian rất ngắn Cytosix đã tạo ra được một cái bước đột phá cả về trong hợp thuật lẫn trong lĩnh vực về công nghiệp về công nghệ và các cái công ty công nghệ thì đã chuyển đổi hoàn toàn sang cái mô hình Cytosix này"
        },
        {
          "index": 7,
          "video_id": "Chương 8_--JpgsDEL40",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_2： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/--JpgsDEL40",
          "start_time": 297,
          "end_time": 363,
          "text": "Cytosix đã tạo ra được một cái bước đột phá cả về trong hợp thuật lẫn trong lĩnh vực về công nghiệp về công nghệ và các cái công ty công nghệ thì đã chuyển đổi hoàn toàn sang cái mô hình Cytosix này thì điều đó chứng tỏ là cái tính hiệu quả của mô hình này và đồng thời là nó có khả năng dễ dàng mở rộng cho rất nhiều những cái ngôn ngữ khác nhau cũng như là sau này khi có những cái từ khóa mới cũng như là sau này khi có những cái từ khóa mới thì nó cũng có thể dễ dàng học và cập nhật lại được thì đó chính là cái thành tựu của Neural Machine Translation và để đánh giá được cái mô hình dịch máy thì đây là một trong những cái bài toán khó trong cái việc là đánh giá tại vì một cái bản dịch của mình một cái văn bản nguồn của mình thì nó có khả năng nhiều cái cách dịch khác nhau ví dụ như cũng một cái câu đó nhưng mà một cái người theo chuyên ngành về khoa học thì họ sẽ dịch theo một phong cách và người theo chuyên ngành về xã hội thì sẽ dịch theo một cách"
        },
        {
          "index": 8,
          "video_id": "Chương 8_--JpgsDEL40",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_2： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/--JpgsDEL40",
          "start_time": 346,
          "end_time": 409,
          "text": "một cái văn bản nguồn của mình thì nó có khả năng nhiều cái cách dịch khác nhau ví dụ như cũng một cái câu đó nhưng mà một cái người theo chuyên ngành về khoa học thì họ sẽ dịch theo một phong cách và người theo chuyên ngành về xã hội thì sẽ dịch theo một cách hoặc là một người trẻ và một người lớn tuổi họ có thể dịch theo một cái cách khác nhau do đó thì đánh giá một cái mô hình dịch máy thì đây là một cái cách, đây là một cái vấn đề khó nhưng mà khó thì không có nghĩa là không có giải pháp và một trong những cái giải pháp mà phổ biến hiện nay để mà có thể đánh giá được cái mô hình dịch máy của mình nó có tốt không?  đó là sử dụng độ đo blur blur là viết tắt của chữ bilingual evaluation understudy thì blur so sánh cái phiên bản dịch máy với một hoặc là nhiều một thì tự nhiên rồi nhưng mà nó phải là để tăng cái tính khách quan thì nó nên là so với nhiều cái bản dịch khác nhau"
        },
        {
          "index": 9,
          "video_id": "Chương 8_--JpgsDEL40",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_2： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/--JpgsDEL40",
          "start_time": 396,
          "end_time": 460,
          "text": "thì blur so sánh cái phiên bản dịch máy với một hoặc là nhiều một thì tự nhiên rồi nhưng mà nó phải là để tăng cái tính khách quan thì nó nên là so với nhiều cái bản dịch khác nhau so với nhiều cái bản dịch khác nhau của các cái chuyên gia lưu ý là cái bản dịch này cũng phải là của chuyên gia nha chứ còn những người mà không chuyên về ngôn ngữ thì có thể là sẽ dịch không tốt và sau đó thì sẽ tính được cái độ tương đồng giữa cái bản dịch với lại cái bản của chuyên gia và ở đây cái cách mà người ta so sánh đó là sử dụng trung bình điều hòa của các cái n-ramp precision tức là thay vì chúng ta chỉ so với từng chữ thì ở đây chúng ta sẽ so với cộng n-r chữ ví dụ như là bản dịch là một từ rồi của chuyên gia đó là một cái n-ramp mà ba từ đây là một cái n-ramp hai từ và đây là một cái n-ramp một từ đó thì nó sẽ tính trung bình cho cái n-ramp precision trung bình điều hòa"
        },
        {
          "index": 10,
          "video_id": "Chương 8_--JpgsDEL40",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_2： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/--JpgsDEL40",
          "start_time": 447,
          "end_time": 512,
          "text": "rồi của chuyên gia đó là một cái n-ramp mà ba từ đây là một cái n-ramp hai từ và đây là một cái n-ramp một từ đó thì nó sẽ tính trung bình cho cái n-ramp precision trung bình điều hòa và blur thì mặc dù là hiệu quả nhưng mà không có thật sự là hoàn hảo nó cũng không hoàn hảo tại vì sao tại vì nó sẽ bị bị bias hay là bị chủ quan bởi các cái chuyên gia của mình và như đã đề cập á là dịch máy nó có rất nhiều cái cách dịch khác nhau rất là biển chuyển mình không thể cố định được một cái cách dịch rồi chưa kể là cái yếu tố về tính nhập nhầm của ngôn ngữ nữa thế thì có nhiều cái bản dịch tốt hệ quả đó là gì có nhiều cái bản dịch tốt nhưng mà blur thì lại cho cái score thấp và cái chuyện này thì cũng không phải là hiếm chuyện này cũng không phải là hiếm sẽ ra tuy nhiên cho tới điểm hiện tại thì blur là một trong những cái độ đo đánh giá mà tin cậy nó không hoàn hảo nhưng mà nó vẫn có khả năng thể hiện được cái sự đối sánh"
        },
        {
          "index": 11,
          "video_id": "Chương 8_--JpgsDEL40",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_2： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/--JpgsDEL40",
          "start_time": 499,
          "end_time": 558,
          "text": "tuy nhiên cho tới điểm hiện tại thì blur là một trong những cái độ đo đánh giá mà tin cậy nó không hoàn hảo nhưng mà nó vẫn có khả năng thể hiện được cái sự đối sánh tương đối giữa các cái phương pháp dịch máy với nhau nó thể hiện được cái sự so sánh tương đối ví dụ như phương pháp này tốt hơn phương pháp kia thì cái blur này nó sẽ tốt hơn cái blur kia tuy nhiên tuy nhiên nếu mà cái giá trị blur đó nó có thể hiện được là cái bản dịch này là thật sự đúng không  là tốt hay không thì nó chưa thể hiện được nhưng nó có thể giúp cho chúng ta so được là phương pháp này nó có tốt hơn phương pháp kia hay không rồi và cái nguyên nhân cho cái việc là score thấp đó chính là có ít số lượng cái nram trùng với lại cái bản dịch của chuyên gia thì nếu như chúng ta đưa ra một cái bản dịch mà nó không so khớp được nó không khớp từ nào với lại các chuyên gia thì nó sẽ có cái score thấp"
        },
        {
          "index": 12,
          "video_id": "Chương 8_--JpgsDEL40",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_2： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/--JpgsDEL40",
          "start_time": 550,
          "end_time": 609,
          "text": "thì nếu như chúng ta đưa ra một cái bản dịch mà nó không so khớp được nó không khớp từ nào với lại các chuyên gia thì nó sẽ có cái score thấp và đây là một cái phương pháp này cho các bạn một cái ví dụ đây là bản dịch của máy và đây là bản dịch của một người thì chúng ta sẽ thấy là cái từ the ở đây nó sẽ so khớp nè after the tức là nram trong trường hợp này là n là bằng 2 nè rồi attack là 1 nè nram là trong trường hợp này n là bằng 1 nè rồi so với lại cái bản dịch đối với người thứ hai thì chúng ta thấy là cái cụm từ international airport and is thì ở đây là một cái bản dịch nram đây là cái so khớp nram với n là bằng 4 nè rồi tương tự như vậy cho 4 cái bản dịch và từ đó thì chúng ta sẽ tính ra được cái score trung bình trung bình điều hòa cho 4 cái bản dịch này và theo dòng thời gian"
        },
        {
          "index": 13,
          "video_id": "Chương 8_--JpgsDEL40",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_2： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/--JpgsDEL40",
          "start_time": 597,
          "end_time": 660,
          "text": "và từ đó thì chúng ta sẽ tính ra được cái score trung bình trung bình điều hòa cho 4 cái bản dịch này và theo dòng thời gian thì nếu như trước năm 2015 tức là cái năm 2014 là cái sự ra đời của syscochecker thì đến năm 2015 trở về sau á là các cái hướng tiếp cận của neural machine translation tức là sử dụng ANN á dựa trên ANN á thì nó cho cái tốc độ tăng trưởng cho cái sự gia tăng về cái độ chính xác tăng lên rất là nhiều và chúng ta có thể thấy là cái độ dốc cái độ dốc của cái đường màu xanh đậm này ha làm nó đi rất là dốc tức là cái sự tăng trưởng của đường màu xanh đậm này ha  làm nó đi rất là dốc tức là cái sự tăng trưởng của đường màu xanh đậm này ha về cái độ chính xác của nó trong khi đó các cái hướng tiếp cận mà dựa trên thống kê statistics dựa trên thống kê ví dụ như ở đây là có 2 hướng là dựa trên syntax"
        },
        {
          "index": 14,
          "video_id": "Chương 8_--JpgsDEL40",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_2： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/--JpgsDEL40",
          "start_time": 649,
          "end_time": 705,
          "text": "về cái độ chính xác của nó trong khi đó các cái hướng tiếp cận mà dựa trên thống kê statistics dựa trên thống kê ví dụ như ở đây là có 2 hướng là dựa trên syntax và dựa trên phrase thì chúng ta thấy là cũng có tăng trưởng nhưng mà tăng trưởng rất là thấp cái độ dốc của nó rất là thấp tức là không có cái sự tranh lệch gì nhiều và cũng từ 2017 trở về sau là chúng ta cũng thấy là không còn nhiều cái cái nghiên cứu mà sử dụng phrase-based nhiều cái cái nghiên cứu mà sử dụng phrase-based hoặc là syntax-based machine translation theo cái hướng tiếp cận thống kê nữa mà chúng ta chỉ còn các cái hướng tiếp cận cho sử dụng là neural machine translation mà thôi thì điều này cho thấy là cái tầm ảnh hưởng của cái hướng tiếp cận neural machine translation và nó đã đánh mật những cái phương pháp truyền thống trước đây để tạo ra một cái hướng đi mới và hiệu quả hơn và thậm chí là đã có thể ứng dụng được trong công nghiệp ngôi vụ này"
        },
        {
          "index": 15,
          "video_id": "Chương 8_--JpgsDEL40",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 1_2： Giới thiệu bài toán Dịch máy",
          "video_url": "https://youtu.be/--JpgsDEL40",
          "start_time": 699,
          "end_time": 705,
          "text": "và hiệu quả hơn và thậm chí là đã có thể ứng dụng được trong công nghiệp ngôi vụ này"
        }
      ]
    },
    {
      "video_id": "Chương 8_ROIgZ5tyDFo",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giải thích **cơ chế Attention** trong kiến trúc Sequence-to-Sequence để khắc phục vấn đề thông tin bị dồn (bottleneck) và bị quên khi truyền tuần tự trong các mạng dịch máy / NLP. [1][2][3]\n\n- Các khái niệm sẽ được đề cập:\n  - Vấn đề *bottleneck* trong encoder-decoder thuần túy (toàn bộ thông tin nguồn bị nén vào một vector) và hệ quả quên thông tin. [1][2][3]\n  - Cách tính *attention score* giữa trạng thái decoder và các trạng thái encoder. [4][5]\n  - Chuẩn hóa các score thành *attention distribution* (không gian xác suất). [6][10]\n  - Tổng hợp có trọng số (weighted sum) tạo *attention output* và kết hợp với trạng thái decoder để dự đoán. [8][9][10]\n  - Minh họa bằng ví dụ dịch (cặp từ tiếng Anh -> tiếng Pháp) để thấy hiệu quả của Attention. [11][12][13]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 Vấn đề bottleneck và sự mất thông tin trong Seq2Seq\n- Trong kiến trúc encoder-decoder truyền thống, toàn bộ nội dung câu nguồn bị nén vào một vector cuối cùng của encoder — dẫn đến **bottleneck**: thông tin bị dồn và có khả năng bị mất khi lan truyền theo chuỗi. [1][2]\n- Do quá nhiều lần biến đổi tuần tự (qua nhiều bước RNN/LSTM/GRU hoặc tầng sâu), thông tin từ các token đầu dễ bị *loãng* và *bị quên*, làm giảm độ chính xác khi decoding các token cuối. Đây là vấn đề cố hữu dù có dùng LSTM/GRU, bidirectional hay deep stacking. [2][3]\n\n### 2.2 Ý tưởng cơ bản của Attention\n- Thay vì chỉ dùng một vector nén, khi bắt đầu mỗi bước decode ta tính **attention score** giữa trạng thái ẩn hiện tại của decoder và tất cả các trạng thái ẩn của encoder, để xác định “những từ nguồn nào cần *để tâm*” ở thời điểm đó. [4][5]\n- Cách tính sơ bộ: dùng phép đo độ tương đồng (ví dụ *tích vô hướng* / dot product) giữa trạng thái decoder và từng trạng thái encoder để ra các giá trị scalar gọi là score. [5][10]\n  - Ví dụ (ký hiệu): score_i = s_t · h_i  (score giữa trạng thái decoder tại bước t và trạng thái encoder thứ i). [5][10]\n\n### 2.3 Chuẩn hóa thành Attention Distribution\n- Các score thu được cần được **chuẩn hóa** về một không gian xác suất (giá trị trong đoạn 0..1, tổng bằng 1) để dễ so sánh và làm trọng số. Video gọi bước này là tính *Attention Distribution* (hàm chủ hóa về không gian xác suất). [6][10]\n  - Ký hiệu tổng quát: alpha_i = normalize(score_i)  (normalize đưa các score về phân bố xác suất). [6][10]\n\n### 2.4 Tính Attention Output (context vector) và sử dụng để dự đoán\n- Khi có attention distribution (các alpha_i), ta **tổng hợp có trọng số** các vector trạng thái encoder để tạo ra *attention output* (còn gọi là context vector):  \n  context_t = sum_i alpha_i * h_i. [8][9]\n- Attention output chứa chủ yếu thông tin từ những vị trí nguồn được chú ý (có alpha lớn) và giảm ảnh hưởng của thông tin thừa (alpha nhỏ). [9]\n- Attention output sau đó được phối hợp (kết hợp) với trạng thái ẩn decoder tại bước hiện tại để thu được thông tin đầy đủ hơn phục vụ cho việc dự đoán token tiếp theo. [9][10]\n\n### 2.5 Lợi ích trực tiếp của Attention\n- Giảm hiện tượng quên thông tin gốc do bottleneck: thay vì phải truyền toàn bộ thông tin qua một vector duy nhất, decoder có thể *lấy trực tiếp* thông tin từ mọi vị trí trong encoder theo nhu cầu từng bước. [1][4][9]\n- Giúp mô hình “chú ý” vào các token nguồn có liên quan cho từng vị trí dịch, do đó cải thiện độ chính xác dịch từng token. [6][7][8]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa (từ video): Câu tiếng Anh gồm các từ \"I\", \"am\", \"not\", \"sure\". Trong encoder-decoder truyền thống, thông tin từ các từ này bị dồn vào một vector dẫn đến việc khi decode đến từ cuối thông tin từ \"I\" hoặc \"am\" có thể đã bị loãng/forget. [1][2][3]\n\n- Minh họa cơ chế Attention qua từng bước decode:\n  - Khi decoder cần sinh một token ở vị trí nhất định, nó tính attention scores so sánh trạng thái decoder với các trạng thái encoder (các token nguồn). Nếu score tại vị trí tương ứng với \"I\" lớn hơn, attention distribution sẽ cho trọng số lớn cho \"I\", nghĩa là context vector sẽ chứa nhiều thông tin từ \"I\" tại bước đó. Điều này giúp dự đoán tại vị trí decode đó chính xác hơn so với việc toàn bộ thông tin bị chi phối bởi từ không liên quan hơn (ví dụ \"sure\"). [5][6][7][8]\n  - Cụ thể trong ví dụ dịch sang tiếng Pháp: khi decoder sinh từ tương ứng với phần phủ định, mô hình sẽ tập trung vào token tiếng Anh tương ứng (ví dụ \"not\") để dịch thành các phần phủ định tiếng Pháp (\"ne\", \"pa\" trong video), còn khi sinh động từ tương ứng thì attention sẽ tập vào \"am\". Video trình bày rằng attention distribution đổi theo từng bước decode — ví dụ khi sinh token \"ne\" mô hình chú ý nhiều đến \"not\" và khi sinh token cho động từ thì chú ý nhiều đến \"am\". [11][12][13]\n- Ứng dụng thực tế: cơ chế Attention được dùng rộng rãi trong dịch máy (machine translation) và các bài toán NLP khác để cho phép mô hình tập trung vào các phần quan trọng của input ở từng bước đầu ra. (Ví dụ trong video là minh họa cho song ngữ Anh->Pháp). [11][12][13]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Seq2Seq truyền thống gặp *bottleneck* do nén toàn bộ câu nguồn vào một vector, dẫn đến mất/loãng thông tin khi lan truyền tuần tự. [1][2][3]\n  - Attention giải quyết vấn đề này bằng cách cho phép decoder tính tương đồng với mọi trạng thái encoder, chuẩn hóa thành phân bố xác suất, và tổng hợp có trọng số các trạng thái encoder để tạo context phù hợp cho từng bước decode. [4][5][6][8][9][10]\n  - Cơ chế này làm giảm thông tin thừa, tăng trọng số cho các token liên quan, và cải thiện độ chính xác dịch — được minh họa rõ qua ví dụ dịch các token \"I/am/not/sure\" sang cấu trúc phủ định tiếng Pháp (\"ne\", \"pa\", v.v.). [7][11][12][13]\n\n- Tầm quan trọng: Attention là một bước tiến quan trọng trong xử lý sequence, cho phép mô hình truy cập thông tin nguồn theo cách có chọn lọc thay vì phụ thuộc hoàn toàn vào một vector ngưng tụ duy nhất. Điều này là nền tảng cho nhiều kiến trúc hiện đại trong Deep Learning cho NLP. [1][4][9]\n\n- Liên hệ với các bài giảng khác: Video nhắc lại rằng dù sử dụng các biến thể RNN như LSTM/GRU hay kiến trúc bidirectional/deep stack, vấn đề quên thông tin vẫn tồn tại nếu không có Attention; do đó Attention là một bổ sung quan trọng so với các phương pháp trước đó. [2][3]\n\n---\n\nGhi chú: Các trích dẫn [1]..[13] trong bản tóm tắt tương ứng với các đoạn (chunks) của video theo timestamp được cung cấp, dùng để dẫn lại nội dung từng phần của bài giảng.",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 8_ROIgZ5tyDFo",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_1： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/ROIgZ5tyDFo",
          "start_time": 0,
          "end_time": 59,
          "text": "Trong phần tiếp theo thì chúng ta sẽ cùng tìm hiểu về cơ chế Attention để giúp cho chúng ta giải quyết một số cái vấn đề của mạng ANN trong bài toán dịch máy nói riêng và trong các cái bài toán của NLP nói chung. Thì đầu tiên chúng ta sẽ cùng xem lại cái kiến trúc 626, 727 và chúng ta xem coi cái vấn đề của nó đang mất phải hiện giờ đó là gì. Và tại cái nốt cuối cùng của cái quá trình Encoder chúng ta thấy là toàn bộ nội dung của câu văn nguồn nó đã dồn vô cái vector này. Toàn bộ nội dung của cái câu văn nguồn nó dồn vô cái vector này và như vậy thì nó sẽ gây ra cái điểm ngãn. Nó giống như là chúng ta hình dung cái phẻo của mình vậy đó. Nó hình dung giống như là cái phẻo thông tin. Thì toàn bộ nội dung chúng ta đưa vô đây. Và ở đây thì nó sẽ bị dồn."
        },
        {
          "index": 2,
          "video_id": "Chương 8_ROIgZ5tyDFo",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_1： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/ROIgZ5tyDFo",
          "start_time": 50,
          "end_time": 110,
          "text": "Nó hình dung giống như là cái phẻo thông tin. Thì toàn bộ nội dung chúng ta đưa vô đây. Và ở đây thì nó sẽ bị dồn. Vô cái miệng phẻo, nó gọi là bottleneck. Nó sẽ bị dồn vô. Thế thì ở đây cũng vậy. Toàn bộ thông tin của từ am, từ not, từ the, từ sure. Nó sẽ gọi am, not sure. Dồn hết vô đây. Thì nó sẽ gây ra cái hiện tượng điểm ngãn. Thế thì đó là về mặt hình tượng. Còn về mặt ý nghĩa thực sự của cái điểm ngãn đó là gì? Đó là khi chúng ta xử lý đến cái từ sure. Thì cho dù chúng ta có sử dụng kiến thức. Chúng ta có sử dụng là các cái biến thể. Như là LST. STM. Rồi GRU. Hoặc là bidirectional, deep stack v.v. Thì nó đều không thể giải quyết được cái vấn đề cố hữu. Đó chính là vấn đề về thông tin bị mất. Bị file. Hoặc khi lan truyền theo cái chiều tuần tự này. Khi chúng ta lan truyền tuần tự."
        },
        {
          "index": 3,
          "video_id": "Chương 8_ROIgZ5tyDFo",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_1： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/ROIgZ5tyDFo",
          "start_time": 99,
          "end_time": 161,
          "text": "Đó chính là vấn đề về thông tin bị mất. Bị file. Hoặc khi lan truyền theo cái chiều tuần tự này. Khi chúng ta lan truyền tuần tự. Thì cái thông tin của những cái từ đầu tiên. Nó đã bị mất thông tin. Nó bị file thông tin nhiều. Và dẫn đến đó là. Khi chúng ta lan truyền được đến cái từ cuối cùng ở đây. Đến cái từ cuối cùng ở đây. Để tính ra được cái giá trị ở đây. Thì cái thông tin của cái từ sure. Ví dụ ở đây là cái thông tin của từ sure là cần thiết để mà đưa ra được cái dự đoán đúng không? Thì nó đã bị quên. Nó đã bị quên. Do nó đã bị biến đổi quá nhiều. Biến đổi quá nhiều. Từ sure. Đến đây là bị biến đổi một lần. Hai lần. Ba lần. Bốn lần. Năm lần. Năm lần. Năm lần.  Năm lần. Năm lần. Và qua năm lần biến đổi đó thì hàm lực thông tin nó bị loãng đi. Thì đó chính là cái vấn đề thật sự của Sys2Sys. Và giải pháp làm sao có thể giải quyết được vấn đề này."
        },
        {
          "index": 4,
          "video_id": "Chương 8_ROIgZ5tyDFo",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_1： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/ROIgZ5tyDFo",
          "start_time": 151,
          "end_time": 210,
          "text": "Và qua năm lần biến đổi đó thì hàm lực thông tin nó bị loãng đi. Thì đó chính là cái vấn đề thật sự của Sys2Sys. Và giải pháp làm sao có thể giải quyết được vấn đề này. Thì chúng ta sẽ sử dụng cái cơ chế đó là Attention. Với cái cơ chế Attention. Thì cái cách thức làm của chúng ta sẽ là như sau. Đầu tiên. Đó là cái vết khi chúng ta bắt đầu cái quá trình decode. Khi chúng ta bắt đầu quá trình decode. Thì chúng ta sẽ đi tính cái score của cái trạng thái tại đây. Đúng không? Trạng thái ẩn tại đây. Và đi tính với lại tất cả. Đi tính với lại tất cả các cái score. Trạng thái ẩn của cái code input của mình. Thì ở đây nó gọi là Attention score. Mục tiêu của nó là gì? Mục tiêu của cái việc tính cái Attention score này là. Tại thời điểm tôi. Tôi bắt đầu. Một cái quá trình decode ở đây."
        },
        {
          "index": 5,
          "video_id": "Chương 8_ROIgZ5tyDFo",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_1： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/ROIgZ5tyDFo",
          "start_time": 199,
          "end_time": 260,
          "text": "Thì ở đây nó gọi là Attention score. Mục tiêu của nó là gì? Mục tiêu của cái việc tính cái Attention score này là. Tại thời điểm tôi. Tôi bắt đầu. Một cái quá trình decode ở đây. Thì tôi sẽ để tâm. Cái từ Attention. Tiếng Anh. Thì khi dịch ra tiếng Việt. Mình có thể dùng từ nôn na đó là để tâm. Tôi sẽ để tâm. Đến cái từ nào. Trong 4 cái từ ở đây. Khi tôi bắt đầu dịch. Tại cái vị trí này. Thì. Để mà tính được cái sự. Để tâm đó. Thì chúng ta sẽ dùng cái. Cái công thức tính là độ tương đồng. Có thể là dùng cái độ đo. Đó. Tích vô hướng. Và các cái giá trị Scalar. Các cái giá trị ở đây. Nó thể hiện cho cái sự tương đồng đó. Tuy nhiên các cái giá trị tương đồng này. Nếu như chúng ta sử dụng độ đo tích vô hướng. Thì nó sẽ chưa có được. Chuẩn hóa. Về một cái không gian. Xét xuất. Do đó thì chúng ta sẽ. Tiến hành cái bước tiếp theo."
        },
        {
          "index": 6,
          "video_id": "Chương 8_ROIgZ5tyDFo",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_1： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/ROIgZ5tyDFo",
          "start_time": 248,
          "end_time": 309,
          "text": "Nếu như chúng ta sử dụng độ đo tích vô hướng. Thì nó sẽ chưa có được. Chuẩn hóa. Về một cái không gian. Xét xuất. Do đó thì chúng ta sẽ. Tiến hành cái bước tiếp theo. Đó là. Tính cái Attention Distribution. Attention Distribution. Là nó sẽ. Quy chiếu. Về một cái. Cái không gian. Có cái giá trị là từ 0. Cho đến 1. Đó. Để Normalize. Để chuẩn hóa nó lại. Và đưa về cái không gian phân bố. 1. Thì với cái Distribution này. Chúng ta. Thấy rằng cái cột này. Nó sẽ cao hơn hẳn. So với lại các cái cột này. Thì điều đó có nghĩa là gì. Khi chúng ta bắt đầu. Cái quá trình. Decode. Khi bắt đầu quá trình. Thì. Tại cái thời điểm này. Nó sẽ bắt đầu. Để ý. Cái từ này. Thì chúng ta sẽ là. Chuyển sang cái. Cái cách hiệu khác đó là. Để ý. Để ý đến cái từ I. Nhiều hơn. So với lại những cái từ còn lại."
        },
        {
          "index": 7,
          "video_id": "Chương 8_ROIgZ5tyDFo",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_1： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/ROIgZ5tyDFo",
          "start_time": 299,
          "end_time": 359,
          "text": "Thì chúng ta sẽ là. Chuyển sang cái. Cái cách hiệu khác đó là. Để ý. Để ý đến cái từ I. Nhiều hơn. So với lại những cái từ còn lại. Để ý đến cái từ I nhiều hơn. Và. Khi đó. Thì chúng ta sẽ biết rằng là. Toàn bộ thông tin. Của cái. S1 này nè. Nó nên được. Tổng hợp. Nhiều nhất. Để mà. Đưa ra cái sáng đoán. Đưa ra cái phán đoán tiếp theo. Đưa ra cái phán đoán. Của cái. Quá trình mình dịch. Thay vì là chúng ta đưa thông tin của cái từ sua. Thay vì chúng ta đưa thông tin của từ sua. Thì chúng ta nên đưa thông tin của cái từ I. Nó sẽ giúp cho chúng ta dịch ở chỗ này chính xác hơn. Trong khi đó với cái phiên bản cũ. Thì. Cái thông tin của từ sua ở đây là nhiều nhất. Đúng không? Thông tin của từ sua nhiều nhất và đưa ra đến đây. Thì. Cái việc dự đoán tiếp theo nó sẽ bị ảnh hưởng bởi từ sua. Nhiều hơn là cái từ I."
        },
        {
          "index": 8,
          "video_id": "Chương 8_ROIgZ5tyDFo",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_1： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/ROIgZ5tyDFo",
          "start_time": 348,
          "end_time": 410,
          "text": "Cái thông tin của từ sua ở đây là nhiều nhất. Đúng không? Thông tin của từ sua nhiều nhất và đưa ra đến đây. Thì. Cái việc dự đoán tiếp theo nó sẽ bị ảnh hưởng bởi từ sua. Nhiều hơn là cái từ I. Nhiều hơn là cái từ I. Và khi chúng ta đã tính được cái attention distribution này rồi. Chúng ta biết là chúng ta cần phải quan tâm. Chúng ta phải để ý đến cái từ I này nhiều hơn rồi. Thì chúng ta sẽ đến cái giai đoạn đó là tổng hợp thông tin. Tổng hợp thông tin. Thì cái vector này. Cái vector này. Là tổng. Có trọng số. Của các cái S1, S2, S3. Cho nên S4 này. Theo. Cái trọng số. Theo cái tỷ trọng. Đã được tính toán ở cái attention distribution. Và. Attention. Tổng hợp. Các cái thông tin đó. Thì nó gọi là. Attention output. Và attention output. Thì sử dụng các cái attention distribution này. Để cộng có trọng số các cái vector đầu vào này. Cộng có trọng số các cái vector trạng thay ẩn này. Và attention output sẽ. Liên quan."
        },
        {
          "index": 9,
          "video_id": "Chương 8_ROIgZ5tyDFo",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_1： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/ROIgZ5tyDFo",
          "start_time": 397,
          "end_time": 460,
          "text": "Thì sử dụng các cái attention distribution này. Để cộng có trọng số các cái vector đầu vào này. Cộng có trọng số các cái vector trạng thay ẩn này. Và attention output sẽ. Liên quan. Nó sẽ liên quan đến cái từ. Mà chúng ta cần phải để ý. Tại cái quá trình dịch. Tại đây. Và nó sẽ. Loại bỏ được những cái thông tin thừa. Và. Nó sẽ loại bỏ được những cái thông tin thừa. Thì những cái thông tin mà dư thừa. Những cái thông tin dư thừa. Thì nó sẽ có cái attention score thấp. Hoặc là cái attention distribution thấp. Ví dụ đây là thông tin thừa. Nên cái. Cái chiều cao của nó thấp. Còn những cái thông tin. Của những cái. Từ nào mà có liên quan nhiều. Thì nó sẽ là cao. Ví dụ đây là một cái. Thời minh họa cho cái chuyện đấy. Và. Khi chúng ta tổng hợp được cái thông tin. Của cái attention output này. Phối hợp. Với lại cái thông tin của cái trạng thái ẩn tại đây. Đúng không? Thì chúng ta sẽ có."
        },
        {
          "index": 10,
          "video_id": "Chương 8_ROIgZ5tyDFo",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_1： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/ROIgZ5tyDFo",
          "start_time": 450,
          "end_time": 509,
          "text": "Khi chúng ta tổng hợp được cái thông tin. Của cái attention output này. Phối hợp. Với lại cái thông tin của cái trạng thái ẩn tại đây. Đúng không? Thì chúng ta sẽ có. Đầy đủ thông tin hơn. Chúng ta sẽ có đầy đủ thông tin quan trọng. Để giúp do cái việc đưa ra cái dự đoán. Là y ngã. Một. Ổn. Rồi. Tương tự như vậy. Chúng ta sẽ. Đến cái từ thứ hai. Và chúng ta cũng lấy cái vector ẩn. Trong cái quá trình decode ở đây.  Đi tính. Tích vô hướng. Dot vào đó. Tích vô hướng. Với các cái. Vector ẩn. Của cái encoder. Rồi. Sau đó chúng ta sẽ ra được các cái score. Các cái score này. Chưa được chủng hóa. Do đó chúng ta sẽ dùng cái. Hàm chủng hóa và. Chút nữa thì chúng ta sẽ nói rõ hơn là. Cái công thức chủng hóa như thế nào. Chúng ta sẽ chủng hóa nó. Về. Cái không gian sát xuất như thế này. Và ở đây thì. Nó cho thấy là. Là."
        },
        {
          "index": 11,
          "video_id": "Chương 8_ROIgZ5tyDFo",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_1： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/ROIgZ5tyDFo",
          "start_time": 499,
          "end_time": 560,
          "text": "Cái công thức chủng hóa như thế nào. Chúng ta sẽ chủng hóa nó. Về. Cái không gian sát xuất như thế này. Và ở đây thì. Nó cho thấy là. Là. Cái từ. Tứ. Note. Cái từ note này nè. Là. Chúng ta sẽ để ý nhiều nhất. Do. Cái từ note này. Sẽ để ý nhiều nhất. Còn cái từ am. Thì nó sẽ để ý. Ít hơn. Còn hai cái từ. I. Và sure. Tương ứng ở đây. Thì nó sẽ để ý nhất. Thì trong tiếng Pháp. Cái note này. Là phủ định. Thì. Ở trong tiếng Pháp. Tương ứng nó sẽ là. Hai cái từ là. Nê. Pa. Nê. Cái gì đấy. Nê. Và pa. Đó là. Note. Note. Trong tiếng Anh. Thì cái bắt đầu bằng cái nê này nè. Là nó sẽ chú ý đến. Nó sẽ để ý đến hai cái từ am và từ note. Nó không có chú ý đến từ. I. Và từ sure. Rồi. Tương tự như vậy. Đến cái từ tiếp theo. Nó cần phải dự đoán. Thì sau khi chúng ta đưa vô cái từ nê. Thì cái từ tiếp theo. Chúng ta cần phải dự đoán. Note. Để ý đến cái từ thứ hai nhiều hơn. Tức là từ am nhiều hơn."
        },
        {
          "index": 12,
          "video_id": "Chương 8_ROIgZ5tyDFo",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_1： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/ROIgZ5tyDFo",
          "start_time": 549,
          "end_time": 610,
          "text": "Đến cái từ tiếp theo. Nó cần phải dự đoán. Thì sau khi chúng ta đưa vô cái từ nê. Thì cái từ tiếp theo. Chúng ta cần phải dự đoán. Note. Để ý đến cái từ thứ hai nhiều hơn. Tức là từ am nhiều hơn. Thế qua. Cái chiều cao này. Và. Trong tiếng Pháp. Thì suy. Tức là. Tức là cái động từ to be. Am. Của tiếng Anh. Suy này. Thực ra chính là cái am này của tiếng Anh. Như vậy thì với cái. Attention distribution này. Nó cũng thể hiện đúng là. Khi tôi bắt đầu. Đưa vô cái từ nê. Thì. Tôi sẽ bắt đầu. Để tâm. Để ý đến cái từ thứ hai. Tức là từ am nhiều hơn. Do đó. Cái hàm lượng thông tin của cái từ am. Sẽ được truyền vô đây. Cái attention output này. Nhiều hơn. Dẫn đến là cái việc đưa ra cái dự đoán. I3 nó. Chính xác hơn. Rồi. Tương tự như vậy. Pa. Pa trong tiếng Pháp. Là note. Nó sẽ để ý vô cái. Từ note này. Nhiều hơn. Suy. Sơ. Thì nó trong tiếng Pháp. Tức là. Tương ứng là từ sua. Thì các bạn sẽ thấy là."
        },
        {
          "index": 13,
          "video_id": "Chương 8_ROIgZ5tyDFo",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_1： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/ROIgZ5tyDFo",
          "start_time": 600,
          "end_time": 613,
          "text": "Là note. Nó sẽ để ý vô cái. Từ note này. Nhiều hơn. Suy. Sơ. Thì nó trong tiếng Pháp. Tức là. Tương ứng là từ sua. Thì các bạn sẽ thấy là. Ở đây. Nó sẽ cho cái trọng số. Cao hơn."
        }
      ]
    },
    {
      "video_id": "Chương 8_S8__bXkLSbM",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giải thích chi tiết cơ chế Attention trong mô hình Sequence-to-Sequence — từ công thức tính score, chuẩn hóa thành phân phối attention, tính context vector đến cách kết hợp với trạng thái decoder để sinh output; đồng thời nêu lợi ích (tập trung, giảm vanishing, khả năng giải thích) và làm một bài tập kích thước vector để hiểu rõ kích thước các thành phần. [1][2][3][4][5]  \n- Các khái niệm sẽ được đề cập: encoder outputs S1..SN, decoder hidden states H_t, attention score R (tập các phép nhân vô hướng H_t·S_i), attention distribution α (softmax của R), context vector C_t (tổng có trọng số các S_i), phép nối (concat) C_t và H_t, và cách tính y_t qua softmax trên vector tổng hợp; cùng các khái niệm liên quan như skip connection / giảm vanishing và khả năng trực quan hóa attention. [1][2][3][4][9][11][12][13]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Ký hiệu và bước đầu tính attention score\n- Encoder tạo ra các vector ẩn S1, S2, …, SN; decoder có các trạng thái ẩn H theo trục thời gian (ví dụ H_t là trạng thái tại thời điểm decode t). [1]  \n- Tại thời điểm decode t, ta tính attention score bằng các tích vô hướng giữa H_t và mỗi S_i: R_t = [H_t·S1, H_t·S2, …, H_t·SN]. (mỗi H_t·S_i là một scalar). [1][6]\n\n### 2.2. Chuẩn hóa thành Attention Distribution (α)\n- Để đưa các score R_t về không gian xác suất, dùng hàm softmax: α_t = softmax(R_t). Trong đó α_t là vector phân phối trọng số attention tại thời điểm t (α_ti là trọng số tương ứng với S_i). [2][6]\n\n### 2.3. Tính Context vector C_t (Attention Output)\n- Context vector C_t (còn gọi là Attention Output) được tính là tổng có trọng số các vector ẩn encoder:  \n  C_t = Σ_{i=1..N} α_ti · S_i. [3][7]  \n- Lưu ý: α_ti là scalar; để có thể nhân α_ti với S_i (vector), S_i có cùng chiều với H_t (được giả định là R^D), do đó S_i ∈ R^D và C_t ∈ R^D. [7]\n\n### 2.4. Kết hợp Context với trạng thái decoder và sinh output\n- Sau khi có C_t, ta nối (concatenate) C_t với H_t (ký hiệu [C_t; H_t] hoặc CT.HT trong video) để tạo vector tổng hợp. [4]  \n- Vector tổng hợp này được dùng để tính xác suất output y_t bằng một lớp tuyến tính + softmax chẳng hạn: y_t = softmax( V · [C_t; H_t] ). (Theo mô tả trong video: Softmax của V nhân với vector nối C_t và H_t). [4]  \n- Kích thước: nếu H_t ∈ R^D thì R_t ∈ R^N (N scalar scores), α_t ∈ R^N, C_t ∈ R^D, và [C_t; H_t] ∈ R^{2D}. (Đây là đáp án bài tập kích thước được nêu trong bài). [5][6][7][8]\n\n### 2.5. Ý nghĩa trực quan và lý do hiệu quả của Attention\n- Attention cho phép decoder “nhìn lại” toàn bộ câu nguồn: C_t chứa thông tin từ toàn bộ encoder outputs, nhưng được trọng số hóa để *tập trung* vào những vị trí liên quan nhất cho việc giải mã tại thời điểm t (không phải là trung bình đều). Điều này giúp sinh output chính xác hơn so với phương pháp không có attention. [9][10]  \n- Attention tạo ra các đường tắt (skip connections) giữa encoder và decoder, tương tự cơ chế skip connection trong ResNet, giúp giảm vấn đề vanishing gradient và cải thiện học sâu về mặt thực nghiệm cũng như lý thuyết. [11][12]\n\n### 2.6. Khả năng diễn giải (Interpretability)\n- Vì α_t là phân phối trọng số theo thời gian, ta có thể trực quan hóa ma trận attention (các α_t theo t) để biết mô hình đang chú ý vào từ nguồn nào khi sinh mỗi token đích — cung cấp tính giải thích và trực quan hóa cho kết quả dịch/giải mã. [12][13]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa dịch máy: khi dịch một từ tiếng Anh như \"size\", ma trận attention có thể “phát sáng” (tập trung) ở một số từ tiếng Pháp tương ứng (ví dụ ba từ liên quan) giúp mô hình quyết định cách chia từ trong tiếng Pháp cho thể bị động/quá khứ; trực quan hóa α cho thấy mối liên hệ này. [14]  \n- Ứng dụng thực tế: attention được dùng rộng rãi trong các hệ thống dịch máy (machine translation), các mô hình sequence-to-sequence khác (ví dụ tóm tắt văn bản, chú thích ảnh tuần tự), nơi cần ánh xạ mềm giữa vị trí nguồn và vị trí đích; đồng thời giúp model giải thích được quyết định thông qua ma trận attention. [9][10][13][14]  \n- Bài tập trong video (đã nêu & giải): cho H_t ∈ R^D, xác định kích thước của R_t, α_t, C_t, và [C_t; H_t] — đáp án: R_t ∈ R^N, α_t ∈ R^N, C_t ∈ R^D, [C_t; H_t] ∈ R^{2D}. [5][6][7][8]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính: Attention được xây dựng bằng cách tính score (tích vô hướng H_t·S_i), chuẩn hóa bằng softmax để được α_t, tạo context vector C_t = Σ α_ti S_i, rồi nối C_t với H_t để đưa vào lớp softmax sinh y_t; các thành phần có kích thước: R_t ∈ R^N, α_t ∈ R^N, C_t ∈ R^D, concat ∈ R^{2D}. [1][2][3][4][5][6][7][8]  \n- Tầm quan trọng: Attention cho phép mô hình tập trung vào phần phù hợp của câu nguồn, cải thiện hiệu suất và chống vanishing thông qua cơ chế đường tắt, đồng thời cung cấp khả năng diễn giải thông qua trực quan hóa phân phối attention. [9][10][11][12][13]  \n- Liên hệ với các bài giảng khác: Video nhắc tới mối quan hệ ý tưởng với skip connections / ResNet (từ CNN) như một cơ chế tương tự giúp giảm vanishing, và khẳng định hiệu quả của attention so với các phương pháp trước đó (đã được thử nghiệm trong nghiên cứu). [11][12][9]  \n- Ghi chú cuối: phần minh họa cụ thể về từ “size” và mối liên hệ đến từ tiếng Pháp được trình bày trong video để minh họa cho khả năng can thiệp/ghép từ của attention. [14][15]\n\nNếu bạn muốn, mình có thể:  \n- Viết lại công thức tóm tắt bằng ký hiệu toán học tập trung (latex-like),  \n- Vẽ sơ đồ luồng tính attention (H_t → scores → α_t → C_t → concat → y_t), hoặc  \n- Tách chi tiết từng bước tính trên ví dụ cụ thể (ví dụ N=4, D=3) để tính từng giá trị số.",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 8_S8__bXkLSbM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_2： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/S8__bXkLSbM",
          "start_time": 0,
          "end_time": 60,
          "text": "rồi bây giờ chúng ta sẽ đến với cái phần về công thức nãy giờ là chúng ta đang mô phỏng cách thức vận hành của một cái attention còn về công thức tính thì chúng ta sẽ tính như thế nào thì tại đây chúng ta sẽ có các cái hệ thống ký hiệu với encoder chúng ta sẽ ký hiệu bằng chữ S ký hiệu bằng chữ S S1, S2, S3 cho đến SN và quá trình decode thì chúng ta sẽ ký hiệu bằng H quá trình decode thì sẽ ký hiệu bằng H và H ở đây sẽ là đi theo cái trục thời gian rồi và tại cái thời điểm đầu tiên thì T, thời gian của mình nó sẽ là bằng 1 và tiếp theo thì nó sẽ lấy cái HT này đi nhân tích vô hướng với lại các cái giá trị S này thì là HT sẽ nhân tích vô hướng với lại S1, HT, S2, HT, S3, HT, SN sau khi chúng ta tính xong chúng ta được các cái hệ thống ký hiệu chúng ta sẽ chuẩn hóa nó và để chuẩn hóa về không gian sát xuất"
        },
        {
          "index": 2,
          "video_id": "Chương 8_S8__bXkLSbM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_2： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/S8__bXkLSbM",
          "start_time": 49,
          "end_time": 110,
          "text": "với lại S1, HT, S2, HT, S3, HT, SN sau khi chúng ta tính xong chúng ta được các cái hệ thống ký hiệu chúng ta sẽ chuẩn hóa nó và để chuẩn hóa về không gian sát xuất thì không hàm nào khác chúng ta đã từng học đó chính là chúng ta sử dụng hàm Sopax chúng ta sẽ sử dụng hàm Sopax rồi và ký hiệu cho toàn bộ cái nội dung của cái tính Attention Score là chúng ta dùng cái ký hiệu là R rồi để tính cái Attention Distribution thì chúng ta sẽ ký hiệu là chữ Alpha Alpha là thể hiện cái trọng số đã được chuẩn hóa của R như vậy thì Alpha T sẽ là bằng Sopax của R T Alpha chính là cái đã chuẩn hóa của R T rồi sau khi chúng ta đã có được cái bộ trọng số Alpha này rồi thì chúng ta sẽ bắt đầu tổng hợp thông tin cho cái Attention Output tổng hợp thông tin cho Attention Output và ký hiệu nó bằng chữ C C này thì còn có một cái ý nghĩa khác nó chỉ là Context"
        },
        {
          "index": 3,
          "video_id": "Chương 8_S8__bXkLSbM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_2： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/S8__bXkLSbM",
          "start_time": 99,
          "end_time": 161,
          "text": "thì chúng ta sẽ bắt đầu tổng hợp thông tin cho cái Attention Output tổng hợp thông tin cho Attention Output và ký hiệu nó bằng chữ C C này thì còn có một cái ý nghĩa khác nó chỉ là Context và C này sẽ là tổng có trọng số của các cái trọng số của R T và cái trạng thái của các cái vector ẩn của End Coder của cái quá trình End Coder chính là cái S1, S2, S3, Sn còn trọng số tương ứng của nó đó chính là cái Alpha Ti trong đó T là cái đại diện cho cái tại cái thời điểm T này T là đại diện cho cái thời điểm T mà mình bắt đầu cái quá trình decode còn Y là chúng ta sẽ duyệt từ 1 cho đến N đây duyệt từ đầu cho đến cuối cuối cái đoạn của End Coder rồi và cái vector output Attention Output này đó là CT nó sẽ được thực hiện contrast"
        },
        {
          "index": 4,
          "video_id": "Chương 8_S8__bXkLSbM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_2： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/S8__bXkLSbM",
          "start_time": 150,
          "end_time": 209,
          "text": "rồi và cái vector output Attention Output này đó là CT nó sẽ được thực hiện contrast ký hiệu là cái dấu chấm phẩy ha tức là nó nối chuỗi nó contrast lại với nhau rồi để tạo ra nó nối với lại cái HT để tạo ra một cái vector tổng hợp và từ cái vector tổng hợp này thì chúng ta sẽ đi tính tổng hợp này để tính toán cái giá trị đi tính cái giá trị Y-T và đây chính là cái cách tính mà dựa hoàn toàn vào cái tình huống là không có Attention tức là Y-T thì nó sẽ là bằng Softmax của V nhân với lại cái vector này nhân với cái vector là CT.HT Được rồi! Thì đây chính là cái"
        },
        {
          "index": 5,
          "video_id": "Chương 8_S8__bXkLSbM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_2： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/S8__bXkLSbM",
          "start_time": 200,
          "end_time": 261,
          "text": "nhân với cái vector là CT.HT Được rồi! Thì đây chính là cái cách tính khi mà không có Attention thì nó cũng giống như là trong trường hợp là Attention hay Softmax Rồi và ở đây thì chúng ta sẽ có một cái bài tập đó là nếu như chúng ta giả định cái vector HT này nó có kích thước là RD tức là H là một cái vector D chiều thì hỏi RT là một cái vector có kích thước bao nhiêu alpha T sẽ là vector có kích thước bao nhiêu rồi cái vector nối CT và HT sẽ là kích thước bao nhiêu rồi C cái vector Attention output nó sẽ có kích thước là bao nhiêu"
        },
        {
          "index": 6,
          "video_id": "Chương 8_S8__bXkLSbM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_2： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/S8__bXkLSbM",
          "start_time": 249,
          "end_time": 310,
          "text": "cái vector nối CT và HT sẽ là kích thước bao nhiêu rồi C cái vector Attention output nó sẽ có kích thước là bao nhiêu rồi bây giờ chúng ta sẽ tính toán cái dấu chấm hỏi này nó sẽ là các giá trị gì nếu như bạn nào mà nhanh chí thì có thể nhìn vô đây là R này là tập hợp của các cái dấu hình tròn này đúng không? thì ở đây có bao nhiêu? có N có N phần tử như vậy ở đây R này sẽ là RN và tương ứng là và mỗi cái phần tử HT nhân với ST nó là một cái Scalar một cái giá trị vô hướng và tổ hợp của các cái giá trị vô hướng nó sẽ tạo ra một cái vector và có N cái giá trị vô hướng như vậy ở đây sẽ là N alpha là cái vector là Attention distribution"
        },
        {
          "index": 7,
          "video_id": "Chương 8_S8__bXkLSbM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_2： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/S8__bXkLSbM",
          "start_time": 299,
          "end_time": 359,
          "text": "nó sẽ tạo ra một cái vector và có N cái giá trị vô hướng như vậy ở đây sẽ là N alpha là cái vector là Attention distribution của cái vector đã được trưởng hóa của RT do đó thì cái số chiều của alpha T nó không thay đổi so với RT do đó nếu ở đây là là RN thì ở đây cũng sẽ là RN bước tiếp theo là chúng ta sẽ thực hiện cái phép contrast nhưng mà để contrast được chúng ta phải có cái CT nhưng mà chúng ta chưa có CT như vậy chúng ta phải tính cái này trước CT bản chất là tổng trọng số của các cái ST đây là giá trị Scalar đây là giá trị Scalar còn đây là vector mà vector ST thì để mà có thể nhân được cái S với lại cái H đúng không? để mà S và H có thể nhân được với nhau thì tụi nó phải có cùng số chiều"
        },
        {
          "index": 8,
          "video_id": "Chương 8_S8__bXkLSbM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_2： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/S8__bXkLSbM",
          "start_time": 350,
          "end_time": 410,
          "text": "thì để mà có thể nhân được cái S với lại cái H đúng không? để mà S và H có thể nhân được với nhau thì tụi nó phải có cùng số chiều nó phải có cùng số chiều do đó thì ST ở đây nó cũng là một cái RD tức là vector D chiều như vậy thì ở đây là vector D chiều tổng trọng số của các cái vector D chiều thì nó sẽ là một cái vector D chiều nó là một cái vector D chiều rồi và khi chúng ta concat 2 cái này lại với nhau CT là một cái vector D chiều HT là một cái vector D chiều thì như vậy ở đây nó sẽ là 2D 2D thì đây là cái đáp án cho cái bài tập của mình và hy vọng là khi các bạn làm được cái bài tập này thì các bạn có thể hiểu được cái rõ hơn cái cơ chế vận hành của cái Extension"
        },
        {
          "index": 9,
          "video_id": "Chương 8_S8__bXkLSbM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_2： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/S8__bXkLSbM",
          "start_time": 396,
          "end_time": 461,
          "text": "thì đây là cái đáp án cho cái bài tập của mình và hy vọng là khi các bạn làm được cái bài tập này thì các bạn có thể hiểu được cái rõ hơn cái cơ chế vận hành của cái Extension và tại sao Extension thì hiệu quả? Extension cho cái hiệu suất cao hơn hẳn so với lại các phương pháp trước đây thì cái hiệu sức cao hơn này nó được thể hiện qua cái việc mà chúng ta thực nghiệm nhưng mà nếu mà nói về mặt lý thuyết nói về mặt lý thuyết thì cái quá trình decoder nó sẽ cho phép là nhìn lại toàn bộ cái câu văn nguồn của mình như chúng ta đã thấy là khi chúng ta tính cái khi chúng ta tính cái y tế này Ook ngã T đúng không thì chúng ta sẽ phải dựa trên cái thông tin của cả khách T và kết hợp với lại cả thông tin CT trong đó thông tin CT là có chứa thông tin của toàn bộ cái câu văn ngụ của mình rồi ngoài ra thì decoder"
        },
        {
          "index": 10,
          "video_id": "Chương 8_S8__bXkLSbM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_2： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/S8__bXkLSbM",
          "start_time": 448,
          "end_time": 513,
          "text": "và kết hợp với lại cả thông tin CT trong đó thông tin CT là có chứa thông tin của toàn bộ cái câu văn ngụ của mình rồi ngoài ra thì decoder nó sẽ cho phép chúng ta tập trung hơn tại một số cái phần nhất định trong câu văn thì ở đây chúng ta quan sát nè để tính ra cái output IT chúng ta sẽ có cái sự tổng hợp thông tin của CT và CT thì nó là tổng trọng số nó là tổng trọng số tổng trọng số của cái attention distribution này với cái vector ẩn như vậy thì nó vừa cho phép chúng ta có thể nhìn lại toàn bộ cái nội dung của cái câu văn ngụ nhưng nó cũng không phải là tổng hợp nó sẽ không đều cộng trung bình cộng tất cả các thông tin này mà nó chỉ chú tâm nó chỉ chú tâm đến những cái vị trí nào của cái encoder mà nó thật sự liên quan đến cái quá trình giải mạ ở đây"
        },
        {
          "index": 11,
          "video_id": "Chương 8_S8__bXkLSbM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_2： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/S8__bXkLSbM",
          "start_time": 498,
          "end_time": 560,
          "text": "nó sẽ không đều cộng trung bình cộng tất cả các thông tin này mà nó chỉ chú tâm nó chỉ chú tâm đến những cái vị trí nào của cái encoder mà nó thật sự liên quan đến cái quá trình giải mạ ở đây ví dụ như ở đây nó sẽ chú tâm đến cái từ đầu tiên là từ I nhiều hơn so với các từ khác đó thì ở đây là cho phép tập trung vào một số phần nhất định chứ không phải là nó sẽ đi nhìn hết toàn bộ cái nội dung của cái câu văn ngụ nó gây loạn và attention nó giải quyết được cái vấn đề điểm ngãn như chúng ta đã đề cập ở những slide đầu rồi rồi attention giúp chúng ta giải quyết được vấn đề valencing radian khi nó tạo được các cái đường tắt thì cái đường tắt này chính là cái skip connection và cái cơ chế skip connection này lại một lần nữa chúng ta nhắc đến nó là sự kế thừa của cái mạng cnn một cái biến thể của cnn đó chính là"
        },
        {
          "index": 12,
          "video_id": "Chương 8_S8__bXkLSbM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_2： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/S8__bXkLSbM",
          "start_time": 551,
          "end_time": 610,
          "text": "và cái cơ chế skip connection này lại một lần nữa chúng ta nhắc đến nó là sự kế thừa của cái mạng cnn một cái biến thể của cnn đó chính là resnet và nó đã được chứng minh trong rất nhiều những cái bài báo khoa học skip connection nó sẽ giúp cho chúng ta chống được cái hiện tượng valencing rất là tốt với cái công thức nó rất là đơn giản là x là bằng một cái hàm g tức là hàm biến đổi cộng cho x đó là thiệt nhờ cái phép cộng với x nó sẽ giúp cho chúng ta giảm được cái hiện tượng valencing và nét xin và attention nó còn cho phép chúng ta một cái khả năng nữa cũng rất là thú vị đó chính là khả năng diễn đạt hay còn gọi là khả năng giải thích kết quả hoặc là trực quan hóa thì ở đây thấy các bạn là"
        },
        {
          "index": 13,
          "video_id": "Chương 8_S8__bXkLSbM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_2： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/S8__bXkLSbM",
          "start_time": 599,
          "end_time": 676,
          "text": "cũng rất là thú vị đó chính là khả năng diễn đạt hay còn gọi là khả năng giải thích kết quả hoặc là trực quan hóa thì ở đây thấy các bạn là cái gì trong cái attention nó giúp cho chúng ta có khả năng diễn đạt được đó chính là cái cái khả năng diễn đạt được    cái khả năng diễn đạt được cái khả năng diễn đạt được  với cái attention distribution thì tại cái thời điểm t chúng ta biết là chúng ta phải để tâm đến cái từ nào thì họ sẽ tìm cách là visualize cái ma trậu alpha đó visualize cái alpha t theo thời gian để từ đó là có khả năng là biết được là tại một thời điểm nào đó thì mô hình của mình cái quá trình decoder của mình nó đang chú ý đến cái từ nào và bên đây chính là một cái cái khả năng diễn đạt được cái nó sẽ"
        },
        {
          "index": 14,
          "video_id": "Chương 8_S8__bXkLSbM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_2： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/S8__bXkLSbM",
          "start_time": 656,
          "end_time": 711,
          "text": "cái nó sẽ minh họa cho cái ma trậu alpha đó chúng ta thấy là cái từ size đúng không thì nó đã phát sáng ở trên ba từ là a, s, size thì cái từ size trong tiếng anh này nó đã được chia theo thì bị động quá khứ và tương ứng trong tiếng pháp để mà chia được bị động quá khứ nó cũng sẽ phải cần có một ba cái chữ này x size thì đây là cái lý giải tại sao cái từ tiếng anh này nó có cái sự liên đối đến cộng các cái từ tiếng pháp"
        },
        {
          "index": 15,
          "video_id": "Chương 8_S8__bXkLSbM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 2_2： Cơ chế Attention trong Sequence-to-Sequence",
          "video_url": "https://youtu.be/S8__bXkLSbM",
          "start_time": 698,
          "end_time": 715,
          "text": "nó cũng sẽ phải cần có một ba cái chữ này x size thì đây là cái lý giải tại sao cái từ tiếng anh này nó có cái sự liên đối đến cộng các cái từ tiếng pháp rồi cho dạ lệ  đây tên là"
        }
      ]
    },
    {
      "video_id": "Chương 8_my3qRjVJ7VM",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Trình bày và so sánh **một số biến thể của attention** (trong video được gọi là *extension*), cách chúng tính điểm (score), chuẩn hóa và tổng hợp thông tin, cũng như các cách ánh xạ giữa không gian vector khác nhau. [1][2][3]\n\n- Các khái niệm sẽ được đề cập:\n  - Query: vector truy vấn (ký hiệu h) được dùng để so sánh với các values. [1][2]\n  - Values: tập các trạng thái S1, S2, …, Sn (các value) được tổng hợp theo trọng số. [1][15]\n  - Ba bước chung của attention: (1) tính score (R), (2) chuẩn hóa score bằng hàm softmax để được phân phối attention, (3) tổng hợp weighted sum để ra output C = Σ αi · si. [3]\n  - Các dạng scoring: dot-product (tích vô hướng), bilinear (nhân ma trận), low-rank factorization (ánh xạ giảm bậc), additive (tương tác qua phép cộng + tanh). [4][5][7][11][12]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Quy trình tổng quát của attention\n- Ba bước cơ bản:\n  1. Tính score R giữa query h và từng value si. [3]  \n  2. Chuẩn hóa R bằng softmax để tạo attention distribution (αi). [3]  \n  3. Tổng hợp ra output C = Σ_i αi · si (C là tổng có trọng số của các value). [3]\n\n  (Biểu diễn tắt không dùng chỉ số thời gian t để dễ thảo luận các biến thể.) [3][4]\n\n### 2.2. Dot-product (tích vô hướng)\n- Phương pháp tính score đơn giản: score = h · si (tích vô hướng). [4][5]\n- Yêu cầu: hai vector phải cùng kích thước (d1 = d2) để thực hiện tích vô hướng. Nếu không cùng kích thước thì không thể trực tiếp nhân vô hướng. [4][5]\n\n### 2.3. Scoring bằng ma trận (Bilinear / nhân ma trận)\n- Khi d1 ≠ d2, cần ánh xạ một vector về không gian của vector kia bằng một ma trận W, rồi tính tích: score = h^T W si (hoặc h^T W si tuỳ cách biểu diễn). [6][7]\n- Kích thước của W sẽ tương ứng để ánh xạ giữa hai không gian — ví dụ W có kích thước d2 × d1 (chiếu h về không gian của si hoặc ngược lại). [6]\n- Tên gọi: dạng này còn gọi là *bilinear attention* hay *attention song tuyến*. W là tham số học được trong quá trình huấn luyện. [7]\n\n### 2.4. Ánh xạ qua không gian trung gian — Low-rank factorization (nhân ma trận giảm bậc)\n- Ý tưởng: thay vì trực tiếp ánh xạ từ không gian này sang không gian kia, đưa cả hai về một không gian trung gian có chiều k (mặt tranh) rồi so sánh trong không gian đó. [8][9]\n- Cách thực hiện (một cách tường minh):\n  - r_i = (U · h) và s'_i = (V · s_i), với U ∈ R^{k×d_h}, V ∈ R^{k×d_s} (k nhỏ hơn nhiều so với d_h, d_s). Sau đó score = (U h) · (V s_i). [8][9]\n- Lợi ích:\n  - U và V có hạng thấp (k nhỏ) => giảm số tham số so với ma trận đầy đủ, tiết kiệm bộ nhớ và tính toán. [9][10]\n  - Giảm nguy cơ overfitting và cải thiện hiệu suất huấn luyện khi tham số quá lớn. [10]\n  - Minh họa: U và V là hai ma trận low-rank; khi nhân U và V lại có thể xấp xỉ ma trận d2×d1 ở phiên bản đầy đủ. [11]\n\n### 2.5. Additive attention (tích ma trận + phép cộng + tanh)\n- Thay vì nhân vô hướng trực tiếp, ta có thể ánh xạ và cộng rồi đưa qua hàm phi tuyến:\n  - score_i = v^T · tanh(W1 · si + W2 · h)  (W1, W2, v là các tham số học được). [11][12]\n- Kích thước W1, W2, v được chọn tương ứng để phù hợp với chiều của si, h và không gian ẩn của phép tanh. [11][12]\n\n### 2.6. Ghi chú về việc chọn biến thể\n- Các biến thể đều là cách khác nhau nhằm đưa query và values về cùng không gian để có thể so sánh/đo lường tương đồng. [6][8][12]\n- Phiên bản low-rank (ánh xạ về không gian k nhỏ) là một biến thể quan trọng và được **sử dụng trong các công thức của Transformer** (được nhấn mạnh trong bài). [12][18][19]\n\n### 2.7. Attention như một kỹ thuật tổng quát\n- Attention không chỉ áp dụng cho seq-to-seq (dịch máy) mà còn dùng rộng rãi trong thị giác máy tính (CNN), để tăng hiệu năng và trực quan hóa kết quả (explainability). [13][14]\n- Attention có thể giúp trực quan hóa vùng input mà mô hình “chú ý”, tức là khả năng giải thích vì sao mô hình đưa ra dự đoán đó. [14]\n\n### 2.8. Mapping sang bài toán tìm kiếm / seq2seq\n- Định nghĩa tổng quát: cho một tập các vector value và một vector query, attention tính trọng số của các value dựa trên query — tương tự như thao tác truy vấn tìm kiếm (search engine). [15]\n- Trong seq2seq (ví dụ: mô hình encoder-decoder), mọi trạng thái ẩn của encoder là values và trạng thái ẩn của decoder là query; tại mỗi bước decode, decoder tra cứu các value để quyết định cần chú ý value nào nhiều hơn. [16]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Bài toán mẫu được nhắc đến: **dịch máy** — là động lực chính để phát triển attention do tính khó và tính tổng quát cao của nó. Mô hình giải được dịch máy có thể áp dụng cho tóm tắt văn bản, chatbot, tạo code, v.v. [17][16]\n- Ứng dụng trong thị giác máy tính:\n  - Dùng attention trong mạng CNN để tăng hiệu năng và trực quan hóa vùng ảnh mà mô hình quan tâm (visualization/explainability). [13][14]\n- Các trường hợp sử dụng khác:\n  - Tóm tắt văn bản (text summarization), chatbot, code synthesis (synthcode), và các bài toán sequence-to-sequence khác. [14][16]\n- Lợi ích thực tế:\n  - Giải quyết vấn đề phụ thuộc dài hạn (long-range dependencies) và vấn đề vanishing gradient nhờ cơ chế chú ý trực tiếp giữa vị trí xa nhau. [17][18]\n  - Cải thiện khả năng giải thích (visualize attention maps) để hiểu vì sao mô hình đưa ra dự đoán. [14]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Attention (gọi là *extension* trong video) là một kỹ thuật tổng quát gồm ba bước: tính score R, chuẩn hóa (softmax), và tổng hợp weighted sum C = Σ αi si. [3]\n  - Có nhiều biến thể của scoring: dot-product (nhanh, cần cùng chiều), bilinear (nhân ma trận W khi chiều khác nhau), low-rank factorization (ánh xạ về không gian k nhỏ bằng U và V để giảm tham số), và additive attention (W1, W2, tanh, vector v). [4][6][8][11][12]\n  - Việc lựa chọn biến thể phụ thuộc vào chiều không gian, giới hạn tham số và hiệu năng mong muốn; phiên bản low-rank đặc biệt được sử dụng trong Transformer. [9][12][18][19]\n\n- Tầm quan trọng:\n  - Attention cải thiện khả năng mô hình xử lý thông tin dài hạn, giảm các vấn đề huỷ gradient, và cung cấp khả năng giải thích (visualization). [17][18][14]\n  - Attention là kỹ thuật tổng quát, áp dụng cho nhiều kiến trúc (seq2seq, Transformer, CNN trong thị giác, v.v.) chứ không chỉ dành cho dịch máy. [13][16]\n\n- Liên hệ với các bài giảng khác:\n  - Các ý tưởng về ánh xạ không gian và biến thể low-rank được liên hệ với các slide/bài trước (đã thảo luận trước đó) và là phần quan trọng trong các kiến trúc như Transformer mà bài trước/sau có đề cập. [12][13][18]\n\n---\n\nGhi chú: Các phần trên tóm tắt toàn bộ nội dung trong các đoạn đã cung cấp, bao gồm định nghĩa tổng quát, ba bước attention, và chi tiết các biến thể (dot-product, bilinear, low-rank, additive), cùng các ứng dụng thực tế. Các ý chính được trích từ các đoạn tương ứng trong video. [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19]",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 0,
          "end_time": 61,
          "text": "cuối cùng trong bài này đó là chúng ta sẽ tìm hiểu một số cái biến thể của attention chúng ta cùng nhìn lại attention thì nếu như các cái giá trị S1, S2 cho đến Sn chúng ta gọi nó là value nó là các cái trạng thái ẩn của decoder thì chúng ta gọi là value thì trong cái quá trình decode cái vector h này chúng ta sẽ ký hiệu nó là h mà không có chỉ số để cho nó đơn giản thôi để cho nó đơn giản để sau này chúng ta dễ thảo luận các cái biến thể thôi thì đối với cái quá trình mà chúng ta truy vấn tức là từ cái vector h chúng ta sẽ đi truy vấn đến tất cả các cái h S1, S2 cho đến Sn và xem xem cái vector nào là cái vector chúng ta quan tâm nhiều hơn thì cái quá trình mà đi so sánh này"
        },
        {
          "index": 2,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 48,
          "end_time": 109,
          "text": "S1, S2 cho đến Sn và xem xem cái vector nào là cái vector chúng ta quan tâm nhiều hơn thì cái quá trình mà đi so sánh này thì nó gọi là truy vấn do đó thì cái vector h nó sẽ gọi là query và một cách tổng quát thì 2 cái bộ giá trị là S1 cho đến Sn nó sẽ vector truy vấn H nó sẽ có cái số chiều khác nhau đây là một cách tổng quát trong cái ví dụ ở đây thì chúng ta để cho 2 cái vector này là cùng số chiều này là 2, và chúng ta có số chiều nhưng mà một cách tổng quát thì chúng ta có thể để nó là 2 cái vector có số chiều khác nhau và các cái bước thực hiện của cái extension của mình nó sẽ thực hiện 3 bước bước đầu tiên đó là nó sẽ đi tính cái extension score nó sẽ đi tính cái extension score và nó sẽ tạo ra nó sẽ tạo ra một cái vector là R"
        },
        {
          "index": 3,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 98,
          "end_time": 159,
          "text": "nó sẽ thực hiện 3 bước bước đầu tiên đó là nó sẽ đi tính cái extension score nó sẽ đi tính cái extension score và nó sẽ tạo ra nó sẽ tạo ra một cái vector là R sau đó cái extension score sẽ được đưa vào để thực hiện tính cái extension distribution với cái hàm shopping score đó là R và hàm shopping score này thực ra đây chính là một cái quá trình chuẩn hóa chuẩn hóa cái R để tạo ra một cái bộ trọng số trước khi chúng ta tổng hợp thông tin ở cái extension output rồi và bước cuối cùng chính là chúng ta tổng hợp thông tin ra cái extension output thì C ở đây chính là tổng trọng số của alpha i và sc thì với cái cách mà biểu diễn ht bằng h chúng ta sẽ thấy là cái công thức của mình nhìn nó sẽ gọn gàng hơn chúng ta sẽ đỡ để tâm đến cái chỉ số t này hơn tại vì cái việc mà chúng ta tính toán extension này nó thực hiện hoàn toàn tương tự trong suyên suốt"
        },
        {
          "index": 4,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 149,
          "end_time": 210,
          "text": "chúng ta sẽ đỡ để tâm đến cái chỉ số t này hơn tại vì cái việc mà chúng ta tính toán extension này nó thực hiện hoàn toàn tương tự trong suyên suốt chúng ta tính toán cái ht với t là bằng 1 bằng 2 giống nhau chúng ta giảng được đi bớt cái chỉ số t này đi và chúng ta nhìn vô đây chúng ta thấy cái việc tính extension score này nó sẽ có rất nhiều cái phiên bản khác nhau là do cái sự khác biệt giữa cái số chiều của s và số chiều của h s, i nó sẽ có số chiều là d1 h nó sẽ có số chiều là d2 như vậy thì chúng ta sẽ xem các cái biến thể của nó là gì biến thể đầu tiên đó chính là chúng ta sẽ tính tích vô hướng tính tích vô hướng   đây chính là cái biến thể mà chúng ta đã bàn trong cách đây x foot thì để có thể thực hiện được cái phép tích vô hướng này"
        },
        {
          "index": 5,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 199,
          "end_time": 260,
          "text": "chúng ta sẽ tính tích vô hướng tính tích vô hướng   đây chính là cái biến thể mà chúng ta đã bàn trong cách đây x foot thì để có thể thực hiện được cái phép tích vô hướng này tính cái h nhân với s, i này thì 2 cái d1 và d2 nó phải có kích thước bằng nhau thì phải có cái điều kiện này thì chúng ta mới có thể thực hiện được nhân vô hướng thì đây chính là cái phiên bản đã nói trước đây sang cái biến thể tiếp 2 đó là extension khi nhân với ma trận như vậy thì chúng ta cho cái phiên bản này vào đây trong cái tình huống tiếp theo là d1 nó khác d2 thì sao d1 mà khác d2 thì cái phép nhân này nó sẽ không thể nhân được với nhau hay nói vui đó là d1 r d1 tức là cái không gian của trái đất còn r d2 nó sẽ làm ở một cái hành tinh khác ví dụ như đó là sao vậy và để 2 cái vector này có thể so sánh được với nhau"
        },
        {
          "index": 6,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 250,
          "end_time": 309,
          "text": "r d2 nó sẽ làm ở một cái hành tinh khác ví dụ như đó là sao vậy và để 2 cái vector này có thể so sánh được với nhau thực hiện phép tương đồng thì chúng ta sẽ có thể  chúng ta sẽ phải chiếu một cái vector này về cái từ cái không gian sao mã về không gian trái đất hoặc vượt lại từ trái đất về sao mã tức là chúng ta sẽ phải đưa về cùng một cái không gian thì chúng ta sẽ chèn thêm vô một cái ma trận w ở giữa thì h chuyển vị nhân với w đó chính là chúng ta đang ánh xạ chúng ta đang ánh xạ hoặc là chiếu h về cái không gian của s i chiếu về cùng không gian thì khi đó chúng ta mới thực hiện được cái phép nhân scoring này thì khi đó cái ma trận w này của mình nó sẽ là một cái ma trận có kích thước là d2 x d1"
        },
        {
          "index": 7,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 298,
          "end_time": 361,
          "text": "thì khi đó chúng ta mới thực hiện được cái phép nhân scoring này thì khi đó cái ma trận w này của mình nó sẽ là một cái ma trận có kích thước là d2 x d1 và đây chính là cái ma trận trọng số của một cái mô hình mà mình sẽ phải huấn luyện về sau đây cũng sẽ là cái tham số mà mình có thể huấn luyện và mô hình extension với nhân ma trận này thì còn một cái tên gọi khác đó là bilinear extension hay còn gọi là extension song tuyến chúng ta sẽ qua các cái phiên bản khác của extension đó là chúng ta sẽ giảm bật thì cái idea của cái việc mà extension với nhân ma trận giảm bật này nếu như rd1 này là trái đất query h này là sao hỏa thì trong cái phiên bản trước chúng ta sẽ phải map sao hỏa map vector h về"
        },
        {
          "index": 8,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 351,
          "end_time": 411,
          "text": "query h này là sao hỏa thì trong cái phiên bản trước chúng ta sẽ phải map sao hỏa map vector h về cái không gian trái đất hoặc ngược lại map trái đất về không gian sao hỏa để có thể nhân với nhau thì cái phiên bản nhân ma trận giảm bật này nó sẽ là tìm một cái không gian trung gian ví dụ như đây là mặt tranh lấy ví dụ như đây là mặt tranh  ví dụ bạn thì cái vector h này sẽ chiếu lên trên cái không gian này và các cái s này sẽ chiếu lên trên cái không gian này chiếu lại cùng một cái không gian mặt tranh thì khi đó các cái vector này của mình nó mới có cùng chiều và nó có thể thực hiện cái phép tính tích vô hướng được ở đây chúng ta sẽ thấy là ri sẽ là bằng u nhân với h tức là chúng ta đang biến cái h này về cái không gian mặt tranh"
        },
        {
          "index": 9,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 400,
          "end_time": 460,
          "text": "ở đây chúng ta sẽ thấy là ri sẽ là bằng u nhân với h tức là chúng ta đang biến cái h này về cái không gian mặt tranh v nhân với si tức là chúng ta cũng đang ánh xạ cái si này về không gian của mặt tranh đưa về cùng một cái không gian để khi đó chúng ta có thể thực hiện được nhân hai cái kết quả này nhân tích vô hướng với nhau được và cái này thì khi chúng ta chọn được thì cái vector u và v này cái vector u và v này nó nên là những cái ma trận low rank tức là có cái h nó thấp tức là chúng ta nên chọn cái k chúng ta nên chọn cái k nó bé hơn so với d1 d2 rất là nhiều thì cái việc chọn k sao cho cái hạng nó thấp để cho cái u và v này có cái hạng thấp nó sẽ giúp cho chúng ta rất nhiều việc khi k mà thấp thì đồng nghĩa u của chúng ta u của chúng ta sẽ là x tham số"
        },
        {
          "index": 10,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 449,
          "end_time": 510,
          "text": "để cho cái u và v này có cái hạng thấp nó sẽ giúp cho chúng ta rất nhiều việc khi k mà thấp thì đồng nghĩa u của chúng ta u của chúng ta sẽ là x tham số tương tự như vậy v của chúng ta cũng sẽ x tham số và cái ma trận u và v là hai cái ma trận trọng số để cho cái quá trình huấn luyện của mình nếu như mà x tham số thì rõ ràng là chúng ta sẽ tránh được cái vấn đề về overfitting không phải tránh mà là chúng ta sẽ giảm bớt giảm bớt được cái vấn đề về overfitting giảm bớt được cái vấn đề về overfitting thì đây là cái dạng biểu diễn dưới dạng là hình ảnh trực quan để chúng ta hình dung được là các cái ma trận low rank là như thế nào đây là ma trận h nè và ma trận u thì đây sẽ là cái chiều k của mình và k này k nhỏ thì k tốt tương tự như vậy v v của mình"
        },
        {
          "index": 11,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 499,
          "end_time": 560,
          "text": "đây là ma trận h nè và ma trận u thì đây sẽ là cái chiều k của mình và k này k nhỏ thì k tốt tương tự như vậy v v của mình thì cũng sẽ là có số chiều là k là low rank k nó phải thấp thì nó sẽ là hai ma trận có cái hạn thấp và khi nhân hai cái thằng này lại với nhau thì nhận u với lại v thì chúng ta sẽ có cái chiều k mà trận này ma trận này sẽ có cái chích thước giống với lại cái chích thước trong cái phiên bản trước đó là d2 nhân cho d1 rồi và phiên bản thứ ba à xin lỗi phiên bản thứ tư đó là extension với các cái tích ma trận tức là chúng ta sẽ có cái công thức như sau bản chất là chúng ta cũng ánh xạ cái se và h về cùng một cái không gian nhưng mà thay vì chúng ta thực hiện cái thao tác tích vô hướng thì chúng ta thực hiện cái phép cộng thực hiện cái phép cộng rồi sau đó qua cái hàm tanh"
        },
        {
          "index": 12,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 549,
          "end_time": 608,
          "text": "về cùng một cái không gian nhưng mà thay vì chúng ta thực hiện cái thao tác tích vô hướng thì chúng ta thực hiện cái phép cộng thực hiện cái phép cộng rồi sau đó qua cái hàm tanh rồi nhân với lại cái vector v thì trong cái trường hợp này w1 w2 và v sẽ có các cái kích thước như trên và đây sẽ là các cái trọng số của mô hình của mình đây là các cái trọng số của mô hình của mình rồi như vậy thì chúng ta sẽ chú ý đến cái phiên bản cái biến thể này của extension từ nay trở về sau là cái biến thể extension với nhân ma trận giảm bật được sử dụng trong các cái công thức của transformer tại vì cái ý tưởng của nó rất là hay và nó tập quát tức là các cái vector ở các cái không gian khác nhau sẽ được map về cùng một cái không gian chúng ta sẽ tìm cái ma trận u và v để map nó về cùng một không gian để từ đó là có thể"
        },
        {
          "index": 13,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 599,
          "end_time": 659,
          "text": "sẽ được map về cùng một cái không gian chúng ta sẽ tìm cái ma trận u và v để map nó về cùng một không gian để từ đó là có thể thực hiện được cái thao tác nhân tích vô hướng so hoặc là có thể so sánh được có thể so sánh tương đồng được với nhau thì đó chính là các cái biến thể của extension và extension nó là một cái kỹ thuật rất là tổng quát và nó có thể sử dụng cho những cái kiến trúc cho nhiều cái kiến trúc khác nhau chứ không chỉ là sys-to-sys ví dụ như trong lĩnh vực deep learning của hình ảnh mạng cnl cũng sử dụng extension rất là nhiều và thường sử dụng extension để mà tăng cái hiệu năng của cái việc của cái kết quả trong cái bài toán của bên thị giác máy tính cũng như là dùng extension để phục vụ cho cái việc là trực quan hóa"
        },
        {
          "index": 14,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 650,
          "end_time": 707,
          "text": "việc của cái kết quả trong cái bài toán của bên thị giác máy tính cũng như là dùng extension để phục vụ cho cái việc là trực quan hóa các cái kết quả của bên mạng cnl thì như chúng ta đã đề cập ở trong những slide trước một trong những cái tính năng của extension đó là cái tính năng có thể giải thích được và nó giúp cho chúng ta visualize cái kết quả của mình để biết được là cái mô hình của mình tại sao nó đưa ra được cái trọ lượng đó thì thì nó sẽ chú ý đến cái vùng nào trong cái thông tin đầu vào của mình thì đây là một kỹ thuật rất là hay và áp dụng trong không chỉ trong xử lý ngôn ngữ tự nhiên mà cả trong sử dụng ảnh cũng dùng rất là nhiều rồi nó dùng cho rất nhiều bài toán chứ không phải dịch máy thì cái này chúng ta cũng đã nói ở trong những slide trước nó có thể áp dụng cho bài toán tấm tắt văn bản chatbot hoặc là syncod và"
        },
        {
          "index": 15,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 699,
          "end_time": 760,
          "text": "ở trong những slide trước nó có thể áp dụng cho bài toán tấm tắt văn bản chatbot hoặc là syncod và định nghĩa một cách tổng quát cho extension đó là chúng ta sẽ cho trước chúng ta sẽ cho trước một cái tập các cái vector value và một cái vector query và extension là một cái kỹ thuật để giúp cho chúng ta tính cái trọng số của các cái value dựa trên cái query này tức là query này giống như là quá trình mà chúng ta truy vấn tìm kiếm trên mạng internet đúng không đưa vào các cái từ khóa và nó sẽ đi so nó sẽ đi so sánh với các cái từ khóa ở trên các trang web rồi sau đó nó sẽ tổng hợp thông tin lại để trả về cho chúng ta thì đó chúng ta so sánh nó với lại cái hệ thống search engine và trong cái mô hình siktosyx cộng với extension thì mỗi một cái trạng thái ẩn của decoder"
        },
        {
          "index": 16,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 747,
          "end_time": 809,
          "text": "thì đó chúng ta so sánh nó với lại cái hệ thống search engine và trong cái mô hình siktosyx cộng với extension thì mỗi một cái trạng thái ẩn của decoder thì nó sẽ được gọi là query và mỗi và tất cả các cái trạng thái ẩn của encoder thì nó gọi là value như vậy thì ở đây chúng ta đang tổng quát hóa cho cái extension với cái quá trình mà encode thì các cái giá trị ẩn của mình nó sẽ được gọi là query còn trong cái quá trình decode tức là trong cái quá trình chúng ta bắt đầu giải mã chúng ta phải thực hiện cái công đoạn gọi là group up và tra cứu tra cứu vào những cái đoạn văn đồ vào để chúng ta nhìn lại xem là ứng với cái thời điểm hiện tại chúng ta cần phải để tâm đến cái value nào nhiều thì đó chính là một cái quá trình search, một cái quá trình tìm kiếm thì extension có thể sít to sít với extension nó sẽ được liên tưởng đến cái bài toán tìm kiếm như vậy và như vậy thì trong cái bài hôm nay"
        },
        {
          "index": 17,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 800,
          "end_time": 860,
          "text": "sít to sít với extension nó sẽ được liên tưởng đến cái bài toán tìm kiếm như vậy và như vậy thì trong cái bài hôm nay chúng ta đã cùng tìm hiểu qua về bài toán dịch máy tại sao chúng ta tìm hiểu về bài toán này đó là vì đây là một cái bài toán khó và nó đồng thời có cái tính tổng quát nó sẽ có cái tính tổng quát cao nó sẽ các cái mô hình mà có thể giải quyết được bài toán dịch máy thì đều có thể áp dụng để giải quyết các cái bài toán tương tự như là ấm tắt văn bản như là chatbot như là tạo sync code và chúng ta đã tìm hiểu về một cái cải tiến của cái mạng ANN đó chính là chúng ta sẽ dùng cái extension một cái kỹ thuật extension extension nó đã giúp cho chúng ta giải quyết rất nhiều vấn đề vấn đề về điểm ngạn thông tin vấn đề về vanishing radian"
        },
        {
          "index": 18,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 850,
          "end_time": 909,
          "text": "extension nó đã giúp cho chúng ta giải quyết rất nhiều vấn đề vấn đề về điểm ngạn thông tin vấn đề về vanishing radian và đồng thời là nó có thể giúp cho chúng ta giải thích được kết quả hoặc là trực quan hóa và trong phần cuối thì chúng ta đã tìm hiểu về một số cái biến thể một số cái biến thể của extension và trong số các cái biến thể này thì chúng ta cần phải nhớ đến cái biến thể đó là extension với cái nhân ma trận low rank ma trận có cái hạn thấp ma trận có cái hạn thấp  và đây sẽ là một cái kỹ thuật extension được sử dụng trong các cái kiến trúc về transformer về sao và đây sẽ là một cái kỹ thuật extension"
        },
        {
          "index": 19,
          "video_id": "Chương 8_my3qRjVJ7VM",
          "chapter": "Chương 8",
          "video_title": "[CS431 - Chương 9] Part 3： Một số biến thể của Attention",
          "video_url": "https://youtu.be/my3qRjVJ7VM",
          "start_time": 899,
          "end_time": 909,
          "text": "ma trận có cái hạn thấp  và đây sẽ là một cái kỹ thuật extension được sử dụng trong các cái kiến trúc về transformer về sao và đây sẽ là một cái kỹ thuật extension"
        }
      ]
    },
    {
      "video_id": "Chương 9_1tCmeHf1Xk0",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giới thiệu về Transformer và một số ứng dụng của Transformer trong xử lý ngôn ngữ tự nhiên, đồng thời làm nền tảng để tiếp tục học các thành tựu của Transformer trong các lĩnh vực khác (ví dụ: hình ảnh, âm thanh, v.v.). [1]  \n- Các khái niệm sẽ được ôn tập trong bài: *Attention* (gồm Attention Score, Attention Distribution, Attention Output), cùng ôn lại các mô hình hồi quy (RNN/LSTM) và cách chúng được dùng trong bài toán NLP trước khi có Transformer. [1][2][3]  \n- Phạm vi: Bài tập trung vào khái niệm Attention và mối liên hệ với kiến trúc encoder–decoder dựa trên LSTM (bidirectional encoder, unidirectional decoder), và ý tưởng mở rộng (extension) để truy xuất bộ nhớ. [2][3][4][7]\n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Khái niệm cơ bản về Attention\n- Attention là cơ chế cho phép mô hình xác định mức độ \"quan tâm\" tới từng vị trí trong chuỗi input khi xử lý tại một vị trí hiện tại; kết quả của Attention được sử dụng để tổng hợp thông tin từ các trạng thái ẩn (values) thành một *Attention Output*. [1][3]  \n- Thành phần chính trong Attention:\n  - *Value*: là các trạng thái ẩn của input (ví dụ các hidden state S1, S2, …, SN). [2]  \n  - *Query*: là vector truy vấn tại vị trí hiện tại, dùng để tính trọng số so với các value. [2]  \n  - Từ Query và Value ta tính *Attention Score*, sau đó chuẩn hóa thành *Attention Distribution* (trọng số chú ý), rồi nhân với Values để lấy *Attention Output*. [1][2][3]\n\n### 2.2. Vai trò của Attention Output trong Encoder–Decoder\n- Attention Output tại vị trí hiện tại được dùng làm thông tin bổ sung để tính toán giá trị output trong bước Decoder — tức là decoder dùng Attention Output để tập trung vào các vị trí input quan trọng khi sinh từ. [3]  \n- Những token không được quan tâm nhiều sẽ có trọng số thấp hơn trong Attention Distribution nên đóng góp nhỏ hơn vào Attention Output. [2][3]\n\n### 2.3. Mô hình hồi quy (RNN/LSTM) truyền thống cho NLP\n- Trước khi có Transformer, các bài toán NLP thường dùng kiến trúc hồi quy với cell là LSTM (Long Short-Term Memory). [3][4]  \n- Encoder thường được triển khai theo dạng *bidirectional* (đọc cả trái→phải lẫn phải→trái) để thu thập ngữ cảnh đầy đủ của input; các trạng thái ẩn của encoder được biểu diễn (trong bài giảng) bằng các vector màu xanh. [3][4]  \n- Decoder khi sinh output hoạt động theo chiều đơn (unidirectional) vì trong quá trình sinh ta không được phép nhìn thấy các từ tương lai (chưa biết đáp án), nên quá trình suy đoán phải diễn ra lần lượt từng token một. Decoder thường được biểu diễn bằng màu khác (màu đỏ) để phân biệt với encoder. [5][6]\n\n### 2.4. Ý tưởng mở rộng (extension) — truy xuất bộ nhớ linh hoạt (tiền thân của Attention)\n- Các mô hình hồi quy truyền thống có thể tương tác với thông tin ở những vị trí xa trong chuỗi nhưng cần đi qua nhiều bước (nhiều biến đổi liên tiếp) nên kém hiệu quả khi truy xuất ngữ cảnh ở xa. [7][8]  \n- Module *extension* (ý tưởng tương tự Attention) cho phép kết nối trực tiếp đến các vị trí trong bộ nhớ (ví dụ truy xuất token đầu tiên) chỉ bằng một phép biến đổi, nhờ đó truy xuất thông tin từ xa trở nên ngắn gọn và hiệu quả hơn so với phải đi nhiều bước. [7][8]  \n- Điểm mạnh của extension: cho phép truy xuất linh hoạt đến bộ nhớ, giảm số bước cần thiết để liên hệ thông tin từ các vị trí xa. [7][8]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa cơ chế Attention: Hình minh họa trong bài cho thấy các trạng thái ẩn S1..SN làm Value và các vector Query dùng để tính trọng số, từ đó tổng hợp Attention Output để biết \"ta đang quan tâm đến từ nào\" trong chuỗi input tại vị trí hiện tại. [2][3]  \n- Ứng dụng thực tế được đề cập: Transformer và cơ chế Attention không chỉ dùng trong NLP mà còn có thể áp dụng cho các lĩnh vực khác như xử lý hình ảnh và xử lý âm thanh — bài giảng nhấn mạnh Transformer là nền tảng cho các ứng dụng này. [1]  \n- Trường hợp sử dụng của extension/Attention trong mô hình encoder–decoder: khi decode tại một vị trí, nếu không có extension thì để truy xuất thông tin từ token ở đầu chuỗi phải đi qua nhiều bước; với extension/attention có thể truy xuất trực tiếp chỉ bằng một phép biến đổi, giúp cải thiện khả năng truy xuất ngữ cảnh dài. [7][8]\n\n---\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính: Bài ôn tập các khái niệm Attention (Score, Distribution, Output), mô tả vai trò Query/Value trong tính toán Attention, nêu lại cấu trúc encoder–decoder truyền thống với encoder *bidirectional* và decoder *unidirectional*, và giới thiệu ý tưởng extension (tiền đề cho cơ chế Attention) giúp truy xuất bộ nhớ linh hoạt và hiệu quả hơn. [1][2][3][4][5][6][7][8]  \n- Tầm quan trọng: Hiểu rõ Attention và hạn chế của mô hình hồi quy truyền thống là nền tảng để học tiếp về Transformer và các ứng dụng của nó trên nhiều lĩnh vực (NLP, hình ảnh, âm thanh, ...). [1]  \n- Liên hệ với các bài giảng khác: Bài này được trình bày là nền tảng để tiếp tục học các thành tựu của Transformer trong các lĩnh vực khác nhau (sẽ được tiếp tục ở các phần sau). [1]",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 9_1tCmeHf1Xk0",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 1： Ôn tập kiến trúc mạng RNN và cơ chế Attention",
          "video_url": "https://youtu.be/1tCmeHf1Xk0",
          "start_time": 13,
          "end_time": 62,
          "text": "Chào các bạn, hôm nay chúng ta sẽ cùng đến với bài Transformer và một số ứng dụng của Transformer trong xử lý ngôn ngữ tự nhiên. Đây có thể nói là một trong những bài rất là quan trọng, nó sẽ là nền tảng cho chúng ta để có thể học tiếp những thành tựu của Transformer trong các lĩnh vực khác, không phải chỉ trong lĩnh vực về xử lý ngôn ngữ tự nhiên mà cũng có thể là dùng trong lĩnh vực về hình ảnh, về xử lý âm thanh, v.v. Nội dung của bài hôm nay chúng ta sẽ cùng đầu tiên đó là chúng ta sẽ ôn tập về một số cái khái niệm ví dụ như là Attention là gì, rồi chúng ta sẽ tính cái Attention Score, Attention Distribution và Attention Output đó là gì."
        },
        {
          "index": 2,
          "video_id": "Chương 9_1tCmeHf1Xk0",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 1： Ôn tập kiến trúc mạng RNN và cơ chế Attention",
          "video_url": "https://youtu.be/1tCmeHf1Xk0",
          "start_time": 44,
          "end_time": 111,
          "text": "Nội dung của bài hôm nay chúng ta sẽ cùng đầu tiên đó là chúng ta sẽ ôn tập về một số cái khái niệm ví dụ như là Attention là gì, rồi chúng ta sẽ tính cái Attention Score, Attention Distribution và Attention Output đó là gì. Thì trong cái hình ở đây chúng ta thấy đó là các cái trạng thái ẩn là S1, S2 cho đến SN của mình thì nó sẽ được gọi là Value. Còn các cái vector truy vấn thì chúng ta sẽ gọi là Query, như khác ở đây thì được gọi là Query. Và chúng ta sẽ đi lần lượt tính cái giá trị trọng số của cái Query với lại cái vector Output này để từ đó là chúng ta biết là tại cái vị trí hiện tại chúng ta sẽ quan tâm đến cái từ nào trong cái chuỗi Input của mình. Và sau đó chúng ta sẽ cùng tổng hợp cái Attention Output để lấy cái trạng thái ẩn của cái từ nào mà chúng ta đang quan tâm nhiều. Còn những cái từ nào mà chúng ta không có quan tâm nhiều thì cái trọng số của nó sẽ thấp hơn."
        },
        {
          "index": 3,
          "video_id": "Chương 9_1tCmeHf1Xk0",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 1： Ôn tập kiến trúc mạng RNN và cơ chế Attention",
          "video_url": "https://youtu.be/1tCmeHf1Xk0",
          "start_time": 97,
          "end_time": 170,
          "text": "Và sau đó chúng ta sẽ cùng tổng hợp cái Attention Output để lấy cái trạng thái ẩn của cái từ nào mà chúng ta đang quan tâm nhiều. Còn những cái từ nào mà chúng ta không có quan tâm nhiều thì cái trọng số của nó sẽ thấp hơn. Đó. Thì từ cái Attention Output này chúng ta sẽ được sử dụng để đi tính toán các cái giá trị Output ở cái bước Decoder. Thì đây chính là cái ý tưởng của Attention. Và ngoài ra thì chúng ta cũng ôn tập về một số cái mô hình, về cái cách thức mà chúng ta sử dụng mô hình của Hồi Quy cho bài toán NLP. Thì đầu tiên đó là cái chuỗi Input của mình. Nó sẽ được encode 2 chiều. 3D có nghĩa là 2 chiều. Để đảm bảo cho cái đầu vào của mình, mình có thể đọc theo trình tự từ trái sang phải và từ phải sang trái. Chúng ta có đầy đủ được cái thông tin về mặt ngữ cảnh của cái Input một cách đầy đủ. Và trong cái môi nồi quy mà khi chưa có cái Transformer ra đời thì chúng ta sẽ sử dụng cái kiến trúc ANN với cái cell là LSTM, Long Short Term Memory."
        },
        {
          "index": 4,
          "video_id": "Chương 9_1tCmeHf1Xk0",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 1： Ôn tập kiến trúc mạng RNN và cơ chế Attention",
          "video_url": "https://youtu.be/1tCmeHf1Xk0",
          "start_time": 144,
          "end_time": 211,
          "text": "Để đảm bảo cho cái đầu vào của mình, mình có thể đọc theo trình tự từ trái sang phải và từ phải sang trái. Chúng ta có đầy đủ được cái thông tin về mặt ngữ cảnh của cái Input một cách đầy đủ. Và trong cái môi nồi quy mà khi chưa có cái Transformer ra đời thì chúng ta sẽ sử dụng cái kiến trúc ANN với cái cell là LSTM, Long Short Term Memory. Thì đây chính là cái ý tưởng chung, những cái kiến trúc chung cho các cái mô hình Hồi Quy sử dụng cho các cái bài toán NLP. Và ở trong hình đây thì chúng ta thấy đó là cái bước encoder thì nó sẽ được thực hiện tính toán. Đó là 2 chiều, bidirectional. Và ở đây chúng ta ký hiệu bằng các cái trạng thái ẩn là bằng các cái vector màu xanh tương ứng lại encoder. Rồi. Để tính toán cho cái output thì chúng ta cũng sẽ biểu diễn output dưới dạng chuỗi. Input của mình nó đã là chuỗi rồi thì output của mình nó cũng sẽ dạng chuỗi."
        },
        {
          "index": 5,
          "video_id": "Chương 9_1tCmeHf1Xk0",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 1： Ôn tập kiến trúc mạng RNN và cơ chế Attention",
          "video_url": "https://youtu.be/1tCmeHf1Xk0",
          "start_time": 201,
          "end_time": 260,
          "text": "Rồi. Để tính toán cho cái output thì chúng ta cũng sẽ biểu diễn output dưới dạng chuỗi. Input của mình nó đã là chuỗi rồi thì output của mình nó cũng sẽ dạng chuỗi. Và chúng ta sử dụng LSTM. Chúng ta cũng sử dụng LSTM để sinh ra kết quả. Tuy nhiên ở đây chúng ta có một cái nhận xét đó là cái LSTM này thì nó sẽ đi theo một chiều chứ nó không có đi 2 chiều. Tại vì về nguy tắc là ở cái quá trình output. Chúng ta sẽ không thấy trước cái kết quả của mình. Ví dụ như trong trường hợp encoder chúng ta có thể đi theo chiều ngược lại là vì chúng ta được phép thấy cái dữ kiện của mình ở phía sau truyền lên phía trước và phía trước truyền phía sau. Nhưng mà khi chúng ta tính cái giá trị output đó chúng ta không được phép thấy những cái giá trị phía sau. Chúng ta chỉ phải lần lượt suy đoán từ từng cái từ một. Chúng ta suy đoán ở đây. Rồi. Sau đó mới đến đây."
        },
        {
          "index": 6,
          "video_id": "Chương 9_1tCmeHf1Xk0",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 1： Ôn tập kiến trúc mạng RNN và cơ chế Attention",
          "video_url": "https://youtu.be/1tCmeHf1Xk0",
          "start_time": 246,
          "end_time": 310,
          "text": "Nhưng mà khi chúng ta tính cái giá trị output đó chúng ta không được phép thấy những cái giá trị phía sau. Chúng ta chỉ phải lần lượt suy đoán từ từng cái từ một. Chúng ta suy đoán ở đây. Rồi. Sau đó mới đến đây. Rồi sau đó mới đến đây. Chứ không có chuyện là chúng ta nhận được thông tin từ cái giá trị cuối truyền lên đầu. Tại vì lúc đó chúng ta chưa có biết cái đáp án. Thì đó là lý do tại sao cái phần output chúng ta sẽ ký hiệu bằng một cái màu riêng. Nó là màu đỏ và chúng ta chỉ có một chiều. Một chiều. Thay vì là hai chiều giống như trên đây. Ở trên đây là hai chiều. Tối cùng đó là các cái mô hình hồi quy cho bài toán NLP có sử dụng một cái kỹ thuật đó là extension để linh hoạt truy xuất cái mẫu nhớ của mình. Để linh hoạt truy xuất mẫu nhớ của mình. Thì ở đây chúng ta sẽ set đến cái quá trình là chúng ta decode tại cái vị trí này."
        },
        {
          "index": 7,
          "video_id": "Chương 9_1tCmeHf1Xk0",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 1： Ôn tập kiến trúc mạng RNN và cơ chế Attention",
          "video_url": "https://youtu.be/1tCmeHf1Xk0",
          "start_time": 291,
          "end_time": 361,
          "text": "Tối cùng đó là các cái mô hình hồi quy cho bài toán NLP có sử dụng một cái kỹ thuật đó là extension để linh hoạt truy xuất cái mẫu nhớ của mình. Để linh hoạt truy xuất mẫu nhớ của mình. Thì ở đây chúng ta sẽ set đến cái quá trình là chúng ta decode tại cái vị trí này. Đúng không? Thì tại đây nếu như không có cái extension module này á. Thì các cái thông tin của những cái từ này.  Từ ở rất là xa. Nó sẽ tương tác được nhưng mà phải thông qua cái số bước di chuyển rất là dài. Trong khi đó nếu nhờ cái extension. Nếu nhờ cái module extension này. Thì chúng ta có thể linh hoạt truy xuất được các cái thông tin. Từ đầu. Của những cái từ ở đầu tiên. Chúng ta có thể truy xuất được những cái từ đầu tiên. Một cách dễ dàng và với cái số bước rất là ngắn. Ví dụ trong cái hình này. Trong cái quá trình này. Tại cái vị trí này. Đúng không? Chúng ta có thể truy xuất cái từ đầu tiên. Chỉ thông qua một phép biến đổi. Đó là tại đây. Một phép biến đổi. Trong khi đó nếu như chúng ta thực hiện tại đây. Theo không có cái extension output."
        },
        {
          "index": 8,
          "video_id": "Chương 9_1tCmeHf1Xk0",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 1： Ôn tập kiến trúc mạng RNN và cơ chế Attention",
          "video_url": "https://youtu.be/1tCmeHf1Xk0",
          "start_time": 348,
          "end_time": 396,
          "text": "Chúng ta có thể truy xuất cái từ đầu tiên. Chỉ thông qua một phép biến đổi. Đó là tại đây. Một phép biến đổi. Trong khi đó nếu như chúng ta thực hiện tại đây. Theo không có cái extension output. Chúng ta sẽ phải đi một lần, hai lần, ba lần, bốn lần, năm lần. Chúng ta phải mất năm lần xử lý. Năm lần biến đổi thì mới đến được đến cái vị trí mà chúng ta cần phải xử lý. Trong khi đó với cái output. Với cái extension output. Thì tại đây chúng ta kết nối trực tiếp. Rồi sau đó chúng ta sẽ đưa ra cái giá trị tính toán tiếp theo. Thì đây chính là cái ý tưởng của extension và cái điểm mạnh của extension. Đó là cho phép mình có thể linh hoạt truy xuất đến cái bộ nhớ của mình."
        }
      ]
    },
    {
      "video_id": "Chương 9_5DE5HXG8FWk",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Trình bày **động lực** dẫn tới việc đề xuất kiến trúc Transformer, bao gồm (1) hiểu vì sao cần Attention/Transformer để xử lý sự tương tác giữa các từ trong câu, (2) nêu các điểm yếu của kiến trúc tuần tự hiện có, và (3) đề cập đến một số ứng dụng / thành tựu liên quan. [1]\n\n- Các khái niệm sẽ được đề cập: **kiến trúc Transformer**, **module Attention**, **độ dài tương tác (sequence length / T)**, **tối đa hóa thao tác song song (parallelism)**, và **vấn đề vanishing gradient / phụ thuộc dài** trong mô hình tuần tự. [1][2][3]\n\n## 2. Các điểm chính (Main Points)\n\n### A. Động lực cơ bản: tương tác giữa các từ trong ngôn ngữ tự nhiên\n- Trong một đoạn input, *mọi cặp từ* có thể tương tác về mặt thông tin; để hiểu nội dung và sinh output phù hợp, các từ cần tham chiếu lẫn nhau (ví dụ: quan hệ ý nghĩa giữa từ này và từ kia). Đây là động lực chính cho việc cần có cơ chế Attention. [1][2]\n\n### B. Mục tiêu của Transformer: giảm độ dài tương tác và tăng song song\n- Mục tiêu chính của Transformer là **tối thiểu hóa độ dài của con đường tương tác** giữa hai từ bất kỳ trong câu (giảm số bước mà thông tin phải lan truyền) và **tối đa hóa khả năng tính toán song song**, để khai thác hiệu quả sức mạnh của GPU. [2][3]\n\n### C. Minh họa bằng ví dụ chuỗi (sequence) và kí hiệu chiều dài\n- Ví dụ minh họa: câu kiểu \"In France I had a great day ... language\" — để điền vào chỗ trống (predict word related to *language*), cần tương tác thông tin giữa từ *France* và vị trí chỗ trống; trong kiến trúc tuần tự, thông tin đó phải lan truyền qua toàn bộ chuỗi, tốn số bước bằng **chiều dài sequence** (ký hiệu T trong bài giảng). [3][4]\n- Khi thông tin của một từ (ví dụ *France*) lan truyền tới vị trí cần điền, quá trình này tốn T bước (sequence length), và trong quá trình đó thông tin có thể bị giảm chất lượng (degraded) do nhiều phép biến đổi trung gian. [4]\n\n### D. Hạn chế của kiến trúc tuần tự: vanishing gradient và khó huấn luyện\n- Kiến trúc tuần tự (ví dụ các biến thể RNN) có **phụ thuộc dài** => dẫn tới vấn đề *vanishing gradient*: các đạo hàm thành phần nhỏ khi nhân nhiều lần sẽ tạo ra giá trị rất nhỏ, làm giảm bước cập nhật tham số (parameter update) và khiến việc huấn luyện trở nên khó khăn khi chuỗi dài. [5][6]\n\n### E. Vấn đề song song hóa và chi phí tính toán\n- Kiến trúc tuần tự buộc phải thực hiện các phép tính theo thứ tự (từ trái sang phải hoặc ngược lại), nên không tận dụng tốt các bộ xử lý song song (GPU) vì GPU cần các phép toán độc lập để phân bổ cho các core. Các trạng thái ẩn quá khứ phải được tính xong mới tính trạng thái hiện tại, tạo ra sự phụ thuộc tuần tự. [7][8]\n- Chi phí tính toán trong các phép toán không song song được biểu diễn bằng ký hiệu được nhắc đến trong bài (ví dụ \"O-SQL LEN\" / O(seq_len)) — tức là phép toán tốn chi phí tỷ lệ với chiều dài chuỗi; để tính tới trạng thái cuối cùng thường cần thực hiện khoảng T phép tính trước đó. [6][7]\n\n### F. Kết luận động lực: cần một kiến trúc cho tương tác ngắn và song song\n- Do các vấn đề trên (phụ thuộc dài, vanishing gradient, không tận dụng GPU, chi phí tăng theo chiều dài chuỗi), động lực cuối cùng là đề xuất một kiến trúc (Transformer) cho phép **rút ngắn con đường tương tác giữa các từ** và **thực hiện nhiều phép tính song song**, giải quyết giới hạn của mô hình tuần tự. [2][3][8][9]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa chính trong bài: câu ví dụ \"In France I had a great day ... language\" dùng để nhấn mạnh rằng để predict từ ở chỗ trống (liên quan tới *language*) cần tương tác thông tin từ *France* đến vị trí chỗ trống; trong kiến trúc tuần tự, thông tin này phải truyền qua toàn bộ sequence và tốn T bước. [3][4]\n\n- Ứng dụng / hậu quả thực tế nêu trong bài:\n  - Vì các biến thể của ANN tuần tự không khai thác được GPU hiệu quả, **không thể huấn luyện trên các dataset cực lớn với số tham số rất lớn** nếu vẫn giữ cách tính tuần tự; đó là một trong những lý do cần Transformer để có thể huấn luyện các mô hình lớn trên dữ liệu lớn. [8][9]\n  - Bài giảng còn dự định đề cập đến một số ứng dụng và thành tựu của Transformer (như phần cuối của bài), nhưng trong các đoạn cung cấp chỉ nêu ý sẽ nói về ứng dụng và thành tựu chứ không liệt kê chi tiết. [1]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt ý chính:\n  - Ngôn ngữ tự nhiên đòi hỏi các tương tác giữa các từ; kiến trúc tuần tự dẫn tới đường truyền thông tin dài (chiều dài sequence T) và chi phí tính toán tăng theo độ dài chuỗi. [1][2][4]\n  - Phụ thuộc tuần tự gây ra *vanishing gradient* và khó khăn khi huấn luyện, đồng thời không tận dụng tốt khả năng tính toán song song của GPU. [5][6][7][8]\n  - Do đó, cần một kiến trúc mới (Transformer) nhằm **rút ngắn độ dài tương tác giữa từ** và **tối đa hóa thao tác song song**, để huấn luyện hiệu quả trên dữ liệu và mô hình cỡ lớn. [2][3][8][9]\n\n- Tầm quan trọng: Nội dung này giải thích động lực lý thuyết và thực tiễn dẫn tới sự ra đời của Transformer — bước chuyển quan trọng để xử lý các phụ thuộc dài trong ngôn ngữ và để tận dụng phần cứng hiện đại trong đào tạo mô hình lớn. [2][8]\n\n- Liên hệ với các bài giảng khác: bài giảng này là phần tiếp nối trong chương (đã đề cập là Part 2 của Chương 10 / Chương 9 trong danh mục), sẽ được tiếp tục bằng phần trình bày chi tiết về kiến trúc Transformer, các điểm yếu cụ thể của Transformer, và một số ứng dụng/thành tựu (những phần này được nhắc sẽ có trong nội dung hôm nay). [1]\n\n",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 9_5DE5HXG8FWk",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 2： Động lực của kiến trúc Transformer",
          "video_url": "https://youtu.be/5DE5HXG8FWk",
          "start_time": 0,
          "end_time": 60,
          "text": "Nội dung hôm nay thì chúng ta gồm có 3 phần Đầu tiên đó là chúng ta sẽ cùng tìm hiểu về kiến trúc Transformer Sau đó chúng ta sẽ cùng tìm hiểu về một số cái điểm yếu của Transformer Và cuối cùng đó là một số ứng dụng cũng như là thành tựu Đầu tiên thì về kiến trúc thì chúng ta sẽ xét đến cái động lực Tại sao chúng ta cần phải có cái kiến trúc mạng Transformer Động lực đầu tiên xuất phát từ việc đó là Giữa 2 cái từ bất kỳ Giữa 2 cái từ bất kỳ Trong cái đoạn vang input của mình Chúng ta tương tác với nhau, tương tác về mặt thông tin với nhau Chúng ta phải tốn rất nhiều thao tác Ví dụ, ở đây chúng ta sẽ có 2 cái từ này Thì rõ ràng là trong sự liên ngôn ngữ tự nhiên Nó sẽ có tình huống đó là các cái từ Nó phải có cái sự liên hệ về mặt ý nghĩa với nhau Thì từ đó chúng ta mới có thể hiểu Hiểu rõ được cái nội dung của cái input của mình Là gì? Khi đó chúng ta mới có thể đi tính"
        },
        {
          "index": 2,
          "video_id": "Chương 9_5DE5HXG8FWk",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 2： Động lực của kiến trúc Transformer",
          "video_url": "https://youtu.be/5DE5HXG8FWk",
          "start_time": 48,
          "end_time": 110,
          "text": "Nó sẽ có tình huống đó là các cái từ Nó phải có cái sự liên hệ về mặt ý nghĩa với nhau Thì từ đó chúng ta mới có thể hiểu Hiểu rõ được cái nội dung của cái input của mình Là gì? Khi đó chúng ta mới có thể đi tính Toán ra các cái giá trị output cho nó phù hợp Thì ở đây cũng vậy Nếu như không có cái module Attention này Thì giữa 2 từ bất kỳ Giữa 2 từ bất kỳ Trong một cái câu của mình Nó tương tác với nhau Thông qua Phải là số cái từ trong câu của mình tức là SQLen Thì chút nữa chúng ta sẽ nói kỹ hơn Và như vậy thì động lực của Transformer đó là làm sao chúng ta có thể Tối thiểu hóa Tức là làm giảm bớt Cái độ dài của cái sự tương tác này Giảm bớt được cái sự Cái độ dài của cái sự tương tác giữa 2 từ bất kỳ trong câu Và chúng ta sẽ tối đa hóa Cái thao tác song song"
        },
        {
          "index": 3,
          "video_id": "Chương 9_5DE5HXG8FWk",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 2： Động lực của kiến trúc Transformer",
          "video_url": "https://youtu.be/5DE5HXG8FWk",
          "start_time": 98,
          "end_time": 161,
          "text": "Cái độ dài của cái sự tương tác này Giảm bớt được cái sự Cái độ dài của cái sự tương tác giữa 2 từ bất kỳ trong câu Và chúng ta sẽ tối đa hóa Cái thao tác song song Tại vì Deep Learning Muốn mà hiện quả thì nó phải Khai thác được các cái sức mạnh của các cái thiết bị tính toán song song Nhưng hiện tại thì Kiến trúc hiện tại nếu như chúng ta thực hiện Tính toán một cách tuần tự Từ trái sang phải hoặc từ phải sang trái Thì khi đó không có khai thác được cái Điểm mạnh Của GPU Của các cái bộ vi sử lý song song Và Đối với cái ý đó là tối thiểu hóa Độ dài tương tác giữa các cái cặp từ Thì chúng ta sẽ lấy một cái ví dụ sau In France I had a great day  Great time and I Chấm chấm chấm Ở đây là chúng ta sẽ điền vô chỗ trống Language Thì ở đây chúng ta sẽ thấy là có cái từ language Từ France và cái từ này chúng ta cần phải điền vào Thì khi đó Chúng ta đang muốn điền cái thông tin vào Cái chỗ chỗ trống này"
        },
        {
          "index": 4,
          "video_id": "Chương 9_5DE5HXG8FWk",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 2： Động lực của kiến trúc Transformer",
          "video_url": "https://youtu.be/5DE5HXG8FWk",
          "start_time": 150,
          "end_time": 211,
          "text": "Language Thì ở đây chúng ta sẽ thấy là có cái từ language Từ France và cái từ này chúng ta cần phải điền vào Thì khi đó Chúng ta đang muốn điền cái thông tin vào Cái chỗ chỗ trống này Chúng ta cần phải có cái sự Tương tác thông tin giữa từ France Và từ language Đúng không? Và cả cái từ mà chúng ta cần phải điền vào chỗ trống này Thì khi đó Thông tin của từ France Khi mà lan truyền Được đến đây Khi mà lan truyền được đến cái vị trí này Thì nó đã tốn một cái Chi phí đó là Sequence Sequence là chiều dài của chuỗi Và trong các cái hệ thống ký hiệu của mình Sequence của mình nó là T Hay còn gọi là Thay vì O, Sequence thì chúng ta sẽ ký hiệu là O, T Tức là chúng ta sẽ tốn T bước Và trong cái quá trình mà thông tin Của cái từ France nó lan truyền Đến được đây Thì nó đã bị mang một thông tin rất là nhiều rồi Do đó thì Với cái kiến trúc hiện tại"
        },
        {
          "index": 5,
          "video_id": "Chương 9_5DE5HXG8FWk",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 2： Động lực của kiến trúc Transformer",
          "video_url": "https://youtu.be/5DE5HXG8FWk",
          "start_time": 200,
          "end_time": 260,
          "text": "Và trong cái quá trình mà thông tin Của cái từ France nó lan truyền Đến được đây Thì nó đã bị mang một thông tin rất là nhiều rồi Do đó thì Với cái kiến trúc hiện tại Đó là tuần tự Thì rất khó để huấn luyện Do có cái sự phụ thuộc dài Từ language Rồi chỗ trống ở đây Nó sẽ phụ thuộc vào cái từ France Để điền vô cái này là từ French Muốn có được cái thông tin ở đây Thì chúng ta phải có được cái thông tin Từ France Và cái việc huấn luyện này Cái việc khó thuận là  Nó xuất phát từ cái vấn đề về Vanishing Gradient Tức là khi cái hàm của mình Hàm biến đổi của mình mà càng dài Thì các cái đạo hàm thành phần Của mình là càng bé Các cái đạo hàm thành phần của mình nó bé Thì khi chúng ta nhân lần lượt Tất cả các cái đạo hàm thành phần này lại với nhau Thì các cái giá trị bé nó nhân lại với nhau Nó sẽ tạo ra những cái giá trị vô cùng bé Nó làm giảm mất cái"
        },
        {
          "index": 6,
          "video_id": "Chương 9_5DE5HXG8FWk",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 2： Động lực của kiến trúc Transformer",
          "video_url": "https://youtu.be/5DE5HXG8FWk",
          "start_time": 248,
          "end_time": 310,
          "text": "Thì khi chúng ta nhân lần lượt Tất cả các cái đạo hàm thành phần này lại với nhau Thì các cái giá trị bé nó nhân lại với nhau Nó sẽ tạo ra những cái giá trị vô cùng bé Nó làm giảm mất cái Gọi là bước nhảy của cái tham số của mình Thì đó là cái bước nhảy của cái tham số của mình Và cái lý do tại sao Khi có cái sự phụ thuộc dài Thì cái mô hình của mình nó huấn luyện không còn hiệu quả nữa Rồi Và cái tiếp theo Của cái động lực tại sao chúng ta phải có Phải đề xuất ra cái kiến trúc mạng Transformer Đó chính là Chúng ta phải tối đa hóa Cái số phép xử lý xong xong Thì trong cái quá trình mà Fit Forward Hoặc là Make Work Thì chúng ta sẽ cần phải Tốn cái chi phí Đó là O-SQL LEN Tức là chúng ta phải cần có O-SQL LEN Cái phép toán không song song Cái phép toán không song song Thì ở trong cái hình ở đây Chúng ta sẽ thấy nè Là nếu như chúng ta thực hiện tần tự Từ trái sang phải"
        },
        {
          "index": 7,
          "video_id": "Chương 9_5DE5HXG8FWk",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 2： Động lực của kiến trúc Transformer",
          "video_url": "https://youtu.be/5DE5HXG8FWk",
          "start_time": 299,
          "end_time": 360,
          "text": "Cái phép toán không song song Cái phép toán không song song Thì ở trong cái hình ở đây Chúng ta sẽ thấy nè Là nếu như chúng ta thực hiện tần tự Từ trái sang phải Forward Hoặc là sau này khi chúng ta huấn luyện Là Backward Thì ở đây Ở bên trong cái ô này chúng ta sẽ ký hiệu là Nó sẽ phụ thuộc vào Những cái phép tính Trước đó Là cần phải phụ thuộc vào bao nhiêu phép tính trước đó Ví dụ Tại đây chúng ta thấy là nó điền vào số 1 Là vì nó bị phụ thuộc vào Một cái phép tính trước đó là đây Còn ở đây là bằng 0 Là vì chúng ta tính trực tiếp luôn Chúng ta không có bị phụ thuộc vào cái phép tính nào trước đó Thì ở đây chúng ta sẽ bị phụ thuộc Hai phép tính Do bị phụ thuộc ở đây là một phép tính Và cái phép tính ở đây Giá trị output ở đây nó lại bị phụ thuộc Bởi một cái phép tính Trước đó Như vậy thì khi chúng ta tính toán Đến cái phần tưởng cuối cùng Đến cái trạng thái ẩn cuối cùng"
        },
        {
          "index": 8,
          "video_id": "Chương 9_5DE5HXG8FWk",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 2： Động lực của kiến trúc Transformer",
          "video_url": "https://youtu.be/5DE5HXG8FWk",
          "start_time": 348,
          "end_time": 410,
          "text": "Và cái phép tính ở đây Giá trị output ở đây nó lại bị phụ thuộc Bởi một cái phép tính Trước đó Như vậy thì khi chúng ta tính toán Đến cái phần tưởng cuối cùng Đến cái trạng thái ẩn cuối cùng Thì chúng ta cần phải thực hiện T cái phép tính trước đó Tức là có cái sự phụ thuộc Tuy nhiên Ở đây thì chúng ta sẽ Thấy ra là GPU là cái cái mộ vi xử lý Xong xong thì nó chỉ có thể Thực hiện được các cái phép độc lập Tức là nó sẽ phân rã Các cái thao tác tính toán Cho từng cái co xử lý Và các cái co xử lý nó phải độc lập nhau Thì khi đó nó mới tính toán được Hiệu quả Trong khi đó ANN Hoặc là các biến thể của ANN Thì cái trạng thái ẩn Các cái trạng thái ẩn của mình trong quá khứ Nó sẽ tính xong Thì khi đó mới tính được những cái trạng thái hiện tại Tức là trạng thái hiện tại sẽ bị Phụ thuộc vào cái trạng thái ẩn trong quá khứ Thì dẫn đến là Không thể huấn luyện Trên những cái data set cực lớn được"
        },
        {
          "index": 9,
          "video_id": "Chương 9_5DE5HXG8FWk",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 2： Động lực của kiến trúc Transformer",
          "video_url": "https://youtu.be/5DE5HXG8FWk",
          "start_time": 398,
          "end_time": 429,
          "text": "Thì khi đó mới tính được những cái trạng thái hiện tại Tức là trạng thái hiện tại sẽ bị Phụ thuộc vào cái trạng thái ẩn trong quá khứ Thì dẫn đến là Không thể huấn luyện Trên những cái data set cực lớn được Tức là những cái Biến thức biến thể của ANN Nó không khai thác được GPU Dẫn đến là sau này chúng ta không thể Sử dụng được cái sức mạnh của GPU Để tính toán trên data set cực lớn Với cái số tham số cực lớn Thì đó chính là Cái động lực tại sao chúng ta cần phải Có transformer"
        }
      ]
    },
    {
      "video_id": "Chương 9_NsWX_5oV8bY",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giải thích **cơ chế self-attention** trong Transformer — khái niệm, khác biệt với attention giữa encoder–decoder, và lợi ích về khả năng *song song hóa* khi tính toán. [1][2]  \n- Các khái niệm sẽ được đề cập: *attention* (query truy xuất giá trị từ encoder), *self-attention* (các token trong cùng một pha — encoder hoặc decoder — tự chú ý lẫn nhau, bao gồm cả chú ý tới chính nó), và vấn đề phụ thuộc/khả năng song song giữa các phép tính lớp. [1][4][5][2]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Attention trong kiến trúc encoder–decoder\n- Trong mô hình encoder–decoder, phần *attention* được hiểu là khi một query (thường từ bước decode) truy xuất và tổng hợp thông tin từ tập các giá trị (values) của encoder — tức là decoder \"lookup\" vào các token đã được mã hóa ở encoder để lấy thông tin cần thiết. [1][2]\n\n### 2.2. Định nghĩa và bản chất của Self-Attention\n- *Self-attention* là cơ chế tương tự attention nhưng thực hiện **trong cùng một pha** (trong encoder hoặc trong decoder): mỗi từ/token trong một pha sẽ \"chú ý\" đến tất cả các token khác trong cùng pha đó, và cũng có thể chú ý đến chính nó. Điều này có nghĩa là các token trong một input (hoặc output) trao đổi thông tin lẫn nhau mà không cần tra cứu sang pha còn lại. [4][5]\n\n### 2.3. Khả năng song song và phụ thuộc tính toán\n- Một điểm mạnh nổi bật của self-attention là **khả năng thực hiện song song**: các phép tính trong cùng một lớp self-attention không phụ thuộc lẫn nhau tuần tự như trong RNN, vì vậy có thể tính song song cho nhiều vị trí/token cùng lúc. [2]  \n- Tuy nhiên, giữa các lớp (layer) vẫn có phụ thuộc: để tính giá trị ở một lớp tiếp theo cần thông tin đầu ra từ lớp trước đó. Video minh họa các lớp khác nhau (layer 1, 2, 3, 4) và nêu rõ rằng mặc dù trong mỗi lớp phép tính nội tại có thể song song, nhưng lớp sau cần kết quả của lớp trước để tiếp tục. [3]\n\n### 2.4. Phân biệt attention và self-attention\n- *Attention* nói chung bao gồm trường hợp query ở decoder truy vấn encoder (cross-attention). *Self-attention* đặc biệt đề cập đến trường hợp các token tự chú ý lẫn nhau trong cùng một pha (encoder với encoder hoặc decoder với decoder). Đây là điểm khác biệt chính giữa hai khái niệm. [4][5]\n\n- (Không có công thức toán học cụ thể nào được trình bày trong các đoạn trích được cung cấp.) [tổng hợp nội dung]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa trong video:\n  - Trường hợp truyền thống encoder → decoder: một query từ bước decode truy xuất và tổng hợp thông tin từ các values của encoder (ví dụ khi dịch, token đang decode sẽ tham khảo các embedding đã mã hóa). [1][2]\n  - Trường hợp self-attention nội tại: toàn bộ token trong pha encoder tự chú ý với nhau (mỗi token có thể nhìn vào chính nó và các token khác trong cùng input); tương tự trong decoder, các token trong phần output tự chú ý lẫn nhau. [4][5]\n\n- Ứng dụng / trường hợp sử dụng (theo nội dung bài giảng):\n  - Self-attention được dùng bên trong Transformer để cho phép mỗi vị trí trong chuỗi thu thập thông tin từ mọi vị trí khác trong cùng pha, đồng thời tận dụng lợi ích song song khi huấn luyện và suy luận. [1][2][4]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - *Attention* ở ngữ cảnh encoder–decoder là cơ chế query (decoder) truy xuất values (encoder). [1][2]  \n  - *Self-attention* là khi các token trong cùng một pha (encoder hoặc decoder) tự chú ý lẫn nhau, bao gồm chú ý đến chính token đó. [4][5]  \n  - Self-attention cho phép **tính toán song song** trong mỗi lớp, nhưng vẫn tồn tại phụ thuộc giữa các lớp liên tiếp. [2][3]\n\n- Tầm quan trọng: Self-attention là thành phần then chốt trong Transformer vì nó vừa cho phép trao đổi thông tin toàn cục giữa các vị trí trong chuỗi vừa tối ưu hoá khả năng song song hoá phép tính so với các mô hình tuần tự như RNN. [2][4]\n\n- Liên hệ với bài giảng trước: Video nhắc rằng ở bài trước đã trình bày attention (đặc biệt attention giữa encoder và decoder), và self-attention được giới thiệu là một biến thể trong đó sự chú ý xảy ra nội tại trong cùng một pha. [1][4]\n\nGhi chú trích dẫn (mapping citation → khoảng thời gian trong video):\n- [1] [00:01 - 01:01]  \n- [2] [00:49 - 01:49]  \n- [3] [01:38 - 02:44]  \n- [4] [02:31 - 03:29]  \n- [5] [03:22 - 03:51]",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 9_NsWX_5oV8bY",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 3： Cơ chế Self-Attention",
          "video_url": "https://youtu.be/NsWX_5oV8bY",
          "start_time": 1,
          "end_time": 61,
          "text": "trong transformer chúng ta sẽ bắt gặp một cái khái niệm dùng rất là thường xuyên đó chính là self-attention tức là cái kỹ thuật tự chú ý thì ở trong cái bài trước chúng ta đã nói về cái khái niệm là attention tức là một cái truy vấn một cái từ truy vấn query của mình ở cái bước decoder nó sẽ truy xuất và tổng hợp nó sẽ truy xuất và tổng hợp thông tin từ các cái tập giá trị value của cái encoder thì như chúng ta thấy là đây là cái giai đoạn là encode và đây là cái giai đoạn decode thì trong cái giai đoạn decode này thì chúng ta sẽ phải lookup chúng ta sẽ phải tra vào tất cả những cái từ ở trong cái giai đoạn encode còn đây là encoder còn đây là cái quá trình là decode thì khi đó là nó gọi là attention tức là cái sự truy vấn của một cái query"
        },
        {
          "index": 2,
          "video_id": "Chương 9_NsWX_5oV8bY",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 3： Cơ chế Self-Attention",
          "video_url": "https://youtu.be/NsWX_5oV8bY",
          "start_time": 49,
          "end_time": 109,
          "text": "còn đây là encoder còn đây là cái quá trình là decode thì khi đó là nó gọi là attention tức là cái sự truy vấn của một cái query ở cái bước decode vào truy xuất vào và tổng hợp vào từ cái thông tin của các cái giá trị ở lớp encode từ decode mình sẽ truy xuất vào encode thì ở đây chúng ta thấy rằng là cái điểm mạnh của cái cell attention đó chính là cái khả năng song song cái khả năng song song trong cái sơ đồ trước đây chúng ta thấy là cái vị trí này chúng ta sẽ bị phụ thuộc vào T cái phép tính trước đó trong khi đó tại đây thì chúng ta chỉ cần phụ thuộc vào hai phêp tính tại sao tại vì để tính ra được cái giá trị ở đây chúng ta sẽ phải cần tính trước các giá trị ở layer trước đó ở đây là cái layer số 2"
        },
        {
          "index": 3,
          "video_id": "Chương 9_NsWX_5oV8bY",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 3： Cơ chế Self-Attention",
          "video_url": "https://youtu.be/NsWX_5oV8bY",
          "start_time": 98,
          "end_time": 164,
          "text": "để tính ra được cái giá trị ở đây chúng ta sẽ phải cần tính trước các giá trị ở layer trước đó ở đây là cái layer số 2 ở đây là layer số 3 đây là layer số 2 và đây là layer số 4 but which is other than in sheep hood thì để tính được cái layer số 3 chúng ta sẽ phải cần thông tin của các layer số 1 và thông tin của layer số 1 thì lại cần thông tin của layer số 2 và ở layer số 2 thì lại cần thông tin của layer số 1 thì tại đây chúng ta thấy là cái trạng thái ẩn này nó sẽ bị phụ thuộc bởi một cái phép tính trước đó đó là đây và các cái phép tính này thì thực hiện song song chính vì nó thực hiện song song nên không có cái phép tính nào phụ thuộc với phép tính nào rồi và ở đây thì chúng ta sẽ có cái khái niệm self-attention thì thật ra nó cũng chính là self-attention"
        },
        {
          "index": 4,
          "video_id": "Chương 9_NsWX_5oV8bY",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 3： Cơ chế Self-Attention",
          "video_url": "https://youtu.be/NsWX_5oV8bY",
          "start_time": 151,
          "end_time": 209,
          "text": "chính vì nó thực hiện song song nên không có cái phép tính nào phụ thuộc với phép tính nào rồi và ở đây thì chúng ta sẽ có cái khái niệm self-attention thì thật ra nó cũng chính là self-attention nhưng mà thay vì chúng ta là cái mối quan hệ giữa encode encoder và decoder đó là cái attention mà trong bài trước thì ở đây đó là cơ chế attention trên cái đoạn encode và encode encoder và encoder hoặc là decoder với decoder tức là mỗi từ nó sẽ chú ý đến nhau trong cùng một cái input hoặc là trong output thì giả sử như đây là nguyên cái giai đoạn encode đó thì các cái từ nó sẽ tự chú ý với nhau trong đó nó cũng có cả chú ý đến chính nó"
        },
        {
          "index": 5,
          "video_id": "Chương 9_NsWX_5oV8bY",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 3： Cơ chế Self-Attention",
          "video_url": "https://youtu.be/NsWX_5oV8bY",
          "start_time": 202,
          "end_time": 231,
          "text": "đó thì các cái từ nó sẽ tự chú ý với nhau trong đó nó cũng có cả chú ý đến chính nó chú ý đến chính nó ở đây và chú ý đến những cái từ còn lại trong cái giai đoạn encode của mình hoặc là trong cái giai đoạn decode tức là nó sẽ tự chú ý đến những cái từ trong cái giai đoạn decode của mình đó là cái sự khác biệt giữa cái khái niệm attention và self-attention"
        }
      ]
    },
    {
      "video_id": "Chương 9_UXxELgk5Vws",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng  \n  Giải thích **kiến trúc Transformer** tập trung vào **bộ Encoder**, nêu lý do ra đời (khắc phục hạn chế của RNN tuần tự) và trình bày chi tiết cơ chế *self-attention* (giảng viên gọi là \"shareattention\") cùng triển khai vector hóa để tính toán song song trên GPU. [1][2][4][5]\n\n- Các khái niệm sẽ được đề cập  \n  - Hạn chế của RNN (lan truyền tuần tự, mất thông tin và khó song song hóa). [1]  \n  - Ý tưởng song song và kết nối dày đặc trong Transformer. [2][3]  \n  - Cơ chế *attention* (query, key, value), phân phối attention (alpha), và tổng hợp value theo trọng số. [5][6][8][9][12]  \n  - Triển khai vector hóa (ma trận X, X_q, X_k, X_v) và công thức attention ở dạng ma trận. [13][14][15][16][17]  \n\n(Trích dẫn tương ứng theo từng nội dung ở trên.)  \n\n---\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Vấn đề của RNN và lợi thế của Transformer\n- RNN xử lý tuần tự: để truyền thông tin từ từ đầu chuỗi đến vị trí cuối cùng cần nhiều bước tuần tự, dẫn tới mất mát thông tin và khó tận dụng song song của GPU. [1]  \n- Transformer nhờ cơ chế *self-attention* cho phép các node ở cùng layer được xử lý độc lập (không phụ thuộc vào giá trị tính toán theo thứ tự), do đó dễ thực hiện song song và không phụ thuộc nhiều vào chiều dài chuỗi. [2][3]\n\n### 2.2. Tổng quan kiến trúc Transformer (Encoder vs Decoder)\n- Hình vẽ tổng quát của Transformer gồm hai thành phần chính: **Encoder** và **Decoder**. Bài giảng tập trung đi vào từng module, bắt đầu từ module chính của Encoder là *shareattention* (self-attention). [4][5]\n\n### 2.3. Ý tưởng cơ bản của Attention (Query, Key, Value)\n- Attention được so sánh với cơ chế truy vấn trong hệ quản trị cơ sở dữ liệu: có *query* (từ khóa tìm kiếm), *key* (chỉ mục/tựa đề), *value* (nội dung trả về). Tương tự, trong attention mỗi token sinh ra bộ (query, key, value) để đối chiếu và lấy thông tin từ các token khác. [6][7]  \n- Khác với truy vấn một cặp key-value duy nhất, trong attention mỗi query sẽ khớp với nhiều cặp key-value và lấy tổng có trọng số của các value đó; trọng số phản ánh mức độ liên quan (attention weight). [8][9]\n\n### 2.4. Các bước tính toán *(per-token)* trong self-attention (phiên bản mô tả từng bước)\n1. Embedding: mỗi từ/token được biểu diễn bằng một vector embedding. [10]  \n2. Chiếu (projection) embedding thành ba thành phần: *query*, *key*, *value* bằng cách nhân embedding với các ma trận tương ứng (W_Q, W_K, W_V). [10][11]  \n   - Mỗi token x_i → q_i = x_i W_Q, k_i = x_i W_K, v_i = x_i W_V. [11][16]\n3. Tính score (mối quan hệ) giữa query và key: dot-product giữa q_y và k_chi cho mọi cặp (y, chi). [11][15]  \n4. Chuẩn hóa scores thành phân phối xác suất bằng softmax → alpha (attention distribution). [12][16]  \n5. Tổng hợp output cho query y: weighted sum các value theo alpha: output_y = Σ_chi alpha_{y,chi} * v_chi. [12]\n\n(Phần mô tả các bước trên dựa trên trình tự và giải thích trong video.) [10][11][12]\n\n### 2.5. Triển khai vector hóa (song song hóa trên ma trận)\n- Thay vì xử lý từng token riêng lẻ, gom tất cả embedding x_i thành ma trận X (các x_i là cột/hoặc hàng tuỳ cách biểu diễn). [13][14]  \n- Tạo ma trận X_Q = X W_Q, X_K = X W_K, X_V = X W_V (chiếu đồng thời cho tất cả token). [14][16]  \n- Tính ma trận scores cho mọi cặp token bằng: S = X_Q · X_K^T (mỗi phần tử S_{y,chi} là dot(q_y, k_chi)). [15][16]  \n- Chuẩn hóa hàng của S bằng softmax → ma trận A (ma trận attention distribution). [16][17]  \n- Kết quả đầu ra cho toàn bộ sequence là: Output = A · X_V. [17]  \n  (Phiên bản ngắn gọn công thức vector hóa: Output = softmax(X_Q X_K^T) X_V.) [17]\n\n### 2.6. Vai trò của các trọng số attention\n- Những cặp key-value có trọng số lớn đóng góp nhiều hơn vào output; các cặp không liên quan có trọng số thấp và tác động nhỏ. Video minh hoạ bằng các V1,V3,V4 có weight cao, còn V0,V2,V5,V6 weight thấp. [9]  \n- Toàn bộ quá trình này là giai đoạn **cell attention** trong encoder, tức là bước tổng hợp thông tin của các từ trong giai đoạn encode. [5][18]\n\n---\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa bằng hệ thống tìm kiếm multimedia:  \n  - Query: \"transformer architecture\" (là *query*). [7]  \n  - Key: tiêu đề các video trong kho (dùng để so khớp). [7]  \n  - Value: nội dung/miêu tả video (thông tin trả về). [7]  \n  - Khi tính attention, query sẽ khớp với nhiều key; các key liên quan sẽ có trọng số cao và do đó giá trị tương ứng (value) đóng góp nhiều vào kết quả. [6][8][9]\n\n- Ứng dụng thực tế (trong ngữ cảnh video):  \n  - Cơ chế này cho phép mỗi vị trí trong câu “tương tác” trực tiếp với mọi vị trí khác (bao gồm chính nó) trong cùng một layer mà không cần lan truyền tuần tự nhiều bước như RNN, vì vậy phù hợp cho các bài toán cần model mối quan hệ dài hạn giữa tokens và tận dụng tính song song của GPU. [2][3][15][18]\n\n- Trường hợp sử dụng: mọi bài toán NLP/sequence modeling cần bắt mối quan hệ giữa từ xa (long-range dependencies) đều hưởng lợi từ self-attention của encoder như trình bày. (Lưu ý: nội dung này lược theo giải thích về lợi ích song song và kết nối dày đặc trong video.) [1][2][3]\n\n---\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:  \n  - Transformer khắc phục nhược điểm của RNN bằng cách dùng *self-attention* để cho phép xử lý song song, kết nối dày đặc giữa các token và truy xuất thông tin trực tiếp giữa các vị trí trong vài layer thay vì lan truyền tuần tự dài. [1][2][3]  \n  - Trong encoder, mỗi token được chiếu thành query/key/value; attention score = dot(query, key) → softmax → weighted sum các value cho ra output. [10][11][12]  \n  - Việc vector hóa (gom các embedding thành ma trận X và tính X_Q, X_K, X_V) cho phép tính attention cho toàn bộ câu một cách song song trên GPU với công thức tổng quát: Output = softmax(X_Q X_K^T) X_V. [13][14][15][17]\n\n- Tầm quan trọng:  \n  - Giải thích cơ chế attention ở cấp độ encoder là nền tảng để hiểu toàn bộ Transformer (vì encoder-decoder đều dựa trên các khối attention này). Việc nắm rõ chi tiết triển khai (Q/K/V, ma trận X, softmax, tổng hợp V) giúp hiểu cách Transformer xử lý mối quan hệ giữa tokens và tận dụng sức mạnh tính toán song song. [4][5][13][17][18]\n\n- Liên hệ với các bài giảng khác:  \n  - Video này là bước phân tích sâu **bộ Encoder** trong kiến trúc Transformer; theo lời giảng viên, sẽ tiếp tục phân tích từng thành phần khác của Transformer (ví dụ decoder và các mô-đun phụ) ở các phần tiếp theo. [4]\n\n---\n\nGhi chú: Tất cả các điểm trên được tóm lược trực tiếp từ nội dung video theo từng đoạn thời gian đã cung cấp. [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18]",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 9_UXxELgk5Vws",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/UXxELgk5Vws",
          "start_time": 0,
          "end_time": 62,
          "text": "ý tưởng của transformer đó là nếu như trong các cái kiến trúc cũ là recurrent neural network thì trong quá trình tính toán chúng ta sẽ phải lan truyền thông tin một cách tuần tự nghĩa là tại cái từ ở đây khi chúng ta xử lý tính toán đến cái từ cuối cùng trong cái giao đoạn encode thì chúng ta sẽ phải lan truyền tuần tự đi lên đi qua đi lên, đi qua như vậy thì nó sẽ tốn rất nhiều bước xử lý tuần tự và thông tin của cái từ đầu tiên này khi đến được đến đây thì nó cũng đã bị may một cũng đã bị mất đi rất là nhiều thì đó chính là cái điểm yếu của recurrent neural network và đồng thời cái việc thực hiện tuần tự này cũng sẽ khiến cho chúng ta không thể xử lý song song sử dụng các cái sức mạnh của GPU còn kiến trúc của transformer thì nhờ cái cơ chế cell extension thì nó cho phép chúng ta có thể xử lý song song"
        },
        {
          "index": 2,
          "video_id": "Chương 9_UXxELgk5Vws",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/UXxELgk5Vws",
          "start_time": 50,
          "end_time": 112,
          "text": "còn kiến trúc của transformer thì nhờ cái cơ chế cell extension thì nó cho phép chúng ta có thể xử lý song song tại vì khi chúng ta tính toán tại đây đúng không thì chúng ta sẽ không cần phải phụ thuộc vào các cái giá trị được tính toán tại đây tức là các cái node ở trên cùng một cái layer sẽ được thực hiện một cách độc lập với nhau còn ở đây chúng ta muốn tính toán tại vị trí này tại hidden này thì chúng ta sẽ phải tính toán ở đây trước rồi sau đó mới đến đây tính đến đây xong chúng ta mới đến đây được rồi còn ở đây là các cái node ở đây là tính độc lập mà độc lập thì có thể sử dụng GPU được do đó thì mỗi cái số phép tính song song của mình là nó sẽ không phụ thuộc vào cái chiều dài của chuỗi tức là khi cái chuỗi này cái chuỗi này mà dài rất là dài thì nó vẫn có thể thực hiện song song được và đồng thời là chúng ta thấy các cái kết nối dày đặc này"
        },
        {
          "index": 3,
          "video_id": "Chương 9_UXxELgk5Vws",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/UXxELgk5Vws",
          "start_time": 99,
          "end_time": 161,
          "text": "là nó sẽ không phụ thuộc vào cái chiều dài của chuỗi tức là khi cái chuỗi này cái chuỗi này mà dài rất là dài thì nó vẫn có thể thực hiện song song được và đồng thời là chúng ta thấy các cái kết nối dày đặc này nó sẽ cho phép chúng ta có thể tương tác tương tác giữa các cái từ nếu như ở đây để mà có thể tương tác giữa các cái từ  để tương tác được cái từ đầu tiên và cái từ cuối cùng cái từ đầu tiên và cái từ cuối cùng này tại đây đi thì chúng ta sẽ phải cần có rất nhiều bước tương tự mới lan truyền để mà có thể tương tác được trong khi đó tại đây cái từ đầu tiên cái từ đầu tiên này nó đã có thể tương tác được thông qua cái nớp trước đó là lớp số 1 lớp số 2 sẽ dùng thông tin của lớp số 1 đồng thời nó cũng dựa trên cái thông tin của cái từ cuối cùng của cái lớp số 1 tức là tại cái layer số 2 thì nó đã có thể truy xuất đến thông tin của cái từ đầu tiên"
        },
        {
          "index": 4,
          "video_id": "Chương 9_UXxELgk5Vws",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/UXxELgk5Vws",
          "start_time": 149,
          "end_time": 209,
          "text": "cái thông tin của cái từ cuối cùng của cái lớp số 1 tức là tại cái layer số 2 thì nó đã có thể truy xuất đến thông tin của cái từ đầu tiên và cái từ cuối cùng của lớp trước đó một cách trực tiếp mà không cần phải thực hiện một cách tương tự thì đây chính là những cái ưu điểm của transformer và hình vẽ ở trên đây đó chính là cái sơ đồ kiến trúc của transformer thì khi chúng ta mới bắt đầu chúng ta nhìn vô cái sơ đồ này chúng ta sẽ rất là rối vì nó có quá nhiều cái module và chúng ta cũng không biết tại sao nó lại có những cái module này thế thì bây giờ tại cái bước này tại cái hình vẽ này thì chúng ta chỉ cần hình dung đó là transformer bao gồ 2 thành phần đó là encoder và decoder đây là encoder và đây là decoder và đây là cái kiến trúc của encoder và đây là kiến trúc của decoder chúng ta sẽ cùng đến với từng cái thành phần của transformer"
        },
        {
          "index": 5,
          "video_id": "Chương 9_UXxELgk5Vws",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/UXxELgk5Vws",
          "start_time": 198,
          "end_time": 261,
          "text": "đây là encoder và đây là decoder và đây là cái kiến trúc của encoder và đây là kiến trúc của decoder chúng ta sẽ cùng đến với từng cái thành phần của transformer đầu tiên đó chính là encoder thì cái module xử lý tính toán đầu tiên của cái encoder đó chính là shareattention shareattention chính là một cái module một cái module chính của transformer đó là một cái module chính và với cái dữ kiện đầu vào là cái từ của mình hoặc là cái token của mình qua cái input embedding tức là chúng ta sẽ biến một cái từ trong một cái văn bản thành một cái vector biểu diễn và cái vector biểu diễn này thì sẽ đến cái module shareattention nó sẽ đến cái module gọi là shareattention và shareattention nó dựa trên cái cơ chế của attention thế thì attention nó cũng na ná nó cũng tương đương với lại một cái phép"
        },
        {
          "index": 6,
          "video_id": "Chương 9_UXxELgk5Vws",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/UXxELgk5Vws",
          "start_time": 247,
          "end_time": 310,
          "text": "nó sẽ đến cái module gọi là shareattention và shareattention nó dựa trên cái cơ chế của attention thế thì attention nó cũng na ná nó cũng tương đương với lại một cái phép truy vấn trong cái bảng dữ liệu của mình có điều nếu như truy vấn trong cái bảng dữ liệu của mình chúng ta có một cái query ở đây chúng ta sẽ tra trong cái cơ sở dữ liệu của mình các cái value thông qua cái chúng ta sẽ sort up dựa trên các cái key để chúng ta lấy thông tin của cái value thì ở đây chúng ta sẽ có các khí niệm là query, key và value thì ở đây chúng ta sẽ hình dung nó liên quan đến một cái ứng dụng trong thực tế đó chính là các cái hệ thống tìm kiếm về multimedia query của mình đó nó chính là các cái keyword khi mà mình tìm kiếm ví dụ như chúng ta muốn tìm kiếm một cái video về chủ đề đó là transformer thì cái keyword của mình sẽ là"
        },
        {
          "index": 7,
          "video_id": "Chương 9_UXxELgk5Vws",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/UXxELgk5Vws",
          "start_time": 301,
          "end_time": 361,
          "text": "khi mà mình tìm kiếm ví dụ như chúng ta muốn tìm kiếm một cái video về chủ đề đó là transformer thì cái keyword của mình sẽ là transformer architecture và key nó chính là các cái tiêu đề tiêu đề của các cái video trong cái kho trên cái kênh youtube của mình   chẳng hạn và cái value của mình trả về nó chính là những cái nội dung của video của mình đó chính là nội dung video rồi ví dụ như là mô tả mô tả cái video thì đó chính là các cái value của mình thì đây chính là cái sự liên tưởng đến cái hệ thống tìm kiếm các cái hệ thống truy vấn mà trong cái thế giới thực của mình còn attention của mình đó nó sẽ khác so với lại cái truy vấn trong cái video của mình trong bảng dữ liệu của mình ở chỗ đó là chúng ta sẽ phải"
        },
        {
          "index": 8,
          "video_id": "Chương 9_UXxELgk5Vws",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/UXxELgk5Vws",
          "start_time": 350,
          "end_time": 410,
          "text": "các cái hệ thống truy vấn mà trong cái thế giới thực của mình còn attention của mình đó nó sẽ khác so với lại cái truy vấn trong cái video của mình trong bảng dữ liệu của mình ở chỗ đó là chúng ta sẽ phải trích xuất ra thông tin của rất nhiều của rất nhiều những cái key và value ở đây là chúng ta trích xuất một lột chúng ta sẽ lấy ánh xạ chúng ta sẽ lấy mỗi cái query của mình nó sẽ ánh xạ đến một cặp key và value tức là query sẽ ánh xạ một lột đến một cái cặp key và value trong khi đó thì mỗi một cái query của mình mỗi một query của mình nó sẽ khớp với mỗi key nó sẽ so khớp với các cái key này của mình và nó sẽ trả về tổng tất cả các cái value có điều ở đây chúng ta sẽ thấy là nó sẽ có trọng số nha thì những cái cặp key và value nào có liên quan đến cái query này thì nó mới"
        },
        {
          "index": 9,
          "video_id": "Chương 9_UXxELgk5Vws",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/UXxELgk5Vws",
          "start_time": 400,
          "end_time": 460,
          "text": "có điều ở đây chúng ta sẽ thấy là nó sẽ có trọng số nha thì những cái cặp key và value nào có liên quan đến cái query này thì nó mới có cái trọng số lớn còn những cái key và và value mà có cái trọng số thấp tức là x có cái sự liên quan thì khi nó cộng lại thì nó sẽ x tham gia vào cái giá trị output của mình thì ở trong hình này chúng ta thấy đây là những cái giá trị mà có trọng số thấp đây là những cái giá trị có trọng số thấp đây là những cái giá trị có trọng số thấp còn những cái key value mà chúng ta tô đỏ ở đây chính là những cái mà có trọng số cao thì khi đó cái tỷ trọng của cái key value  trọng lượng thông tin của V1, V3, V4 khi chúng ta tổng hợp thông tin sẽ là nhiều nhất còn V0, V2, V5, V6 thì cái hàm lượng thông tin của mình tổng hợp chúng ta khi tổng hợp thì nó sẽ rất là thấp thì đó là cái sự khác nhau"
        },
        {
          "index": 10,
          "video_id": "Chương 9_UXxELgk5Vws",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/UXxELgk5Vws",
          "start_time": 448,
          "end_time": 510,
          "text": "sẽ là nhiều nhất còn V0, V2, V5, V6 thì cái hàm lượng thông tin của mình tổng hợp chúng ta khi tổng hợp thì nó sẽ rất là thấp thì đó là cái sự khác nhau như attention với lại cái truy vống trong bảng dữ liệu của mình và khi này thì chúng ta sẽ có cái công thức cho cái cell attention trong cái encoder của mình bước số 1 đó là với mỗi một cái từ với một cái từ thì cái này chính là cái embedding embedding vector của mình đây là cái embedding vector của một cái từ thì nó sẽ được chia ra nó sẽ được tức là với mỗi từ nó sẽ chia ra thành 3 cái giá trị đó là query, key và value tương ứng là cái màu ha chúng ta sẽ theo dõi dựa trên màu cho dễ hình dục thì query nó sẽ phải ánh xạ từ cái embedding vector đó về cái không gian ánh xạ về cái không gian của query của mình rồi xa nó sẽ nhân với ma trận k để ánh xạ về cái không gian"
        },
        {
          "index": 11,
          "video_id": "Chương 9_UXxELgk5Vws",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/UXxELgk5Vws",
          "start_time": 497,
          "end_time": 561,
          "text": "nó sẽ phải ánh xạ từ cái embedding vector đó về cái không gian ánh xạ về cái không gian của query của mình rồi xa nó sẽ nhân với ma trận k để ánh xạ về cái không gian của cái key của mình của cái key của mình và xa nhân với v để ánh xạ về cái không gian của cái value của mình và sang cái bước thứ 2 đó là chúng ta sẽ tính cái attention score giữa cái query và key thì ở trong trường hợp này query và key của mình nó đã có cùng một cái số chiều nó phải đưa về cùng một cái số chiều thì khi đó chúng ta chỉ việc thực hiện cái phép tích vô hướng tích vô hướng giữa một cái query và một cái key thứ chi bất kỳ và chúng ta sẽ trả về là cái relation tức là cái sự liên hệ giữa query và cái key này query thứ y và cái key thứ chi này sau đó chúng ta sẽ chuẩn hóa bước thứ 3 là chúng ta sẽ chuẩn hóa cái giá trị này về cái không gian sát xuất thì thông qua cái hàm softmax"
        },
        {
          "index": 12,
          "video_id": "Chương 9_UXxELgk5Vws",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/UXxELgk5Vws",
          "start_time": 549,
          "end_time": 611,
          "text": "query thứ y và cái key thứ chi này sau đó chúng ta sẽ chuẩn hóa bước thứ 3 là chúng ta sẽ chuẩn hóa cái giá trị này về cái không gian sát xuất thì thông qua cái hàm softmax và công thức của softmax ở đây thì chúng ta sẽ có được cái alpha e g chính là cái attention distribution hay là attention score mà chúng ta đã được chuẩn hóa và sang bước số 4 là chúng ta sẽ tính tổng trọng số của các cái value tức là các cái trọng số alpha e g này sẽ nhân với value tương ứng để chúng ta trả kết quả về cái output e tức là output cho cái query thứ y output cho cái query thứ y của mình query thứ y của mình và khi này thì chúng ta sẽ có nếu chúng ta thực hiện trên cái vector, cái dạng vector hóa tức là chúng ta sẽ gom chúng ta sẽ gom"
        },
        {
          "index": 13,
          "video_id": "Chương 9_UXxELgk5Vws",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/UXxELgk5Vws",
          "start_time": 599,
          "end_time": 660,
          "text": "và khi này thì chúng ta sẽ có nếu chúng ta thực hiện trên cái vector, cái dạng vector hóa tức là chúng ta sẽ gom chúng ta sẽ gom các cái query lại với nhau thì bước số 1 nè các cái từ xy sẽ được gộp lại vào x ở đây chúng ta sẽ thực hiện theo bài hát thực hiện theo khối trong cái slide trước là chúng ta tính trên từng từ thì ở trong cái slide này thì ở trong cái slide này chúng ta sẽ gom tất cả các cái từ trong cái code input của mình vào thành một cái ma trận là ma trận x thì khi đó chúng ta tính toán trên cái ma trận x này thì nó sẽ tính toán nhanh hơn và có thể thực hiện được một cách song song do cái sức mạnh tính toán của GPU thì cái xy này nó sẽ là một cái vector dạng cột như thế này của mình và chúng ta sẽ gom tất cả các cái xy này lại với nhau rồi chúng ta sẽ gom tất cả các xy lại với nhau thì chúng ta sẽ có được là một"
        },
        {
          "index": 14,
          "video_id": "Chương 9_UXxELgk5Vws",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/UXxELgk5Vws",
          "start_time": 648,
          "end_time": 708,
          "text": "nó sẽ là một cái vector dạng cột như thế này của mình và chúng ta sẽ gom tất cả các cái xy này lại với nhau rồi chúng ta sẽ gom tất cả các xy lại với nhau thì chúng ta sẽ có được là một cái ma trận thì toàn bộ cái xy gom lại thì nó sẽ là ma trận x thì nguyên cái tổ hợp của các cái xy sẽ là ma trận x và khi đó chúng ta cũng có cái công thức này tương tự như vậy x mà nhân với lại cái ma trận y x nhân với ma trận y thì chúng ta sẽ có xy tức là x tương ứng trong không gian query xk tương ứng là x khi nhân với lại k thì chúng ta sẽ có là trong không gian key và xv tức là trong không gian value sang bước thứ 2 thì chúng ta sẽ tính cái attention score giữa query và key của mình"
        },
        {
          "index": 15,
          "video_id": "Chương 9_UXxELgk5Vws",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/UXxELgk5Vws",
          "start_time": 701,
          "end_time": 761,
          "text": "sang bước thứ 2 thì chúng ta sẽ tính cái attention score giữa query và key của mình thì ở đây chúng ta sẽ có ma trận là xq nhân với lại xk khi này thì chúng ta sẽ tính là giữa các query giữa các query và các key chúng ta sẽ tính trên một chuỗi tất cả các cái cặp query và key với nhau nhưng mà lưu ý là ở cái bước sell attention này thì query và key của mình nó sẽ có tính là nó sẽ là nó sẽ tên ở đây chúng ta sẽ có các cái vector sau khi chúng ta đã sau khi chúng ta đã chiếu về cái không gian của query key và value thì mỗi cái từ này nó sẽ đi so với lại các cái từ và thậm chí là nó so với cả chính nó nữa để tính ra cái score"
        },
        {
          "index": 16,
          "video_id": "Chương 9_UXxELgk5Vws",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/UXxELgk5Vws",
          "start_time": 750,
          "end_time": 809,
          "text": "chiếu về cái không gian của query key và value thì mỗi cái từ này nó sẽ đi so với lại các cái từ và thậm chí là nó so với cả chính nó nữa để tính ra cái score xq nhân với xk và triển khai ra thì xq của mình nó chính là x nhân với lại ma trận q rồi xk chính là x nhân với lại ma trận k tất cả chuyển vị thì khi chúng ta triển khai cái chuyển vị nào vào vào bên trong cái dấu hoạt này thì nó sẽ là x chuyển vị đem ra và k chuyển vị đem lên trước như vậy thì cái công thức của ma trận biến đổi x của ma trận attention score của mình đó chính là xq nhân với lại k chuyển vị nhân với x chuyển vị và sang bước thứ 3 chúng ta sẽ tính cái extension distribution với cái hàm softmax thì chúng ta sẽ có cái ma trận a và sang bước số 4 thì chúng ta sẽ tổng hợp là output là bằng cái công thức ở đây"
        },
        {
          "index": 17,
          "video_id": "Chương 9_UXxELgk5Vws",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/UXxELgk5Vws",
          "start_time": 798,
          "end_time": 861,
          "text": "với cái hàm softmax thì chúng ta sẽ có cái ma trận a và sang bước số 4 thì chúng ta sẽ tổng hợp là output là bằng cái công thức ở đây rồi thì đây là cái công thức ở dạng vector hóa cho cell attention và khi chúng ta triển khai hết thì chúng ta sẽ có output là bằng softmax của xq k chuyển vị và x chuyển vị xq k chuyển vị x chuyển vị rồi qua cái hàm softmax xong để tính ra được đây là cái fan file này thì chúng ta sẽ nhân với lại cái xv chúng ta sẽ nhân với lại cái xv để tổng hợp thông tin để tổng hợp thông tin đây sẽ là trọng số và toàn bộ cái này sẽ là tổng hợp thông tin tổng hợp toàn bộ những cái thông tin của word cái giai đoạn là cell attention tức là giai đoạn encode"
        },
        {
          "index": 18,
          "video_id": "Chương 9_UXxELgk5Vws",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/UXxELgk5Vws",
          "start_time": 848,
          "end_time": 862,
          "text": "và toàn bộ cái này sẽ là tổng hợp thông tin tổng hợp toàn bộ những cái thông tin của word cái giai đoạn là cell attention tức là giai đoạn encode tức là giai đoạn encode"
        }
      ]
    },
    {
      "video_id": "Chương 9_JGxo_olUl2U",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giải thích chi tiết kiến trúc Encoder trong Transformer — các thành phần quan trọng (feedforward nonlinearity / MLP, activation, việc nhân bản nhiều layer, residual connection, layer normalization, scale dot‑product attention, và positional encoding) và lý do tại sao mỗi thành phần cần thiết. [1][2][3][4][5][12][14]\n\n- Các khái niệm sẽ được đề cập:\n  - Vấn đề của attention nếu thiếu biến đổi phi tuyến (chỉ là tổng có trọng số) và giải pháp dùng feedforward (MLP). [1][2]\n  - Cấu trúc MLP (ma trận tham số W1, W2) và activation PReLU. [2][3]\n  - Lập chồng nhiều layer (stacking) của module Encoder, số layer điển hình (ví dụ 6) và khả năng thay đổi. [3][4][5]\n  - Residual connection (phép cộng skip connection). [5][6]\n  - Layer normalization để ổn định phân phối đầu vào từng layer. [7][8][9]\n  - Scale‑dot‑product attention (chia cho sqrt(d_k)). [10][11][12]\n  - Positional encoding / position embedding (mã hóa thông tin thứ tự vào Q/K/V, phép cộng vs concat). [12][13][14][15][16][17][18]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Vấn đề khi thiếu biến đổi phi tuyến ở đầu ra attention\n- Nếu không có biến đổi phi tuyến sau phép tổng hợp attention, phần output chỉ là tổng có trọng số các vector value — tức *chỉ là sự tổng hợp tuyến tính* của các đặc trưng (weighted average), không chuyển đổi sang dạng đặc trưng mới. [1]\n\n### 2.2. Giải pháp: Feedforward network (MLP) áp lên output của attention\n- Sau khi có output_i (đầu ra của attention cell), ta đưa qua một multilayer perceptron (feedforward network) để thu được phép biến đổi phi tuyến. [2]\n- Cấu trúc cơ bản (như mô tả): output_i nhân tuyến tính với ma trận W1, qua hàm kích hoạt, rồi tiếp tuyến tính với W2 — W1 và W2 là tham số cần huấn luyện. (công thức dạng: y = W2 · activation(W1 · output_i) ) [2]\n\n- Hàm kích hoạt được khuyến nghị: PReLU (parametric ReLU) — giúp huấn luyện nhanh hơn và giảm hiện tượng vanishing gradients. [3]\n\n### 2.3. Lập chồng nhiều layer (Stacking layers)\n- Một mẹo từ Deep Learning là dựng mạng sâu (nhiều lớp) để học đặc trưng đa cấp: low‑level → mid‑level → high‑level. Trong Transformer, điều này thực hiện bằng cách lặp lại module Encoder (và Decoder) nhiều lần. [3][4]\n- Số layer điển hình: thường là 6 lớp trong nhiều triển khai gốc, nhưng có thể thay đổi tùy nhu cầu — từ 1–2 lớp cho mô hình nhẹ, đến vài chục/hàng trăm lớp cho bài toán phức tạp. [4][5]\n\n### 2.4. Residual connections (skip connections)\n- Thiết kế residual: layer tiếp theo được tính bằng input của layer trước cộng với phép biến đổi F(x_{l-1}), tức x_l = x_{l-1} + F(x_{l-1}). [6]\n- Lợi ích: giúp huấn luyện các mạng rất sâu, giảm thời gian huấn luyện và hỗ trợ chống overfitting (giảm khó khăn huấn luyện). [6]\n\n### 2.5. Layer normalization\n- Vấn đề: khi input tới một layer có phân bố thay đổi lớn giữa các minibatch/lớp, việc học các tham số trở nên khó khăn. [7]\n- Giải pháp: chuẩn hóa theo cấp độ layer (layer norm) — tại layer l, ta trừ trung bình m_l rồi chia cho sigma_l để đạt mean = 0 và std = 1 cho các phần tử trong layer đó. Công thức tổng quát (theo mô tả): x̂ = (x − μ) / σ, với μ là mean trên layer và σ là standard deviation. [8][9]\n- Thiết đặt thứ tự: trong slide, norm được thực hiện sau phép cộng residual và trước bước biến đổi tiếp theo (chuẩn hóa để ổn định đầu vào cho bước tiếp). [9]\n\n### 2.6. Scale‑dot‑product attention (scale.product)\n- Sau khi chuẩn hóa các vector (Q, K, V), khi tính tích vô hướng Q·K^T, tổng các tích phần tử có thể lớn nếu chiều d_k lớn — dẫn tới giá trị attention quá lớn. [10][11]\n- Giải pháp: chia kết quả nhân vô hướng cho sqrt(d_k) (d_k = chiều của key/query) để đưa output về phân bố hợp lý trước softmax. Đây là phép \"scale\" trong scale‑dot‑product attention. [11][12]\n\n### 2.7. Vấn đề thứ tự (position) và Positional Encoding\n- Attention xử lý các token song song và độc lập theo bản chất, nên *không* tự có thông tin về thứ tự (order) của các từ. Tuy nhiên thứ tự trong ngôn ngữ rất quan trọng (ví dụ: \"do you understand\" vs \"you do understand\" khác nghĩa). [12][13]\n- Giải pháp: tạo một vector biểu diễn vị trí p_i cho mỗi vị trí i (position embedding) và kết hợp nó vào Q/K/V. Cách đơn giản và thường dùng là **cộng** p_i vào các vector embeddings ban đầu:\n  - v'_i = v_i + p_i, k'_i = k_i + p_i, q'_i = q_i + p_i. [14][15][16]\n- Thay thế concat: có thể nối (concat) p_i với vector từ (v_i), nhưng điều này tăng kích thước vector và tốn kém tính toán; do đó cộng thường được dùng để giữ kích thước không đổi. [16][17]\n- Position embedding đảm bảo mỗi vị trí có một vector riêng (không trùng) để tránh nhập nhằng vị trí/từ. [17][18]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa tầm quan trọng của thứ tự: cụm từ \"do you understand\" (câu hỏi) khác nghĩa với \"you do understand\" (khẳng định) — attention cần biết thứ tự để phân biệt ý nghĩa. [13]\n\n- Ứng dụng thực tế của từng thành phần:\n  - MLP (feedforward) sau attention: cho phép ánh xạ các đặc trưng attention thành biểu diễn mới, cần thiết cho tính biểu diễn (representation) mạnh hơn. [1][2]\n  - PReLU: tăng tốc huấn luyện, giảm vanishing gradient — áp dụng trong nhiều tầng feedforward. [3]\n  - Stacking nhiều lớp Encoder: dùng cho các tác vụ phức tạp (dịch máy, hiểu ngôn ngữ sâu), hoặc giảm số layer cho mô hình nhẹ/triển khai hạn chế tài nguyên. [4][5]\n  - Residual + LayerNorm: thiết lập phổ biến để huấn luyện ổn định các Transformer sâu. [6][7][8][9]\n  - Scale‑dot‑product: cần thiết để attention hoạt động ổn định khi chiều embedding lớn. [11][12]\n  - Positional encoding: bắt buộc khi xử lý ngôn ngữ để mô hình phân biệt trật tự token. [14][15][17]\n\n- Trường hợp sử dụng:\n  - Mô hình nhẹ (edge hoặc inference giới hạn): giảm số encoder layers (1–2). [5]\n  - Bài toán ngôn ngữ phức tạp (dịch máy, hiểu ngữ cảnh dài): tăng số layer (vài chục/hàng trăm) kết hợp residual + layer norm để giữ khả năng huấn luyện. [4][5][6]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Attention cần có biến đổi phi tuyến (feedforward MLP) để không chỉ là tổng tuyến tính của value vectors. [1][2]\n  - Sử dụng activation (PReLU), stacking nhiều layer, residual connection, và layer normalization là các “mẹo” từ Deep Learning giúp Transformer học hiệu quả và ổn định. [3][4][6][7]\n  - Scale‑dot‑product (chia cho sqrt(d_k)) là cần thiết để giữ phân phối hợp lý khi tính attention. [11][12]\n  - Positional encoding (position embedding) là cách đưa thông tin thứ tự vào Q/K/V; thường dùng phép cộng để tránh tăng kích thước biểu diễn. [14][15][16][17][18]\n\n- Tầm quan trọng của nội dung:\n  - Những thành phần và kỹ thuật này là nền tảng để Encoder trong Transformer hoạt động hiệu quả trên các tác vụ ngôn ngữ tự nhiên, cho phép mô hình vừa học biểu diễn mạnh vừa huấn luyện được sâu. [1][2][3][6][11][14]\n\n- Liên hệ với các bài giảng khác:\n  - Hàm kích hoạt PReLU được nhắc là đã được trình bày ở \"bài trước\" (liên hệ với nội dung về activation và kỹ thuật huấn luyện trước đó). [3]\n  - Tổng hợp ở đây là ứng dụng các thành tựu Deep Learning trước (stacking layers, residual, norm, scale attention, pos encoding) vào kiến trúc Transformer Encoder. [3]\n\n(Toàn bộ nội dung tóm tắt dựa trên các đoạn trích từ video: [1] … [18].)",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 9_JGxo_olUl2U",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/JGxo_olUl2U",
          "start_time": 0,
          "end_time": 61,
          "text": "rồi và extension có phải là tất cả những gì chúng ta cần hay không thì câu trả lời đó là không phải câu trả lời là không phải đầu tiên đó là chúng ta sẽ phải xem cái problem ở đây là gì extension nếu như chúng ta không có biến đổi phi tuyến tính thì nó chỉ là cái sự tổng hợp trọng số của các cái vector value thôi nó chỉ là cái extension của mình nó chỉ là cái sự tổng hợp trọng số của các cái vector value nó không có cái sự biến đổi từ cái dạng đặc trưng này sang một dạng đặc trưng khác nó chỉ là cộng dồn các cái đặc trưng chính xác là nó chỉ là tính trung bình có trọng số của các cái đặc trưng thôi thì giải pháp ở đây đó là chúng ta sẽ biến đổi feedforward tức là một cái mạng neural network chúng ta sẽ biến đổi feedforward hay là một mạng neural network với cái đầu ra của cái extension của cái cell extension và cùng với cái hàm hàm kích hoạt phi tuyến thì trong cái feedforward này nó đã có"
        },
        {
          "index": 2,
          "video_id": "Chương 9_JGxo_olUl2U",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/JGxo_olUl2U",
          "start_time": 49,
          "end_time": 109,
          "text": "chúng ta sẽ biến đổi feedforward hay là một mạng neural network với cái đầu ra của cái extension của cái cell extension và cùng với cái hàm hàm kích hoạt phi tuyến thì trong cái feedforward này nó đã có cái phép biến đổi là phi tuyến trong đó thì đây là cái công thức của mình giả sử như output i là giá trị đầu ra của cell extension thì chúng ta sẽ qua cái multilayer perceptron thì cái công thức của multilayer perceptron nó sẽ có dạng như sau output i nhân tuyến tính với lại một cái ma trận w ma trận w1 này chính là một cái bộ tham số tham số của mô hình mà mình sẽ phải huấn luyện ha tương file binance 1 cũng g ling một cái tham số của mô hình rồi w2 cũng là tham số của mình để mà huấn luyện chúng ta sẽ cần phải huấn luyện các cái bộ tham số này và chúng ta sẽ sử dụng lower hàm kích hoạt"
        },
        {
          "index": 3,
          "video_id": "Chương 9_JGxo_olUl2U",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/JGxo_olUl2U",
          "start_time": 102,
          "end_time": 158,
          "text": "chúng ta sẽ cần phải huấn luyện các cái bộ tham số này và chúng ta sẽ sử dụng lower hàm kích hoạt đó là prelu thì cái hàm kích hoạt tr trelu này như trong bài trước nó sẽ giúp cho cái việc huấn luyện nhanh hơn và hạn chế được hiện tượng Vanishing Radiance Rồi, và cho đến bây giờ thì chúng ta sẽ bắt đầu sử dụng một loạt các cái mẹo trong các cái thành tựu của Deep Learning trước đây Mẹo đầu tiên đó chính là trồng nhiều lớp Khi chúng ta trồng nhiều lớp với nhau thì nó sẽ giúp cho chúng ta tạo ra được rất nhiều những cái đặc trưng từ các cái cấp độ Low Level, tức là đặc trưng cấp thấp cho đến đặc trưng cấp cao à đặc trưng cấp giữa và đặc trưng cấp cao thì việc trồng nhiều lớp này nó sẽ giúp cho chúng ta tổng hợp tổng hợp được cái đặc trưng"
        },
        {
          "index": 4,
          "video_id": "Chương 9_JGxo_olUl2U",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/JGxo_olUl2U",
          "start_time": 147,
          "end_time": 211,
          "text": "à đặc trưng cấp giữa và đặc trưng cấp cao thì việc trồng nhiều lớp này nó sẽ giúp cho chúng ta tổng hợp tổng hợp được cái đặc trưng nhiều cấp của học sau của Deep Learning bao gồm là Low Level rồi Mid Level và High Level Feature thì đây là cái mẹo đầu tiên và mẹo này nó sẽ được thực hiện bằng cách chúng ta sẽ lập đi lập lại cái module Encoder hoặc là Decoder này chúng ta sẽ nối tiếp rồi thực hiện đi thực hiện lại cái quá trình tính toán này nhiều lần và trong trường hợp này chúng ta sẽ lập là 6 lần hay tổng số layer của mình sẽ là bằng 6 thì tại sao nó lại là bằng 6 thì đây chính là cái chúng ta hoàn toàn có thể thay đổi cái số lớp này tùy vào cái kiến trúc của cái Transformer này của mình nếu như chúng ta muốn cái Transformer này của mình nó nhẹ ít tham số"
        },
        {
          "index": 5,
          "video_id": "Chương 9_JGxo_olUl2U",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/JGxo_olUl2U",
          "start_time": 200,
          "end_time": 260,
          "text": "chúng ta hoàn toàn có thể thay đổi cái số lớp này tùy vào cái kiến trúc của cái Transformer này của mình nếu như chúng ta muốn cái Transformer này của mình nó nhẹ ít tham số thì layer của mình có thể là 1 layer, 2 layer nhưng nếu chúng ta muốn cái kiến trúc Transformer này có thể giải quyết những cái bài tán thức tạp hơn với nhiều cái thông tin hơn thì khi đó cái số layer này có thể lên đến vài chục thậm chí là hàng trăm layer và mẹo thứ 2 đó chính là sử dụng cái residual connect à sử dụng cái residual connect và cái layer tiếp theo sẽ được tính bằng cái layer trước đó là layer thứ l trừ 1 cộng với lại cái phép biến đổi cộng với lại cái output của cái phép biến đổi thì ở đây chúng ta sẽ có các cái đường màu đỏ chúng ta thực hiện phép cộng tức là đầu vào ở đây nè sau khi chúng ta thực hiện cell tension xong"
        },
        {
          "index": 6,
          "video_id": "Chương 9_JGxo_olUl2U",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/JGxo_olUl2U",
          "start_time": 250,
          "end_time": 311,
          "text": "thì ở đây chúng ta sẽ có các cái đường màu đỏ chúng ta thực hiện phép cộng tức là đầu vào ở đây nè sau khi chúng ta thực hiện cell tension xong sau khi thực hiện cell tension xong thì chúng ta sẽ có cái layer này đó là layer thứ 1 nè đúng không? cái F của x L trừ 1 rồi sau đó chúng ta lấy 9 cái đầu vào tức là x L trừ 1 này chúng ta lại đi cộng lại với nhau lấy 2 cái giá trị này cộng lại để được cái Fx và chúng ta lại tương tự như vậy chúng ta lại có cái residual connect ở đây, chúng ta lại có cái phép cộng ở đây và nhờ cái residual connect này nó sẽ giúp cho chúng ta huấn luyện được với những cái mạng rất là sâu và giảm được rất nhiều cái thời gian huấn luyện cũng như là chống được cái hiện tượng overfitting cái mẹo thứ 3 đó chính là layer norm chúng ta sẽ chuẩn hóa theo cấp độ layer cái problem của cái việc là chúng ta phải sử dụng cái layer norm"
        },
        {
          "index": 7,
          "video_id": "Chương 9_JGxo_olUl2U",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/JGxo_olUl2U",
          "start_time": 300,
          "end_time": 361,
          "text": "cái mẹo thứ 3 đó chính là layer norm chúng ta sẽ chuẩn hóa theo cấp độ layer cái problem của cái việc là chúng ta phải sử dụng cái layer norm cái vấn đề đó là nó sẽ khó huấn luyện cái tham số của mình và khi các cái layer của mình nó có cái input biến động liên tục nghĩa là sao? khi chúng ta thực hiện cái phép biến đổi cell retention ở đây thì điều gì xảy ra nếu như cái x của mình cái x đầu vào của mình nó có cái giải giá trị biến động rất là khác biệt thì nó dẫn đến là mô hình nó khó huấn luyện và giải pháp ở đây chúng ta sẽ làm đó chính là đưa cái giá trị của mình về cái dạng chủng hóa đưa về cùng một cái không gian có cái phân bố chuẩn đó là 0 1 norm 0 1 thì như vậy thì giải pháp của mình là giảm cái sự biến động đầu vào giảm cái sự biến động của cái dữ liệu đầu vào bằng cách đó là đưa về cái phân bố chuẩn trên mỗi cái layer của mình"
        },
        {
          "index": 8,
          "video_id": "Chương 9_JGxo_olUl2U",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/JGxo_olUl2U",
          "start_time": 350,
          "end_time": 410,
          "text": "thì như vậy thì giải pháp của mình là giảm cái sự biến động đầu vào giảm cái sự biến động của cái dữ liệu đầu vào bằng cách đó là đưa về cái phân bố chuẩn trên mỗi cái layer của mình và đây chính là cái công thức cho cái hàm biến đổi của mình x phải là cái giá trị sau khi chúng ta đã chủng hóa và nó sẽ là bằng x của cùng một cái layer ở đây là cùng một cái layer l tức là chúng ta sẽ chủng hóa trên từng layer chứ không có sự tương tác của nó giữa các layer với nhau thế thì tại layer số l chúng ta sẽ trừ cho l tức là trung bình cộng của các cái x này trên cái layer l rồi chia cho sigma thì cái việc mà trừ cho m xíu gọi trừ cho cái m nó sẽ giúp cho mình đạt được cái phân bố đó là min bằng 0 và chia cho sigma nó sẽ giúp cho chúng ta đạt được cái phân bố đó là standard deviation của mình là bằng 1 thì cái x phải của mình"
        },
        {
          "index": 9,
          "video_id": "Chương 9_JGxo_olUl2U",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/JGxo_olUl2U",
          "start_time": 400,
          "end_time": 460,
          "text": "min bằng 0 và chia cho sigma nó sẽ giúp cho chúng ta đạt được cái phân bố đó là standard deviation của mình là bằng 1 thì cái x phải của mình sau khi chúng ta biến đổi xong nó sẽ đạt được cái phân bố chỗ này và giúp cho cái việc huấn luyện của mình nó sẽ ít bị biến động hơn nó sẽ bền vững hơn huấn luyện nó sẽ nhanh hơn và trong slide trước thì chúng ta thấy nè cái việc thực hiện norm này sẽ được thực hiện là sau khi thực hiện cái phép biến đổi cái phép cộng của mình sau khi chúng ta cộng xong chúng ta sẽ chủng hóa để chuẩn bị bước qua cái phép biến đổi tiếp theo là phép cộng của mình rồi ở đây chúng ta cộng xong chúng ta sẽ chủng hóa để chuẩn bị biến đổi sang cái nấp biến đổi tiếp theo và cái mẹo thứ 4 đó chính là scale.product thì sau khi chúng ta chủng hóa xong thì từng cái phần tử của mình nó đã được đưa từng cái phần tử trong cái vector của mình"
        },
        {
          "index": 10,
          "video_id": "Chương 9_JGxo_olUl2U",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/JGxo_olUl2U",
          "start_time": 450,
          "end_time": 512,
          "text": "và cái mẹo thứ 4 đó chính là scale.product thì sau khi chúng ta chủng hóa xong thì từng cái phần tử của mình nó đã được đưa từng cái phần tử trong cái vector của mình nó đã được đưa về cái phân bố chuẩn tuy nhiên ở đây chúng ta sẽ set 2 cái vector chúng ta sẽ set 2 cái vector xe và xg của mình à à à rồi trước khi chúng ta thực hiện cái phép biến đổi tích vô hướng thì 2 cái xe và xg này đã được chủng hóa tức là nó đã được đưa về cái phân bố chuẩn tuy nhiên sau khi chúng ta thực hiện cái phép tích vô hướng sau sau khi chúng ta thực hiện cái phép biến đổi tích vô hướng thì cái kết quả của mình nó sẽ là 1 cái tích vô hướng nó vẫn lớn cái kết quả của cái phép tích vô hướng này nó sẽ ra cái giá trị rất là lớn và cái việc cái giá trị này lớn nó xuất phát là từ"
        },
        {
          "index": 11,
          "video_id": "Chương 9_JGxo_olUl2U",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/JGxo_olUl2U",
          "start_time": 499,
          "end_time": 560,
          "text": "cái kết quả của mình nó sẽ là 1 cái tích vô hướng nó vẫn lớn cái kết quả của cái phép tích vô hướng này nó sẽ ra cái giá trị rất là lớn và cái việc cái giá trị này lớn nó xuất phát là từ cái số chiều dk này lớn, số chiều dk tức là số phần tử trong cái vector xe này của mình là nhiều thì khi các cái giá trị này nhân với nhau và cái dk này mà dài càng dài thì cái việc mà chúng ta cộng cái giá trị tích này lại với nhau nó sẽ ra 1 cái giá trị nó lớn thì giải pháp đó là chúng ta sẽ sau khi chúng ta thực hiện cái phép nhân là xe với xc này xong chúng ta sẽ chia cho căn của dk thì cái việc chia cho căn của dk này nó cũng giúp cho chúng ta đưa về cái phân bố chuẩn đưa cái output của mình về cái phân bố chuẩn và đây chính là chia cho căn của dk như vậy đây là cái công thức của mình để chúng ta có thể  đưa về cái phân bố chuẩn của mình sau khi đã được chuẩn hóa"
        },
        {
          "index": 12,
          "video_id": "Chương 9_JGxo_olUl2U",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/JGxo_olUl2U",
          "start_time": 547,
          "end_time": 610,
          "text": "nó cũng giúp cho chúng ta đưa về cái phân bố chuẩn đưa cái output của mình về cái phân bố chuẩn và đây chính là chia cho căn của dk như vậy đây là cái công thức của mình để chúng ta có thể  đưa về cái phân bố chuẩn của mình sau khi đã được chuẩn hóa thì nó gọi là scale.product attention và 1 trong những cái vấn đề lớn khác của cái mạng transformer đó là hình như chúng ta chưa xét đến yếu tố về mặt thứ tự chưa xét về yếu tố về mặt thứ tự ở đây chúng ta thấy nè các cái từ của mình được đưa vào xử lý đưa vào xử lý song song với nhau từ này biến đổi độc lập với từ này từ này biến đổi độc lập với từ này nó thực hiện một cách song song nó không có yếu tố thứ tự ở cái việc ở đây chúng ta nhìn trên cái sơ đồ này chúng ta thấy là từ này trước từ này sau nhưng nó không có cái gì đảm bảo được là khi chúng ta tổng hợp thông tin ở đây thì từ nào xuất hiện trước từ nào xuất hiện sau đó thì cái tính thứ tự này nó có quan trọng hay không"
        },
        {
          "index": 13,
          "video_id": "Chương 9_JGxo_olUl2U",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/JGxo_olUl2U",
          "start_time": 598,
          "end_time": 660,
          "text": "nhưng nó không có cái gì đảm bảo được là khi chúng ta tổng hợp thông tin ở đây thì từ nào xuất hiện trước từ nào xuất hiện sau đó thì cái tính thứ tự này nó có quan trọng hay không thì trong đại đa số các cái ngôn ngữ của mình tính thứ tự rất là quan trọng không phải chỉ trong tiếng Việt mà kể cả tiếng Anh thì đây chúng ta sẽ lấy một cái ví dụ đó là cũng 3 từ you, do, understand nhưng mà chúng ta sắp xếp theo cái trình tự khác nhau thì nó sẽ ra 2 cái ý nghĩa khác nhau thì nó sẽ ra 2 cái ý nghĩa khác nhau ví dụ do you understand với lại you do understand thì rõ ràng đây là một cái câu hỏi trong khi ở đây là một câu khẳng định thì đây là ý nghĩa khác nhau hoàn toàn ở đây chỉ là một cái ví dụ để minh họa cho cái tầm quan trọng của những câu hỏi này của thứ tự vậy thì giải pháp để giải quyết cái vấn đề này đó chính là chúng ta sẽ có một cái vector biểu diễn cho cái yếu tố vị trí bình thường chúng ta sẽ có cái xy"
        },
        {
          "index": 14,
          "video_id": "Chương 9_JGxo_olUl2U",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/JGxo_olUl2U",
          "start_time": 649,
          "end_time": 710,
          "text": "vậy thì giải pháp để giải quyết cái vấn đề này đó chính là chúng ta sẽ có một cái vector biểu diễn cho cái yếu tố vị trí bình thường chúng ta sẽ có cái xy thì y này chính là cái chỉ số về mặt vị trí y này sẽ là vị trí y này sẽ là vị trí thì bây giờ chúng ta làm sao có thể biến cái y này thành một cái vector biểu diễn luôn thì bản thân cái cell attention là nó không có quan tâm đến yếu tố về mặt vị trí như đã giải thích trong cái slide trước nó không có quan tâm đến yếu tố về mặt vị trí các cái từ nó được thực hiện một cách độc lập nhau do đó chúng ta cần phải mã hóa cái thứ tự trong cái query key và value của mình qi rồi ki và vi thì chúng ta làm sao mã hóa được cái chỉ số này và đưa vào bên trong cái vector của mình thì ở đây chúng ta giả sử là các cái chỉ số thứ tự y sẽ được mã hóa bằng một cái vector là pi"
        },
        {
          "index": 15,
          "video_id": "Chương 9_JGxo_olUl2U",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/JGxo_olUl2U",
          "start_time": 700,
          "end_time": 760,
          "text": "mã hóa được cái chỉ số này và đưa vào bên trong cái vector của mình thì ở đây chúng ta giả sử là các cái chỉ số thứ tự y sẽ được mã hóa bằng một cái vector là pi cái chỉ số y sẽ được mã hóa bằng một cái vector tên là pi và pi này thì có d chiều và y của mình sẽ là các cái chỉ số chạy từ 1 cho đến t với t là cái độ dài của cái code hoặc là cái đoạn văn, độ vào của mình thì khi đó các cái vector value, key và query mới của mình thì nó sẽ được tính là bằng vi bằng vi ngã i cộng cho pi tức là chúng ta sẽ có cái sự tham gia của cái thông tin về mặt vị trí pi nó chứa cái thông tin về mặt vị trí của mình và vi ngã i k ngã i và qi ngã i nó chính là các cái value key và query cũ rồi chưa có cái thông tin về mặt vector vị trí"
        },
        {
          "index": 16,
          "video_id": "Chương 9_JGxo_olUl2U",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/JGxo_olUl2U",
          "start_time": 748,
          "end_time": 809,
          "text": "và vi ngã i k ngã i và qi ngã i nó chính là các cái value key và query cũ rồi chưa có cái thông tin về mặt vector vị trí vi ngã i vi ngã ki và xin lỗi k ngã i và quy ngã i đây chính là những cái vector biểu diễn cho các cái từ cũ của mình cho các cái value key và query cũ của mình và khi chúng ta cộng thêm cái pi thì như vậy cái thông tin vi ki và qi của mình nó sẽ có được cái thông tin về mặt vị trí rồi và chúng ta có thể ở đây thì chúng ta sử dụng cái phép là phép cộng ở đây là chúng ta sử dụng là phép cộng và chúng ta hoàn toàn có thể thực hiện cái phép concat thì vi ngã i của mình đây là vi ngã i chúng ta thực hiện cái phép cộng với lại cái vector biểu diễn của pi"
        },
        {
          "index": 17,
          "video_id": "Chương 9_JGxo_olUl2U",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/JGxo_olUl2U",
          "start_time": 799,
          "end_time": 860,
          "text": "thì vi ngã i của mình đây là vi ngã i chúng ta thực hiện cái phép cộng với lại cái vector biểu diễn của pi nhưng chúng ta cũng có thể là nối vậy thì nếu chúng ta nối thì điều gì sẽ xảy ra đây là vi ngã i và đây là pi thì khi chúng ta nối thì cái kích thước của vector của mình nó sẽ lớn lên dẫn đến là cái khối lượng tính toán của mình nó sẽ lớn do đó thì chúng ta sử dụng cái phong thức cộng để giảm phát sinh cái chi phí tính toán về sau để giảm phát sinh cái chi phí tính toán về sau rồi như vậy thì chúng ta đã có cái solution đó là chúng ta làm sao có được cái vector biểu diễn đó là chúng ta làm sao có được cái vector biểu diễn thì ở đây nó sẽ có cái khái niệm nó gọi là position embedding hoặc là position embedding nó sẽ giúp cho chúng ta mẻ hóa nó sẽ giúp cho chúng ta mẻ hóa cái vị trí bằng cái vector biểu diễn cái vector biểu diễn và mỗi một cái vị trí thì nó sẽ là một cái vector không trùng nhau đúng rồi, tại vì nếu như"
        },
        {
          "index": 18,
          "video_id": "Chương 9_JGxo_olUl2U",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/JGxo_olUl2U",
          "start_time": 850,
          "end_time": 899,
          "text": "nó sẽ giúp cho chúng ta mẻ hóa nó sẽ giúp cho chúng ta mẻ hóa cái vị trí bằng cái vector biểu diễn cái vector biểu diễn và mỗi một cái vị trí thì nó sẽ là một cái vector không trùng nhau đúng rồi, tại vì nếu như nó là cái vector trùng nhau dẫn đến đó là nó sẽ bị nhập nhằn thông tin là từ nào ở vị trí nào do đó thì cái vector vị trí nó phải đảm bảo một yếu tố đó là mỗi vị trí sẽ có một cái vector riêng không trùng thì ở đây nó chính là cái chỗ này đây chính là cái position embedding và bản chất nó chỉ là một cái vector tạo ra là một cái vector về mặt vị trí sau đó nó sẽ cộng với input embedding của mình input embedding của mình"
        }
      ]
    },
    {
      "video_id": "Chương 9_jKnjyvvXzXI",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng  \n  - Giải thích chi tiết kiến trúc **Decoder** trong Transformer: cơ chế autoregressive của decoder, cách ngăn không cho decoder “nhìn thấy” các token tương lai, cách hiện thực hoá việc này sao cho vẫn tận dụng được tính song song của self-attention, và sự khác biệt chính giữa encoder và decoder (cross-attention). [1][2][3]\n\n- Các khái niệm sẽ được đề cập  \n  - Autoregressive decoding và vấn đề nhìn thấy token tương lai; masking trong self-attention; multi-head masked self-attention; encoder–decoder (cross) attention; cơ chế Add & Norm, Feedforward, positional embedding, linear projection và softmax để sinh xác suất từ tiếp theo. [1][2][3][6][11][15][16]\n\n## 2. Các điểm chính (Main Points)\n\n### A. Tính chất tuần tự của decoding và vấn đề “nhìn trước” (autoregressive constraint)\n- Decoding là quá trình tuần tự: khi dự đoán token ở vị trí i, ta không được phép dùng thông tin từ các token ở vị trí > i (tương lai). Nếu dùng self-attention như encoder thì các đường attention từ token hiện tại tới token tương lai sẽ làm “rò rỉ đáp án” (leak future information). [1][2]  \n- Một cách biểu diễn trực quan: khi dùng self-attention thuần trên decoder, các cạnh màu đỏ (từ hiện tại sang tương lai) vi phạm nguyên tắc này và phải bị loại bỏ/che đi. [2][3]\n\n### B. Cách hiểu tuần tự: mở dần tập key/value khi decode từng bước\n- Nếu decode hoàn toàn tuần tự (không song song), tại mỗi bước i ta chỉ mở rộng tập key và value đến token i (tức là chỉ dùng các token hiện tại và quá khứ). Điều này tôn trọng nguyên tắc autoregressive nhưng gây mất khả năng song song hoá. [3][4][5]\n\n### C. Giải pháp: Masked Multi-Head Self-Attention để vừa đúng nguyên tắc vừa song song hóa\n- Ý tưởng: vẫn tính attention song song cho toàn chuỗi bằng cách **che (mask)** các vị trí tương lai trong ma trận attention—gán score của các cặp (query i, key j) với j > i bằng -∞ trước khi softmax. Việc này làm cho các attention weights tương ứng trở thành 0 sau softmax, vì softmax(-∞) → 0. [6][9]  \n- Nhờ việc mask tại cấp độ ma trận attention, toàn bộ các node ở layer có thể tính toán **song song** trên GPU mà không vi phạm nguyên tắc autoregressive. [6][10]  \n- Cách tiến hành tính toán (tóm tắt): tính toàn bộ attention scores QK^T, áp mask (đặt giá trị j>i = -∞), sau đó softmax để ra weights, rồi nhân với V. [9][6]\n\n  - Công thức attention score sơ lược: score_{i,j} = Q_i · K_j (dot product). Nếu j > i thì score_{i,j} := -∞ (mask). Sau softmax, các vị trí bị mask góp 0. [9]\n\n### D. Ví dụ minh hoạ cách mask hoạt động (Start / Do / You / Understand)\n- Ví dụ chuỗi \"Start Do You Understand\":  \n  - Khi dự đoán token tại vị trí \"Start\": không được phép thấy các token \"Do\", \"You\", \"Understand\". [7]  \n  - Khi dự đoán token \"Do\": được thấy \"Start\" nhưng không được thấy \"Do\",\"You\",\"Understand\". (Trong ví dụ hình ảnh: ô trắng = được nhìn thấy; ô màu = bị che.) [7][8]  \n  - Khi dự đoán tới \"Understand\": được thấy \"Start\",\"Do\",\"You\" nhưng không được thấy chính \"Understand\". [8]\n\n- Nhờ áp mask theo hàng-cột (j>i), multi-head masked attention vẫn có thể thực thi song song mà đảm bảo tính autoregressive. [9][10]\n\n### E. Multi-Head Masked Self-Attention — lý do dùng multi-head\n- Sử dụng multi-head giúp mô hình học nhiều “view”/khía cạnh khác nhau của sự phụ thuộc giữa token (multiple subspaces), đồng thời tận dụng tốt khả năng song song hoá trên GPU. Masking vẫn được áp dụng cho từng head. [6][10]\n\n### F. Cross-Attention (Encoder–Decoder Attention)\n- Ngoài masked self-attention, decoder có thêm **encoder–decoder attention** (cross-attention): ở block này, **query** đến từ decoder (các trạng thái nội bộ h1,h2,...), còn **key** và **value** lấy từ output của encoder (s1,s2,...). Đây là cơ chế để decoder truy vấn (look up) thông tin đã mã hoá từ encoder. [11][12][13][14]  \n- Về công thức: attention(query=from decoder, key=value=from encoder) có dạng tương tự attention chuẩn (Q·K^T → softmax → V), chỉ khác là nguồn tạo Q và (K,V) khác nhau (Q: decoder; K,V: encoder). [13][14]\n\n### G. Các bước còn lại trong block decoder (Add & Norm, Feedforward, Projection)\n- Sau mỗi multi-head attention (masked self-attn và cross-attn) thường có các khối: **Add & Norm** (residual + layer norm), sau đó **Feedforward** (position-wise FFN) và lại **Add & Norm**. Những bước này giúp ổn định huấn luyện, tránh vanishing gradient và overfitting. [14][15]  \n- Input vào decoder vẫn dùng **token embedding** cộng **positional embedding** như encoder. [15]  \n- Sau các tầng attention + feedforward, decoder dùng một **linear layer** để chiếu không gian đặc trưng của mô hình thành không gian output cần thiết (ví dụ không gian từ vựng hoặc nhãn), rồi áp **softmax** để tính xác suất token tiếp theo. [15][16][17]\n\n### H. Hai khác biệt chính giữa Decoder và Encoder\n- Hai thành phần khác biệt lớn nhất của decoder so với encoder là:  \n  1) **Masked Multi-Head Self-Attention** (chặn tương lai) để đảm bảo autoregressive. [15]  \n  2) **Encoder–Decoder (Cross) Attention**: key/value từ encoder, query từ decoder để truy vấn thông tin mã hoá. [15][11][14]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh hoạ cụ thể trong video: chuỗi \"Start, Do, You, Understand\" để minh hoạ rõ ràng phạm vi token mà mỗi vị trí được phép “nhìn thấy” trong masked self-attention (Start không thấy Do,...; Do thấy Start nhưng không thấy You,...; v.v.). [7][8]  \n- Ứng dụng thực tế: decoder dạng này dùng cho các tác vụ sinh ngôn ngữ autoregressive như language modeling, machine translation (lúc giải mã), text generation, hoặc các nhiệm vụ cần dự đoán token/tập nhãn tuần tự. Khi cần dự báo token tiếp theo, decoder tính xác suất trên không gian output (từ điển / nhãn) bằng linear + softmax. [16]  \n- Trường hợp sử dụng: mọi hệ thống Transformer cần sinh câu đầu ra theo thứ tự (ví dụ hệ thống dịch, chatbot, summarization ở bước generate) sẽ áp dụng masked multi-head self-attention trong decoder kết hợp cross-attention với encoder. [11][15][16]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính  \n  - Decoder trong Transformer khác encoder ở chỗ phải **tôn trọng tính autoregressive** (không được dùng thông tin tương lai) bằng cách **mask** các vị trí tương lai trong self-attention; đồng thời decoder có thêm **encoder–decoder cross-attention** để lấy thông tin từ encoder. [1][2][9][11][15]  \n  - Việc gán attention score = -∞ cho các vị trí tương lai trước softmax khiến các trọng số attention tương ứng trở thành 0, từ đó vừa đảm bảo tính tuần tự vừa cho phép tính toán song song trên GPU thông qua multi-head attention. [6][9][10]  \n  - Các thành phần chuẩn khác vẫn giữ: embeddings + positional embeddings, Add & Norm, Feedforward, linear projection và softmax để sinh xác suất token tiếp theo. [15][16][17]\n\n- Tầm quan trọng của nội dung  \n  - Hiểu rõ cơ chế masked self-attention và cross-attention là then chốt để triển khai chính xác decoder của Transformer cho các nhiệm vụ sinh tuần tự, đồng thời tối ưu hoá tính song song trong huấn luyện và suy luận. [6][9][11][15]\n\n- Liên hệ với các bài giảng khác  \n  - Nội dung này là phần tiếp nối trực tiếp từ kiến thức về encoder và cơ chế self-attention nói chung (đã trình bày trước đó), và chuẩn bị cho các phần nói về chi tiết huấn luyện/generation và tối ưu hoá Transformer ở các chương sau. [1][6][15]\n\n(Phần tóm tắt đã sử dụng đầy đủ các đoạn trích từ video để mô tả toàn bộ nội dung về Decoder trong Transformer; các citation [1]…[17] chỉ tới các đoạn tương ứng trong video.)",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 9_jKnjyvvXzXI",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder",
          "video_url": "https://youtu.be/jKnjyvvXzXI",
          "start_time": 0,
          "end_time": 61,
          "text": "về lý thuyết thì decoder nó cũng sẽ tương tự như là encoder tuy nhiên cái output của decoder nó sẽ có một cái vấn đề như thế này đó là decode là một cái quá trình mà chúng ta dạy mã tuần tự dạy mã tuần tự chúng ta không thể nào thực hiện song song được tại vì cái việc song song nó tư đương với cái việc là chúng ta có thể nhìn thấy đáp án ở phía sau tức là tại một cái quá trình decode chúng ta đưa ra cái output tại đây rồi sau đó chúng ta mới đưa ra cái output tại đây đây là lần thứ một đây là lần thứ hai và chúng ta không được phép thấy cái từ ở thứ ba thứ tư thế thì những cái đường màu đỏ ở đây nếu như chúng ta sử dụng self-attention trên cái decode tương tự như là cái self-attention của encode"
        },
        {
          "index": 2,
          "video_id": "Chương 9_jKnjyvvXzXI",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder",
          "video_url": "https://youtu.be/jKnjyvvXzXI",
          "start_time": 48,
          "end_time": 108,
          "text": "cái từ ở thứ ba thứ tư thế thì những cái đường màu đỏ ở đây nếu như chúng ta sử dụng self-attention trên cái decode tương tự như là cái self-attention của encode thì những cái đường màu đỏ này nó sẽ vi phạm đó là chúng ta đã nhìn thấy đáp án phía sau đúng không tại vì cái thông tin tại cái vị trí số một này nó nhận được cái thông tin tại cái layer tại cái vị trí này tức là cái từ thứ hai đưa vô rồi từ thứ ba đưa vô đây như vậy nó đã thấy trước đáp án như vậy là không có đáp án không có được phép như vậy vậy thì chúng ta phải bỏ đi các cái cạnh nối màu đỏ này đi chúng ta phải đảm bảo như vậy thì khi đó cái quá trình decode nó mới thật sự là đúng như cái quy tắc của mình đó là chúng ta lần lượt đưa ra các cái dự đoán cho từ từ từ của mình chứ không được phép"
        },
        {
          "index": 3,
          "video_id": "Chương 9_jKnjyvvXzXI",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder",
          "video_url": "https://youtu.be/jKnjyvvXzXI",
          "start_time": 99,
          "end_time": 160,
          "text": "nó mới thật sự là đúng như cái quy tắc của mình đó là chúng ta lần lượt đưa ra các cái dự đoán cho từ từ từ của mình chứ không được phép tổng hợp thông tin của những cái từ ở trong tương lai tức là chúng ta thấy trước những cái từ trong tương lai như vậy thì giải pháp đó là gì tại mỗi bước của decoder thì ta sẽ lần chúng ta sẽ dần dần tạo ra cái mở rộng cái tập key và value của mình tức là trong cái quá trình mà chúng ta decode là chúng ta có cái query rồi tại đây thì chúng ta sẽ có cái query của mình thì chúng ta sẽ phải dần dần mở rộng ra cái tập key và value tại vì chúng ta xử lý đến đâu decode đến đâu thì chúng ta sẽ thấy đến đó chứ chúng ta không được phép thấy những cái từ tiếp theo ví dụ tại đây thì chúng ta chỉ được thấy những cái từ hiện tại nè"
        },
        {
          "index": 4,
          "video_id": "Chương 9_jKnjyvvXzXI",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder",
          "video_url": "https://youtu.be/jKnjyvvXzXI",
          "start_time": 150,
          "end_time": 210,
          "text": "xử lý đến đâu decode đến đâu thì chúng ta sẽ thấy đến đó chứ chúng ta không được phép thấy những cái từ tiếp theo ví dụ tại đây thì chúng ta chỉ được thấy những cái từ hiện tại nè cái từ hiện tại nè và quá khứ nè là cái quá trình mà chúng ta cái từ mà chúng ta đã suy đoán ở trước đó chứ không được phép những cái từ của tương lai nè là chúng ta không được phép và ở đây thì chúng ta cần phải che các cái trạng thái sau đúng không tại cái vị trí thứ hai đây chúng ta sẽ phải che các cái trạng thái sau chúng ta không được phép thấy tại vị trí số 2 chúng ta có thể truyền thông tin ra sau nhưng không nhận được thông tin từ đằng trước về thì đó là cái nguyên tắc và ở đây chúng ta sẽ có một cái hiệu ứng để minh mại cho cái việc này đầu tiên đó là kết thúc quá trình encode thì chúng ta sẽ bắt đầu đưa ra cái dự đoán cho cái từ tiếp theo"
        },
        {
          "index": 5,
          "video_id": "Chương 9_jKnjyvvXzXI",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder",
          "video_url": "https://youtu.be/jKnjyvvXzXI",
          "start_time": 199,
          "end_time": 260,
          "text": "để minh mại cho cái việc này đầu tiên đó là kết thúc quá trình encode thì chúng ta sẽ bắt đầu đưa ra cái dự đoán cho cái từ tiếp theo rồi sau đó chúng ta mở rộng ra và chúng ta sẽ lan truyền cái thông tin đến cái sau khi chúng ta đã dự đoán xong chúng ta sẽ lan truyền thông tin đến cái query tiếp theo cứ như vậy thì là lan truyền và mở rộng dần ra như vậy thì với cái việc mà dần dần mở rộng ra thì nó sẽ bị vấn đề gì đó chính là cái tính tuần tự mà tính tuần tự thì nó vi phạm cái nguyên lý hoặc là cái mong muốn của transformer đó là chúng ta đang muốn song song hóa càng nhiều càng tốt cái tính tuần tự này dẫn đến là không có song song không thể song song hóa được không thể tinh sáng song song như vậy thì giải pháp là như thế nào"
        },
        {
          "index": 6,
          "video_id": "Chương 9_jKnjyvvXzXI",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder",
          "video_url": "https://youtu.be/jKnjyvvXzXI",
          "start_time": 247,
          "end_time": 310,
          "text": "cái tính tuần tự này dẫn đến là không có song song không thể song song hóa được không thể tinh sáng song song như vậy thì giải pháp là như thế nào chúng ta biết rằng là self-attention sở dĩ có thể song song hóa được mà chúng ta biết là point of view là não tính tính tính củagoodness Đó là vì cái từ tại thời điểm hiện tại có thể nhìn được những cái từ của tương lai, những cái từ ở đằng sau đó. Đó thì nó mới có thể tính toán song song được. Vậy thì bây giờ làm sao để cái việc mà tính toán của các cái node ở cái layer, các cái trạng thái ẩn của các cái tầng của mình, nó vẫn thực hiện được một cách song song. Nó vẫn thực hiện được một cách song song. Đó chính là chúng ta sẽ sử dụng cơ chế Mesh, Multihead, Self-Attention. Thì chúng ta sẽ che các cái Attention của các cái từ phía sau bằng cách gán cái Attention Score của nó bằng trừ của cuối cùng. Vậy thì chúng ta sẽ có công thức ở đây."
        },
        {
          "index": 7,
          "video_id": "Chương 9_jKnjyvvXzXI",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder",
          "video_url": "https://youtu.be/jKnjyvvXzXI",
          "start_time": 300,
          "end_time": 359,
          "text": "Thì chúng ta sẽ che các cái Attention của các cái từ phía sau bằng cách gán cái Attention Score của nó bằng trừ của cuối cùng. Vậy thì chúng ta sẽ có công thức ở đây. Lỡ chúng ta sẽ có cái ví dụ ở đây. Start, tức là bắt đầu cái quá trình decode. Do you understand? Thế thì tại cái vị trí Start này. Đây là cái quá trình. Đi code nha. Thì tại cái vị trí Start. Thì chúng ta sẽ không được phép thấy từ Start, từ Do, từ You, từ Understand. Tại vì chúng ta đang cần phải predict. Đây là cái chúng ta cần predict, cần đoán ra đúng không? Cần predict, cần dự đoán cái từ này. Thì chúng ta không được thấy cái từ đáp án của nó. Rồi, đến cái quá trình mà decode. Cho cái từ Do. Chúng ta sẽ Ở đây chỗ này là màu trắng. Chỗ này là màu trắng, tức là chúng ta được phép thấy cái từ Start"
        },
        {
          "index": 8,
          "video_id": "Chương 9_jKnjyvvXzXI",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder",
          "video_url": "https://youtu.be/jKnjyvvXzXI",
          "start_time": 351,
          "end_time": 410,
          "text": "Cho cái từ Do. Chúng ta sẽ Ở đây chỗ này là màu trắng. Chỗ này là màu trắng, tức là chúng ta được phép thấy cái từ Start Chúng ta được phép thấy cái từ Start. Nhưng không được phép thấy cái từ Do. Và không được thấy cái từ You, từ Understand. Trong quá trình mà decode cái từ Do. Chúng ta sẽ được thấy cái từ Start. Được thấy cái từ Do, nhưng không được thấy cái từ You, từ Understand. Và trong quá trình mà decode cái từ Understand. Chúng ta sẽ được thấy hết các từ Start, Do, You nhưng không được thấy từ Understand. Được hãy để hnae biểu気 Fish disappear is original. chính là cái mass multi-head cell tension và về công thức tính toán thì cũng rất là đơn giản nếu như cái kỳ nếu như cái kG của mình mà bé hơn Y tức là cái kỳ của mình là những cái từ đã thấy đã thấy thì chúng ta sẽ giữ nguyên cái công thức attention score"
        },
        {
          "index": 9,
          "video_id": "Chương 9_jKnjyvvXzXI",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder",
          "video_url": "https://youtu.be/jKnjyvvXzXI",
          "start_time": 399,
          "end_time": 459,
          "text": "tức là cái kỳ của mình là những cái từ đã thấy đã thấy thì chúng ta sẽ giữ nguyên cái công thức attention score là QI nhân kỳ nhưng nếu Y mình lớn hơn hoặc bằng Z tức là tại cái vị trí thứ Y trở đi thì chúng ta không được phép tái chúng ta sẽ gán là trừ vô cùng và tại sao chúng ta lại gán bằng trừ vô cùng mà không phải cộng vô cùng đó là vì sau khi chúng ta chủng hóa nó thì ở đây cái trừ vô cùng nó sẽ biến thành số 0 sau khi thực hiện hàm sốt mắt thì trừ vô cùng của mình sẽ biến thành số 0 tức là chúng ta sẽ sẽ không tổng hợp cái thông tin của cái từ thứ Z với Z lớn hơn Y những cái từ nào mà từ nó trở về sau là không được phép tổng hợp thông tin như vậy thì giải pháp đó là sử dụng multihead cell attention sẽ giúp cho chúng ta có thể song song hóa được quá trình tính toán một cách dễ dàng với GPU"
        },
        {
          "index": 10,
          "video_id": "Chương 9_jKnjyvvXzXI",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder",
          "video_url": "https://youtu.be/jKnjyvvXzXI",
          "start_time": 448,
          "end_time": 512,
          "text": "những cái từ nào mà từ nó trở về sau là không được phép tổng hợp thông tin như vậy thì giải pháp đó là sử dụng multihead cell attention sẽ giúp cho chúng ta có thể song song hóa được quá trình tính toán một cách dễ dàng với GPU mà vẫn không vi phạm cái nguyên tắc đó là không được phép những cái thường của tương lai rồi và tương tự như vậy ha thì chúng ta cũng sẽ thực hiện cái decoder hoàn toàn giống với lại decoder đây chính là cái khác lớn nhất của mình à nó còn một cái khác nữa là trong cái slide tiếp theo ha rồi thì sau khi chúng ta thực hiện cái mask multihead attention thì chúng ta sẽ thực hiện cái add và non nó cũng giống như bên đây bên đây là multihead attention thì ngay sau đó là add và non thì ở đây cũng vậy add và non rồi và bây giờ chúng ta sẽ có một cái khác nữa là   cũng khá là lớn trong cái bước gọi là decoder đó chính là encoder decoder attention và tên ngắn gọn của nó đó là cross attention"
        },
        {
          "index": 11,
          "video_id": "Chương 9_jKnjyvvXzXI",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder",
          "video_url": "https://youtu.be/jKnjyvvXzXI",
          "start_time": 499,
          "end_time": 560,
          "text": "rồi và bây giờ chúng ta sẽ có một cái khác nữa là   cũng khá là lớn trong cái bước gọi là decoder đó chính là encoder decoder attention và tên ngắn gọn của nó đó là cross attention cross là cái sự chuyển đổi từ cái sort xin gọi là sự chuyển đổi giữa cái encode và cái decode ánh xạ giữa encode với decode thì nó gọi là cross attention thì trong cái cơ chế attention thì key nó sẽ đến từ decoder key của mình nó sẽ đến từ cái decoder còn value là cái decoder đây chính là cái key của mình đây chính là cái key và value của mình thì nó đến từ encoder 2 cái mũi tên này đến từ encoder tương ứng là cái se transformer cũng vậy transformer cũng vậy giả sử như chúng ta có s1 s2 cho đến s t thuộc rd tức là cái output của encoder đây chính là cái output của encoder"
        },
        {
          "index": 12,
          "video_id": "Chương 9_jKnjyvvXzXI",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder",
          "video_url": "https://youtu.be/jKnjyvvXzXI",
          "start_time": 550,
          "end_time": 610,
          "text": "transformer cũng vậy transformer cũng vậy giả sử như chúng ta có s1 s2 cho đến s t thuộc rd tức là cái output của encoder đây chính là cái output của encoder đây là encoder và h1 h2 ht là các cái input decoder tức là chúng ta sẽ có cái ki h i ở đây đây là cái input cho cái quá trình decoder input cho cái quá trình decoder khi đó thì cái cặp các bộ key value và query của mình thì nó sẽ có cái công thức như sau key thì nó sẽ lấy từ se đây là ở đây trong cái công thức này thì xin lỗi ở đây là chúng ta nhầm đây là query query của mình chứ không phải là key rồi ở đây chính là query"
        },
        {
          "index": 13,
          "video_id": "Chương 9_jKnjyvvXzXI",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder",
          "video_url": "https://youtu.be/jKnjyvvXzXI",
          "start_time": 600,
          "end_time": 661,
          "text": "query của mình chứ không phải là key rồi ở đây chính là query đây chính là query còn đây chính là key và value thì cái query này là cái query này chúng ta sẽ đi truy vấn trong cái tập key ở đây để từ đó chúng ta sẽ tổng hợp thông tin rồi thì công thức ở đây là đúng rồi và key của mình nó sẽ lấy từ se se là đến từ encoder là từ encoder rồi se ở đây cũng đến từ encoder và h1 h2 h3 là đến từ encoder và h3 h4 là đến từ encoder rồi thì chúng ta vẫn sẽ có bộ 3 là kvi rồi thì chúng ta vẫn sẽ có bộ 3 là kvi"
        },
        {
          "index": 14,
          "video_id": "Chương 9_jKnjyvvXzXI",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder",
          "video_url": "https://youtu.be/jKnjyvvXzXI",
          "start_time": 650,
          "end_time": 711,
          "text": "và h3 h4 là đến từ encoder rồi thì chúng ta vẫn sẽ có bộ 3 là kvi rồi thì chúng ta vẫn sẽ có bộ 3 là kvi thì công thức nó cũng y chang là se attention nhưng mà ở đây là gross attention tức là key và value thì lấy từ key và value thì lấy từ phần cuối tờ phần output của encoder phần cuối tờ phần output của encoder còn query thì chúng ta sẽ lấy từ cái input vào của encoder và query thì chúng ta sẽ lấy từ input vào của encoder decoder và tương tự như vậy chúng ta cũng sẽ thực hiện add norm đây là cái trick, cái mẹo để giúp cho thuấn luyện không có hiện tượng overfitting cũng như là tránh được cái hiện tượng vanishing gradient rồi như vậy thì cho đến bây giờ chúng ta đã gần như đã hoàn thành cái decoder rồi 2 cái sự khác biệt lớn nhất của cái decoder đó đó chính là cái math multi head attention và gross attention"
        },
        {
          "index": 15,
          "video_id": "Chương 9_jKnjyvvXzXI",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder",
          "video_url": "https://youtu.be/jKnjyvvXzXI",
          "start_time": 697,
          "end_time": 760,
          "text": "chúng ta đã gần như đã hoàn thành cái decoder rồi 2 cái sự khác biệt lớn nhất của cái decoder đó đó chính là cái math multi head attention và gross attention đó là 2 cái lớn nhất ngoài ra trong cái output ở trong cái decoder chúng ta vẫn sử dụng embedding như bình thường và đây vẫn là position positional embedding positional embedding như bình thường như bình thường thì đầu tiên đó là chúng ta sẽ thêm feedforward và add norm thì cũng tương tự cho cái ở đây tất cả các bước cell attention ở đây bản chất chỉ là sự tổng hợp thông tin một cách có trọng số thôi nó chưa có thực sự biến đổi sang một cái thông tin mới chưa thực sự biến đổi sang một thông tin mới do đó chúng ta sẽ thêm cái feedforward và add norm ở đây sau đó chúng ta sẽ thêm cái linear để chiếu từ không gian đặc trưng toàn bộ cái encoder và decoder này"
        },
        {
          "index": 16,
          "video_id": "Chương 9_jKnjyvvXzXI",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder",
          "video_url": "https://youtu.be/jKnjyvvXzXI",
          "start_time": 748,
          "end_time": 810,
          "text": "chưa thực sự biến đổi sang một thông tin mới do đó chúng ta sẽ thêm cái feedforward và add norm ở đây sau đó chúng ta sẽ thêm cái linear để chiếu từ không gian đặc trưng toàn bộ cái encoder và decoder này đó là không gian đặc trưng nó chưa phải là cái không gian output của mình sang cái không gian output của mình thì trong trường hợp này nó có thể là không gian từ điển nó có thể là không gian từ điển hoặc là cái keyword mà chúng ta cần trả về tại vì trong một số bài toán nó không phải là trả về một từ điển mà nó có thể là trả về cái nhãn từ loại tóm lại đó là nó chuyển từ không gian đặc trưng sang cái không gian output cái không gian mà chúng ta cần phải trả kết quả về và cuối cùng đó là chúng ta sẽ qua cái hàm để tính cái xác suất của cái từ tiếp theo mà mình dự đoán đó là gì chúng ta sẽ tính ra cái xác suất của cái từ tiếp theo rồi thì đây chính là những cái bước cuối cùng của decoder"
        },
        {
          "index": 17,
          "video_id": "Chương 9_jKnjyvvXzXI",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder",
          "video_url": "https://youtu.be/jKnjyvvXzXI",
          "start_time": 799,
          "end_time": 810,
          "text": "mà mình dự đoán đó là gì chúng ta sẽ tính ra cái xác suất của cái từ tiếp theo rồi thì đây chính là những cái bước cuối cùng của decoder"
        }
      ]
    },
    {
      "video_id": "Chương 9_fEGw6eEre2I",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Tổng kết và phân tích một số **vấn đề** (weaknesses) của kiến trúc Transformer cùng một số **hướng cải tiến** đã được đề xuất. [1]  \n- Các khái niệm sẽ được đề cập:  \n  - Chi phí tính toán của cơ chế self‑attention theo độ dài chuỗi. [1][2]  \n  - Vấn đề biểu diễn vị trí (positional representation), bao gồm vị trí tuyệt đối vs. vị trí tương đối và các phương pháp cải tiến (ví dụ: Rotary, EFM). [2][3][4][5]  \n  - Một số kiến trúc biến thể nhằm giảm chi phí tính toán như Linformer và BigBird. [5][6][7][8]  \n  - Nhận xét về hiệu quả thực tế của nhiều biến thể Transformer (không tăng mạnh độ chính xác). [9][10]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Chi phí tính toán của Self‑Attention\n- Self‑attention phải tính tương tác giữa tất cả các cặp token trong chuỗi, do đó chi phí tăng theo bình phương độ dài chuỗi: O(T^2). [1]  \n- Ngoài ra còn phụ thuộc vào D (số chiều của vector biểu diễn), nên tổng chi phí thường biểu diễn có sự phụ thuộc thêm vào D. [2]\n\n(Đề cập công thức: chi phí tính toán của self‑attention tăng theo O(T^2) và còn phụ thuộc vào kích thước chiều của vector D.) [1][2]\n\n### 2.2. Vấn đề biểu diễn vị trí (Positional representation)\n- Vị trí *tuyệt đối* (absolute position) không phải luôn quan trọng; trong nhiều bài toán, **vị trí tương đối** giữa token với các token lân cận mới mang thông tin quan trọng (ví dụ: T−1, T−2, T+1, T+2). [2][3]  \n- Công trình của Shaw et al. (2018) chỉ ra vai trò quan trọng của việc sử dụng **relative position** trong attention, giúp cải thiện độ chính xác trên một số task. [3]  \n- Ngoài vị trí tương đối theo thứ tự tuyến tính, ta còn có thể tận dụng **cấu trúc cú pháp (syntactic tree)**: vị trí trong cây cú pháp (chủ từ, động từ, object, tính từ, ...) cung cấp thông tin vị trí theo ngữ nghĩa hơn là chỉ số vị trí tuyệt đối. [4]  \n- Một số phương pháp biểu diễn vị trí/cải tiến được đề cập: *Rotary*, *EFM* (và các phương pháp khác) — nhằm cho phép mô hình học embedding vị trí có ý nghĩa hơn thay vì chỉ là vector số cứng. [4][5]\n\n### 2.3. Các phương án giảm chi phí tính toán của Attention\n- Linformer: ý tưởng là **chiếu** (projection) không gian chiều T xuống một không gian nhỏ hơn kích thước K (K << T hoặc K cố định), từ đó giảm độ phức tạp tính toán. Module chính là phép projection từ không gian T chiều về K chiều. [5][6]  \n  - Khi dùng Linformer, độ phức tạp tính toán được mô tả giảm từ phụ thuộc vào T về một biểu thức liên quan tới K và D (K là con số nhỏ hơn so với T, thậm chí có thể cố định), do đó thời gian inference gần như không tăng khi T thay đổi; ý tưởng cốt lõi là giảm chiều tính toán bằng phép chiếu. [5][6]\n  - (Trong bài giảng nhắc rằng độ phức tạp lúc này “chỉ còn là K D K bình D” — ý nghĩa là chi phí liên quan tới K và D thấp hơn so với T; tham khảo cụ thể trong video). [5][6]\n\n- BigBird: thay vì tính đầy đủ tất cả các cặp, BigBird kết hợp ba kiểu lựa chọn cặp tương tác:  \n  - *Random* (chọn ngẫu nhiên một số cặp),  \n  - *Window* (chỉ tương tác cục bộ giữa các token gần nhau),  \n  - *Global* (một số token có khả năng tương tác với toàn bộ chuỗi).  \n  Sự kết hợp này cho phép giảm chi phí tính toán nhưng vẫn giữ khả năng nắm bắt các cặp quan trọng (cục bộ, toàn cục và một phần ngẫu nhiên). [7][8]  \n- Tổng hợp: Linformer tập trung vào giảm chiều không gian bằng projection; BigBird giảm số cặp cần tính bằng chiến lược chọn con; cả hai đều nhằm giảm chi phí so với Transformer gốc. [5][6][7][8]\n\n### 2.4. Hiệu quả thực tế của các biến thể Transformer\n- Có rất nhiều biến thể khác của Transformer đã được thử nghiệm (được nhắc tên như các biến thể trong hành trình phát triển Transformer), nhưng **đa phần không cải thiện đáng kể độ chính xác** trên các benchmark được đề cập. [9]  \n- Trong ví dụ số liệu được đưa ra, độ chính xác dao động quanh khoảng 26 → 27 (ví dụ: 26.8 → 27), không có nhiều thay đổi lớn giữa các biến thể. Điều này chỉ ra rằng mặc dù có nhiều cải tiến kiến trúc để giảm chi phí, sự thay đổi về độ chính xác có hạn. [9][10]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa chi phí tính toán: đối với một chuỗi dài T, self‑attention phải tính tương tác cho tất cả các cặp token dẫn tới chi phí O(T^2) (kèm phụ thuộc D) — điều này làm cho transformer khó mở rộng cho chuỗi rất dài. [1][2]  \n- Ví dụ về cải tiến vị trí: bài báo của Shaw et al. (2018) là một minh chứng thực nghiệm rằng **relative position** giúp cải thiện hiệu năng trên một số task, tức là áp dụng positional encoding dạng tương đối sẽ có lợi hơn positional tuyệt đối trong nhiều trường hợp. [3]  \n- Ví dụ kiến trúc thay thế:  \n  - Linformer áp dụng phép projection từ không gian T xuống K để giảm chi phí và giữ thời gian inference gần như không đổi khi T thay đổi. [5][6]  \n  - BigBird dùng tổ hợp Random + Window + Global attention để giảm số cặp phải tính mà vẫn giữ khả năng nắm bắt thông tin cục bộ và toàn cục. [7][8]  \n- Ứng dụng thực tế / trường hợp sử dụng: các phương pháp như Linformer và BigBird được thiết kế để làm cho Transformer có thể xử lý chuỗi dài hơn (ví dụ: tài liệu dài, xử lý văn bản dài) với chi phí tính toán và bộ nhớ thấp hơn so với Transformer gốc. [5][6][7][8]\n\n## 4. Kết luận (Conclusion)\n\n- Tổng kết các ý chính:  \n  - Một điểm yếu chính của Transformer là **chi phí tính toán tăng theo bình phương độ dài chuỗi** do self‑attention phải xét tất cả cặp token; chi phí còn phụ thuộc vào chiều D của vector. [1][2]  \n  - **Biểu diễn vị trí** là một vấn đề quan trọng: vị trí tương đối và các thông tin cú pháp (syntactic positions) thường quan trọng hơn vị trí tuyệt đối; đã có nhiều phương pháp (Shaw et al., Rotary, EFM, ...) để cải thiện embedding vị trí. [2][3][4][5]  \n  - Các kiến trúc như **Linformer** và **BigBird** là ví dụ hướng giải quyết cho vấn đề chi phí: Linformer giảm chiều bằng projection (T → K), BigBird giảm số cặp bằng chiến lược Random/Window/Global. [5][6][7][8]  \n  - Tuy nhiên, nhiều **biến thể** khác nhau không đem lại cải thiện đáng kể về độ chính xác trên các benchmark được nêu (độ chính xác dao động quanh ~26–27). Điều này cho thấy mặc dù có nhiều tối ưu về hiệu năng tính toán, cải thiện về mặt độ chính xác không phải lúc nào cũng rõ rệt. [9][10]\n\n- Tầm quan trọng: Hiểu rõ các hạn chế (chi phí O(T^2), cách biểu diễn vị trí) và các giải pháp thay thế là cần thiết để thiết kế mô hình phù hợp với ứng dụng thực tế (nhất là khi xử lý chuỗi dài hoặc khi yêu cầu tiết kiệm tài nguyên). [1][2][5][7]\n\n- Liên hệ với các bài giảng khác: Bài giảng này là phần phân tích nhược điểm và cải tiến của Transformer — liên quan trực tiếp tới các nội dung về cơ chế self‑attention, positional encoding và các kiến trúc biến thể đã được trình bày trong các chương trước/sau của khóa CS431 (tham khảo các bài giảng về self‑attention và positional embedding). [1][2][3][5]\n\n---\n\nGhi chú: Các trích dẫn [1]…[10] trong bản tóm tắt tương ứng với các đoạn (chunk) và mốc thời gian trong video:\n- [1] [00:01 - 01:01]  \n- [2] [00:48 - 01:49]  \n- [3] [01:40 - 02:41]  \n- [4] [02:27 - 03:30]  \n- [5] [03:20 - 04:21]  \n- [6] [04:07 - 05:12]  \n- [7] [04:57 - 06:01]  \n- [8] [05:44 - 06:50]  \n- [9] [06:36 - 07:42]  \n- [10] [07:25 - 07:56]",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 9_fEGw6eEre2I",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 6： Một số vấn đề của Transformer",
          "video_url": "https://youtu.be/fEGw6eEre2I",
          "start_time": 1,
          "end_time": 61,
          "text": "phần tiếp theo thì chúng ta sẽ cùng lượt qua một số điểm yếu của kiến trúc transformer thì đầu tiên đó là cái chi phí tính toán nó sẽ phải tăng dần theo cái độ dài của chuỗi và cụ thể đó là nó sẽ phụ thuộc với một cái hàm mật 2 theo cái độ dài của chuỗi, tại sao lại như vậy tại vì ở đây chúng ta thấy là cái thao tác self-attention của mình nó sẽ phải đi thực hiện trên tất cả các cái cặp ví dụ cái chuỗi này mà càng dài, chuỗi này là có t t là 1, 2, 3 thì trong cái bước tiếp theo chúng ta sẽ phải đi tính tất cả các cái tính attention trên tất cả các cái từ của mình và chúng ta sẽ phải thực hiện cái việc này trên tất cả các cái cặp như vậy chi phí của mình sẽ là O, T bình phương"
        },
        {
          "index": 2,
          "video_id": "Chương 9_fEGw6eEre2I",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 6： Một số vấn đề của Transformer",
          "video_url": "https://youtu.be/fEGw6eEre2I",
          "start_time": 48,
          "end_time": 109,
          "text": "và chúng ta sẽ phải thực hiện cái việc này trên tất cả các cái cặp như vậy chi phí của mình sẽ là O, T bình phương chưa kể là chúng ta sẽ phải có thêm cái D là cái số chiều của cái vector này của mình thì đây chính là cái điểm yếu đầu tiên là chi phí tính toán nó sẽ thay đổi theo cái độ dài của cái chuỗi của mình do phải tính trên tất cả các cái cặp tương tác của self-attention và vấn đề tiếp theo đó là vấn đề về biểu diễn vị trí thì trong cái positional emitting thì chúng ta đã từng nhận xét đó là cái tính vị trí tuyệt đối nó sẽ không quan trọng vị trí tuyệt đối nó không quan trọng mà đôi khi cái vị trí tương đối vị trí tương đối trong cái attention nó là quan trọng vị trí tương đối là gì là vị trí giữa một cái từ thứ T với lại những cái từ"
        },
        {
          "index": 3,
          "video_id": "Chương 9_fEGw6eEre2I",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 6： Một số vấn đề của Transformer",
          "video_url": "https://youtu.be/fEGw6eEre2I",
          "start_time": 100,
          "end_time": 161,
          "text": "vị trí tương đối trong cái attention nó là quan trọng vị trí tương đối là gì là vị trí giữa một cái từ thứ T với lại những cái từ xung quanh đó đó là từ thứ T trừ 1 T trừ 2 rồi T cộng 1 T cộng 2 thì cái vị trí tương đối của mình trong trường hợp này nó sẽ là trường 1 trường 2 cộng 1 cộng 2 là những cái vị trí tương đối so với những cái từ xung quanh nó mới là những cái thể hiện được cái ý nghĩa trong yếu tố về một vị trí chứ không phải là cái con số tuyệt đối là 1 2 3 cho đến T ở đây và cái bài báo của Shaw và các cộng sự vào năm 2018 thì cho thấy là cái vai trò của vị trí tương đối trong cái extension của mình quan trọng như thế nào và nó đã giúp cho cải tiến cái độ chính xác của hệ thống lên trong một số cái task rất là đáng kể"
        },
        {
          "index": 4,
          "video_id": "Chương 9_fEGw6eEre2I",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 6： Một số vấn đề của Transformer",
          "video_url": "https://youtu.be/fEGw6eEre2I",
          "start_time": 147,
          "end_time": 210,
          "text": "thì cho thấy là cái vai trò của vị trí tương đối trong cái extension của mình quan trọng như thế nào và nó đã giúp cho cải tiến cái độ chính xác của hệ thống lên trong một số cái task rất là đáng kể tiếp theo đó là cái vị trí của mình nó sẽ dựa trên cái cây cú pháp thuộc tức là chúng ta sẽ có các cái chủ từ rồi động từ object vân vân tính từ vân vân thì ở đây nó sẽ là cái cây cú pháp và tùy theo cái vị trí trong cái cây cú pháp này của mình mà mình sẽ có cái ở đây sẽ có một cái ví dụ thôi nha chứ không chắc là cái cây này đúng nha thì tùy vào cái cấu thức của cái cây này nè thì mình sẽ có được cái thông tin về mặt vị trí khác nhau chứ chúng ta không phải dựa trên cái chỉ số chúng ta sẽ dựa trên cái vai trò về mặt cú pháp về mặt của pháp trong câu rồi chúng ta sẽ có những cái phương pháp cải tiến khác như là Rotary, EFM, etc. thì đây là những cái phương pháp biểu diễn vị trí"
        },
        {
          "index": 5,
          "video_id": "Chương 9_fEGw6eEre2I",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 6： Một số vấn đề của Transformer",
          "video_url": "https://youtu.be/fEGw6eEre2I",
          "start_time": 200,
          "end_time": 261,
          "text": "cú pháp về mặt của pháp trong câu rồi chúng ta sẽ có những cái phương pháp cải tiến khác như là Rotary, EFM, etc. thì đây là những cái phương pháp biểu diễn vị trí và cho phép chúng ta có thể đạt được cái độ chính xác cao và cho phép mô hình của mình có thể học và tạo ra được những cái position embedding đúng nghĩa chứ không phải là những cái hàng số những cái vector ở dạng hàng số không có sự thay đổi thì đây chính là những cái vấn đề lớn của transformer và khi nói về cái vấn đề về giảm chỉnh của cái mặt này thì khi chúng ta có cái chi phí tính toán trên CRTension thì Linformer đây là một cái kiến trúc do Quang và khách công sự năm 2020 thay vì chúng ta tính toán trên cái không gian T chiều thì chúng ta sẽ giảm số chiều này xuống còn K chiều và khi đó thì độ phúc tạo của mình lúc này nó chỉ còn là K D K bình D và K này là con số nhỏ hơn"
        },
        {
          "index": 6,
          "video_id": "Chương 9_fEGw6eEre2I",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 6： Một số vấn đề của Transformer",
          "video_url": "https://youtu.be/fEGw6eEre2I",
          "start_time": 247,
          "end_time": 312,
          "text": "thì chúng ta sẽ giảm số chiều này xuống còn K chiều và khi đó thì độ phúc tạo của mình lúc này nó chỉ còn là K D K bình D và K này là con số nhỏ hơn so với T rất là nhiều và có thể là con số cố định luôn tức là khi T thay đổi thì K này vẫn có thể là cố định khi đó thì chúng ta thấy với cái sô đồ này ở đây chúng ta sẽ có cái cặp số là độ dài của chuỗi và cái max size thì ở đây chúng ta thấy là với cái transformer phiên bản gốc đây là transformer gốc này thì cái độ phức tạp khi chúng ta inference cái thời gian chúng ta inference của mình tăng lên nhưng với Linformer khi K cố định chúng ta thấy là gần như là gần như không thay động nó đi ngang thì cái thời gian inference của mình là gần như không đổi và cái module chính của nó đó chính là cái module projection ở đây đó là biến từ chiếu từ cái không gian T chiều về cái không gian nhỏ hơn"
        },
        {
          "index": 7,
          "video_id": "Chương 9_fEGw6eEre2I",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 6： Một số vấn đề của Transformer",
          "video_url": "https://youtu.be/fEGw6eEre2I",
          "start_time": 297,
          "end_time": 361,
          "text": "thì cái thời gian inference của mình là gần như không đổi và cái module chính của nó đó chính là cái module projection ở đây đó là biến từ chiếu từ cái không gian T chiều về cái không gian nhỏ hơn đó là idea của ý tưởng của Linformer rồi với BitBird thì thay vì chúng ta sẽ phải tính tất cả cái cái cặp nếu như chúng ta vẽ trong cái ma trận ha tức là chúng ta sẽ phải tính trên tất cả những loại vỏ bóng giống mà chúng ta có thể tính ra Should be, if we draw in all the centers, if it's a full circle, it's all in one place, you have to calculate the space, the space, etc. là những cái cặp tương tác chúng ta sẽ phải tính full trên toàn bộ cặp tương tác thế thì chúng ta sẽ sử dụng một cái tổ hợp các cái cặp tương tác ví dụ như random tức là chúng ta sẽ random các cái vị trí các cái cặp của mình chúng ta kết hợp với lại Windows Windows tức là những cái cặp nào mà gần nhau thôi ví dụ như tại vị trí này chúng ta sẽ lấy những cái từ trước đó và từ sau đó đó là những cái cặp mà cục bộ ở gần nhau là Windows và Global tức là chúng ta sẽ có những cái cặp tương"
        },
        {
          "index": 8,
          "video_id": "Chương 9_fEGw6eEre2I",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 6： Một số vấn đề của Transformer",
          "video_url": "https://youtu.be/fEGw6eEre2I",
          "start_time": 344,
          "end_time": 410,
          "text": "mà gần nhau thôi ví dụ như tại vị trí này chúng ta sẽ lấy những cái từ trước đó và từ sau đó đó là những cái cặp mà cục bộ ở gần nhau là Windows và Global tức là chúng ta sẽ có những cái cặp tương tác mà lấy được tất cả những cái từ đầu cho đến cuối từ đầu cho đến cuối thì nó gọi là Global Attention thì như vậy lấy từ tuy nhiên lúc chúng ta sẽ không lấy dây đặc hết tại vì nếu mà lấy dây đặc hết thì nó không khác gì cái Transformer bình thường chúng ta sẽ lấy từ đầu đến cuối nhưng mà ở những cái phần tử đầu tiên ở hai hàng đầu tiên và hai cái cột cuối cùng thôi và BigBird chính là cái sự kết hợp của ba cái loại Attention này như cái hình bên đây thì như vậy thì với cái BigBird thì nó sẽ giúp cho chúng ta đó là tăng cái tốc độ tính toán sẽ Attention nhưng nó sẽ không có tức là nó sẽ vẫn lấy ra được những cái cặp quan trọng và nhưng mà nó sẽ không có lấy hết nó sẽ không lấy hết tất cả các cặp tương tác"
        },
        {
          "index": 9,
          "video_id": "Chương 9_fEGw6eEre2I",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 6： Một số vấn đề của Transformer",
          "video_url": "https://youtu.be/fEGw6eEre2I",
          "start_time": 396,
          "end_time": 462,
          "text": "giúp cho chúng ta đó là tăng cái tốc độ tính toán sẽ Attention nhưng nó sẽ không có tức là nó sẽ vẫn lấy ra được những cái cặp quan trọng và nhưng mà nó sẽ không có lấy hết nó sẽ không lấy hết tất cả các cặp tương tác nó vừa có đủ yếu tố về random vừa có yếu tố về mặt vị trí cục bộ ở trong cái lưng cộng xung quanh mà vừa có cái yếu tố lấy được toàn cục lấy hết ở đây thì đó chính là phương pháp BigBird cuối cùng một cái nhận xét cuối cùng đó chính là có rất nhiều những cái biến thể có rất nhiều những cái biến thể khác nhau của Transformer đã được thử nghiệm và gần như tất cả những cái biến thể ở trong hành trình Transformer, Katarski, Christmas,  các biến thể đó đều không cải tiến nhiều về độ chính xác như chúng ta nhìn thấy đây là cái độ chính xác của mình cứ dao động quanh con số là 26 cuối 26.8 mấy hay sao cho đến 27 tức là nó sẽ không có cái sự dao động nhiều 26 27 26 27 các biến thể này không có làm thay đổi cái độ chính xác của mùi"
        },
        {
          "index": 10,
          "video_id": "Chương 9_fEGw6eEre2I",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 6： Một số vấn đề của Transformer",
          "video_url": "https://youtu.be/fEGw6eEre2I",
          "start_time": 445,
          "end_time": 476,
          "text": "chính xác của mình cứ dao động quanh con số là 26 cuối 26.8 mấy hay sao cho đến 27 tức là nó sẽ không có cái sự dao động nhiều 26 27 26 27 các biến thể này không có làm thay đổi cái độ chính xác của mùi một cách đáng kể thì điều này cho thấy đó là gì đó là trang sông mới ổn định cho thấy trang sông transformer bỏng quát đó chính là những cái nhận xét về tố quyết điểm cũng như là một số cái cải tiến của transformer"
        }
      ]
    },
    {
      "video_id": "Chương 9_iMfkIHkU6NM",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giới thiệu các **ứng dụng** và **thành tựu** của kiến trúc Transformer, đi sâu vào các *foundation models* (BERT, GPT), cách sử dụng chúng cho các downstream tasks (fine-tuning, prompting, instruction tuning), các phương pháp tiết kiệm chi phí tinh chỉnh (Adapter, Prefix Tuning, LoRA) và mở rộng Transformer sang các loại dữ liệu khác (âm thanh, ảnh, multimodal). [1][21]\n\n- Các khái niệm sẽ được đề cập: *Foundation models* (BERT, GPT), *self-supervised learning*, *contextual embeddings*, downstream tasks (classification, QA, translation, generation, NER...), hai cách chính để dùng mô hình nền tảng là **Fine-tuning** và **Prompting** (bao gồm zero/one/few-shot), các kỹ thuật adapter như *Adapter*, *Prefix Tuning*, *LoRA*, và ứng dụng Transformer cho âm thanh (ví dụ Whisper), ảnh (Vision Transformer) và multimodal (ví dụ Stable Diffusion). [1][2][7][12][19][20][21]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1 Foundation models: BERT và GPT\n- BERT và GPT là hai *language model* lớn được huấn luyện theo kiểu *self-supervised* trên dữ liệu không gán nhãn. [1][2]  \n- BERT (Bidirectional Encoder Representations from Transformers) sử dụng phần *Encoder* của Transformer và học bằng cách che (mask) một từ ở giữa và dự đoán từ đó (masked language modeling). [1][5][6]  \n- GPT (Generative Pre-trained Transformer) sử dụng phần *Decoder* (autoregressive) và học bằng cách dự đoán từ tiếp theo (next-word prediction). [1][5][6]  \n- Cả hai model tạo ra *contextual embeddings* (biểu diễn từ theo ngữ cảnh): giá trị biểu diễn của một từ phụ thuộc vào các từ xung quanh (ví dụ: \"Apple\" có thể là trái táo hoặc công ty tùy ngữ cảnh). [2][3]\n\n### 2.2 Downstream tasks (sử dụng model đã huấn luyện)\n- Mặc dù BERT/GPT được huấn luyện cho nhiệm vụ ngôn ngữ (masking/next-word), ta có thể *tận dụng* chúng cho nhiều downstream tasks: phân loại văn bản (sentiment analysis), question answering (QA), named entity recognition (NER), dịch máy (translation), tạo sinh nội dung (generation) v.v. [4][6]  \n- Sự phù hợp về loại task: BERT thích hợp cho các task như phân loại, QA, NER; GPT mạnh ở dịch máy và tạo sinh nội dung tự động nhờ cơ chế autoregressive. [6]\n\n### 2.3 Hai hướng chính để sử dụng model nền tảng: Fine-tuning vs Prompting\n- Fine-tuning (tinh chỉnh tham số):\n  - Fine-tuning là cập nhật tham số mô hình (sử dụng gradient descent) để tối ưu cho một task cụ thể. [7][8]  \n  - Có thể fine-tune toàn bộ mô hình (full fine-tune) hoặc thêm/điều chỉnh *readout head* (Bridge/Readouthead) và chỉ fine-tune phần đầu ra. [8][10]  \n  - Nhược điểm: chi phí tính toán và lưu trữ lớn khi phải cập nhật toàn bộ tham số. [12]  \n- Prompting (không thay đổi tham số):\n  - Prompting là thiết kế các *prompt* (chỉ dẫn/ngữ cảnh) để hướng mô hình sinh đầu ra mong muốn mà không cập nhật tham số mô hình. [7][15]  \n  - Các dạng: *zero-shot* (không cung cấp ví dụ), *one-shot* (1 ví dụ), *few-shot* (vài ví dụ). [16][17]  \n  - Prompting cho phép mô hình “học” cách giải task dựa trên các cặp input-output đưa sẵn trong ngữ cảnh. [15][16]\n\n### 2.4 Các chiến lược trung gian và kỹ thuật tiết kiệm khi tinh chỉnh\n- Readout / Bridge Outhead: thêm module đầu ra (ví dụ Linear + Softmax/Sigmoid) để ánh xạ embedding cuối cùng sang lớp nhãn phù hợp; dùng cho phân loại (softmax hoặc sigmoid tuỳ số lớp) và cho QA cần dự đoán span (start/end). [10][11]  \n- Adapter: thêm một mô-đun nhỏ vào mô hình và chỉ huấn luyện phần thêm vào, giữ nguyên phần còn lại, giúp tiết kiệm chi phí tính toán và lưu trữ so với full fine-tune. [9][12]  \n- Prefix Tuning: cố định các ma trận QKV gốc, và thêm một *prefix* (ma trận bổ sung) vào đầu (ví dụ gắn thêm ma trận vào K hoặc V) rồi huấn luyện chỉ phần prefix này. [12]  \n- LoRA (Low-Rank Adaptation): thay vì cập nhật toàn bộ ma trận lớn (ví dụ Q hoặc V), tách cập nhật thành hai ma trận thấp chiều A và B (A giảm chiều, B khôi phục chiều) và huấn luyện A, B; sau đó cộng kết quả A·B vào ma trận gốc để đạt hiệu quả fine-tuning với chi phí thấp. [13][14]  \n- Tóm lại: các phương pháp adapter (Adapter, Prefix Tuning, LoRA) là những cách tinh chỉnh hiệu quả về tham số, chỉ cập nhật phần nhỏ hoặc low-rank, thay vì toàn bộ mô hình. [9][12][13][14]\n\n### 2.5 Prompting chi tiết và Instruction Tuning\n- Prompting bằng ví dụ (khi cho một số cặp input-output trong ngữ cảnh) cho phép mô hình suy luận output cho sample mới dựa trên mối quan hệ học được trong ngữ cảnh. Ví dụ sentiment: đưa các câu mẫu với nhãn positive/negative/neutral, mô hình sẽ điền nhãn cho câu mới. [15][16]  \n- Các chế độ prompting: zero-shot (không cho mẫu), one-shot (cho 1 mẫu), few-shot (cho vài mẫu). [16][17]  \n- Instruction Tuning: kết hợp fine-tuning và prompting — tinh chỉnh tham số mô hình bằng các nhiệm vụ có chỉ dẫn (instruction) để mô hình có khả năng tổng quát hóa sang các task mới chưa từng thấy. Quy trình: pre-trained model → instruction tuning trên nhiều task → suy luận cho task mới theo template (premise, hypothesis, options). [17][18][19]\n\n### 2.6 Mở rộng Transformer sang các modal khác\n- Transformer không chỉ áp dụng cho văn bản mà còn cho các chuỗi khác:\n  - Âm thanh: ví dụ mô hình *Whisper* của OpenAI (speech-to-text) dùng Transformer cho bài toán nhận dạng giọng nói. [19]  \n  - Ảnh: Vision Transformer (ViT) áp dụng ý tưởng chuỗi lên các patch/pixel của ảnh (chuỗi 2 chiều). [20]  \n  - Multimodal: mô hình kết hợp text và image, ví dụ Stable Diffusion dùng text làm *conditioning* để can thiệp vào không gian latent và sinh/chỉnh sửa ảnh theo nội dung text. [20][21]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa prompting cho Sentiment Analysis:\n  - Cho các cặp input-output: câu \"Circulation revenue has increased by 5% in Finland\" → output \"positive\"; \"paying off the national debt ...\" → \"negative\"; \"…\" → \"neutral\". Với vài ví dụ như vậy trong prompt, mô hình sẽ điền nhãn cho câu mới. [15][16]\n\n- Ví dụ design đầu ra khi fine-tuning:\n  - Phân loại văn bản: thêm một Linear layer + Softmax cho multi-class hoặc Sigmoid cho binary (positive/negative). [10][11]\n  - Question Answering (QA): thiết kế đầu ra theo dạng regression để tìm vị trí *start* và *end* (span) trong đoạn văn. [11]\n\n- Ví dụ adapter / prefix / LoRA:\n  - Prefix Tuning: cố định QKV gốc, thêm ma trận prefix vào K hoặc V ở đầu, chỉ huấn luyện phần prefix. [12]\n  - LoRA: huấn luyện hai ma trận nhỏ A và B để biểu diễn cập nhật low-rank cho Q/V, sau đó cộng A·B vào ma trận gốc để giải quyết task mới với chi phí thấp. [13][14]\n\n- Ứng dụng thực tế:\n  - Speech-to-text: Whisper (OpenAI) — chuyển giọng nói thành văn bản. [19]\n  - Vision tasks: Vision Transformer cho dữ liệu ảnh (xử lý các patch/pixel như chuỗi). [20]\n  - Multimodal image generation/editing: Stable Diffusion sử dụng text conditioning để điều khiển không gian latent và sinh ảnh phù hợp với nội dung text. [21]\n  - Downstream NLP: sentiment analysis, QA, NER, translation, text generation — đều có thể khai thác BERT/GPT qua fine-tuning hoặc prompting. [4][6][15][16]\n\n- Các trường hợp sử dụng (use-cases):\n  - Khi có đủ tài nguyên: full fine-tune để đạt hiệu năng tối ưu cho task cụ thể. [8][12]  \n  - Khi tài nguyên hạn chế hoặc cần lưu trữ nhiều task: dùng Adapter/Prefix/LoRA để tinh chỉnh nhẹ, tiết kiệm bộ nhớ và chi phí. [9][12][13][14]  \n  - Khi muốn triển khai nhanh cho nhiều task không cần huấn luyện: dùng prompting (zero/one/few-shot) hoặc instruction-tuned models để sinh phản hồi theo chỉ dẫn. [15][16][17][18]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Transformer là nền tảng cho nhiều *foundation models* như BERT (encoder, masked LM) và GPT (decoder, autoregressive), cả hai đều huấn luyện theo *self-supervised* và tạo *contextual embeddings*. [1][2][3][5]  \n  - Để dùng mô hình nền tảng cho downstream tasks có hai hướng chính: **Fine-tuning** (cập nhật tham số) và **Prompting** (thiết kế ngữ cảnh/only prompts). Có thêm phương pháp trung gian/instruction tuning để kết hợp lợi ích hai bên. [7][8][15][17][18]  \n  - Adapter techniques (Adapter, Prefix Tuning, LoRA) cho phép tinh chỉnh hiệu quả với chi phí tham số thấp. [9][12][13][14]  \n  - Transformer không chỉ cho dữ liệu văn bản mà còn mở rộng tốt cho âm thanh (Whisper), ảnh (Vision Transformer) và multimodal (Stable Diffusion). [19][20][21]\n\n- Tầm quan trọng:\n  - Kiến trúc Transformer và các chiến lược tinh chỉnh/hướng dẫn (fine-tune, prompt, adapter) tạo ra một hệ sinh thái linh hoạt — cho phép tận dụng mô hình lớn cho vô số ứng dụng thực tế trên nhiều modal dữ liệu với các yêu cầu tài nguyên khác nhau. [1][7][12][19][21]\n\n- Liên hệ với các bài giảng khác:\n  - Bài này liên kết và tiếp nối các nội dung đã trình bày về *motivation* và kiến trúc cơ bản của Transformer, các hạn chế & giải pháp ban đầu, rồi cuối cùng là các ứng dụng và cách áp dụng (fine-tuning, prompting, adapter). Nếu muốn triển khai chi tiết về kiến trúc/thuật toán nội bộ cần xem lại phần kiến trúc kinh điển và các khuyết điểm/giải pháp đã học trước đó. [21]\n\n(Nguồn: tóm tắt toàn bộ các đoạn video cung cấp.) [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21]",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 0,
          "end_time": 58,
          "text": "Trong phần tiếp theo thì chúng ta sẽ cùng tìm hiểu về hướng dụng của Transformer cũng như là các cái thành tựu của nó Đầu tiên đó là các cái mô hình nền tảng BERT và GPT Thì đây là hai cái mô hình ngôn ngữ được huấn luyện trên tập dữ liệu lớn Ở hai bước của Transformer, BERT là cho giai đoạn Encoder và GPT là cho giai đoạn Encoder BERT là viết tắt của bi-directional encoder representation from Transformer Chúng ta thấy đều có cái từ là Transformer GPT là Generative, Generative này chính là Decode, Retrain, Transformer Thì chúng ta cũng thấy là cả hai mô hình này đều based, đều dựa trên cái kiến trúc của Transformer Và điểm chung đó là đều thuộc cái nhóm là tự học, Self-superviling Tức là học Khi không có cái dữ liệu gán nhãn"
        },
        {
          "index": 2,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 48,
          "end_time": 111,
          "text": "Và điểm chung đó là đều thuộc cái nhóm là tự học, Self-superviling Tức là học Khi không có cái dữ liệu gán nhãn Học khi không có dữ liệu gán nhãn Đều sử dụng dữ liệu không gán nhãn Rồi đều dùng để biểu diễn Thì ở đây có thể biểu diễn gì? Chúng ta có thể sử dụng để biểu diễn từ Biểu diễn từ tức là khi chúng ta đưa vào một cái câu Nó sẽ dựa trên ngữ cảnh của những cái từ xung quanh Để đưa ra cái biểu diễn của cái từ đó Nó gọi là Contextual Embedding Đó Tại sao nó lại có cái khái niệm này? Và tại sao phải có yếu tố bề mặt ngữ cảnh? Đó là vì Ví dụ cái từ Apple Nếu như chúng ta không có cái ngữ cảnh của những cái từ xung quanh Chúng ta sẽ không biết Apple ở đây là trái táo Hay Apple ở đây là tên của một công ty"
        },
        {
          "index": 3,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 97,
          "end_time": 160,
          "text": "Và tại sao phải có yếu tố bề mặt ngữ cảnh? Đó là vì Ví dụ cái từ Apple Nếu như chúng ta không có cái ngữ cảnh của những cái từ xung quanh Chúng ta sẽ không biết Apple ở đây là trái táo Hay Apple ở đây là tên của một công ty Chúng ta phải có những cái từ xung quanh chúng ta mới biết được Thế thì đó chính là công dụng của cái Cái từ xung quanh của một công ty Còn là Contextual Embedding Tức là biểu diễn từ khi có yếu tố bề mặt ngữ cảnh Rồi Cả hai Bird và GPT đều sử dụng Transformer như đã đề cập Và Nó đều có thể sử dụng để làm cho các cái Downstream Task Downstream Task có nghĩa là gì? Đó là những cái Task mà không phải là Task chính của Bird và GPT Nó là những cái Task phụ không được Tức là trong quá trình mà huấn luyện Bird và GPT Nó không có Được huấn luyện bởi vì nó không có những cái Task phụ không được Hướng luyện để giải quyết các cái nhiệm vụ này Được sử dụng để huấn luyện Bird và GPT được sử dụng để huấn luyện cho cái bài toán khác Đó là bài toán dự đoán từ Bird thì dự đoán từ ở giữa"
        },
        {
          "index": 4,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 149,
          "end_time": 208,
          "text": "Được sử dụng để huấn luyện Bird và GPT được sử dụng để huấn luyện cho cái bài toán khác Đó là bài toán dự đoán từ Bird thì dự đoán từ ở giữa Từ bị che Còn GPT thì để dự đoán từ tiếp theo Nó không có được huấn luyện Để giải quyết các cái Task ví dụ Task phân loại Sentiment Analysis Hoặc là cho cái Task như là QA Question Answering Rồi trả lời cái câu hỏi Hoặc là dịch máy Translation Thì rõ ràng là các cái Model như là Bird và GPT Nó không được huấn luyện để giải quyết các Task này Nhưng khi chúng ta sử dụng cái Model đã được retrain này Thì chúng ta có thể sử dụng được các cái Task này để giải quyết các cái Task này   Và khai thác nó để giải quyết các cái Task này"
        },
        {
          "index": 5,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 199,
          "end_time": 260,
          "text": "Nhưng khi chúng ta sử dụng cái Model đã được retrain này Thì chúng ta có thể sử dụng được các cái Task này để giải quyết các cái Task này   Và khai thác nó để giải quyết các cái Task này Vốn Bird là không sinh ra để giải quyết các cái Task này Nó huấn luyện để giải quyết bài toán đoán từ Nhưng chúng ta có thể sử dụng cái mô hình này Để cho các cái downstream Task khác Thì đó là ý nghĩa của cái ý cuối này Và cái điểm khác biệt Đó là đối với mô hình Bird Thì đó là một cái mô hình ngôn ngữ Language Model Ổn Còn GPT là mô hình ngôn ngữ tự hồi quy Auto Progressive Thì tức là chúng ta sẽ đoán ra cái từ tiếp theo Còn mô hình ngôn ngữ ẩm Tức là chúng ta sẽ che đi một từ ở giữa Một từ bất kỳ, một từ ngẫu nhiên Nó sẽ phải đoán qua cái từ đó bị che là từ gì Thì đó là hai cái mô hình Và cấu tạo Thì Bird là bao gồm Encoder Trong Transformer Và GPT thì là cấu tạo bởi Decoder Trong Transformer Nhiệm vụ Bird là che cái từ"
        },
        {
          "index": 6,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 249,
          "end_time": 308,
          "text": "Thì Bird là bao gồm Encoder Trong Transformer Và GPT thì là cấu tạo bởi Decoder Trong Transformer Nhiệm vụ Bird là che cái từ Đoán cái từ bị che Là Mask Word Còn đây là GPT sẽ đoán cái từ tiếp theo Là Next Word Và các cái Task, Downstream Task Những cái Task mà có thể Sử dụng để mà giải quyết Với cái mô hình Bird Đó là phân loại văn bản, trả lời câu hỏi Tấm tác văn bản Hoặc là nhận diện thực tế Name, Entity, Recognition Còn các cái Downstream Task cho GPT Mà nó khá là phù hợp đó chính là dịch máy Và tạo sinh nội dung tự động Được Rồi và Để sử dụng 2 cái mô hình nền tảng này Để có thể sử dụng được Các cái mô hình nền tảng thì chúng ta sẽ có 2 cách Cách đầu tiên Đó là File Tuning File Tuning hiểu một cách nôn na Đó chính là chúng ta sẽ huấn luyện lại"
        },
        {
          "index": 7,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 299,
          "end_time": 359,
          "text": "Để có thể sử dụng được Các cái mô hình nền tảng thì chúng ta sẽ có 2 cách Cách đầu tiên Đó là File Tuning File Tuning hiểu một cách nôn na Đó chính là chúng ta sẽ huấn luyện lại Hoặc là chúng ta sẽ thay đổi Các cái tham số của mô hình Của mô hình Còn Prompting là chúng ta sẽ Gần như là Tạo ra các cái chỉ thị chỉ dẫn Cho mô hình Chỉ dẫn cho mô hình Có thể thực thi Cho mô hình Đưa ra các cái phán đoán Và quan trọng đó là Không Làm Thay đổi Cái tham số của mô hình Các cái tham số của mô hình Nó không làm thay đổi Cái tham số của mô hình Với cái phương pháp File Tuning thì chúng ta sẽ phải Sử dụng một cái thuật toán Radian Descent"
        },
        {
          "index": 8,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 350,
          "end_time": 410,
          "text": "Các cái tham số của mô hình Nó không làm thay đổi Cái tham số của mô hình Với cái phương pháp File Tuning thì chúng ta sẽ phải Sử dụng một cái thuật toán Radian Descent Để tối ưu lại cái trọng số Cho một cái task nào đó Và chúng ta sẽ có những cái cách Để File Tuning Đó là chúng ta sẽ File Tuning lại toàn bộ mô hình Hoặc chúng ta sẽ Tải các cái đầu ra của mô hình Tức là với cái File Tuning toàn bộ này thì chúng ta sẽ có cái cách Nó gọi là Bridge Outhead Tức là chúng ta sẽ Tạo ra các cái đầu ra Tạo ra các cái module tải cái đầu ra Sau đó chúng ta sẽ File Tuning trên toàn bộ mô hình Rồi sau đó thì chúng ta Cũng có thể sử dụng một cái kỹ thuật Đó gọi là Ờ Một cái kỹ thuật khác thay cho cái Bridge Out Đó chính là Adapter Rồi thì Với cả hai với với Tất cả các cái phương pháp File Tuning ở đây Thì chúng ta đều phải thay đổi cái tham số của mô hình Dù ít dù nhiều Ở đây là thay đổi toàn bộ"
        },
        {
          "index": 9,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 398,
          "end_time": 461,
          "text": "Đó chính là Adapter Rồi thì Với cả hai với với Tất cả các cái phương pháp File Tuning ở đây Thì chúng ta đều phải thay đổi cái tham số của mô hình Dù ít dù nhiều Ở đây là thay đổi toàn bộ Còn Adapter Thì có thể là chúng ta chỉ thay đổi cho Một phần của mô hình mà thôi Còn Prompting Thì chúng ta sẽ phải thiết kế những cái Prompt đặc biệt Để gợi ý và ràng buộc Mạng để có thể giải quyết Một cái task nào đó Và ở đây là chúng ta sẽ không Có cần phải cập nhật Cái tham số của mô hình Và ở đây chúng ta sẽ là Thay đổi cái cách thức sử dụng mô hình Bên đây là chúng ta sẽ thay đổi tham số mô hình Còn bên đây chúng ta sẽ thay đổi tham số mô hình Và bên đây chúng ta sẽ thay đổi cách sử dụng mô hình Thông qua cái Prompt Và Đối với cái phương pháp Phytuning Phytune với cái đầu ra của mô hình Tức là Readouthead Thì chúng ta sẽ thêm cái đầu ra Và hàm kích hoạt phù hợp Để giải quyết một cái bài toán Ví dụ tại cái đầu ra ở đây"
        },
        {
          "index": 10,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 450,
          "end_time": 510,
          "text": "Phytune với cái đầu ra của mô hình Tức là Readouthead Thì chúng ta sẽ thêm cái đầu ra Và hàm kích hoạt phù hợp Để giải quyết một cái bài toán Ví dụ tại cái đầu ra ở đây Tại cái đầu ra của mô hình Bird Thì chúng ta sẽ Đưa thêm qua một cái Linear Kết hợp với lại Một cái Softmax Để Linear này Cộng cho một cái Softmax Để có thể Tính toán ra được cái Class của cái nhãn đầu vào của mình là gì Ví dụ ở đây là Object chẳng hạn Còn cho cái bài toán Phân loại văn bản đúng không Thì ở đây chúng ta sẽ có Một cái Linear Module và cộng với lại Thay vì ở đây là phân loại đa lớp Thì ở đây chúng ta chỉ cần là Softmax thôi Nếu như ở đây chúng ta Phân lớp ra là Positive và Negative thôi Thì ở đây chúng ta sẽ là hàm Sigmoid thôi Còn nếu như ở đây là phân"
        },
        {
          "index": 11,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 499,
          "end_time": 560,
          "text": "Thì ở đây chúng ta chỉ cần là Softmax thôi Nếu như ở đây chúng ta Phân lớp ra là Positive và Negative thôi Thì ở đây chúng ta sẽ là hàm Sigmoid thôi Còn nếu như ở đây là phân Phân loại văn bản Nhưng mà cho nhiều lớp thì chúng ta có thể là Softmax Thì như vậy là Tùy vào cái đầu ra của cái Task của mình là gì Thì mình sẽ có cái Activation Tương ứng cho nó phù hợp Và cái Module là Linear Cho nó phù hợp Còn cho cái bài toán trả lời câu hỏi Thì ở đây mình sẽ phải tìm ra cái bài toán của mình là gì Phải làm bài toán là Regression Tức là chúng ta sẽ có cái Start và cái End Và cái Span Tức là Start End Tức là cái Đoạn thông tin Ở bên trong cái Đoạn văn đầu vào của mình Span là cái mở rộng ra Để đưa ra cái câu trả lời Tương ứng với lại cái câu hỏi của mình Thì như vậy thì chúng ta sẽ Phải thích ứng"
        },
        {
          "index": 12,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 550,
          "end_time": 609,
          "text": "Span là cái mở rộng ra Để đưa ra cái câu trả lời Tương ứng với lại cái câu hỏi của mình Thì như vậy thì chúng ta sẽ Phải thích ứng Theo từng cái Task của mình Thì từ đó chúng ta thiết kế cái đầu ra Tại cái Bridge Outhead Cho nó phù hợp Và ở đây chúng ta lưu ý là chúng ta sẽ PhyTool toàn bộ PhyTool toàn bộ cái mô hình Như vậy thì cái phương pháp này Có khả năng cái chi phí tính toán của mình Nó sẽ rất là lớn Và PhyTool với Adapter Đó là chúng ta sẽ thêm một cái mô đun nhỏ Vào mô hình ngôn ngữ Và chúng ta chỉ PhyTool cho một mô đun vừa được thêm Như vậy thì cái phương pháp này Nó sẽ rất là tiết kiệm Phí tính toán cũng như là Có thể là dễ dàng lưu được cái mô hình của mình Ví dụ trong cái phương pháp là Prefix Tuning Thì bình thường chúng ta sẽ có các cái ma trận QKV Thì ở đây chúng ta vẫn sẽ cố định QKV Nhưng mà chúng ta sẽ gắn thêm một cái phần nữa Vào đầu"
        },
        {
          "index": 13,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 599,
          "end_time": 662,
          "text": "Thì bình thường chúng ta sẽ có các cái ma trận QKV Thì ở đây chúng ta vẫn sẽ cố định QKV Nhưng mà chúng ta sẽ gắn thêm một cái phần nữa Vào đầu Một cái Prefix mà Chúng ta sẽ gắn thêm một cái ma trận vào đầu Hoặc là con card vào đầu cái ma trận K Con card một cái ma trận Khác vào đầu ma trận V Sau đó chúng ta chỉ đi  Trên cái bộ tham số Của cái phần ma trận màu hồng này và màu tím này thôi Chúng ta không có phy tool cho QKV cũ Chúng ta chỉ phy tool cho những cái phần mới thêm vào này vào Low Rank Adaptation Là một cái phương pháp rất là nổi tiếng Thì tại mỗi cái Ma trận Q và V Ở đây chúng ta sẽ có Tách ra Đây là cái đã được retrain Đây là cái ma trận đã được retrain Chúng ta sẽ kết hợp Với các cái ma trận này Với cái hai cái ma trận là A A và B Là hai cái ma trận Trong đó mục tiêu của A là để giảm chiều Và mục tiêu của B là để khôi phục lại chiều ban đầu"
        },
        {
          "index": 14,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 648,
          "end_time": 711,
          "text": "Đây là cái ma trận đã được retrain Chúng ta sẽ kết hợp Với các cái ma trận này Với cái hai cái ma trận là A A và B Là hai cái ma trận Trong đó mục tiêu của A là để giảm chiều Và mục tiêu của B là để khôi phục lại chiều ban đầu Và đây là cái phương pháp Low Rank Thì sau khi chúng ta huấn luyện A và B này xong Chúng ta sẽ cộng lại để ra được một cái Ma trận tổng Chúng ta sẽ ra được một cái ma trận tổng Của cái retrain Và cái A nhân B Và cái A nhân B này Nó sẽ giúp cho chúng ta Đi giải quyết cái bài toán mới Giải bài toán mới Tương tự như vậy Hai cái ma trận Màu tím, màu hồng ở đây Thì cũng là giúp cho chúng ta Đi giải quyết bài toán mới Thì đây là hai cái phương pháp Điển hình của FileTool với Adapter Và đối với cái phương pháp về Prompting Thì chúng ta sẽ cho Cái mô hình nó học từ ngữ cảnh Nghĩa là sao Ở đây chúng ta sẽ cho một số cái ví dụ về cái task"
        },
        {
          "index": 15,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 697,
          "end_time": 761,
          "text": "Điển hình của FileTool với Adapter Và đối với cái phương pháp về Prompting Thì chúng ta sẽ cho Cái mô hình nó học từ ngữ cảnh Nghĩa là sao Ở đây chúng ta sẽ cho một số cái ví dụ về cái task Cái mô hình nó học từ ngữ cảnh Và mô hình nó sẽ đi tự tìm ra cái cách Để giải quyết Lấy ví dụ, ở đây chúng ta sẽ cho trước Một số cái cặp input và output Ví dụ như ở đây chúng ta sẽ có cái input là một cái câu Circulation revenue has increased by 5% in Finland Circulation revenue has increased by 5% in Finland Circulation revenue has increased by 5% in Finland Thì ở đây Chúng ta sẽ có cái output của mình là positive Chúng ta sẽ có cái output của mình là positive Đó Rồi paying off the national debt National debt will be extremely low Depend full thì ở đây chúng ta sẽ có Output của mình là negative Tương tự ở đây chúng ta sẽ có neutral Và ở đây chúng ta sẽ để thêm The company anticipated Is operating profit Is operating profit to improve Thì chúng ta sẽ để dấu xuyệt xuyệt Để chống Thì mô hình nó sẽ tự biết là"
        },
        {
          "index": 16,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 750,
          "end_time": 808,
          "text": "The company anticipated Is operating profit Is operating profit to improve Thì chúng ta sẽ để dấu xuyệt xuyệt Để chống Thì mô hình nó sẽ tự biết là Ở trên đây là positive, ở trên đây là neutral Ở đây là negative Thì tự nó sẽ điền vô chỗ chống này là Positive hay negative hay neutral Nó sẽ tự biết mối quan hệ giữa Cái cặp input output này Mô hình nó sẽ tự học theo cái ngữ cảnh Đây là 3 cái context để giúp cho mình Đưa ra cái phán đoán tại cái vị trí Output cho cái sample mới này Tương tự như vậy cho cái bài toán Là phân loại văn mãn Chúng ta sẽ cho ở trước Đây là chủ đề về finance, chủ đề về sports Chủ đề về tech Thì bên đây nó sẽ từ cái Cái input này nó sẽ đưa ra Cái phán đoán output của mình Thì đây là phương pháp prompting Và chúng ta sẽ có một số Thuật ngữ trong cái phương pháp này Đó là zero Zero sort prompting"
        },
        {
          "index": 17,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 799,
          "end_time": 860,
          "text": "Và chúng ta sẽ có một số Thuật ngữ trong cái phương pháp này Đó là zero Zero sort prompting Tức là chúng ta sẽ không cần cho Mẫu này luôn Chúng ta sẽ không cần cho mẫu Chúng ta sẽ hỏi nó là The company anticipated is operating profit to improve Thì nó là Positive, negative hay neutral Cái sentiment của nó là Positive, negative hay neutral Chúng ta sẽ hỏi nó luôn One sort Tức là chúng ta sẽ cho nó một mẫu Và few sort Là chúng ta sẽ cho nó nhiều mẫu Few sort prompting Và chúng ta sẽ có cái phương pháp Kết hợp giữa file tool Với lại prompting Nó gọi là instruction tuning Chúng ta sẽ vừa Có cái sự tinh chỉnh mô hình Nhưng ở đây chúng ta sẽ tinh chỉnh mô hình Để trả lời cho các cái câu hỏi Và mô hình nó sẽ tự có thể tổng quát hóa"
        },
        {
          "index": 18,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 850,
          "end_time": 910,
          "text": "Chúng ta sẽ vừa Có cái sự tinh chỉnh mô hình Nhưng ở đây chúng ta sẽ tinh chỉnh mô hình Để trả lời cho các cái câu hỏi Và mô hình nó sẽ tự có thể tổng quát hóa Để có thể giải cho các cái task khác Giải cho các cái task khác Chưa từng thấy Thì chúng ta sẽ có cái pre-trained language model Chúng ta sẽ instruction tool Trên các cái tham số Trên các cái task bcd Đây là cái task Củ Và chúng ta sẽ thay đổi Chúng ta sẽ tinh chỉnh Cái tham số Của mô hình Và sang cái giai đoạn suy luận Thì chúng ta có thể thực hiện trên một cái task mới hoàn toàn Đó là task A Và cái template Cái template cho mình Đó là sẽ bao gồm là premise Tức là cái nội dung Ngữ cảnh mình đưa vào Hypothesis Và các cái option output của mình là gì Thì đây chúng ta sẽ có"
        },
        {
          "index": 19,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 900,
          "end_time": 960,
          "text": "Tức là cái nội dung Ngữ cảnh mình đưa vào Hypothesis Và các cái option output của mình là gì Thì đây chúng ta sẽ có 4 cái template Ví dụ Cho các cái task để mà chúng ta file tool mô hình Chúng ta đưa vô Chúng ta file tool xong thì cái Mô hình đã được chỉnh sửa Đã được tinh chỉnh tham số Thì nó sẽ có thể khả năng Là giải quyết được cho cái task mới Task A Và transformer Nó không chỉ làm cho dữ liệu văn bản Không chỉ làm cho dữ liệu văn bản Mà transformer còn có thể Mở rộng cho các dữ liệu dạng chuỗi khác Chúng ta có thể kể đến Ví dụ như là transformer thực hiện được trên dữ liệu âm thanh Và điển hình cho cái dữ liệu âm thanh Đó là chúng ta có cái mô hình Whisper của OVN Thì đây là một trong những cái mô hình Style of the art Cho cái bài toán là Speak to text Bài toán nhận dịp giọng nói, tư giọng nói Biến thành văn bản"
        },
        {
          "index": 20,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 948,
          "end_time": 1009,
          "text": "Whisper của OVN Thì đây là một trong những cái mô hình Style of the art Cho cái bài toán là Speak to text Bài toán nhận dịp giọng nói, tư giọng nói Biến thành văn bản Rồi, ở đây thì mình ghi nhầm Đó là chúng ta sẽ có Mô hình vision Transformer Mô hình vision transformer Đây Và dữ liệu chuỗi đây Chúng ta cũng có thể hiểu đó là dữ liệu ảnh Các cái pixel Hoặc là chuỗi các cái path Path này đến trước, path này đến sau Và ở đây chúng ta sẽ lưu ý yếu tố đó là 2 chiều Chuỗi này của chúng ta là đi theo 2 chiều Rồi Và cuối cùng Đó chính là Chúng ta có một cái ví dụ đó là Trên multimodal Tức là vừa có sự kết hợp của cả ảnh và text Thì trong cái mô hình Cái mô hình là stable diffusion"
        },
        {
          "index": 21,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 1003,
          "end_time": 1061,
          "text": "Tức là vừa có sự kết hợp của cả ảnh và text Thì trong cái mô hình Cái mô hình là stable diffusion Chúng ta thấy là có cái sự tham gia Của text Là đóng vai trò là conditioning để can thiệp vào Cái không gian latent Để cho chúng ta có thể chỉnh sửa cái nội dung của tấm ảnh Theo cái mong muốn của cái text Của cái nội dung text này Thì đó chính là Một số cái thành tựu Của transformer Không chỉ trên lĩnh vực về Văn bản mà nó còn Có thể làm trên được các loại dữ liệu như là âm thanh Hình ảnh hoặc là multimodal Ví dụ như là hình ảnh Kết hợp với lại văn bản Thì như vậy là trong bài ngày hôm nay Chúng ta đã tìm hiểu qua Về cái Cái motivation Của cái kiến trúc transformer Rồi chúng ta đồng thời Cũng đã tìm hiểu về cái kiến trúc"
        },
        {
          "index": 22,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 1049,
          "end_time": 1109,
          "text": "Của cái kiến trúc transformer Rồi chúng ta đồng thời Cũng đã tìm hiểu về cái kiến trúc Kinh điển Của transformer Chúng ta đã tìm hiểu qua về Các cái khuyết điểm Một số cái vấn đề Một số cái vấn đề Còn tồn tại Và một số cái giải pháp Ban đầu của transformer Và cuối cùng đó là Những cái ứng dụng Rồi Ứng dụng của transformer Thông qua việc đó là chúng ta có khả năng PhyTool PhyTool để giải quyết các cái task Giải quyết cái downstream task Chúng ta có thể Prompting Để chỉ dẫn cho mô hình Hiểu cái context hiểu cái ngũi cảnh Và chúng ta có cái kiểu là Zero"
        },
        {
          "index": 23,
          "video_id": "Chương 9_iMfkIHkU6NM",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer",
          "video_url": "https://youtu.be/iMfkIHkU6NM",
          "start_time": 1102,
          "end_time": 1136,
          "text": "Để chỉ dẫn cho mô hình Hiểu cái context hiểu cái ngũi cảnh Và chúng ta có cái kiểu là Zero One shot Few shot Rồi đồng thời là Chúng ta có thể áp dụng Trong cái lĩnh vực không chỉ là văn bản Mà nó có thể áp dụng trong cái Loại dữ liệu là âm thanh Rồi hình ảnh Và kết hợp cả Ảnh Cộng với lại văn bản Thì đó chính là Nội dung của bài học ngày hôm nay Hãy subscribe cho kênh La La School Để không bỏ lỡ những video hấp dẫn"
        }
      ]
    },
    {
      "video_id": "Chương 9_7AZr_li6ZtA",
      "summary": "## 1. Giới thiệu (Introduction)\n\n- Mục tiêu chính của bài giảng: Giải thích chi tiết cách biểu diễn vị trí (positional encoding) trong Transformer đời đầu và trình bày thành phần *multi‑head self‑attention* trong bộ Encoder của Transformer, cùng các tính chất, ưu/nhược điểm và cách tổng hợp thông tin từ nhiều head. [1][3][6]  \n- Các khái niệm sẽ được đề cập: *sinusoidal positional encoding*, tính chất không lặp lại và cố định của positional vector; khái niệm *self‑attention* mở rộng thành *multi‑head attention* (Q, K, V, linear projections, scaled dot‑product, concat + linear); kích thước ma trận theo số head; mối quan hệ giữa encoder và decoder (decoder truy vấn đặc trưng từ encoder). [1][2][6][10][12][13]\n\n## 2. Các điểm chính (Main Points)\n\n### 2.1. Sinusoidal positional encoding (mã hóa vị trí dạng sóng)\n- Transformer đời đầu dùng hàm sinh (sinusoidal) để sinh positional encoding cho mỗi vị trí Y, theo cặp để tạo vector có kích thước D chiều; tổng cộng có D/2 cặp sin/cos nên vector cuối là D chiều. [1][2]  \n- Công thức (dạng mô tả trong bài giảng): vị trí Y được chia theo các tần số dựa trên 10.000 và phân bố theo từng cặp chỉ số (ví dụ cặp (1,1), (2,2), ..., (D/2, D/2)), dẫn tới vector PI có D chiều. (Mô tả công thức trong video dùng tỉ lệ với 10.000 theo chỉ số chiều). [1][2]  \n- Hai thuộc tính chính của sinusoidal encoding:\n  - Tránh trùng lặp: khi Y chạy từ 0 tới 10.000, các giá trị thay đổi theo bước nhỏ (ví dụ tăng 1/10.000 …) nên khả năng trùng lặp thấp trên khoảng này. Điều này giúp biểu diễn được chuỗi dài mà không lặp lại nhanh. [2][5]  \n  - Phân bố đều/“chuẩn” của các phần tử trong vector PI (hàm tuần hoàn) — tức là vị trí tuyệt đối không được yêu cầu phải tăng đơn điệu trong vector (hàm tuần hoàn lên/xuống), nhưng hệ thống vẫn hoạt động tốt với dạng này. *Ý niệm là thông tin vị trí tuyệt đối (monotonic increase) không cần thiết nếu dùng hàm tuần hoàn*. [3][4]\n- Nhược điểm: positional encoding dạng sinusoidal là một vector cố định do thiết kế (không học từ dữ liệu). Đây là điểm yếu của cách biểu diễn vị trí này vì các vector PI không được tối ưu hóa trực tiếp qua học. [5][6]\n\n### 2.2. Từ single‑head đến multi‑head self‑attention — động cơ\n- Động cơ: Một từ trong câu thường có nhiều mối quan hệ ngữ nghĩa khác nhau (ví dụ vừa tham chiếu tới một tên riêng, vừa là chủ thể cho một hành động). Do đó dùng một *single‑head attention* có thể không đủ để nắm hết các mối quan hệ này; cần thực hiện attention nhiều lần trên các “khía cạnh” khác nhau và kết hợp chúng — đó là *multi‑head attention*. Ví dụ minh hoạ: câu “tôi có hẹn với Bảo nhưng ... anh ấy nhắn đến muộn” cho thấy từ “anh ấy” có ít nhất hai mối quan hệ khác nhau trong câu. [7][8]\n\n### 2.3. Cấu trúc tính toán của multi‑head attention\n- Với multi‑head, ta có các ma trận linear để sinh Q, K, V cho từng head; mỗi head thực hiện attention độc lập trên một không gian chiều hẹp hơn, rồi kết quả của các head được concat lại và biến đổi tuyến tính để thu được vector tổng hợp cuối cùng. [8][9][11]  \n- Kích thước và chỉ số:\n  - Gọi h là số head; chỉ số l chạy từ 1..h để đánh dấu từng head. Mỗi head có bộ tham số riêng (các ma trận tuyến tính Q_l, K_l, V_l). [10][11]  \n  - Thông thường mỗi head có chiều d/h (để khi concat h head ta trở về kích thước d). Ma trận biến đổi cuối cùng có kích thước d × d (để nhân với vector concat và cho output kích thước d). (Video nêu dạng ma trận có kích thước d nhân cho d/h và kết quả concat rồi nhân tuyến tính để hợp thành thông tin tổng hợp). [10][11][12]\n- Công thức tính attention từng head (mô tả từ video):\n  - Đầu tiên lấy Q_l, K_l, V_l bằng phép biến đổi tuyến tính từ input; rồi tính score = Q_l K_l^T, áp dụng scale (chia cho sqrt(dim_k) — video gọi đó là *scale*), sau đó softmax và nhân với V_l: output_l = softmax( (Q_l K_l^T) / sqrt(...) ) V_l. [11]  \n  - Tổng hợp: concat(output_1, ..., output_h) rồi nhân tuyến tính bởi một ma trận W^O (kích thước d × d) để ra output cuối của multi‑head. [11][12]\n\n### 2.4. Vai trò của multi‑head trong Encoder và liên hệ với Decoder\n- Mỗi head là một “góc nhìn” khác nhau của ngôn ngữ; kết hợp nhiều head giúp mô hình nắm bắt đa dạng mối quan hệ trong câu. [9][11]  \n- Kiến trúc encoder sau khi thêm positional encoding và multi‑head attention là tương đối hoàn chỉnh theo nội dung bài giảng. Decoder sẽ thực hiện truy vấn (query) lên các đặc trưng đã được encoder tính toán; tức là decoder dùng các đặc trưng từ encoder làm nguồn thông tin để sinh đầu ra (video nhắc tới decoder sẽ truy vấn trên tầng đặc trưng lấy từ encoder và các tầng tiếp theo, ví dụ tới tầng thứ 6). [12][13]\n\n## 3. Ví dụ & Ứng dụng (Examples & Applications)\n\n- Ví dụ minh họa mối quan hệ nhiều chiều trong câu: “tôi có hẹn với Bảo nhưng ... anh ấy nhắn đến muộn” — từ “anh ấy” vừa tham chiếu tới “Bảo” vừa là chủ thể hành động nhắn tin. Multi‑head attention cho phép biểu diễn đồng thời những mối quan hệ khác nhau này bằng các head khác nhau rồi tổng hợp lại. [7][8]  \n- Ứng dụng thực tế (theo ngữ cảnh video): phần Encoder với positional encoding sinusoidal và multi‑head attention là thành phần cơ bản trong Transformer để xử lý chuỗi dài và nắm bắt phụ thuộc ngữ cảnh — phục vụ các tác vụ như dịch máy, mô hình ngôn ngữ, v.v. (Video nhắc đến ý rằng decoder sẽ truy vấn các đặc trưng encoder khi sinh đầu ra). [5][12][13]  \n- Trường hợp sử dụng: khi cần biểu diễn vị trí nhưng không muốn học vector vị trí (muốn một biểu diễn cố định để có tính tổng quát với chuỗi dài), sinusoidal positional encoding được dùng; khi muốn nắm bắt nhiều mối quan hệ song song giữa từ và các thành phần khác trong câu thì dùng multi‑head attention. [5][6][7][9]\n\n## 4. Kết luận (Conclusion)\n\n- Tóm tắt các ý chính:\n  - Positional encoding dạng sinusoidal tạo vector PI D‑chiều bằng các cặp tuần hoàn, giúp biểu diễn vị trí và hỗ trợ chuỗi dài mà ít trùng lặp; nhược điểm là là cố định, không học từ dữ liệu. [1][2][3][5][6]  \n  - Multi‑head self‑attention là phần mở rộng của self‑attention để bắt nhiều mối quan hệ khác nhau song song; mỗi head có Q,K,V riêng, thực hiện scaled dot‑product attention, sau đó concat và biến đổi tuyến tính để thu kết quả cuối cùng. Kích thước các ma trận phụ thuộc vào số head h, mỗi head thường có chiều d/h để khi concat về lại được d. [7][8][10][11][12]  \n  - Encoder với positional encoding + multi‑head attention là gần như hoàn chỉnh; Decoder sẽ query các biểu diễn này khi sinh output. [12][13]\n- Tầm quan trọng: Những thành phần này (positional encoding và multi‑head attention) là lõi trong khả năng của Transformer nắm bắt phụ thuộc dài hạn và nhiều loại mối quan hệ ngữ cảnh, từ đó cho phép các mô hình xử lý ngôn ngữ hiệu quả trên nhiều nhiệm vụ. [3][5][9]  \n- Liên hệ với các bài giảng khác: Video kết thúc bằng việc chuyển sang vai trò của Decoder — tức phần sau của bài giảng/chuỗi bài sẽ phân tích chi tiết cách decoder truy vấn lên các đặc trưng encoder (video nêu ý này như bước tiếp theo). [12][13]\n\nGhi chú: Tóm tắt trên dựa hoàn toàn trên nội dung các đoạn trích từ video (các chunk [1]…[13]) và trích dẫn ngay sau các ý tương ứng.",
      "sources": [
        {
          "index": 1,
          "video_id": "Chương 9_7AZr_li6ZtA",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 2] Part 4_3： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/7AZr_li6ZtA",
          "start_time": 1,
          "end_time": 59,
          "text": "Trong phiên bản của transformer đời đầu thì nó sẽ sử dụng các hàm sinh code Cứ lần lượt sinh code sinh code sinh code Và giá trị Y Và bên trong hàm sinh của mình đó chính là Y chia cho 10.000 bình phương Nhân cho 1 phần D Thế thì cái PI này của mình nó sẽ có kích thước là D chiều Nó sẽ có kích thước là D chiều Tại vì ở đây là D chi 2 Ở đây là D 1, 1, D chi 2, D chi 2 Thì như vậy là nó sẽ là D chi 2 Mà nhân 2 lên Tại vì nó sẽ là 1 cặp Nó sẽ là 1 cặp D nhân 2 Mỗi cái này là cho 1 cái chỉ số tăng Là cho 1 cái chỉ số Ví dụ đây là 1, 1 Tiếp theo sẽ là 2, 2 Đến đây sẽ là D phần 2, D phần 2 Thì chúng ta sẽ có tất cả là D phần 2 cái cặp như vậy"
        },
        {
          "index": 2,
          "video_id": "Chương 9_7AZr_li6ZtA",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 2] Part 4_3： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/7AZr_li6ZtA",
          "start_time": 46,
          "end_time": 110,
          "text": "Mỗi cái này là cho 1 cái chỉ số tăng Là cho 1 cái chỉ số Ví dụ đây là 1, 1 Tiếp theo sẽ là 2, 2 Đến đây sẽ là D phần 2, D phần 2 Thì chúng ta sẽ có tất cả là D phần 2 cái cặp như vậy D phần 2 cặp Thì D phần 2 nhân 2 sẽ là bằng D Như vậy thì kích thước output của cái positional embedding này Sẽ là 1 cái vector D chiều Cái ý tiếp theo chúng ta cần phải Đề cập đến đây đó là Cái chỉ số Y Cái chỉ số Y này của mình Nó tương ứng là cái index Cái vị trí của từ Vị trí của cái từ của mình Và với cái việc Chúng ta cho cái mẫu số là 10.000 này Nó sẽ giúp cho Cái khả năng Là các cái positional embedding của mình Nó không có nặp lại Với Y của mình chạy từ 0 cho đến 10.000 Tại vì với Y chạy từ 0 cho đến 10.000 Thì cái giá trị này nó sẽ là từ 0 Rồi nó nhảy lên là 1 phần 10.000 Rồi nhảy lên 2 phần 10.000"
        },
        {
          "index": 3,
          "video_id": "Chương 9_7AZr_li6ZtA",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 2] Part 4_3： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/7AZr_li6ZtA",
          "start_time": 100,
          "end_time": 160,
          "text": "Tại vì với Y chạy từ 0 cho đến 10.000 Thì cái giá trị này nó sẽ là từ 0 Rồi nó nhảy lên là 1 phần 10.000 Rồi nhảy lên 2 phần 10.000 Vâng vâng nhảy cho đến 1 Nó cứ nhảy lên Thì cái giá trị này nó sẽ là từ 0 Cái việc mà chúng ta đang xem Cái sinh cốt này Nó sẽ có thêm 1 cái tác dụng nữa Tức là cái việc mà Giúp tránh các giá trị embedding nó trùng nhau Đó là cái ý thứ nhất Ý thứ 2 đó là đảm bảo cho PI Nó sẽ đi theo cái phân bố Là phân bố chuẩn Các cái phân tử của PI Các cái phân tử trong cái vector PI này của mình Nó tuân theo phân bố là chuẩn Thì ở đây là ưu điểm Là hàng Hàm tuần hoàng Hàm tuần hoàng Cho thấy là vị trí Vị trí tiệt đối nó không có quan trọng Tức là gì Chúng ta hoàn toàn có thể thay 1 cái hàm tuần hoàng này Bằng 1 cái hàm khác Hàm ý của nó là Chúng ta có thể sử dụng"
        },
        {
          "index": 4,
          "video_id": "Chương 9_7AZr_li6ZtA",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 2] Part 4_3： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/7AZr_li6ZtA",
          "start_time": 148,
          "end_time": 211,
          "text": "Vị trí tiệt đối nó không có quan trọng Tức là gì Chúng ta hoàn toàn có thể thay 1 cái hàm tuần hoàng này Bằng 1 cái hàm khác Hàm ý của nó là Chúng ta có thể sử dụng Cái giá trị của mình nó sẽ là Thay đổi lên xuống lên xuống lên xuống Như vậy thì cái thông tin về mặt vị trí tiệt đối nó không quan trọng Tức là thông tin về mặt chỉ số Y Rồi Y cộng 1 Y cộng 2 v.v. Nó phải tăng đúng không Nếu mà xét về mặt Ở vị trí tiệt đối Thì là nó phải tăng Nhưng mà hàm tuần hoàng thì nó lại là lên xuống lên xuống Thì như vậy là Nó khẳng định cái việc đó là Khi chúng ta chọn với hàm tuần hoàng Mà cái độ chính xác của hệ thống này nó vẫn tốt Tức là cái vị trí tiệt đối Cái sự tăng dần của cái chỉ số này Cho cái position of adding Là không cần thiết Tức là P.I. của mình nó phải là 1 cái vị trí tiệt đối Cho cái hàm tăng là không cần thiết Và Ưu điểm thứ 2 Đó là nó có thể biểu diễn được cái chuỗi rất dài"
        },
        {
          "index": 5,
          "video_id": "Chương 9_7AZr_li6ZtA",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 2] Part 4_3： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/7AZr_li6ZtA",
          "start_time": 198,
          "end_time": 260,
          "text": "Cho cái position of adding Là không cần thiết Tức là P.I. của mình nó phải là 1 cái vị trí tiệt đối Cho cái hàm tăng là không cần thiết Và Ưu điểm thứ 2 Đó là nó có thể biểu diễn được cái chuỗi rất dài Thì thể hiện qua cái 10.000 Thì Y của mình cứ thay đổi Thì cái giá trị này của mình sẽ tăng theo Và thậm chí là cho đến khi Y nó chạm được đến 10.000 Và vượt qua khoảng 10.000 Thì cái giá trị này của mình nó cũng sẽ Cái vector amending của mình nó cũng sẽ không lặp lại Nó không có trùn nhờ Tại vì để trùn Thì nó sẽ phải có thêm một cái đại lượng là P nữa Nó phải có thêm một cái đại lượng là P Còn ở đây là không có P vô Nên cái khả năng mà nó trùn rất là thấp Rồi Và đương nhiên nó cũng sẽ có một số cái khuyết điểm Như chúng ta thấy ở đây Và cái vector position amending này nó là một cái Vector cố định"
        },
        {
          "index": 6,
          "video_id": "Chương 9_7AZr_li6ZtA",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 2] Part 4_3： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/7AZr_li6ZtA",
          "start_time": 249,
          "end_time": 312,
          "text": "Và đương nhiên nó cũng sẽ có một số cái khuyết điểm Như chúng ta thấy ở đây Và cái vector position amending này nó là một cái Vector cố định Với một cái Y cố định thì chúng ta sẽ có một cái P.I. cố định này là 1 cái vector cố định   Và cái này nó là một cái hàm do chúng ta thiết kế Là một cái tổ hợp của các cái hàm tồn hoàng Nó không phải học từ dữ liệu Nó không học từ dữ liệu Thì đây chính là cái điểm yếu của cái cách biểu diễn vị trí dưới dạng các cái đường sinh Và ở đây thì chúng ta sẽ xuất hiện thêm một cái khái niệm nữa Đó là multihead cell attention Trước đây thì là cell attention là một cái khái niệm nữa đó là multihead cell attention  Còn bây giờ chúng ta sẽ là multihead cell attention Còn bây giờ chúng ta sẽ là multihead cell attention Thì ở đây nó xuất phát từ một cái góc nhìn Đó là một từ nó sẽ có thể có nhiều cái mối quan hệ trong câu Đúng không? Và chúng ta sẽ thực hiện cái cell attention này nhiều lần"
        },
        {
          "index": 7,
          "video_id": "Chương 9_7AZr_li6ZtA",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 2] Part 4_3： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/7AZr_li6ZtA",
          "start_time": 299,
          "end_time": 360,
          "text": "Thì ở đây nó xuất phát từ một cái góc nhìn Đó là một từ nó sẽ có thể có nhiều cái mối quan hệ trong câu Đúng không? Và chúng ta sẽ thực hiện cái cell attention này nhiều lần Ứng với một lần nó sẽ thực hiện cho một cái mối quan hệ của một từ trong câu Và chúng ta sẽ kết hợp các cái kết quả này lại với nhau Chúng ta sẽ kết hợp lại Lấy ví dụ tôi có hẹn với Bảo nhưng mà bây giờ tôi không có hẹn với Bảo Nhưng anh ấy nhắn đến muộn Thì chúng ta thấy cái từ anh ấy nè Cái từ anh ấy Nó sẽ có hai cái mối quan hệ Mối quan hệ đầu tiên nó sẽ là một cái tham chiếu đến cái từ là bảo Là cái người tên là Bảo Đồng thời anh ấy sẽ là cái chủ thể cho cái hành động nhắn tin Như vậy thì chúng ta thấy là với một cái từ này nè Nó có đến những hai cái mối quan hệ Nó có đến những hai cái mối quan hệ Và một cách tổng quát thì chúng ta có thể gọi là mối quan hệ đó là 2 mối quan hệ này nè một từ có thể có rất nhiều cái mối quan hệ trong câu do đó chúng ta không sẽ không sử dụng một cái"
        },
        {
          "index": 8,
          "video_id": "Chương 9_7AZr_li6ZtA",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 2] Part 4_3： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/7AZr_li6ZtA",
          "start_time": 350,
          "end_time": 410,
          "text": "Nó có đến những hai cái mối quan hệ Và một cách tổng quát thì chúng ta có thể gọi là mối quan hệ đó là 2 mối quan hệ này nè một từ có thể có rất nhiều cái mối quan hệ trong câu do đó chúng ta không sẽ không sử dụng một cái single head attention mà chúng ta sẽ sử dụng multi head thì ở đây chúng ta vẫn sẽ là hệ thống ký hiệu VK và Q tương ứng là value key và query chúng ta đưa qua cái Linear này bản chất đó chính là cái cái phép nhân nhân tuyến tính rồi sau khi chúng ta nhân tuyến tính xong chúng ta thực hiện cái kéo đó rồi đó chúng ta thực hiện cái kéo đó product để mà cuối cùng là chúng ta sẽ nhân với lại tuyến tính để tính ra cái giá trị vector tổng hợp đây chính là cái attention à à à à à à à à à à à à à rồi thì ở bên đây sẽ là cái multi head"
        },
        {
          "index": 9,
          "video_id": "Chương 9_7AZr_li6ZtA",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 2] Part 4_3： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/7AZr_li6ZtA",
          "start_time": 394,
          "end_time": 464,
          "text": "tính để tính ra cái giá trị vector tổng hợp đây chính là cái attention à à à à à à à à à à à à à rồi thì ở bên đây sẽ là cái multi head cell attention đây là một cái lá cắt đúng không chúng ta sẽ thực hiện trên một cái khía cạnh của của câu của mình của một cái từ trong câu của mình và chúng ta sẽ thực hiện nhiều cái khía cạnh khác nhau sau đó đến đây chúng ta sẽ con cat thông tin của các cái kéo đó product attention này lại với nhau và sau đó chúng ta mới thực hiện cái phép biến đổi bật s Pattern vous cho mình đùa       MP3 Pero coi à à à cho người già sức khỏe trong bản v chasing erw Meter W X4 bộ tính sợ cutting là journalists common Sweep, cell Tension like the rest of the same age have more heads V공 Szon design like this, famas, cell tension normalties have many skills, when we choose gold, we need to save also We only have three exact things which are the signal at a quantity. if we have many more head bör nữa foreximately our initial wealth started to slow will have more head. Well, with the number L, we have various steps, we need to get more things above it. . gluttime sometimes we will also have the other level of L. theiorism is below it inDIGRADI"
        },
        {
          "index": 10,
          "video_id": "Chương 9_7AZr_li6ZtA",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 2] Part 4_3： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/7AZr_li6ZtA",
          "start_time": 439,
          "end_time": 509,
          "text": "cho người già sức khỏe trong bản v chasing erw Meter W X4 bộ tính sợ cutting là journalists common Sweep, cell Tension like the rest of the same age have more heads V공 Szon design like this, famas, cell tension normalties have many skills, when we choose gold, we need to save also We only have three exact things which are the signal at a quantity. if we have many more head bör nữa foreximately our initial wealth started to slow will have more head. Well, with the number L, we have various steps, we need to get more things above it. . gluttime sometimes we will also have the other level of L. theiorism is below it inDIGRADI và cái ma trận này thì nó đều có cái kích thước đó là d nhân cho d chia chỉ h trong đó h là số cái đầu extension của mình thì số lượng hết của mình rồi và l của mình nó sẽ chạy từ nó sẽ chạy từ 1 cho đến h l của mình sẽ là từ 1 cho đến h tức là mỗi cái bộ ql,kl,vl nó tương ứng là một cái nó tương ứng là một cái bộ này một cái biến đội tiến tính ở đây, bộ cái tham số cho cái biến đội tiến tính ở đây thì ở đây có bao nhiêu hết, ở đây có 3 hết thì trong trường hợp này h của mình là bằng 3 và l của mình sẽ là một cái chỉ số chạy"
        },
        {
          "index": 11,
          "video_id": "Chương 9_7AZr_li6ZtA",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 2] Part 4_3： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/7AZr_li6ZtA",
          "start_time": 499,
          "end_time": 560,
          "text": "cái biến đội tiến tính ở đây thì ở đây có bao nhiêu hết, ở đây có 3 hết thì trong trường hợp này h của mình là bằng 3 và l của mình sẽ là một cái chỉ số chạy l trong trường hợp này là bằng 1 l trong trường hợp này là bằng 2 l trong trường hợp này là bằng 3 l là cái chỉ số chạy thì mỗi một cái lá cắt như vậy thì chúng ta sẽ có một cái bộ trọng số bộ tham số cho mỗi mình cần phải hữu luyện và output lúc này của mình theo từng cái hết output theo từng hết của mình nó sẽ là softmax của x nhân quy l kl chuyển vị, x chuyển vị chia cho, đây chính là scale đó rồi sau đó nhân cho x nhân cho vl và như vậy thì output này nó sẽ ra là một cái vector có kích thước là dh và ở cái bước tổng hợp này nè ở cái bước concat này nè thì chúng ta sẽ tổng hợp lại output 1 output 2 output h chúng ta sẽ concat lại với nhau"
        },
        {
          "index": 12,
          "video_id": "Chương 9_7AZr_li6ZtA",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 2] Part 4_3： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/7AZr_li6ZtA",
          "start_time": 549,
          "end_time": 610,
          "text": "và ở cái bước tổng hợp này nè ở cái bước concat này nè thì chúng ta sẽ tổng hợp lại output 1 output 2 output h chúng ta sẽ concat lại với nhau sau đó chúng ta sẽ nhân tuyến tính với lại cái my.y để tổng hợp thông tin thì đây sẽ là cái thông tin tổng hợp cuối cùng của multihead của nhiều đầu của nhiều đầu của nhiều đầu của nhiều đầu của nhiều đầu nhiều đầu thông tin và như vậy thì cái output của mình à kia xin lỗi cái vector y cái ma trận y của mình nó sẽ là một cái ma trận có kích thước là d x d để có thể thực hiện được cái phép nhân này thì mỗi cái đầu extension của mình nó sẽ là một cái góc nhìn của ngôn ngữ như đã giải thích ở trên và như vậy thì đến đây kiến trúc encoder của chúng ta đã tương đối là đầy đủ rồi vậy thì decoder của mình sẽ ra sao? thì ở đây"
        },
        {
          "index": 13,
          "video_id": "Chương 9_7AZr_li6ZtA",
          "chapter": "Chương 9",
          "video_title": "[CS431 - Chương 2] Part 4_3： Kiến trúc Transformer： Bộ Encoder",
          "video_url": "https://youtu.be/7AZr_li6ZtA",
          "start_time": 599,
          "end_time": 642,
          "text": "và như vậy thì đến đây kiến trúc encoder của chúng ta đã tương đối là đầy đủ rồi vậy thì decoder của mình sẽ ra sao? thì ở đây decoder Potato nó sẽ được tiến hành truy vấn trên tầng h noise . Ở đây decoder sẽ là tiến hành truy vấn trên cái đặt trưng được lấy từ cái encoder encoder kêu chúng ta tổng hợp thông tin và tính toán sau đến cái tầng thứ 6 b Entonces tú y thì chúng ta sẽ tính toán và chúng ta sẽ bắt đầu cái cái doesn't work out to get done? Quá trình giải mẽ Cho cái decoder của mình"
        }
      ]
    }
  ]
}