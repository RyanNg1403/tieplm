"""
Q&A Evaluation Service

Evaluates Q&A responses using simplified metrics:
1. Exact Match - For MCQ questions (A/B/C/D matching)
2. Answer Correctness - Semantic similarity with ground truth (cosine + LLM score)
3. Citation Accuracy - Ground truth source in retrieved chunks
"""
import os
import sys
import numpy as np
import re
import json
from typing import Dict, Any, List, Optional
from datetime import datetime

# Add backend to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../backend'))

from app.shared.embeddings.embedder import OpenAIEmbedder
from app.shared.llm.client import LLMClient
from app.shared.rag.retriever import get_rag_retriever
from app.shared.rag.reranker import get_local_reranker

# Evaluation-specific prompts (ngáº¯n gá»n, khÃ´ng dÃ i dÃ²ng nhÆ° prompt cho users)
EVAL_QA_SYSTEM_PROMPT = """Báº¡n lÃ  trá»£ lÃ½ AI cho khÃ³a há»c CS431 - Deep Learning.

NHIá»†M Vá»¤: Tráº£ lá»i cÃ¢u há»i dá»±a vÃ o cÃ¡c nguá»“n transcript video Ä‘Æ°á»£c cung cáº¥p.

QUY Táº®C:
1. Tráº£ lá»i NGáº®N Gá»ŒN, Ä‘i tháº³ng vÃ o váº¥n Ä‘á»
2. Chá»‰ tráº£ lá»i nhá»¯ng gÃ¬ cÃ³ trong nguá»“n
3. Vá»›i cÃ¢u há»i tráº¯c nghiá»‡m: chá»‰ cáº§n tráº£ lá»i "A", "B", "C" hoáº·c "D" kÃ¨m giáº£i thÃ­ch ngáº¯n
4. Vá»›i cÃ¢u há»i tá»± luáº­n: tráº£ lá»i sÃºc tÃ­ch trong 2-3 cÃ¢u
5. KhÃ´ng cáº§n format markdown phá»©c táº¡p
"""

EVAL_QA_USER_PROMPT_TEMPLATE = """# NGUá»’N TÃ€I LIá»†U:
{sources}

# CÃ‚U Há»I:
{query}

# TRáº¢ Lá»œI:
(Tráº£ lá»i ngáº¯n gá»n, Ä‘i tháº³ng vÃ o váº¥n Ä‘á»)
"""


class QAEvaluationService:
    """Service for evaluating Q&A task performance."""
    
    def __init__(self):
        self.embedder = OpenAIEmbedder()
        self.llm = LLMClient()
        self.retriever = get_rag_retriever()
        self.reranker = get_local_reranker()
        
    async def evaluate_question(
        self,
        question: str,
        ground_truth_answer: str,
        ground_truth_videos: List[str],
        ground_truth_timestamps: List[str],
        chapters: Optional[List[str]] = None,
        question_type: str = "short_answer",  # "mcq" hoáº·c "short_answer"
        ground_truth_options: Optional[str] = None  # Cho MCQ
    ) -> Dict[str, Any]:
        """
        Evaluate a single Q&A question.
        
        Args:
            question: User question
            ground_truth_answer: Expected answer from ground truth
            ground_truth_videos: List of expected video URLs
            ground_truth_timestamps: List of expected timestamp ranges
            chapters: Optional chapter filter
            question_type: "mcq" or "short_answer"
            ground_truth_options: Options string for MCQ (e.g., "a) ... b) ... c) ... d) ...")
            
        Returns:
            Dict with evaluation metrics and details
        """
        # Step 1: Retrieve vÃ  rerank chunks
        print(f"  ğŸ“š Retrieving chunks...")
        retrieved_chunks = await self.retriever.retrieve(
            query=question,
            top_k=150,
            chapter_filter=chapters,
            use_bm25=True
        )
        
        # Rerank to get top 10
        print(f"  ğŸ”„ Reranking to top 10...")
        reranked_chunks = self.reranker.rerank(question, retrieved_chunks, top_k=10)
        
        # Format sources cho prompt
        sources_text = ""
        for i, chunk in enumerate(reranked_chunks, 1):
            metadata = chunk.get('metadata', {})
            sources_text += f"[{i}] {metadata.get('video_title', 'Unknown')}\n"
            start_time = metadata.get('start_time', 0)
            end_time = metadata.get('end_time', 0)
            sources_text += f"Timestamp: {self._format_timestamp(start_time)} - {self._format_timestamp(end_time)}\n"
            sources_text += f"{metadata.get('text', '')}\n\n"
        
        # Build eval prompt
        query_with_options = question
        if question_type == "mcq" and ground_truth_options:
            query_with_options += f"\n\nCÃ¡c phÆ°Æ¡ng Ã¡n:\n{ground_truth_options}"
        
        eval_prompt = EVAL_QA_USER_PROMPT_TEMPLATE.format(
            sources=sources_text,
            query=query_with_options
        )
        
        # Step 2: Generate answer vá»›i eval prompt
        print(f"  ğŸ¤– Generating answer with LLM...")
        generated_answer = await self.llm.generate_async(
            prompt=eval_prompt,
            system_prompt=EVAL_QA_SYSTEM_PROMPT,
            max_tokens=500
        )
        
        # Convert chunks to sources format
        generated_sources = []
        for chunk in reranked_chunks:
            metadata = chunk.get('metadata', {})
            start_time = metadata.get('start_time', 0)
            end_time = metadata.get('end_time', 0)
            generated_sources.append({
                "video_title": metadata.get("video_title", ""),
                "video_url": metadata.get("video_url", ""),
                "timestamp": f"{self._format_timestamp(start_time)} - {self._format_timestamp(end_time)}",
                "text": metadata.get("text", "")
            })
        
        # Step 3: Calculate metrics
        print(f"  ğŸ“Š Calculating metrics...")
        metrics = {}
        
        # Metric 1: Exact Match (chá»‰ cho MCQ)
        if question_type == "mcq":
            metrics["exact_match"] = self._calculate_exact_match(
                generated_answer, ground_truth_answer
            )
        
        # Metric 2: Answer Correctness (cho cáº£ MCQ vÃ  tá»± luáº­n)
        metrics["answer_correctness"] = await self._calculate_answer_correctness(
            question, generated_answer, ground_truth_answer
        )
        
        # Metric 3: Citation Accuracy (Ä‘Æ¡n giáº£n - kiá»ƒm tra ground truth source cÃ³ trong retrieved khÃ´ng)
        metrics["citation_accuracy"] = self._calculate_citation_accuracy_simple(
            generated_sources, ground_truth_videos, ground_truth_timestamps
        )
        
        # Step 4: Return evaluation result
        return {
            "question": question,
            "question_type": question_type,
            "generated_answer": generated_answer,
            "ground_truth_answer": ground_truth_answer,
            "generated_sources": generated_sources,
            "ground_truth_videos": ground_truth_videos,
            "ground_truth_timestamps": ground_truth_timestamps,
            "metrics": metrics,
            "timestamp": datetime.now().isoformat()
        }
    
    def _calculate_exact_match(
        self,
        generated_answer: str,
        ground_truth_answer: str
    ) -> Dict[str, Any]:
        """
        Metric cho MCQ: Exact Match
        
        TrÃ­ch xuáº¥t lá»±a chá»n (a, b, c, d) tá»« cÃ¢u tráº£ lá»i vÃ  so sÃ¡nh.
        
        Returns:
            Dict with:
            - predicted_choice: str (a/b/c/d)
            - ground_truth_choice: str (a/b/c/d)
            - is_correct: bool
            - score: float (1.0 hoáº·c 0.0)
        """
        # Extract choice tá»« generated answer (tÃ¬m a, b, c, d Ä‘áº§u tiÃªn)
        predicted_choice = None
        generated_lower = generated_answer.lower()
        
        # TÃ¬m pattern "a)", "b)", "c)", "d)" hoáº·c Ä‘Æ¡n giáº£n "a", "b", "c", "d"
        for choice in ['a', 'b', 'c', 'd']:
            if f"{choice})" in generated_lower[:50] or f"{choice}." in generated_lower[:50]:
                predicted_choice = choice
                break
            # Fallback: tÃ¬m chá»¯ Ä‘Æ¡n
            if generated_lower.strip().startswith(choice):
                predicted_choice = choice
                break
        
        # Extract choice tá»« ground truth
        ground_truth_choice = None
        ground_truth_lower = ground_truth_answer.lower()
        for choice in ['a', 'b', 'c', 'd']:
            if ground_truth_lower.strip().startswith(f"{choice})") or ground_truth_lower.strip().startswith(f"{choice}."):
                ground_truth_choice = choice
                break
        
        is_correct = (predicted_choice == ground_truth_choice) if predicted_choice and ground_truth_choice else False
        
        return {
            "predicted_choice": predicted_choice,
            "ground_truth_choice": ground_truth_choice,
            "is_correct": is_correct,
            "score": 1.0 if is_correct else 0.0
        }
    
    async def _calculate_answer_correctness(
        self,
        question: str,
        generated: str, 
        ground_truth: str
    ) -> Dict[str, Any]:
        """
        Metric: Answer Correctness
        
        Uses both embedding similarity and LLM-based evaluation.
        
        Returns:
            Dict with:
            - cosine_similarity: float (0-1)
            - llm_score: float (0-1)
            - combined_score: float (0-1)
            - explanation: str
        """
        # Embedding-based similarity
        gen_embedding = self.embedder.embed(generated)
        gt_embedding = self.embedder.embed(ground_truth)
        
        # Cosine similarity
        embedding_sim = float(np.dot(gen_embedding, gt_embedding) / 
                            (np.linalg.norm(gen_embedding) * np.linalg.norm(gt_embedding)))
        
        # LLM-based evaluation (thÃªm question Ä‘á»ƒ LLM hiá»ƒu context)
        llm_prompt = f"""ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¢u tráº£ lá»i Ä‘Æ°á»£c generate so vá»›i Ä‘Ã¡p Ã¡n ground truth.

# CÃ¢u há»i gá»‘c:
{question}

# ÄÃ¡p Ã¡n Ground Truth:
{ground_truth}

# CÃ¢u tráº£ lá»i Ä‘Æ°á»£c Generate:
{generated}

# YÃªu cáº§u Ä‘Ã¡nh giÃ¡:
1. So sÃ¡nh ná»™i dung semantic (Ã½ nghÄ©a) cá»§a hai cÃ¢u tráº£ lá»i
2. Kiá»ƒm tra xem cÃ¢u tráº£ lá»i generated cÃ³ Ä‘á»§ thÃ´ng tin quan trá»ng tá»« ground truth khÃ´ng
3. ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c vá» máº·t ká»¹ thuáº­t (thuáº­t ngá»¯, Ä‘á»‹nh nghÄ©a)
4. XÃ©t trong context cá»§a cÃ¢u há»i gá»‘c

# Output format (JSON):
{{
    "score": <float 0-1>,
    "explanation": "<giáº£i thÃ­ch ngáº¯n gá»n>"
}}

Chá»‰ tráº£ vá» JSON, khÃ´ng giáº£i thÃ­ch thÃªm.
"""
        
        llm_response = await self.llm.generate_async(
            prompt=llm_prompt,
            system_prompt="You are an evaluation assistant. Return only valid JSON.",
            max_tokens=500
        )
        
        # Parse LLM response
        try:
            llm_eval = json.loads(llm_response.strip())
            llm_score = float(llm_eval.get("score", 0.0))
            explanation = llm_eval.get("explanation", "")
        except:
            llm_score = 0.0
            explanation = "Failed to parse LLM evaluation"
        
        # Combined score (weighted average)
        combined_score = 0.4 * embedding_sim + 0.6 * llm_score
        
        return {
            "cosine_similarity": round(embedding_sim, 4),
            "llm_score": round(llm_score, 4),
            "combined_score": round(combined_score, 4),
            "explanation": explanation
        }
    
    def _calculate_citation_accuracy_simple(
        self,
        generated_sources: List[Dict[str, Any]],
        ground_truth_videos: List[str],
        ground_truth_timestamps: List[str]
    ) -> Dict[str, Any]:
        """
        Metric: Citation Accuracy (ÄÆ¡n giáº£n hÃ³a)
        
        Kiá»ƒm tra xem ground truth source cÃ³ náº±m trong 10 chunks retrieved khÃ´ng.
        Má»—i cÃ¢u há»i chá»‰ cÃ³ 1 source, RAG retrieve 10 chunks.
        
        Returns:
            Dict with:
            - ground_truth_in_retrieved: bool
            - retrieved_count: int
            - score: float (1.0 náº¿u cÃ³, 0.0 náº¿u khÃ´ng)
        """
        if not ground_truth_videos or not generated_sources:
            return {
                "ground_truth_in_retrieved": False,
                "retrieved_count": len(generated_sources),
                "score": 0.0,
                "details": "No ground truth or no retrieved sources"
            }
        
        # Láº¥y ground truth video URL (chá»‰ cÃ³ 1)
        gt_video_url = ground_truth_videos[0]
        gt_timestamp = ground_truth_timestamps[0] if ground_truth_timestamps else None
        
        # Extract video ID tá»« ground truth
        gt_video_id = self._extract_video_id(gt_video_url)
        
        # Kiá»ƒm tra xem cÃ³ chunk nÃ o match khÃ´ng
        found = False
        for source in generated_sources:
            source_video_id = self._extract_video_id(source.get("video_url", ""))
            
            if source_video_id == gt_video_id:
                # Náº¿u cÃ³ timestamp, kiá»ƒm tra overlap
                if gt_timestamp:
                    source_timestamp = source.get("timestamp", "")
                    if self._check_timestamp_overlap(gt_timestamp, source_timestamp):
                        found = True
                        break
                else:
                    # KhÃ´ng cÃ³ timestamp thÃ¬ chá»‰ cáº§n video ID khá»›p
                    found = True
                    break
        
        return {
            "ground_truth_in_retrieved": found,
            "retrieved_count": len(generated_sources),
            "score": 1.0 if found else 0.0,
            "ground_truth_video": gt_video_url,
            "ground_truth_timestamp": gt_timestamp
        }
    
    def _extract_video_id(self, url: str) -> str:
        """Extract YouTube video ID from URL."""
        if "youtu.be/" in url:
            return url.split("youtu.be/")[-1].split("?")[0]
        elif "youtube.com/watch?v=" in url:
            return url.split("v=")[-1].split("&")[0]
        return url
    
    def _check_timestamp_overlap(self, gt_timestamp: str, source_timestamp: str) -> bool:
        """Kiá»ƒm tra xem 2 timestamp cÃ³ overlap khÃ´ng."""
        try:
            # Parse ground truth timestamp (format: "00:26:00 - 00:27:00")
            if " - " in gt_timestamp:
                gt_start_str, gt_end_str = gt_timestamp.split(" - ")
                gt_start = self._parse_timestamp(gt_start_str.strip())
                gt_end = self._parse_timestamp(gt_end_str.strip())
            else:
                # Single timestamp
                gt_start = gt_end = self._parse_timestamp(gt_timestamp.strip())
            
            # Parse source timestamp (format: "00:26:15")
            if " - " in source_timestamp:
                src_start_str, src_end_str = source_timestamp.split(" - ")
                src_start = self._parse_timestamp(src_start_str.strip())
                src_end = self._parse_timestamp(src_end_str.strip())
            else:
                src_start = src_end = self._parse_timestamp(source_timestamp.strip())
            
            # Check overlap
            return not (src_end < gt_start or src_start > gt_end)
        except:
            return False
    
    def _format_timestamp(self, seconds: float) -> str:
        """Convert seconds to HH:MM:SS format."""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"
    
    def _parse_timestamp(self, timestamp: str) -> int:
        """Convert timestamp string (HH:MM:SS) to seconds."""
        parts = timestamp.split(":")
        if len(parts) == 3:
            return int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])
        elif len(parts) == 2:
            return int(parts[0]) * 60 + int(parts[1])
        return 0
