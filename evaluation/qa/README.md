# Q&A Evaluation

Evaluation framework for the Q&A task using 3 simplified metrics.

## ğŸ“Š Evaluation Metrics

### 1. **Exact Match** (cho MCQ, 0-1, higher is better)
- **Predicted Choice**: Lá»±a chá»n (a/b/c/d) Ä‘Æ°á»£c AI tráº£ lá»i
- **Ground Truth Choice**: ÄÃ¡p Ã¡n Ä‘Ãºng tá»« ground truth
- **Score**: 1.0 náº¿u khá»›p, 0.0 náº¿u sai

### 2. **Answer Correctness** (cho cáº£ MCQ vÃ  tá»± luáº­n, 0-1, higher is better)
- **Cosine Similarity**: Cosine similarity giá»¯a embeddings cá»§a generated answer vÃ  ground truth
- **LLM Score**: GPT-5-mini Ä‘Ã¡nh giÃ¡ semantic correctness (cÃ³ thÃªm cÃ¢u há»i gá»‘c Ä‘á»ƒ hiá»ƒu context)
- **Combined Score**: Weighted average (40% cosine + 60% LLM)

### 3. **Citation Accuracy** (0-1, higher is better)
- **Ground Truth in Retrieved**: Kiá»ƒm tra xem ground truth source cÃ³ náº±m trong 10 chunks retrieved khÃ´ng
- **Score**: 1.0 náº¿u cÃ³, 0.0 náº¿u khÃ´ng
- **Chi tiáº¿t**: Má»—i cÃ¢u há»i chá»‰ cÃ³ 1 source, RAG retrieve 10 chunks

---

## ğŸ”„ Thay Ä‘á»•i so vá»›i version cÅ©

**ÄÆ N GIáº¢N HÃ“A:**
1. **MCQ**: ThÃªm metric Exact Match Ä‘á»ƒ so sÃ¡nh trá»±c tiáº¿p A/B/C/D
2. **Answer Correctness**: ThÃªm cÃ¢u há»i gá»‘c vÃ o prompt LLM, lÆ°u riÃªng cosine + LLM score
3. **Citation Accuracy**: ÄÆ¡n giáº£n hÃ³a - chá»‰ kiá»ƒm tra ground truth source cÃ³ trong retrieved chunks khÃ´ng
4. **Loáº¡i bá»**: Source Relevance (F1 phá»©c táº¡p) vÃ  Hallucination Rate (LLM Ä‘Ã£ Ä‘Ã¡nh giÃ¡ á»Ÿ Answer Correctness)
5. **Prompt riÃªng cho eval**: Ngáº¯n gá»n, khÃ´ng dÃ i dÃ²ng nhÆ° prompt cho users

---

## ğŸš€ Quick Start

### 1. Prepare Test Questions

Create `test_questions.json` with ground truth data:

```json
[
  {
    "chapter": "7",
    "question": "RNN lÃ  viáº¿t táº¯t cá»§a thuáº­t ngá»¯ gÃ¬ trong há»c sÃ¢u?",
    "options": null,
    "answer": "RNN lÃ  tÃªn viáº¿t táº¯t cá»§a máº¡ng Recurrent Neural Network.",
    "video_urls": ["https://youtu.be/_KvZN8-SyvQ"],
    "timestamps": ["00:00:10 - 00:00:40"]
  }
]
```

### 2. Run Evaluation

```bash
# Full evaluation (all questions)
cd evaluation/qa
python run_eval.py

# Evaluate specific chapter
python run_eval.py --chapter 7

# Limit number of questions
python run_eval.py --limit 5

# Custom test file
python run_eval.py --test-file my_questions.json --output-dir my_results/
```

### 3. View Results

Results are saved in `evaluation/qa/results/run_TIMESTAMP/`:
- `evaluations.json`: Individual question evaluations
- `summary.json`: Aggregated statistics

---

## ğŸ“ Project Structure

```
evaluation/qa/
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ __init__.py                # Python package init
â”œâ”€â”€ eval_service.py            # Core evaluation logic
â”œâ”€â”€ run_eval.py                # CLI runner script
â”œâ”€â”€ test_questions.json        # Ground truth test questions
â””â”€â”€ results/                   # Evaluation results
    â”œâ”€â”€ run_20251116_140530/
    â”‚   â”œâ”€â”€ evaluations.json
    â”‚   â””â”€â”€ summary.json
    â””â”€â”€ ...
```

---

## ğŸ“‹ Example Output

### Summary Statistics

```json
{
  "total_questions": 8,
  "successful_evaluations": 8,
  "failed_evaluations": 0,
  "average_metrics": {
    "answer_correctness": 0.8234,
    "citation_accuracy": 0.9500,
    "source_relevance_f1": 0.7821,
    "hallucination_score": 0.1200
  },
  "by_chapter": {
    "7": {
      "count": 8,
      "answer_correctness": 0.8234,
      "citation_accuracy": 0.9500,
      "source_relevance_f1": 0.7821,
      "hallucination_score": 0.1200
    }
  }
}
```

### Individual Evaluation

```json
{
  "question": "RNN lÃ  viáº¿t táº¯t cá»§a thuáº­t ngá»¯ gÃ¬ trong há»c sÃ¢u?",
  "generated_answer": "RNN lÃ  viáº¿t táº¯t cá»§a Recurrent Neural Network[1]...",
  "ground_truth_answer": "RNN lÃ  tÃªn viáº¿t táº¯t cá»§a máº¡ng Recurrent Neural Network.",
  "metrics": {
    "answer_correctness": {
      "embedding_similarity": 0.9234,
      "llm_score": 0.9500,
      "combined_score": 0.9394,
      "explanation": "CÃ¢u tráº£ lá»i chÃ­nh xÃ¡c vÃ  Ä‘áº§y Ä‘á»§"
    },
    "citation_accuracy": {
      "has_citations": true,
      "citation_count": 3,
      "valid_citation_count": 3,
      "citation_coverage": 1.0,
      "accuracy_score": 1.0
    },
    "source_relevance": {
      "video_match_count": 1,
      "timestamp_overlap_count": 1,
      "precision": 0.8000,
      "recall": 1.0000,
      "f1_score": 0.8889
    },
    "hallucination_rate": {
      "hallucination_score": 0.0500,
      "has_hallucination": false,
      "hallucination_examples": []
    }
  }
}
```

---

## ğŸ¯ Evaluation Guidelines

### What Makes a Good Q&A Response?

1. **Accurate**: Matches ground truth semantically
2. **Cited**: All claims have [N] citations
3. **Grounded**: Uses correct video sources and timestamps
4. **Faithful**: No hallucinated information

### Threshold Recommendations

- **Answer Correctness**: â‰¥ 0.7 (good), â‰¥ 0.8 (excellent)
- **Citation Accuracy**: â‰¥ 0.9 (all citations should be valid)
- **Source Relevance (F1)**: â‰¥ 0.6 (decent), â‰¥ 0.8 (excellent)
- **Hallucination Score**: â‰¤ 0.2 (acceptable), â‰¤ 0.1 (excellent)

---

## ğŸ”§ Customization

### Add New Metrics

Edit `eval_service.py` and add methods:

```python
async def _calculate_new_metric(self, generated, ground_truth):
    # Your metric logic here
    return {
        "score": 0.0,
        "details": {}
    }
```

### Modify LLM Prompts

Update prompts in `eval_service.py`:
- `_calculate_answer_correctness()`: LLM evaluation prompt
- `_calculate_hallucination_rate()`: Hallucination detection prompt

---

## ğŸ“Š Integration with CI/CD

Run automated evaluation after model updates:

```bash
# In your CI/CD pipeline
python evaluation/qa/run_eval.py --limit 10
```

Set thresholds in deployment scripts:

```python
summary = json.load(open('results/latest/summary.json'))
if summary['average_metrics']['answer_correctness'] < 0.7:
    raise Exception("Q&A quality below threshold!")
```

---

## ğŸ› Troubleshooting

**Issue**: `ModuleNotFoundError: No module named 'app'`
- **Solution**: Run from `evaluation/qa/` directory or set PYTHONPATH

**Issue**: OpenAI API rate limits
- **Solution**: Add delays or use `--limit` flag

**Issue**: Low source relevance scores
- **Solution**: Check if ground truth video URLs match Qdrant data

---

## ğŸ“š References

- [DeepEval Framework](https://docs.confident-ai.com/)
- [RAGAs Metrics](https://docs.ragas.io/en/stable/concepts/metrics/)
- [Anthropic's Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)

---

**Estimated Time Per Question:** ~10-15 seconds (depends on LLM latency)

**Team Can Iterate Quickly** - Run eval after each prompt change! ğŸš€
