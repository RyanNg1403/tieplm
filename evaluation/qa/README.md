# Q&A Evaluation

Evaluation framework for the Q&A task with support for multiple metrics and flexible question sampling.

## Features

- **Metrics**:
  - **Exact Match** (MCQ only): Checks if predicted answer matches ground truth
  - **Answer Correctness** (Open-ended only): LLM-as-Judge combining cosine similarity (30%) and LLM score (70%)
  - **Citation Accuracy**: Checks if ground truth video is in retrieved chunks
  - **MRR (Mean Reciprocal Rank)**: Rank of first relevant chunk from source video

- **Question Types**:
  - Multiple Choice Questions (MCQ) - when `"Ph∆∞∆°ng √°n (n·∫øu c√≥)"` has options
  - Open-ended Questions - when `"Ph∆∞∆°ng √°n (n·∫øu c√≥)"` is null

- **Flexible Sampling**:
  - Sample specific number of questions (`--n-questions`)
  - Random sampling (`--random`)
  - Filter by chapters (`--chapters`)

## Usage

### Basic Usage

Run evaluation on all questions:
```bash
python run_eval.py
```

### Filter by Chapters

Evaluate only questions from Ch∆∞∆°ng 2 and 3:
```bash
python run_eval.py --chapters 2 3
```

### Sample Questions

Evaluate first 10 questions from Ch∆∞∆°ng 2:
```bash
python run_eval.py --n-questions 10 --chapters 2
```

### Random Sampling

Randomly sample 20 questions from Ch∆∞∆°ng 2 and 3:
```bash
python run_eval.py --n-questions 20 --random --chapters 2 3
```

### Custom Output Directory

```bash
python run_eval.py --output-dir my_results --n-questions 5 --chapters 2
```

## Arguments

- `--test-file`: Path to test questions JSON file (default: `test_questions.json`)
- `--output-dir`: Output directory for results (default: `results/run_TIMESTAMP`)
- `--n-questions`: Number of questions to evaluate (default: all questions)
- `--random`: Randomly sample n questions instead of taking first n
- `--chapters`: Filter by chapters (e.g., `--chapters 2 3` for Ch∆∞∆°ng 2 and 3)

## Test Data Format

The `test_questions.json` file follows this format:

```json
[
    {
        "Ch∆∞∆°ng": 2,
        "N·ªôi dung c√¢u h·ªèi": "Question text...",
        "Ph∆∞∆°ng √°n (n·∫øu c√≥)": "a) ... b) ... c) ... d) ..." or null,
        "ƒê√°p √°n": "Answer text...",
        "Link Video": "https://youtu.be/...",
        "Timestamps": "00:00:30‚Äì00:00:50",
        "Video Title": "Video title..." (optional, only for Ch∆∞∆°ng 2 and 3)
    }
]
```

## Output Files

Results are saved to the output directory:

- `evaluations.json`: Detailed results for each question
- `summary.json`: Aggregate statistics and metrics by chapter

## Example Output

```
============================================================
Q&A Evaluation Runner
============================================================
üìù Total questions: 20
üìö Filtered by chapters: [2, 3]
üé≤ Random sampling: Yes
üíæ Output directory: results/run_20251117_123456
============================================================

[1/20] Evaluating question from Chapter 2...
‚ùì Trong m√¥ h√¨nh m√°y h·ªçc c√≥ gi√°m s√°t t·ªïng qu√°t, gi√° tr·ªã d·ª± ƒëo√°n y~‚Äã...
  üìö Retrieving chunks...
  üîÑ Reranking to top 10...
  ü§ñ Generating answer with LLM...
  üìä Calculating metrics...
  ‚úÖ Exact Match: 1.000 (Predicted: b, GT: b)
  üìé Citation Accuracy: 1.000 (GT in retrieved: True)
  üéØ MRR: 1.000 (Rank: 1)

...

============================================================
‚úÖ Evaluation Complete!
============================================================
üìä Summary Statistics:
  ‚Ä¢ Exact Match (MCQ): 0.850
  ‚Ä¢ Answer Correctness: 0.742
  ‚Ä¢ Citation Accuracy: 0.900
  ‚Ä¢ MRR (Mean Reciprocal Rank): 0.815
```

## Dataset Statistics

Total questions: 306
- Ch∆∞∆°ng 2: 66 questions
- Ch∆∞∆°ng 3: 32 questions
- Ch∆∞∆°ng 4: 22 questions
- Ch∆∞∆°ng 5: 18 questions
- Ch∆∞∆°ng 6: 45 questions
- Ch∆∞∆°ng 7: 45 questions
- Ch∆∞∆°ng 8: 38 questions
- Ch∆∞∆°ng 9: 40 questions

Question types:
- MCQ (with options): 190
- Open-ended (null options): 116

Ch∆∞∆°ng 2 & 3 (with Video Titles): 98 questions
