{
  "run_info": {
    "run_id": "run_20251117_111017",
    "timestamp": "20251117_111017",
    "total_cases": 50,
    "successful_cases": 50,
    "failed_cases": 0,
    "question_type": "open_ended"
  },
  "statistics": {
    "mean": 0.8031,
    "median": 0.8115,
    "min": 0.4972,
    "max": 0.985,
    "std": 0.0972
  },
  "scores": [
    {
      "case_id": "quiz_eval_001",
      "question": "Vai trò chính của module projection trong Linformer và ảnh hưởng của việc giảm không gian từ T xuống K (K << T) tới độ phức tạp tính toán và thời gian inference là gì?",
      "embedding_similarity": 0.8929,
      "reference_answer": "Module projection chiếu các đại diện từ không gian chiều T xuống không gian nhỏ hơn K (K << T), giảm số chiều cần tính tương tác. Khi K cố định, độ phức tạp tính toán giảm đáng kể và thời gian inference gần như không đổi khi T thay đổi.",
      "qa_answer": "Module projection là thành phần chính chịu trách nhiệm chiếu không gian từ chiều T xuống không gian nhỏ hơn K; việc giảm từ T xuống K (K << T) làm giảm đáng kể độ phức tạp tính toán và — khi K cố định — giữ thời gian inference gần như không đổi khi T thay đổi."
    },
    {
      "case_id": "quiz_eval_002",
      "question": "Trong ví dụ MobileNet (độ sâu vào 32, đầu ra 64, kernel 3x3), hãy nêu ngắn gọn vì sao depthwise separable convolution giảm số lượng tham số so với convolution chuẩn và giảm khoảng bao nhiêu lần.",
      "embedding_similarity": 0.9157,
      "reference_answer": "Depthwise separable chia phép toán thành depthwise (3x3x32 ≈ 288 tham số) và pointwise (1x1x32x64 ≈ 2048 tham số), tổng ≈ 2k so với convolution chuẩn 3x3x32x64 ≈ 18k, tức giảm khoảng 9 lần.",
      "qa_answer": "Bởi vì depthwise separable chia thành depthwise (3x3 trên mỗi trong 32 kênh) và pointwise (1x1×32×64) nên tham số = 3×3×32 + 1×1×32×64 ≈ 2000, trong khi convolution chuẩn là 3×3×32×64 ≈ 18000; tức giảm khoảng 9 lần."
    },
    {
      "case_id": "quiz_eval_003",
      "question": "Tại sao Transformer cố gắng giảm 'độ dài tương tác' giữa mọi cặp từ trong câu và lợi ích chính của điều này là gì?",
      "embedding_similarity": 0.8195,
      "reference_answer": "Giảm 'độ dài tương tác' rút ngắn đường truyền thông tin giữa các từ, giúp mô hình nắm bắt dependencies dài hạn nhanh hơn và cho phép thực hiện tính toán song song để tận dụng GPU, từ đó tăng tốc xử lý.",
      "qa_answer": "Transformer giảm \"độ dài tương tác\" giữa các cặp từ để rút ngắn đường truyền thông tin (ít bước tuần tự) giữa các từ. Lợi ích chính là tận dụng được tính toán song song của GPU và giảm chi phí so với kiến trúc tuần tự."
    },
    {
      "case_id": "quiz_eval_004",
      "question": "LSTM tạo hidden state từ cell state như thế nào và vai trò của output gate trong việc dự đoán là gì?",
      "embedding_similarity": 0.7651,
      "reference_answer": "Hidden state S_t được tính bằng cách lấy tanh(C_t) nhân với output gate O_t (S_t = O_t * tanh(C_t)); sau đó S_t được nhân với vector trọng số V và qua hàm kích hoạt giống ANN để dự đoán output.",
      "qa_answer": "LSTM tạo hidden state ST bằng cách cho cell state CT đi qua hàm tanh rồi nhân với output gate OT, trong đó OT quyết định có lấy thông tin từ CT hay không. Hidden state ST sau đó được nhân với vector V và qua hàm kích hoạt giống ANN để dự đoán output."
    },
    {
      "case_id": "quiz_eval_005",
      "question": "Trong cơ chế attention áp dụng cho dịch máy, vì sao trạng thái ẩn của decoder được coi là 'query' trong khi các trạng thái ẩn của encoder được coi là 'value'?",
      "embedding_similarity": 0.8112,
      "reference_answer": "Vì khi giải mã, trạng thái ẩn của decoder biểu diễn thông tin cần tìm (query) còn các trạng thái ẩn của encoder chứa các thông tin nguồn sẵn có (value); attention sử dụng query để tra cứu và gán trọng số cho các value phù hợp.",
      "qa_answer": "Bởi vì trong dịch máy quá trình giải mã là một phép tra cứu: trạng thái ẩn của decoder đóng vai trò \"query\" để tìm/so khớp các trạng thái ẩn của encoder, còn các trạng thái encoder là \"value\" tức là thông tin được tra cứu."
    },
    {
      "case_id": "quiz_eval_006",
      "question": "Một giá trị loss liên tục giảm trong quá trình huấn luyện cho biết điều gì về quá trình học và hội tụ của mô hình?",
      "embedding_similarity": 0.6396,
      "reference_answer": "Loss liên tục giảm thường cho thấy mô hình đang học và tiến tới hội tụ với hiệu suất trên tập huấn luyện được cải thiện. Tuy nhiên cần kiểm tra loss trên tập validation để xác định xem có overfitting hay không.",
      "qa_answer": "Cho thấy mô hình đang học tốt và đang hội tụ (giá trị loss giảm dần), đồng thời cho phép theo dõi tốc độ hội tụ qua history và biểu đồ."
    },
    {
      "case_id": "quiz_eval_007",
      "question": "Trong bài toán dịch máy, mô tả ngắn gọn cách các loss tại các time-step được sử dụng để cập nhật các trọng số U, V, W của mô hình.",
      "embedding_similarity": 0.8659,
      "reference_answer": "Các loss từ từng time-step được tổng hợp và lan truyền ngược (BPTT) xuyên suốt mạng, và các gradient thu được từ mọi bước thời gian được dùng để cập nhật các trọng số U, V, W.",
      "qa_answer": "Các loss ở các time-step được lan truyền ngược (backpropagation) xuyên suốt mạng và toàn bộ các loss này được dùng để cập nhật các trọng số U, V, W."
    },
    {
      "case_id": "quiz_eval_008",
      "question": "Tại sao phép toán vector king − man + woman ≈ queen cho thấy Word2Vec nắm được mối quan hệ ngữ nghĩa giữa từ?",
      "embedding_similarity": 0.7296,
      "reference_answer": "Vì Word2Vec học các embedding sao cho mối quan hệ ngữ nghĩa được mã hóa dưới dạng hướng và độ lệch trong không gian vector; phép cộng/trừ dịch chuyển vector theo hướng biểu diễn quan hệ (ví dụ 'nam→nữ'), nên vec gần nhất sau phép toán là từ tương ứng (queen).",
      "qa_answer": "Bởi vì embedding biểu diễn mối quan hệ ngữ nghĩa dưới dạng các offset vector: hiệu king − man trích ra đặc tính tương ứng và khi cộng woman thu được một vector gần queen, cho thấy các quan hệ ngữ nghĩa được mã hóa nhất quán trong không gian vector."
    },
    {
      "case_id": "quiz_eval_009",
      "question": "Tại sao trong công thức cross-entropy của softmax regression lại có dấu âm, và vì sao log loss bằng 0 khi nhãn one-hot trùng khớp hoàn toàn với dự đoán Ŷ?",
      "embedding_similarity": 0.6711,
      "reference_answer": "Dấu âm làm cho giá trị -sum(y * log(Ŷ)) dương vì log(Ŷ) thường là số âm khi Ŷ<1; nếu nhãn one-hot trùng khớp hoàn toàn (prob của lớp đúng = 1) thì log(1)=0 nên log loss bằng 0.",
      "qa_answer": "Công thức cross-entropy được viết kèm dấu âm (lưu ý dấu âm trong công thức cross-entropy). Khi nhãn ở dạng one-hot trùng khớp hoàn toàn với dự đoán Ŷ thì theo giải thích trong video giá trị log loss bằng 0."
    },
    {
      "case_id": "quiz_eval_010",
      "question": "Trong Softmax Regression, khi nào nên sử dụng biểu diễn one-hot cho nhãn y và khi nào nên dùng binary coding?",
      "embedding_similarity": 0.8786,
      "reference_answer": "One-hot được dùng khi mỗi mẫu có đúng một nhãn duy nhất (multi-class), còn binary coding dùng khi một mẫu có thể thuộc nhiều lớp cùng lúc (multi-label).",
      "qa_answer": "Dùng one-hot khi mỗi mẫu chỉ có đúng một nhãn (single-label). Dùng binary coding khi là bài toán đa nhãn, tức một mẫu có thể thuộc nhiều lớp cùng lúc."
    },
    {
      "case_id": "quiz_eval_011",
      "question": "Tại sao việc thêm kết nối skip (cộng x) trong ResNet giúp giảm hiện tượng vanishing gradient và đồng thời cải thiện việc giữ thông tin đặc trưng?",
      "embedding_similarity": 0.7012,
      "reference_answer": "Skip connection tạo một đường truyền đạo hàm trực tiếp (identity shortcut) làm tăng giá trị đạo hàm để gradient không bị biến mất; đồng thời nó giữ lại đặc trưng gốc x và kết hợp với các đặc trưng mới do lớp residual học được.",
      "qa_answer": "Vì trong ResNet việc cộng thêm x (skip connection) làm tăng giá trị đạo hàm của các thành phần trong hàm loss nên giảm hiện tượng vanishing gradient; đồng thời phép cộng này giữ lại đặc trưng gốc (input) kết hợp với các đặc trưng mới nên cải thiện việc bảo tồn thông tin."
    },
    {
      "case_id": "quiz_eval_012",
      "question": "Tại sao hai feature map cùng phát hiện biên theo chiều dòng nhưng lại phản hồi khác nhau (một kích hoạt khi biên từ sáng→tối, còn một kích hoạt khi từ tối→sáng)?",
      "embedding_similarity": 0.7508,
      "reference_answer": "Vì chúng tương ứng với hai filter khác nhau, mỗi filter nhạy cảm với một polarity của biên (một xác định chuyển từ sáng→tối, còn kia xác định chuyển từ tối→sáng), nên cùng kiểu biên nhưng phản ứng khác nhau.",
      "qa_answer": "Vì chúng là kết quả của hai filter khác nhau: một filter phát hiện biên chuyển từ sáng→tối và kích hoạt, còn filter kia phát hiện biên chuyển từ tối→sáng và kích hoạt."
    },
    {
      "case_id": "quiz_eval_013",
      "question": "Tại sao kiến trúc Bidirectional RNN được dùng khi biểu diễn từ trong một câu?",
      "embedding_similarity": 0.8575,
      "reference_answer": "Bidirectional RNN tổng hợp ngữ cảnh từ cả hai phía (trái→phải và phải→trái) để nắm được ảnh hưởng của cả từ trước và từ sau lên ý nghĩa của một từ. Thường thực hiện bằng cách ghép (concatenate) vector trạng thái ẩn từ hai chiều để có biểu diễn đầy đủ cho mỗi vị trí từ.",
      "qa_answer": "Vì cần tổng hợp ngữ cảnh cả trái và phải: Bidirectional RNN duyệt chuỗi theo cả hai chiều và ghép (concatenate) các vector trạng thái ẩn trái→phải và phải→trái để có biểu diễn đầy đủ cho từng từ, giúp hiểu các từ phụ thuộc ngữ cảnh sau như “terribly” khi có “exciting” phía sau."
    },
    {
      "case_id": "quiz_eval_014",
      "question": "Trong phép so sánh attention với một search engine, trạng thái ẩn của decoder tương ứng với gì, các trạng thái ẩn của encoder tương ứng với gì, và quá trình decode được xem như hoạt động gì?",
      "embedding_similarity": 0.9534,
      "reference_answer": "Trạng thái ẩn của decoder được xem như query, các trạng thái ẩn của encoder được xem như values (hoặc keys/values), và quá trình decode là một phép tra cứu/tìm kiếm để chọn value cần chú ý.",
      "qa_answer": "Trạng thái ẩn của decoder tương ứng với query, các trạng thái ẩn của encoder tương ứng với values, và quá trình decode được xem như một quá trình tra cứu (search) để xác định value cần chú ý."
    },
    {
      "case_id": "quiz_eval_015",
      "question": "Tại sao ở lớp output của mô hình phân lớp 10 lớp ta dùng activation softmax thay vì sigmoid?",
      "embedding_similarity": 0.648,
      "reference_answer": "Softmax chuyển logits thành một phân phối xác suất trên các lớp (tổng bằng 1), phù hợp cho phân lớp nhiều lớp, trong khi sigmoid cho xác suất độc lập thường dùng cho phân lớp nhị phân hoặc multi-label.",
      "qa_answer": "Vì đây là bài phân lớp đa lớp với 10 class nên dùng activation softmax cho phân lớp đa lớp; sigmoid chỉ dùng cho phân lớp nhị phân."
    },
    {
      "case_id": "quiz_eval_016",
      "question": "Trong biến thể attention 'matrix reduction', vì sao ta ánh xạ query h và key/side s vào một không gian trung gian bằng các ma trận u, v, và tại sao thường chọn u, v có rank thấp?",
      "embedding_similarity": 0.7113,
      "reference_answer": "Ánh xạ giúp đưa h và s về cùng một không gian để có thể thực hiện tích vô hướng giữa chúng; chọn u, v có rank thấp giảm số tham số và chi phí tính toán trong khi vẫn nắm bắt được cấu trúc tương quan chính.",
      "qa_answer": "Để chiếu query h và key/side s lên cùng một không gian trung gian (\"mặt tranh\") nhằm cho chúng cùng chiều để thực hiện phép tích vô hướng bằng các ma trận u, v. Ta thường chọn u, v có rank thấp (k << d1,d2) để giảm số tham số và chi phí tính toán."
    },
    {
      "case_id": "quiz_eval_017",
      "question": "Trong cơ chế cross-attention của decoder trong Transformer, query, key và value lần lượt được lấy từ đâu?",
      "embedding_similarity": 0.9741,
      "reference_answer": "Query được lấy từ input/hidden state hiện tại của decoder; key và value được lấy từ các hidden states/output của encoder (s_e, h1..ht).",
      "qa_answer": "Query được lấy từ input của decoder; key và value được lấy từ output/hidden states của encoder (s_e, h1..ht)."
    },
    {
      "case_id": "quiz_eval_018",
      "question": "Khi chuẩn bị y_train ở dạng 'one-hot 100', điều này có nghĩa là gì?",
      "embedding_similarity": 0.748,
      "reference_answer": "Nó có nghĩa là các nhãn được mã hóa dưới dạng vector one-hot có chiều dài 100, trong đó mỗi vector có giá trị 1 tại chỉ mục của lớp đúng và 0 ở các vị trí còn lại, tương ứng với 100 lớp.",
      "qa_answer": "y_train ở dạng \"one-hot 100\" nghĩa là các nhãn được mã hoá thành vector one-hot có 100 phần tử."
    },
    {
      "case_id": "quiz_eval_019",
      "question": "Trong ví dụ phép convolution với ảnh 5x5 và filter 3x3 chỉ có giá trị 0/1, chức năng của các phần tử 0 và 1 trong filter khi tính một giá trị trên feature map là gì?",
      "embedding_similarity": 0.8164,
      "reference_answer": "Phần tử 1 giữ và cộng các giá trị tương ứng trên vùng ảnh (đóng góp vào tổng), còn phần tử 0 loại bỏ các giá trị đó (không đóng góp); sau khi nhân từng ô với filter, các tích được cộng lại để cho giá trị trên feature map và filter được trượt để tính các vị trí khác.",
      "qa_answer": "Các phần tử 0 khi nhân với pixel tương ứng sẽ loại bỏ/bỏ qua các giá trị đó, còn phần tử 1 giữ và cộng các thành phần tương ứng vào tổng để tính giá trị trên feature map."
    },
    {
      "case_id": "quiz_eval_020",
      "question": "Nêu trade-off chính giữa phương pháp một giai đoạn (ví dụ YOLO v3) và hai giai đoạn (ví dụ Faster R-CNN) trong bài toán phát hiện đối tượng.",
      "embedding_similarity": 0.968,
      "reference_answer": "Trade-off chính là tốc độ đổi lấy độ chính xác: phương pháp một giai đoạn như YOLO v3 nhanh hơn (≈45 FPS) nhưng thường kém chính xác hơn khoảng ~10% so với phương pháp hai giai đoạn như Faster R-CNN (≈7 FPS).",
      "qa_answer": "Trade-off chính là tốc độ so với độ chính xác: phương pháp một giai đoạn (YOLO v3) nhanh hơn (~45 FPS) nhưng có độ chính xác thấp hơn, trong khi phương pháp hai giai đoạn (Faster R-CNN) chậm hơn (~7 FPS) nhưng khoảng 10% chính xác hơn."
    },
    {
      "case_id": "quiz_eval_021",
      "question": "Tại sao gradient theo ma trận W trong RNN có xu hướng \"biến mất\" khi lan truyền ngược qua nhiều bước thời gian?",
      "embedding_similarity": 0.783,
      "reference_answer": "Vì khi dùng quy tắc chuỗi, ta nhân liên tiếp các đạo hàm trạng thái giữa các bước thời gian và nhiều đạo hàm thành phần có trị tuyệt đối <1, nên tích các đạo hàm thu nhỏ dần khiến gradient theo W (và tương tự U, V) trở nên rất nhỏ.",
      "qa_answer": "Vì đạo hàm theo W được tính theo quy tắc chuỗi là tích các đạo hàm giữa các bước thời gian (Fn theo Fn-1, Fn-1 theo Fn-2, …), và hầu hết các đạo hàm thành phần có trị tuyệt đối < 1 nên tích này co lại khi lan truyền ngược qua nhiều bước thời gian (vanishing gradient)."
    },
    {
      "case_id": "quiz_eval_022",
      "question": "Giải thích mục đích của hệ số 1/(2n) và lý do xuất hiện số 2 trong công thức MSE trong hồi quy tuyến tính.",
      "embedding_similarity": 0.8654,
      "reference_answer": "Hệ số 1/(2n) chuẩn hóa tổng sai số theo số mẫu để giá trị loss không phụ thuộc vào kích thước dữ liệu; số 2 được thêm vào vì khi lấy đạo hàm của bình phương lỗi nó sẽ đem xuống và triệt tiêu với 1/2, giúp công thức gradient đơn giản hơn.",
      "qa_answer": "Hệ số 1/(2n) dùng để chuẩn hóa theo số mẫu (lấy trung bình lỗi) nhằm có thang đo hợp lý cho sai số; số 2 xuất hiện do bình phương sai số và được đặt để khi tính đạo hàm đem 2 xuống sẽ triệt tiêu với 1/2, làm công thức đạo hàm gọn hơn."
    },
    {
      "case_id": "quiz_eval_023",
      "question": "Tại sao gradient trong RNN có xu hướng biến mất khi lan truyền qua nhiều bước thời gian?",
      "embedding_similarity": 0.8708,
      "reference_answer": "Vì khi nhân liên tiếp các đạo hàm có giá trị nhỏ hơn 1 (ví dụ 0.9^20), tích này suy giảm theo cấp số nhân tiến về 0, khiến gradient trở nên rất nhỏ và biến mất qua nhiều bước thời gian.",
      "qa_answer": "Vì các đạo hàm thành phần <1 khi nhân liên tiếp qua nhiều bước thời gian tương đương lấy lũy thừa (ví dụ 0.9^20), nên tích các đạo hàm sẽ tiến về 0 và làm gradient biến mất."
    },
    {
      "case_id": "quiz_eval_024",
      "question": "Khi thiết kế mô hình hồi quy tuyến tính, nêu ba thành phần chính cần xác định.",
      "embedding_similarity": 0.8118,
      "reference_answer": "Ba thành phần chính là: (1) chọn hàm dự đoán (ví dụ y = ax + b), (2) xác định hàm mất mát để đánh giá sai số, và (3) tìm tham số θ tối ưu (ví dụ bằng gradient descent).",
      "qa_answer": "Ba thành phần chính là: thiết kế hàm dự đoán, thiết kế hàm độ lỗi, và tìm tham số theta sao cho hàm độ lỗi được tối thiểu (ví dụ bằng gradient descent)."
    },
    {
      "case_id": "quiz_eval_025",
      "question": "Khi trực quan hóa một filter thấy bên trái tối và bên phải sáng (hoặc ngược lại), filter đó đang phát hiện đặc trưng gì trong ảnh và ý nghĩa của chuyển từ tối→sáng so với sáng→tối là gì?",
      "embedding_similarity": 0.7149,
      "reference_answer": "Đó là một bộ lọc phát hiện cạnh theo chiều dọc; chuyển từ tối→sáng phát hiện cạnh từ vùng tối sang vùng sáng, còn chuyển từ sáng→tối phát hiện cạnh theo hướng ngược lại.",
      "qa_answer": "Filter đó phát hiện các biên (mép) theo chiều dọc; khác nhau ở chiều chuyển cường độ: một filter bắt vùng chuyển từ tối→sáng còn filter kia bắt vùng chuyển từ sáng→tối."
    },
    {
      "case_id": "quiz_eval_026",
      "question": "Tại sao cần gọi lớp flatten trên S4 trước khi thêm lớp dense (fully connected)?",
      "embedding_similarity": 0.6861,
      "reference_answer": "Flatten chuyển tensor nhiều chiều từ S4 thành vector 1D để lớp dense có thể nhận đầu vào và kết nối đầy đủ; nó chỉ thay đổi hình dạng chứ không thay đổi giá trị.",
      "qa_answer": "Bởi vì để thực hiện phép fully connected cần một bước flatten trên S4 để tạo ra FC4 trước khi thêm lớp dense. Bước flatten ở đây chỉ reshape (không thực hiện biến đổi gì) dữ liệu từ S4."
    },
    {
      "case_id": "quiz_eval_027",
      "question": "Trong logistic regression với log loss, nếu y = 1 nhưng mô hình dự đoán ŷ → 0, điều gì xảy ra với giá trị log loss và tại sao log loss nhạy hơn mean squared error trong trường hợp dự đoán sai này?",
      "embedding_similarity": 0.8407,
      "reference_answer": "Log loss sẽ tăng tới +∞ vì loss = -log(ŷ) và log của một số rất nhỏ cho kết quả rất lớn; log loss nhạy hơn MSE vì nó phạt nặng các dự đoán xác suất thấp cho nhãn đúng, đặc biệt khi mô hình quá tự tin nhưng sai.",
      "qa_answer": "Log loss → +∞ vì khi y=1 và ŷ→0 thì loss = −log(ŷ) diverges (→ +∞). Log loss nhạy hơn mean squared error vì nó phạt mạnh các dự đoán sai (đặc biệt khi mô hình rất tự tin sai), dẫn đến lỗi rất lớn."
    },
    {
      "case_id": "quiz_eval_028",
      "question": "Vai trò chính của Atrous Spatial Pyramid Pooling (ASPP) trong DeepLab3 là gì?",
      "embedding_similarity": 0.756,
      "reference_answer": "ASPP kết hợp các đặc trưng từ nhiều receptive field bằng các dilated convolution với các rates khác nhau để nắm bắt ngữ cảnh đa quy mô, rồi dùng 1x1 convolution để tổng hợp và upsample cho kết quả phân đoạn độ phân giải cao.",
      "qa_answer": "Vai trò chính của Atrous Spatial Pyramid Pooling (ASPP) trong DeepLab3 là kết hợp thông tin đa tỉ lệ bằng các dilated convolution với nhiều rates khác nhau, concat các feature này và tổng hợp bằng 1×1 conv rồi upsampling để ra kết quả phân đoạn độ phân giải cao."
    },
    {
      "case_id": "quiz_eval_029",
      "question": "Trong softmax regression, nhãn dạng one-hot trong công thức cross-entropy cho một mẫu có vai trò gì và vì sao công thức có dấu âm?",
      "embedding_similarity": 0.7345,
      "reference_answer": "Nhãn one-hot chọn xác suất dự đoán tương ứng với lớp thật nên cross-entropy một mẫu trở thành -log(p̂_true). Dấu âm được dùng để chuyển việc tối đa hóa likelihood thành tối thiểu hóa negative log-likelihood và đảm bảo giá trị loss không âm.",
      "qa_answer": "Nhãn one-hot biểu diễn lớp thực tế (ví dụ [0,1,0] là lớp thứ 2) và được so sánh với dự đoán Ŷ, khi trùng nhãn thì log loss bằng 0.  \nKhông tìm thấy thông tin trong nguồn về lý do có dấu âm trong công thức."
    },
    {
      "case_id": "quiz_eval_030",
      "question": "Vai trò chính của word embeddings (ví dụ GloVe) trong xử lý ngôn ngữ tự nhiên là gì?",
      "embedding_similarity": 0.8013,
      "reference_answer": "Word embeddings chuyển từ biểu diễn chuỗi thành vector số để biểu diễn ngữ nghĩa và quan hệ giữa từ, cho phép thực hiện các phép toán đại số trên từ. Chúng thường được dùng làm đầu vào cho các mô hình học tiếp theo.",
      "qa_answer": "Chuyển từ dạng chuỗi sang vector để mô hình học có thể thực hiện các phép toán đại số; đồng thời embeddings được dùng làm đầu vào cho các mô hình học tiếp theo."
    },
    {
      "case_id": "quiz_eval_031",
      "question": "Vì sao Faster R-CNN bị cho là chậm và ý tưởng chính của các mô hình one-stage (ví dụ YOLO) là gì?",
      "embedding_similarity": 0.9385,
      "reference_answer": "Faster R-CNN chậm vì tách thành hai giai đoạn: RPN sinh region proposals rồi detector tinh chỉnh vị trí và phân loại; one-stage như YOLO thực hiện dự đoán bounding boxes và classes trực tiếp end-to-end từ ảnh đầu vào để tăng tốc.",
      "qa_answer": "Faster R-CNN bị cho là chậm vì tách làm hai giai đoạn: RPN sinh bounding box rồi detector tinh chỉnh vị trí và gán nhãn, nên phải chạy hai bước. Ý tưởng của mô hình one-stage (ví dụ YOLO) là thực hiện end-to-end trong một giai đoạn, trực tiếp từ ảnh đầu vào dự đoán bounding boxes và classes."
    },
    {
      "case_id": "quiz_eval_032",
      "question": "Khi so sánh hai vector từ (ví dụ 'win' và 'kai'), phép đo nào thường được dùng và tại sao cần so sánh giá trị similarity với nhiều từ khác nhau?",
      "embedding_similarity": 0.9073,
      "reference_answer": "Thường dùng tích vô hướng hoặc cosine similarity để đo độ tương đồng hướng giữa hai vector; cần so sánh với nhiều từ khác nhau vì giá trị similarity tuyệt đối khó diễn giải một mình và cần ngữ cảnh tương đối để đánh giá mức độ giống nhau.",
      "qa_answer": "Thường dùng tích vô hướng / cosine similarity để so sánh vector từ; cần so sánh giá trị similarity với nhiều từ khác nhau vì một giá trị đơn lẻ khó hiểu và phải đối chiếu với các từ khác để đánh giá mức tương đồng."
    },
    {
      "case_id": "quiz_eval_033",
      "question": "Trong mô hình Continuous Bag-of-Words (CBOW) của Word2Vec, vector ngữ cảnh h được tính như thế nào và được dùng để làm gì?",
      "embedding_similarity": 0.7479,
      "reference_answer": "Vector h được tạo bằng cách cộng hoặc lấy trung bình các vector của các từ ngữ cảnh. H sau đó được đưa vào một mạng neural và dùng softmax để dự đoán từ ở giữa (target).",
      "qa_answer": "Trong CBOW, vector ngữ cảnh h được tính bằng cách lấy tổng hoặc trung bình các vector tương ứng của các từ ngữ cảnh; h sau đó được đưa vào một mạng neural để dự đoán từ ở giữa (target) bằng softmax."
    },
    {
      "case_id": "quiz_eval_034",
      "question": "Mục đích của việc sử dụng padding khi chuẩn bị dữ liệu chuỗi cho mô hình RNN/LSTM là gì?",
      "embedding_similarity": 0.4972,
      "reference_answer": "Padding chuẩn hóa độ dài các chuỗi bằng cách chèn giá trị (ví dụ 0) vào đầu hoặc cuối để đưa tất cả chuỗi về cùng kích thước, giúp đóng gói batch và xử lý đồng thời trong mô hình.",
      "qa_answer": "Mục đích của padding là đưa các đoạn chuỗi có độ dài khác nhau về cùng một kích thước để có thể đóng gói và đưa vào mô hình RNN/LSTM."
    },
    {
      "case_id": "quiz_eval_035",
      "question": "Giải thích ngắn gọn tại sao chế độ huấn luyện \"work-to-back\" có thể nhanh hơn \"work-to-end\" khi dùng RNN/LSTM, và vai trò của việc load embedding trong quá trình này?",
      "embedding_similarity": 0.7988,
      "reference_answer": "Work-to-back thường xử lý theo các chunk/ngắn hơn hoặc cho phép thực thi từng phần nên giảm tổng thời gian so với xử lý toàn bộ sequence (work-to-end). Đồng thời embedding layer có thể được load trước hoặc song song với quá trình huấn luyện (hoặc chỉ load một lần), giúp tiết kiệm thời gian khởi tạo.",
      "qa_answer": "Work-to-back chạy nhanh hơn trong ví dụ (khoảng 3–4 phút so với ~16 phút của work-to-end). Trong quá trình work-to-back, lớp embedding được load lên (việc load embedding diễn ra trong quá trình đó)."
    },
    {
      "case_id": "quiz_eval_036",
      "question": "Tại sao ta dùng softmax để chuẩn hóa attention scores thành Attention Distribution và ý nghĩa của một trọng số cao trong phân bố này là gì?",
      "embedding_similarity": 0.7731,
      "reference_answer": "Softmax chuẩn hóa các attention scores thành phân bố xác suất (giá trị trong [0,1] và tổng bằng 1) để decoder biết mức độ quan trọng tương đối của từng từ; một trọng số cao nghĩa là mô hình tập trung nhiều thông tin từ vị trí đó khi tạo context cho bước decode hiện tại.",
      "qa_answer": "Dùng softmax để chuẩn hóa attention scores (dot product) thành Attention Distribution trong không gian [0,1] như một phân bố xác suất, nhằm biết từ nào được “để ý” nhiều hơn khi decode. Trọng số cao trong phân bố này nghĩa là mô hình sẽ chú ý nhiều hơn tới từ đó và tổng hợp thông tin từ vị trí nguồn tương ứng nhiều nhất."
    },
    {
      "case_id": "quiz_eval_037",
      "question": "Tại sao quá trình giải mã (decoding) trong Transformer dẫn đến tính tuần tự và giải pháp nào được dùng để vừa ngăn mô hình thấy từ tương lai vừa cho phép tính toán song song?",
      "embedding_similarity": 0.8007,
      "reference_answer": "Giải mã là tuần tự vì mỗi từ phải được dự đoán dựa trên các từ đã sinh trước đó nên không được dùng thông tin tương lai. Giải pháp là áp dụng multi-head self-attention có masking (gán score rất âm cho vị trí tương lai) để chặn nhìn thấy từ sau và vẫn tính toán song song cho các vị trí hiện tại.",
      "qa_answer": "Quá trình decoding dẫn đến tính tuần tự vì mỗi từ phải được dự đoán theo thứ tự và dự đoán trước được dùng để ảnh hưởng tới dự đoán sau, nên không thể song song hóa trực tiếp. Giải pháp là dùng multi-head self-attention có masking (gán score lớn âm cho các vị trí tương lai) để mô hình không thấy từ sau nhưng vẫn tính toán song song các trạng thái trong từng tầng."
    },
    {
      "case_id": "quiz_eval_038",
      "question": "Mô tả chức năng chính của input gate và output gate trong cell LSTM.",
      "embedding_similarity": 0.985,
      "reference_answer": "Input gate (I) dùng hàm sigmoid để quyết định có nạp thông tin x_t vào cell state hay không (lọc các từ/chi tiết ít quan trọng); output gate (O) dùng sigmoid để quyết định có cho phép thông tin từ cell state C_t ra để tạo output S_t hay không.",
      "qa_answer": "Input gate (I) dùng hàm sigmoid quyết định có nạp thông tin x_t vào cell state hay không, lọc bỏ các từ ít quan trọng (ví dụ mạo từ, giới từ). Output gate (O) dùng hàm sigmoid quyết định có cho phép thông tin từ cell state C_t ra để tạo output S_t hay không."
    },
    {
      "case_id": "quiz_eval_039",
      "question": "Vì sao phép toán trong RNN không thể được song song hóa và hệ quả chính của tính tuần tự này là gì?",
      "embedding_similarity": 0.6976,
      "reference_answer": "Vì mỗi trạng thái ẩn phụ thuộc vào các phép tính trước nên tính toán phải thực hiện tuần tự, dẫn tới vanishing gradients trên các dependency dài và không tận dụng được khả năng song song của GPU, đây là động lực để đề xuất kiến trúc Transformer.",
      "qa_answer": "Phép toán trong RNN không thể song song hóa vì mỗi trạng thái ẩn phụ thuộc vào các phép tính trước đó nên phải tính tuần tự (từ trái sang phải) cả forward và backward. Hệ quả chính là dependency dài gây vanishing gradients và không tận dụng được song song trên GPU, dẫn tới chi phí huấn luyện lớn (O(L^2))."
    },
    {
      "case_id": "quiz_eval_040",
      "question": "Trong ứng dụng trợ lý ảo (ví dụ ChatGPT/GPT-4, Co-Pilot), vai trò chính của Transformer là gì?",
      "embedding_similarity": 0.7513,
      "reference_answer": "Transformer cho phép trợ lý ảo tự động tiếp nối hoặc sinh văn bản (dịch, soạn email, gợi ý hoàn thiện câu) bằng cách dựa trên nội dung trao đổi trước đó để dự đoán phần tiếp theo.",
      "qa_answer": "Vai trò chính của Transformer là tự động tiếp nối văn bản dựa trên nội dung trao đổi trước đó (ví dụ dịch, soạn email, gợi ý hoàn thiện câu)."
    },
    {
      "case_id": "quiz_eval_041",
      "question": "Layer Normalization trong encoder của Transformer được áp dụng ở giai đoạn nào và mục đích chính của nó là gì?",
      "embedding_similarity": 0.8198,
      "reference_answer": "LayerNorm được áp dụng sau phép cộng/residual trước khi bước biến đổi tiếp theo; mục đích chính là chuẩn hóa đầu vào theo từng layer (mean=0, std=1) để giảm biến động và cải thiện độ bền/tốc độ huấn luyện.",
      "qa_answer": "Layer Normalization được áp dụng sau phép cộng (residual) trước bước biến đổi tiếp theo trong encoder. Mục đích là chuẩn hóa đầu vào trên mỗi layer về phân bố chuẩn (mean=0, std=1) để giảm biến động đầu vào và cải thiện độ bền cùng tốc độ huấn luyện."
    },
    {
      "case_id": "quiz_eval_042",
      "question": "Mô tả ngắn gọn nguyên lý hoạt động cơ bản của mô hình encoder–decoder trong bài toán dịch máy.",
      "embedding_similarity": 0.8171,
      "reference_answer": "Encoder mã hóa toàn bộ chuỗi đầu vào thành một vector biểu diễn tổng hợp, còn decoder sử dụng vector này để sinh dần chuỗi đầu ra.",
      "qa_answer": "Mô hình encoder–decoder mã hóa toàn bộ input thành một vector biểu diễn chứa toàn bộ thông tin của input, rồi decoder sinh lần lượt output từ vector đó."
    },
    {
      "case_id": "quiz_eval_043",
      "question": "Viết công thức vector hóa của cơ chế self‑attention trong encoder và nêu ngắn gọn bước tổng hợp đầu ra.",
      "embedding_similarity": 0.82,
      "reference_answer": "Attention(X) = softmax((XW_q)(XW_k)^T) XW_v. Tức là chiếu X thành Q,K,V, tính scores = QK^T, áp softmax để ra ma trận a rồi nhân với V để tổng hợp output.",
      "qa_answer": "Công thức vector hóa: Attention(x) = softmax((xW_q)(xW_k)^T) (xW_v). Bước tổng hợp: chiếu x ra Q,K,V, tính score = QK^T, áp softmax ra ma trận a rồi nhân a với V để thu đầu ra."
    },
    {
      "case_id": "quiz_eval_044",
      "question": "Vai trò chính của vector mà encoder tạo ra trong mô hình encode–decode là gì?",
      "embedding_similarity": 0.8455,
      "reference_answer": "Vector này tích hợp thông tin toàn bộ của input và được decoder sử dụng để sinh ra chuỗi output.",
      "qa_answer": "Vai trò chính là biểu diễn tích hợp toàn bộ thông tin của input dưới dạng một vector, và decoder sẽ dùng vector đó để sinh ra output."
    },
    {
      "case_id": "quiz_eval_045",
      "question": "Mục đích chính của việc sử dụng Deep (stacked) RNN và lợi ích khi kết hợp nó với kiến trúc bidirectional là gì?",
      "embedding_similarity": 0.8634,
      "reference_answer": "Deep (stacked) RNN tăng độ sâu để học các đặc trưng ở nhiều cấp độ (thấp, trung, cao); khi kết hợp với bidirectional, mạng có thể tổng hợp thông tin theo cả hai chiều và ở nhiều cấp độ cùng lúc.",
      "qa_answer": "Mục đích chính là tăng độ sâu (deep/stacked RNN) để mạng học được đặc trưng ở nhiều cấp độ (thấp, trung gian, cao). Kết hợp với kiến trúc bidirectional thì tạo thành bidirectional deep-stacked RNN, giúp tổng hợp thông tin hai chiều và đa cấp."
    },
    {
      "case_id": "quiz_eval_046",
      "question": "Trong ví dụ trực quan hóa CNN, feature map được quan sát phản ánh đặc trưng hình học nào trên áo?",
      "embedding_similarity": 0.6841,
      "reference_answer": "Feature map phản ánh nếp nhăn trên áo chứ không chỉ sự xuất hiện của quần áo; khi nếp nhăn bị phủi mất thì vùng sáng tương ứng trên feature map giảm hoặc biến mất.",
      "qa_answer": "Feature map phản ánh các nếp nhăn (crease/wrinkle) trên áo."
    },
    {
      "case_id": "quiz_eval_047",
      "question": "Thay đổi chính trong biến thể cnsv3 so với phiên bản trước của mạng CNN là gì?",
      "embedding_similarity": 0.8258,
      "reference_answer": "Cnsv3 loại bỏ các lớp pooling và cập nhật code để nối trực tiếp các biến đầu ra của các lớp convolution (ví dụ c1, c3) mà không qua pooling.",
      "qa_answer": "Biến thể cnsv3 loại bỏ lớp pooling và cập nhật mạng để nối trực tiếp các biến giữa các tầng (ví dụ c1, c3 được truyền thẳng)."
    },
    {
      "case_id": "quiz_eval_048",
      "question": "Tại sao khi truy xuất trọng số của mô hình RNN/LSTM trong ví dụ, ta lấy ANN.model.layers[2] thay vì layer 0 hoặc 1?",
      "embedding_similarity": 0.9181,
      "reference_answer": "Vì layer 0 là input và layer 1 là embedding (không chứa tham số huấn luyện chính), nên các tham số huấn luyện chính nằm ở layer 2, do đó ta dùng ANN.model.layers[2] để lấy weights.",
      "qa_answer": "Vì layer 0 là input và layer 1 là embedding không có tham số huấn luyện, tham số huấn luyện chính nằm ở layer 2, nên ta truy xuất ANN.model.layers[2] để lấy trọng số."
    },
    {
      "case_id": "quiz_eval_049",
      "question": "Tại sao trong cấu hình mạng fully connected cho bài toán phân lớp nhị phân ở video, lớp output chỉ dùng 1 nốt và activation = sigmoid?",
      "embedding_similarity": 0.7654,
      "reference_answer": "Vì bài toán là phân lớp nhị phân, một nốt đủ để biểu diễn xác suất thuộc về một lớp; hàm sigmoid chuyển đầu ra về khoảng 0–1, phù hợp để biểu diễn xác suất và dùng với loss như binary cross-entropy.",
      "qa_answer": "Vì đây là bài toán phân lớp nhị phân, nên lớp output được thiết kế là một nốt duy nhất và dùng activation = sigmoid."
    },
    {
      "case_id": "quiz_eval_050",
      "question": "Trong mô hình sequence-to-sequence, vai trò của hidden state của encoder là gì và vì sao cần một token đặc biệt 'start' để khởi tạo decoder?",
      "embedding_similarity": 0.9144,
      "reference_answer": "Hidden state của encoder tổng hợp và mã hóa thông tin của toàn bộ câu nguồn để làm cơ sở cho quá trình giải mã; token đặc biệt 'start' được dùng làm input đầu tiên cho decoder để báo hiệu bắt đầu tạo dự đoán và phải được dùng nhất quán trong dữ liệu, huấn luyện và inference.",
      "qa_answer": "Hidden state của encoder là một tổng hợp thông tin của toàn bộ câu nguồn và được dùng để khởi tạo quá trình decode. Token đặc biệt 'start' được truyền vào decoder để báo hiệu bắt đầu giải mã và phải đồng nhất trong tập dữ liệu, huấn luyện và inference."
    }
  ]
}