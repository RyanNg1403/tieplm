# Evaluation Module

Evaluate performance of 4 AI tasks: Q&A, Text Summary, Video Summary, Quiz Generation.

## ğŸ“ Structure

```
evaluation/
â”œâ”€â”€ text_summary/
â”‚   â”œâ”€â”€ eval_service.py        # Evaluation service
â”‚   â”œâ”€â”€ run_eval.py            # Evaluation runner script
â”‚   â”œâ”€â”€ test_questions.json    # Test dataset (50 questions)
â”‚   â””â”€â”€ results/               # Evaluation results (gitignored)
â”œâ”€â”€ qa/
â”‚   â”œâ”€â”€ eval_service.py        # TODO
â”‚   â”œâ”€â”€ run_eval.py            # TODO
â”‚   â””â”€â”€ results/
â”œâ”€â”€ video_summary/
â”‚   â”œâ”€â”€ eval_service.py        # TODO
â”‚   â”œâ”€â”€ run_eval.py            # TODO
â”‚   â””â”€â”€ results/
â””â”€â”€ quiz/
    â”œâ”€â”€ eval_service.py        # TODO
    â”œâ”€â”€ run_eval.py            # TODO
    â””â”€â”€ results/
```

**Task-Specific Structure:**
Each task folder contains:
- **Evaluation service**: Core evaluation logic
- **Runner script**: Script to execute evaluation
- **Test dataset**: Questions/test cases (JSON)
- **Results folder**: Evaluation results (stored locally, gitignored)

## âœ… Implemented

- âœ… Task-specific folder structure
- âœ… **Text Summary Evaluation**:
  - DeepEval with QAG (Question-Answer Generation) metrics
  - 50 test questions covering all 8 chapters
  - Evaluation service with comprehensiveness-focused prompts
  - Runner script with batch evaluation and statistics
- âœ… **Quiz QAG Evaluation**:
  - Random chunk sampling â†’ quiz question generation via quiz service
  - QA service answers using provided context only (short-answer & MCQ modes)
  - Short-answer metric: embedding cosine similarity
  - MCQ metric: accuracy of selected option (A/B/C/D/IDK)
  - Results saved under `evaluation/quiz/results/`

## âŒ TODO

- âŒ Q&A evaluation
- âŒ Video summary evaluation

## ğŸš€ Usage

### Text Summarization Evaluation

```bash
# Activate virtual environment
source .venv/bin/activate

# Navigate to text_summary folder
cd evaluation/text_summary

# Run all 50 questions
python run_eval.py --all

# Run specific range
python run_eval.py --start 0 --end 10

# Run specific questions
python run_eval.py --question-id sum_001 sum_002

# Results saved to: evaluation/text_summary/results/
```

### Other Tasks (TODO)

Similar structure for qa/, video_summary/, quiz/ when implemented.

## ğŸ“Š Evaluation Metrics

- **Text Summary**: 
  - **QAG-based** (DeepEval SummarizationMetric)
  - Coverage Score: Detail inclusion from original text
  - Alignment Score: Factual accuracy
  - Overall Score: min(coverage, alignment)
  
- **Q&A**: TBD (accuracy, source relevance)
- **Video Summary**: TBD (coverage, coherence)
- **Quiz**: TBD (question quality, difficulty)

## ğŸ”§ Configuration

Add to `.env`:
```bash
# Evaluation Configuration
EVAL_MODEL=gpt-5-mini                    # Model for evaluation
EVAL_SUMMARIZATION_THRESHOLD=0.5         # Pass/fail threshold
```
