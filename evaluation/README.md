# Evaluation Module

Evaluate performance of 4 AI tasks: Q&A, Text Summary, Video Summary, Quiz Generation.

## ğŸ“ Structure

```
evaluation/
â”œâ”€â”€ datasets/         # Test datasets for each task (JSON/CSV format, not in git)
â”œâ”€â”€ scripts/          # Evaluation runner scripts (one per task)
â””â”€â”€ metrics/          # Metric computation and evaluation logic
```

**Expected Folders:**
- **`datasets/`**: Ground truth evaluation data for each AI task (Q&A, Text Summary, Video Summary, Quiz)
- **`scripts/`**: Python scripts to run evaluations and generate reports
- **`metrics/`**: Metric calculators (ROUGE, accuracy, relevance scoring, etc.)

## âœ… Implemented

- âœ… Folder structure (`datasets/`, `scripts/`, `metrics/`)
- âœ… Script skeletons (placeholders for each task)
- âœ… Evaluator class skeleton

## âŒ TODO

- âŒ Create evaluation datasets
- âŒ Q&A evaluation metrics (accuracy, relevance)
- âŒ Summary evaluation (ROUGE, coherence)
- âŒ Video summary evaluation
- âŒ Quiz evaluation (quality, difficulty)
- âŒ Implement evaluation scripts
- âŒ Results aggregation
- âŒ Visualization/reporting

## ğŸš€ Usage (After Implementation)

```bash
cd evaluation

# Run evaluation for specific task
python scripts/<task_eval_script>.py

# Example workflow:
# 1. Prepare ground truth datasets in datasets/
# 2. Run evaluation script (calls main system APIs)
# 3. Compute metrics using metrics/
# 4. Generate reports and visualizations
```

## ğŸ“Š Planned Metrics

- **Q&A**: Answer accuracy, source relevance, timestamp precision
- **Text Summary**: ROUGE scores, factual consistency
- **Video Summary**: Coverage, coherence, key points
- **Quiz**: Question quality, difficulty distribution

**Note**: Build this module after main features are complete.
