{
  "run_info": {
    "run_id": "run_20251114_080029",
    "timestamp": "20251114_080029",
    "total_questions": 5,
    "successful": 5,
    "failed": 0,
    "model": "gpt-5-mini"
  },
  "questions_metadata": {
    "description": "Test dataset for text summarization evaluation - 50 topics covering CS431 Deep Learning course",
    "total_questions": 50
  },
  "generations": [
    {
      "question_id": "sum_001",
      "query": "Transformer Architecture",
      "summary": "Transformer Architecture — Tóm tắt ngắn gọn, toàn diện\n\nGiới thiệu\n- Transformer là kiến trúc mạng thay cho RNN/CNN để xử lý dữ liệu tuần tự, tận dụng cơ chế Attention để mô hình hóa tương tác giữa mọi cặp token trong một sequence, giúp giải quyết hạn chế của RNN về dependency dài hạn và chi phí thao tác tuần tự.\n\nĐịnh nghĩa & khái niệm cơ bản\n- Attention: gồm Attention Score → Attention Distribution → Attention Output. Dùng để tính mức liên hệ (importance) giữa token này với token khác.\n- Key / Query / Value (K/Q/V): thành phần chính trong attention. Trong cross-attention (encoder→decoder): queries đến từ input của decoder (h), keys và values lấy từ output của encoder (s).\n\nKiến trúc chính\n- Encoder stack:\n  - N lớp giống nhau (số lớp tùy chỉnh: 1 → hàng trăm).\n  - Mỗi lớp: Multi-head Self-Attention + Feed-forward layer, dùng residual connections và layer normalization.\n  - Output của encoder là các vector biểu diễn (s1, s2, ..., st).\n- Decoder stack:\n  - Tương tự encoder nhưng có thêm cross-attention: decoder self-attention (thường có mask để chỉ nhìn trái→phải) rồi cross-attention với encoder outputs.\n  - Input decoder là các embedding/tokens (h1, h2, ...).\n- Multi-head Attention: chia không gian embedding thành nhiều head để học các kiểu tương tác khác nhau.\n- Residual connections: cộng đầu vào với output của phép biến đổi, giúp ổn định huấn luyện và cho phép xếp nhiều layer.\n\nĐộng lực so với RNN\n- Attention cho phép mọi cặp token tương tác trực tiếp, giảm số phép toán chuỗi dài phải thực hiện trong RNN; dễ dàng song song hóa và học dependencies dài hạn.\n\nƯu điểm & nhược điểm (theo tài liệu)\n- Ưu điểm:\n  - Mô hình hóa tương tác giữa tokens toàn cục hiệu quả.\n  - Dễ mở rộng (số layer, số head).\n  - Hỗ trợ song song hóa khi huấn luyện.\n- Hạn chế:\n  - Có một số vấn đề tồn tại (tài liệu nêu tổng quát) và cần các giải pháp bổ sung; mô hình có thể nặng nếu nhiều layer/heads.\n\nCác biến thể / so sánh\n- BERT: \"Bidirectional Encoder Representations from Transformers\"\n  - Chỉ dùng phần Encoder, huấn luyện self-supervised bằng task Masked Word (che từ và đoán từ bị che).\n  - Bidirectional (nhìn cả trái và phải).\n  - Phù hợp cho các downstream tasks như phân loại văn bản, hỏi-đáp, NER, tóm tắt.\n- GPT: dựa trên phần Decoder (Generative Transformer)\n  - Huấn luyện bằng Next-Word prediction (từ tiếp theo).\n  - Dùng cho dịch máy, sinh nội dung tự động, generation tasks.\n- Bidirectional vs Unidirectional:\n  - BERT dùng bidirectional encoder; language model truyền thống/ GPT là unidirectional (left-to-right).\n- Vision Transformer (ViT), Whisper (speech-to-text), multimodal models (ví dụ stable diffusion kết hợp ảnh + text): Transformer áp dụng rộng ra ảnh, âm thanh, multimodal.\n\nỨng dụng & cách sử dụng mô hình nền tảng\n- Ứng dụng: language modeling, machine translation (encoder-decoder), classification, QA, NER, text generation, speech-to-text (Whisper), vision (ViT), multimodal generation (stable diffusion).\n- Sử dụng mô hình nền tảng:\n  - Fine-tuning: huấn luyện lại mô hình cho task cụ thể.\n  - Prompting: chỉ dẫn mô hình bằng prompt để hiểu ngữ cảnh; có thể dùng zero-shot/ few-shot.\n\nTóm tắt (Key takeaways)\n- Transformer dùng cơ chế Attention (K/Q/V) để trực tiếp mô hình hóa tương tác giữa mọi token, cho phép học dependency dài hạn và song song hóa.\n- Kiến trúc gồm encoder stack và decoder stack; encoder cho BERT, decoder cho GPT; cross-attention nối hai phần này trong encoder-decoder.\n- Có thể mở rộng số layer/head; dùng residual connections để ổn định huấn luyện.\n- Được ứng dụng rộng rãi: NLP (BERT/GPT), speech (Whisper), vision (ViT), multimodal (stable diffusion); mô hình nền tảng thường dùng self-supervised pretraining rồi fine-tuning hoặc prompting.",
      "original_text": "Video: [CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder\nkhi mà mình tìm kiếm ví dụ như chúng ta muốn tìm kiếm một cái video về chủ đề đó là transformer thì cái keyword của mình sẽ là transformer architecture và key nó chính là các cái tiêu đề tiêu đề của các cái video trong cái kho trên cái kênh youtube của mình   chẳng hạn và cái value của mình trả về nó chính là những cái nội dung của video của mình đó chính là nội dung video rồi ví dụ như là mô tả mô tả cái video thì đó chính là các cái value của mình thì đây chính là cái sự liên tưởng đến cái hệ thống tìm kiếm các cái hệ thống truy vấn mà trong cái thế giới thực của mình còn attention của mình đó nó sẽ khác so với lại cái truy vấn trong cái video của mình trong bảng dữ liệu của mình ở chỗ đó là chúng ta sẽ phải\n\n---\n\nVideo: [CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder\ntransformer cũng vậy transformer cũng vậy giả sử như chúng ta có s1 s2 cho đến s t thuộc rd tức là cái output của encoder đây chính là cái output của encoder đây là encoder và h1 h2 ht là các cái input decoder tức là chúng ta sẽ có cái ki h i ở đây đây là cái input cho cái quá trình decoder input cho cái quá trình decoder khi đó thì cái cặp các bộ key value và query của mình thì nó sẽ có cái công thức như sau key thì nó sẽ lấy từ se đây là ở đây trong cái công thức này thì xin lỗi ở đây là chúng ta nhầm đây là query query của mình chứ không phải là key rồi ở đây chính là query\n\n---\n\nVideo: [CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer\nTrong phần tiếp theo thì chúng ta sẽ cùng tìm hiểu về hướng dụng của Transformer cũng như là các cái thành tựu của nó Đầu tiên đó là các cái mô hình nền tảng BERT và GPT Thì đây là hai cái mô hình ngôn ngữ được huấn luyện trên tập dữ liệu lớn Ở hai bước của Transformer, BERT là cho giai đoạn Encoder và GPT là cho giai đoạn Encoder BERT là viết tắt của bi-directional encoder representation from Transformer Chúng ta thấy đều có cái từ là Transformer GPT là Generative, Generative này chính là Decode, Retrain, Transformer Thì chúng ta cũng thấy là cả hai mô hình này đều based, đều dựa trên cái kiến trúc của Transformer Và điểm chung đó là đều thuộc cái nhóm là tự học, Self-superviling Tức là học Khi không có cái dữ liệu gán nhãn\n\n---\n\nVideo: [CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer\nCủa cái kiến trúc transformer Rồi chúng ta đồng thời Cũng đã tìm hiểu về cái kiến trúc Kinh điển Của transformer Chúng ta đã tìm hiểu qua về Các cái khuyết điểm Một số cái vấn đề Một số cái vấn đề Còn tồn tại Và một số cái giải pháp Ban đầu của transformer Và cuối cùng đó là Những cái ứng dụng Rồi Ứng dụng của transformer Thông qua việc đó là chúng ta có khả năng PhyTool PhyTool để giải quyết các cái task Giải quyết cái downstream task Chúng ta có thể Prompting Để chỉ dẫn cho mô hình Hiểu cái context hiểu cái ngũi cảnh Và chúng ta có cái kiểu là Zero\n\n---\n\nVideo: [CS431 - Chương 10] Part 1： Ôn tập kiến trúc mạng RNN và cơ chế Attention\nChào các bạn, hôm nay chúng ta sẽ cùng đến với bài Transformer và một số ứng dụng của Transformer trong xử lý ngôn ngữ tự nhiên. Đây có thể nói là một trong những bài rất là quan trọng, nó sẽ là nền tảng cho chúng ta để có thể học tiếp những thành tựu của Transformer trong các lĩnh vực khác, không phải chỉ trong lĩnh vực về xử lý ngôn ngữ tự nhiên mà cũng có thể là dùng trong lĩnh vực về hình ảnh, về xử lý âm thanh, v.v. Nội dung của bài hôm nay chúng ta sẽ cùng đầu tiên đó là chúng ta sẽ ôn tập về một số cái khái niệm ví dụ như là Attention là gì, rồi chúng ta sẽ tính cái Attention Score, Attention Distribution và Attention Output đó là gì.\n\n---\n\nVideo: [CS431 - Chương 8] Part 2： Một số biến thể của RNN： Bidirectional RNN\nthì đó là những cái bài toán mà cho phép có khả năng sử dụng được bidirectional và nếu như vậy thì rõ ràng cái language model là nó không phù hợp tại vì language model là chỉ cho phép là nhìn từ trái sang phải đúng không và dự đoán cái từ tiếp theo thôi do đó thì chúng ta không có phù hợp sử dụng bidirectional cho cái gọi là sử dụng cái bidirectional rn cho cái language model và và BERT mô hình BERT là bidirectional encoder representation for transformer thì đây là một trong những cái mô hình mà một cái biến thể của bidirectional nó thể hiện ở trong cái chữ này và vô cùng hiệu quả và rất là được sử dụng rất là phổ biến hiện nay thì hiện nay ở Việt Nam chúng ta có một cái mô hình đó là phở bệt là cũng dựa trên cái cái cái mô hình à của bidirectional này\n\n---\n\nVideo: [CS431 - Chương 10] Part 2： Động lực của kiến trúc Transformer\nNội dung hôm nay thì chúng ta gồm có 3 phần Đầu tiên đó là chúng ta sẽ cùng tìm hiểu về kiến trúc Transformer Sau đó chúng ta sẽ cùng tìm hiểu về một số cái điểm yếu của Transformer Và cuối cùng đó là một số ứng dụng cũng như là thành tựu Đầu tiên thì về kiến trúc thì chúng ta sẽ xét đến cái động lực Tại sao chúng ta cần phải có cái kiến trúc mạng Transformer Động lực đầu tiên xuất phát từ việc đó là Giữa 2 cái từ bất kỳ Giữa 2 cái từ bất kỳ Trong cái đoạn vang input của mình Chúng ta tương tác với nhau, tương tác về mặt thông tin với nhau Chúng ta phải tốn rất nhiều thao tác Ví dụ, ở đây chúng ta sẽ có 2 cái từ này Thì rõ ràng là trong sự liên ngôn ngữ tự nhiên Nó sẽ có tình huống đó là các cái từ Nó phải có cái sự liên hệ về mặt ý nghĩa với nhau Thì từ đó chúng ta mới có thể hiểu Hiểu rõ được cái nội dung của cái input của mình Là gì? Khi đó chúng ta mới có thể đi tính\n\n---\n\nVideo: [CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder\nchúng ta hoàn toàn có thể thay đổi cái số lớp này tùy vào cái kiến trúc của cái Transformer này của mình nếu như chúng ta muốn cái Transformer này của mình nó nhẹ ít tham số thì layer của mình có thể là 1 layer, 2 layer nhưng nếu chúng ta muốn cái kiến trúc Transformer này có thể giải quyết những cái bài tán thức tạp hơn với nhiều cái thông tin hơn thì khi đó cái số layer này có thể lên đến vài chục thậm chí là hàng trăm layer và mẹo thứ 2 đó chính là sử dụng cái residual connect à sử dụng cái residual connect và cái layer tiếp theo sẽ được tính bằng cái layer trước đó là layer thứ l trừ 1 cộng với lại cái phép biến đổi cộng với lại cái output của cái phép biến đổi thì ở đây chúng ta sẽ có các cái đường màu đỏ chúng ta thực hiện phép cộng tức là đầu vào ở đây nè sau khi chúng ta thực hiện cell tension xong\n\n---\n\nVideo: [CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer\nThì Bird là bao gồm Encoder Trong Transformer Và GPT thì là cấu tạo bởi Decoder Trong Transformer Nhiệm vụ Bird là che cái từ Đoán cái từ bị che Là Mask Word Còn đây là GPT sẽ đoán cái từ tiếp theo Là Next Word Và các cái Task, Downstream Task Những cái Task mà có thể Sử dụng để mà giải quyết Với cái mô hình Bird Đó là phân loại văn bản, trả lời câu hỏi Tấm tác văn bản Hoặc là nhận diện thực tế Name, Entity, Recognition Còn các cái Downstream Task cho GPT Mà nó khá là phù hợp đó chính là dịch máy Và tạo sinh nội dung tự động Được Rồi và Để sử dụng 2 cái mô hình nền tảng này Để có thể sử dụng được Các cái mô hình nền tảng thì chúng ta sẽ có 2 cách Cách đầu tiên Đó là File Tuning File Tuning hiểu một cách nôn na Đó chính là chúng ta sẽ huấn luyện lại\n\n---\n\nVideo: [CS431 - Chương 10] Part 7： Một số ứng dụng của kiến trúc mạng Transformer\nWhisper của OVN Thì đây là một trong những cái mô hình Style of the art Cho cái bài toán là Speak to text Bài toán nhận dịp giọng nói, tư giọng nói Biến thành văn bản Rồi, ở đây thì mình ghi nhầm Đó là chúng ta sẽ có Mô hình vision Transformer Mô hình vision transformer Đây Và dữ liệu chuỗi đây Chúng ta cũng có thể hiểu đó là dữ liệu ảnh Các cái pixel Hoặc là chuỗi các cái path Path này đến trước, path này đến sau Và ở đây chúng ta sẽ lưu ý yếu tố đó là 2 chiều Chuỗi này của chúng ta là đi theo 2 chiều Rồi Và cuối cùng Đó chính là Chúng ta có một cái ví dụ đó là Trên multimodal Tức là vừa có sự kết hợp của cả ảnh và text Thì trong cái mô hình Cái mô hình là stable diffusion",
      "chunks_retrieved": 41,
      "chunks_used": 10,
      "generation_time_seconds": 32.630583
    },
    {
      "question_id": "sum_002",
      "query": "Attention Mechanism",
      "summary": "Attention Mechanism — Tóm tắt ngắn gọn và toàn diện\n\nGiới thiệu\n- Attention là cơ chế giúp mô hình sequence-to-sequence “để tâm” (focus) vào những phần quan trọng của chuỗi input khi thực hiện decode, loại bỏ thông tin thừa và cung cấp vector tổng hợp dùng để sinh output.\n\nĐịnh nghĩa & khái niệm cơ bản\n- Query: vector truy vấn (thường là trạng thái ẩn của decoder tại thời điểm t, ký hiệu h_t hoặc Q).\n- Key, Value: thường là các trạng thái ẩn của encoder (S1…SN). Keys để tính độ tương đồng, Values để lấy thông tin.\n- Attention score: độ đo tương đồng giữa Query và mỗi Key (ví dụ tích vô hướng) → ra các scalar chưa chuẩn hóa.\n- Attention distribution (α): chuẩn hóa các score bằng softmax để được phân bố xác suất (giá trị trong [0,1], tổng = 1).\n- Context / Attention output (C_t): tổng có trọng số của các Value theo α: C_t = Σ_i α_i * S_i (vector chiều bằng chiều của Value).\n- Kết hợp với decoder state: thường concat C_t và h_t (hoặc kết hợp khác) rồi tiến hành dự đoán: y_t = softmax(V * [C_t ; h_t]).\n\nCơ chế hoạt động (tóm tắt các bước)\n1. Tính score_i = similarity(Query, Key_i) (ví dụ dot-product).\n2. α = softmax(scores) — chuẩn hóa sang phân bố.\n3. C_t = Σ_i α_i * Value_i — tạo vector tổng hợp chú ý.\n4. Kết hợp C_t với trạng thái decoder hiện tại để tính output tại bước decode.\n\nCác biến thể & mở rộng\n- Cross-attention (encoder-decoder attention): trong decoder, Query từ decoder, Key/Value từ encoder — ánh xạ thông tin encoder → decoder.\n- Multi-head attention (Transformer): chia embedding thành nhiều “head”, thực hiện attention song song ở nhiều không gian con, concat các head rồi biến đổi tuyến tính để thu được biểu diễn phong phú hơn.\n- Trong kiến trúc trước Transformer: encoder thường là bidirectional LSTM để cung cấp ngữ cảnh trước khi áp dụng attention.\n\nƯu điểm (những điểm nêu trong tài liệu)\n- Cho phép mô hình tập trung vào từ/token liên quan cho từng bước decode.\n- Loại bỏ thông tin thừa (score/α thấp với thông tin không liên quan).\n- Dùng trực tiếp để tính output decoder, cải thiện độ chính xác dịch/khớp ngữ cảnh.\n\nTóm tắt (Key takeaways)\n- Attention = tính score giữa Query và Keys → softmax thành α → weighted sum Values = context C_t.\n- C_t phối hợp với trạng thái decoder để sinh output.\n- Cross-attention liên kết encoder và decoder; multi-head mở rộng attention sang nhiều khía cạnh biểu diễn.",
      "original_text": "Video: [CS431 - Chương 9] Part 2_2： Cơ chế Attention trong Sequence-to-Sequence\nrồi và cái vector output Attention Output này đó là CT nó sẽ được thực hiện contrast ký hiệu là cái dấu chấm phẩy ha tức là nó nối chuỗi nó contrast lại với nhau rồi để tạo ra nó nối với lại cái HT để tạo ra một cái vector tổng hợp và từ cái vector tổng hợp này thì chúng ta sẽ đi tính tổng hợp này để tính toán cái giá trị đi tính cái giá trị Y-T và đây chính là cái cách tính mà dựa hoàn toàn vào cái tình huống là không có Attention tức là Y-T thì nó sẽ là bằng Softmax của V nhân với lại cái vector này nhân với cái vector là CT.HT Được rồi! Thì đây chính là cái\n\n---\n\nVideo: [CS431 - Chương 9] Part 2_1： Cơ chế Attention trong Sequence-to-Sequence\nThì sử dụng các cái attention distribution này. Để cộng có trọng số các cái vector đầu vào này. Cộng có trọng số các cái vector trạng thay ẩn này. Và attention output sẽ. Liên quan. Nó sẽ liên quan đến cái từ. Mà chúng ta cần phải để ý. Tại cái quá trình dịch. Tại đây. Và nó sẽ. Loại bỏ được những cái thông tin thừa. Và. Nó sẽ loại bỏ được những cái thông tin thừa. Thì những cái thông tin mà dư thừa. Những cái thông tin dư thừa. Thì nó sẽ có cái attention score thấp. Hoặc là cái attention distribution thấp. Ví dụ đây là thông tin thừa. Nên cái. Cái chiều cao của nó thấp. Còn những cái thông tin. Của những cái. Từ nào mà có liên quan nhiều. Thì nó sẽ là cao. Ví dụ đây là một cái. Thời minh họa cho cái chuyện đấy. Và. Khi chúng ta tổng hợp được cái thông tin. Của cái attention output này. Phối hợp. Với lại cái thông tin của cái trạng thái ẩn tại đây. Đúng không? Thì chúng ta sẽ có.\n\n---\n\nVideo: [CS431 - Chương 10] Part 1： Ôn tập kiến trúc mạng RNN và cơ chế Attention\nVà sau đó chúng ta sẽ cùng tổng hợp cái Attention Output để lấy cái trạng thái ẩn của cái từ nào mà chúng ta đang quan tâm nhiều. Còn những cái từ nào mà chúng ta không có quan tâm nhiều thì cái trọng số của nó sẽ thấp hơn. Đó. Thì từ cái Attention Output này chúng ta sẽ được sử dụng để đi tính toán các cái giá trị Output ở cái bước Decoder. Thì đây chính là cái ý tưởng của Attention. Và ngoài ra thì chúng ta cũng ôn tập về một số cái mô hình, về cái cách thức mà chúng ta sử dụng mô hình của Hồi Quy cho bài toán NLP. Thì đầu tiên đó là cái chuỗi Input của mình. Nó sẽ được encode 2 chiều. 3D có nghĩa là 2 chiều. Để đảm bảo cho cái đầu vào của mình, mình có thể đọc theo trình tự từ trái sang phải và từ phải sang trái. Chúng ta có đầy đủ được cái thông tin về mặt ngữ cảnh của cái Input một cách đầy đủ. Và trong cái môi nồi quy mà khi chưa có cái Transformer ra đời thì chúng ta sẽ sử dụng cái kiến trúc ANN với cái cell là LSTM, Long Short Term Memory.\n\n---\n\nVideo: [CS431 - Chương 9] Part 2_1： Cơ chế Attention trong Sequence-to-Sequence\nNếu như chúng ta sử dụng độ đo tích vô hướng. Thì nó sẽ chưa có được. Chuẩn hóa. Về một cái không gian. Xét xuất. Do đó thì chúng ta sẽ. Tiến hành cái bước tiếp theo. Đó là. Tính cái Attention Distribution. Attention Distribution. Là nó sẽ. Quy chiếu. Về một cái. Cái không gian. Có cái giá trị là từ 0. Cho đến 1. Đó. Để Normalize. Để chuẩn hóa nó lại. Và đưa về cái không gian phân bố. 1. Thì với cái Distribution này. Chúng ta. Thấy rằng cái cột này. Nó sẽ cao hơn hẳn. So với lại các cái cột này. Thì điều đó có nghĩa là gì. Khi chúng ta bắt đầu. Cái quá trình. Decode. Khi bắt đầu quá trình. Thì. Tại cái thời điểm này. Nó sẽ bắt đầu. Để ý. Cái từ này. Thì chúng ta sẽ là. Chuyển sang cái. Cái cách hiệu khác đó là. Để ý. Để ý đến cái từ I. Nhiều hơn. So với lại những cái từ còn lại.\n\n---\n\nVideo: [CS431 - Chương 10] Part 1： Ôn tập kiến trúc mạng RNN và cơ chế Attention\nNội dung của bài hôm nay chúng ta sẽ cùng đầu tiên đó là chúng ta sẽ ôn tập về một số cái khái niệm ví dụ như là Attention là gì, rồi chúng ta sẽ tính cái Attention Score, Attention Distribution và Attention Output đó là gì. Thì trong cái hình ở đây chúng ta thấy đó là các cái trạng thái ẩn là S1, S2 cho đến SN của mình thì nó sẽ được gọi là Value. Còn các cái vector truy vấn thì chúng ta sẽ gọi là Query, như khác ở đây thì được gọi là Query. Và chúng ta sẽ đi lần lượt tính cái giá trị trọng số của cái Query với lại cái vector Output này để từ đó là chúng ta biết là tại cái vị trí hiện tại chúng ta sẽ quan tâm đến cái từ nào trong cái chuỗi Input của mình. Và sau đó chúng ta sẽ cùng tổng hợp cái Attention Output để lấy cái trạng thái ẩn của cái từ nào mà chúng ta đang quan tâm nhiều. Còn những cái từ nào mà chúng ta không có quan tâm nhiều thì cái trọng số của nó sẽ thấp hơn.\n\n---\n\nVideo: [CS431 - Chương 9] Part 2_2： Cơ chế Attention trong Sequence-to-Sequence\nthì chúng ta sẽ bắt đầu tổng hợp thông tin cho cái Attention Output tổng hợp thông tin cho Attention Output và ký hiệu nó bằng chữ C C này thì còn có một cái ý nghĩa khác nó chỉ là Context và C này sẽ là tổng có trọng số của các cái trọng số của R T và cái trạng thái của các cái vector ẩn của End Coder của cái quá trình End Coder chính là cái S1, S2, S3, Sn còn trọng số tương ứng của nó đó chính là cái Alpha Ti trong đó T là cái đại diện cho cái tại cái thời điểm T này T là đại diện cho cái thời điểm T mà mình bắt đầu cái quá trình decode còn Y là chúng ta sẽ duyệt từ 1 cho đến N đây duyệt từ đầu cho đến cuối cuối cái đoạn của End Coder rồi và cái vector output Attention Output này đó là CT nó sẽ được thực hiện contrast\n\n---\n\nVideo: [CS431 - Chương 9] Part 2_1： Cơ chế Attention trong Sequence-to-Sequence\nThì ở đây nó gọi là Attention score. Mục tiêu của nó là gì? Mục tiêu của cái việc tính cái Attention score này là. Tại thời điểm tôi. Tôi bắt đầu. Một cái quá trình decode ở đây. Thì tôi sẽ để tâm. Cái từ Attention. Tiếng Anh. Thì khi dịch ra tiếng Việt. Mình có thể dùng từ nôn na đó là để tâm. Tôi sẽ để tâm. Đến cái từ nào. Trong 4 cái từ ở đây. Khi tôi bắt đầu dịch. Tại cái vị trí này. Thì. Để mà tính được cái sự. Để tâm đó. Thì chúng ta sẽ dùng cái. Cái công thức tính là độ tương đồng. Có thể là dùng cái độ đo. Đó. Tích vô hướng. Và các cái giá trị Scalar. Các cái giá trị ở đây. Nó thể hiện cho cái sự tương đồng đó. Tuy nhiên các cái giá trị tương đồng này. Nếu như chúng ta sử dụng độ đo tích vô hướng. Thì nó sẽ chưa có được. Chuẩn hóa. Về một cái không gian. Xét xuất. Do đó thì chúng ta sẽ. Tiến hành cái bước tiếp theo.\n\n---\n\nVideo: [CS431 - Chương 9] Part 2_2： Cơ chế Attention trong Sequence-to-Sequence\ncái vector nối CT và HT sẽ là kích thước bao nhiêu rồi C cái vector Attention output nó sẽ có kích thước là bao nhiêu rồi bây giờ chúng ta sẽ tính toán cái dấu chấm hỏi này nó sẽ là các giá trị gì nếu như bạn nào mà nhanh chí thì có thể nhìn vô đây là R này là tập hợp của các cái dấu hình tròn này đúng không? thì ở đây có bao nhiêu? có N có N phần tử như vậy ở đây R này sẽ là RN và tương ứng là và mỗi cái phần tử HT nhân với ST nó là một cái Scalar một cái giá trị vô hướng và tổ hợp của các cái giá trị vô hướng nó sẽ tạo ra một cái vector và có N cái giá trị vô hướng như vậy ở đây sẽ là N alpha là cái vector là Attention distribution\n\n---\n\nVideo: [CS431 - Chương 2] Part 4_3： Kiến trúc Transformer： Bộ Encoder\ntính để tính ra cái giá trị vector tổng hợp đây chính là cái attention à à à à à à à à à à à à à rồi thì ở bên đây sẽ là cái multi head cell attention đây là một cái lá cắt đúng không chúng ta sẽ thực hiện trên một cái khía cạnh của của câu của mình của một cái từ trong câu của mình và chúng ta sẽ thực hiện nhiều cái khía cạnh khác nhau sau đó đến đây chúng ta sẽ con cat thông tin của các cái kéo đó product attention này lại với nhau và sau đó chúng ta mới thực hiện cái phép biến đổi bật s Pattern vous cho mình đùa       MP3 Pero coi à à à cho người già sức khỏe trong bản v chasing erw Meter W X4 bộ tính sợ cutting là journalists common Sweep, cell Tension like the rest of the same age have more heads V공 Szon design like this, famas, cell tension normalties have many skills, when we choose gold, we need to save also We only have three exact things which are the signal at a quantity. if we have many more head bör nữa foreximately our initial wealth started to slow will have more head. Well, with the number L, we have various steps, we need to get more things above it. . gluttime sometimes we will also have the other level of L. theiorism is below it inDIGRADI\n\n---\n\nVideo: [CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder\nrồi và bây giờ chúng ta sẽ có một cái khác nữa là   cũng khá là lớn trong cái bước gọi là decoder đó chính là encoder decoder attention và tên ngắn gọn của nó đó là cross attention cross là cái sự chuyển đổi từ cái sort xin gọi là sự chuyển đổi giữa cái encode và cái decode ánh xạ giữa encode với decode thì nó gọi là cross attention thì trong cái cơ chế attention thì key nó sẽ đến từ decoder key của mình nó sẽ đến từ cái decoder còn value là cái decoder đây chính là cái key của mình đây chính là cái key và value của mình thì nó đến từ encoder 2 cái mũi tên này đến từ encoder tương ứng là cái se transformer cũng vậy transformer cũng vậy giả sử như chúng ta có s1 s2 cho đến s t thuộc rd tức là cái output của encoder đây chính là cái output của encoder",
      "chunks_retrieved": 64,
      "chunks_used": 10,
      "generation_time_seconds": 17.211473
    },
    {
      "question_id": "sum_003",
      "query": "Self-Attention",
      "summary": "Self-Attention — Tóm tắt ngắn gọn, toàn diện\n\nGiới thiệu\n- Self-attention là cơ chế “tự chú ý” cho phép mỗi vị trí (token) trong cùng một chuỗi tính mức độ liên quan tới mọi vị trí khác (bao gồm cả chính nó). Đây là thành phần cốt lõi của Transformer.\n\nĐịnh nghĩa & khác biệt với attention thông thường\n- Attention (cross-attention): một query (thường từ decoder) truy xuất và tổng hợp giá trị (values) từ encoder (hai miền khác nhau).\n- Self-attention: query, key, value đều lấy từ cùng một chuỗi (encoder với encoder hoặc decoder với decoder) — mọi token “tự chú ý” lẫn nhau.\n\nCơ chế hoạt động (tóm tắt ý chính)\n- Tại mỗi layer, self-attention tính điểm tương đồng giữa mọi cặp token trong chuỗi rồi dùng điểm này để tổng hợp thông tin.\n- Trong encoder: mọi token có thể nhìn tất cả token khác → dễ song song hóa (không phụ thuộc tính toán theo thứ tự).\n- Trong decoder (autoregressive generation): phải ngăn token hiện tại “nhìn thấy” token tương lai để tránh rò đáp án. Giải pháp:\n  - Causal mask (che/đặt điểm attention của các token phía sau bằng -∞) để loại bỏ các cạnh “nhìn trước”.\n  - Với mask này vẫn có thể thực hiện tính toán song song (ví dụ trong huấn luyện) nhưng đảm bảo tính tuần tự về mặt thông tin.\n- Thường dùng Multi-head Self-Attention để tăng khả năng biểu diễn (đề cập trong nguồn).\n\nTính chất quan trọng\n- Song song hóa: khác RNN, self-attention cho phép tính cho nhiều vị trí một cách song song (encoder rõ rệt).\n- Mỗi token có thể chú ý đến chính nó và mọi token khác trong cùng chuỗi.\n\nƯu điểm\n- Nắm bắt tương tác cặp đôi giữa mọi token (long-range dependencies).\n- Dễ paralell hóa (giảm thời gian huấn luyện so với RNN ở encoder).\n- Linh hoạt cho cả encoding và decoding (với mask cho decoder).\n\nNhược điểm / hạn chế\n- Chi phí tính toán tăng theo bình phương độ dài chuỗi: O(T^2) (chưa kể đến chiều vector D cho các phép chiếu) — gây tốn tài nguyên khi chuỗi dài.\n- Vấn đề biểu diễn vị trí: positional encoding mặc định thể hiện vị trí tuyệt đối; đôi khi vị trí tương đối giữa tokens mới quan trọng — cần xử lý bổ sung.\n- Decoder cần cơ chế masking để duy trì tính tuần tự; nếu không mask sẽ “nhìn thấy” tương lai và vi phạm quy tắc sinh tuần tự.\n\nỨng dụng & vị trí trong Transformer\n- Dùng trong cả encoder và decoder của Transformer.\n- Self-attention trong encoder mã hóa mối quan hệ nội tại của input.\n- Self-attention trong decoder (kết hợp với mask) sinh output theo thứ tự; decoder còn có cross-attention để lấy thông tin từ encoder.\n\nSo sánh ngắn\n- Attention (cross) vs Self-attention: cross truy vấn một miền khác, self là trong cùng một miền.\n- Self-attention vs RNN: self-attention cho song song hóa và dễ bắt dependencies dài hạn; RNN tính tuần tự.\n\nTóm tắt (Key takeaways)\n- Self-attention: mỗi token chú ý đến mọi token trong cùng chuỗi — cơ chế nền tảng của Transformer.\n- Cho phép song song hóa (encoder), nhưng decoder cần causal mask để bảo toàn tính tuần tự.\n- Ưu: bắt được mối quan hệ toàn cục, dễ song song. Nhược: chi phí O(T^2) theo độ dài chuỗi và yêu cầu xử lý vị trí tương đối.",
      "original_text": "Video: [CS431 - Chương 10] Part 3： Cơ chế Self-Attention\nchính vì nó thực hiện song song nên không có cái phép tính nào phụ thuộc với phép tính nào rồi và ở đây thì chúng ta sẽ có cái khái niệm self-attention thì thật ra nó cũng chính là self-attention nhưng mà thay vì chúng ta là cái mối quan hệ giữa encode encoder và decoder đó là cái attention mà trong bài trước thì ở đây đó là cơ chế attention trên cái đoạn encode và encode encoder và encoder hoặc là decoder với decoder tức là mỗi từ nó sẽ chú ý đến nhau trong cùng một cái input hoặc là trong output thì giả sử như đây là nguyên cái giai đoạn encode đó thì các cái từ nó sẽ tự chú ý với nhau trong đó nó cũng có cả chú ý đến chính nó\n\n---\n\nVideo: [CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder\nvề lý thuyết thì decoder nó cũng sẽ tương tự như là encoder tuy nhiên cái output của decoder nó sẽ có một cái vấn đề như thế này đó là decode là một cái quá trình mà chúng ta dạy mã tuần tự dạy mã tuần tự chúng ta không thể nào thực hiện song song được tại vì cái việc song song nó tư đương với cái việc là chúng ta có thể nhìn thấy đáp án ở phía sau tức là tại một cái quá trình decode chúng ta đưa ra cái output tại đây rồi sau đó chúng ta mới đưa ra cái output tại đây đây là lần thứ một đây là lần thứ hai và chúng ta không được phép thấy cái từ ở thứ ba thứ tư thế thì những cái đường màu đỏ ở đây nếu như chúng ta sử dụng self-attention trên cái decode tương tự như là cái self-attention của encode\n\n---\n\nVideo: [CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder\ncái từ ở thứ ba thứ tư thế thì những cái đường màu đỏ ở đây nếu như chúng ta sử dụng self-attention trên cái decode tương tự như là cái self-attention của encode thì những cái đường màu đỏ này nó sẽ vi phạm đó là chúng ta đã nhìn thấy đáp án phía sau đúng không tại vì cái thông tin tại cái vị trí số một này nó nhận được cái thông tin tại cái layer tại cái vị trí này tức là cái từ thứ hai đưa vô rồi từ thứ ba đưa vô đây như vậy nó đã thấy trước đáp án như vậy là không có đáp án không có được phép như vậy vậy thì chúng ta phải bỏ đi các cái cạnh nối màu đỏ này đi chúng ta phải đảm bảo như vậy thì khi đó cái quá trình decode nó mới thật sự là đúng như cái quy tắc của mình đó là chúng ta lần lượt đưa ra các cái dự đoán cho từ từ từ của mình chứ không được phép\n\n---\n\nVideo: [CS431 - Chương 10] Part 3： Cơ chế Self-Attention\nđể tính ra được cái giá trị ở đây chúng ta sẽ phải cần tính trước các giá trị ở layer trước đó ở đây là cái layer số 2 ở đây là layer số 3 đây là layer số 2 và đây là layer số 4 but which is other than in sheep hood thì để tính được cái layer số 3 chúng ta sẽ phải cần thông tin của các layer số 1 và thông tin của layer số 1 thì lại cần thông tin của layer số 2 và ở layer số 2 thì lại cần thông tin của layer số 1 thì tại đây chúng ta thấy là cái trạng thái ẩn này nó sẽ bị phụ thuộc bởi một cái phép tính trước đó đó là đây và các cái phép tính này thì thực hiện song song chính vì nó thực hiện song song nên không có cái phép tính nào phụ thuộc với phép tính nào rồi và ở đây thì chúng ta sẽ có cái khái niệm self-attention thì thật ra nó cũng chính là self-attention\n\n---\n\nVideo: [CS431 - Chương 10] Part 3： Cơ chế Self-Attention\nđó thì các cái từ nó sẽ tự chú ý với nhau trong đó nó cũng có cả chú ý đến chính nó chú ý đến chính nó ở đây và chú ý đến những cái từ còn lại trong cái giai đoạn encode của mình hoặc là trong cái giai đoạn decode tức là nó sẽ tự chú ý đến những cái từ trong cái giai đoạn decode của mình đó là cái sự khác biệt giữa cái khái niệm attention và self-attention\n\n---\n\nVideo: [CS431 - Chương 10] Part 6： Một số vấn đề của Transformer\nphần tiếp theo thì chúng ta sẽ cùng lượt qua một số điểm yếu của kiến trúc transformer thì đầu tiên đó là cái chi phí tính toán nó sẽ phải tăng dần theo cái độ dài của chuỗi và cụ thể đó là nó sẽ phụ thuộc với một cái hàm mật 2 theo cái độ dài của chuỗi, tại sao lại như vậy tại vì ở đây chúng ta thấy là cái thao tác self-attention của mình nó sẽ phải đi thực hiện trên tất cả các cái cặp ví dụ cái chuỗi này mà càng dài, chuỗi này là có t t là 1, 2, 3 thì trong cái bước tiếp theo chúng ta sẽ phải đi tính tất cả các cái tính attention trên tất cả các cái từ của mình và chúng ta sẽ phải thực hiện cái việc này trên tất cả các cái cặp như vậy chi phí của mình sẽ là O, T bình phương\n\n---\n\nVideo: [CS431 - Chương 10] Part 3： Cơ chế Self-Attention\ntrong transformer chúng ta sẽ bắt gặp một cái khái niệm dùng rất là thường xuyên đó chính là self-attention tức là cái kỹ thuật tự chú ý thì ở trong cái bài trước chúng ta đã nói về cái khái niệm là attention tức là một cái truy vấn một cái từ truy vấn query của mình ở cái bước decoder nó sẽ truy xuất và tổng hợp nó sẽ truy xuất và tổng hợp thông tin từ các cái tập giá trị value của cái encoder thì như chúng ta thấy là đây là cái giai đoạn là encode và đây là cái giai đoạn decode thì trong cái giai đoạn decode này thì chúng ta sẽ phải lookup chúng ta sẽ phải tra vào tất cả những cái từ ở trong cái giai đoạn encode còn đây là encoder còn đây là cái quá trình là decode thì khi đó là nó gọi là attention tức là cái sự truy vấn của một cái query\n\n---\n\nVideo: [CS431 - Chương 10] Part 6： Một số vấn đề của Transformer\nvà chúng ta sẽ phải thực hiện cái việc này trên tất cả các cái cặp như vậy chi phí của mình sẽ là O, T bình phương chưa kể là chúng ta sẽ phải có thêm cái D là cái số chiều của cái vector này của mình thì đây chính là cái điểm yếu đầu tiên là chi phí tính toán nó sẽ thay đổi theo cái độ dài của cái chuỗi của mình do phải tính trên tất cả các cái cặp tương tác của self-attention và vấn đề tiếp theo đó là vấn đề về biểu diễn vị trí thì trong cái positional emitting thì chúng ta đã từng nhận xét đó là cái tính vị trí tuyệt đối nó sẽ không quan trọng vị trí tuyệt đối nó không quan trọng mà đôi khi cái vị trí tương đối vị trí tương đối trong cái attention nó là quan trọng vị trí tương đối là gì là vị trí giữa một cái từ thứ T với lại những cái từ\n\n---\n\nVideo: [CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder\ncái tính tuần tự này dẫn đến là không có song song không thể song song hóa được không thể tinh sáng song song như vậy thì giải pháp là như thế nào chúng ta biết rằng là self-attention sở dĩ có thể song song hóa được mà chúng ta biết là point of view là não tính tính tính củagoodness Đó là vì cái từ tại thời điểm hiện tại có thể nhìn được những cái từ của tương lai, những cái từ ở đằng sau đó. Đó thì nó mới có thể tính toán song song được. Vậy thì bây giờ làm sao để cái việc mà tính toán của các cái node ở cái layer, các cái trạng thái ẩn của các cái tầng của mình, nó vẫn thực hiện được một cách song song. Nó vẫn thực hiện được một cách song song. Đó chính là chúng ta sẽ sử dụng cơ chế Mesh, Multihead, Self-Attention. Thì chúng ta sẽ che các cái Attention của các cái từ phía sau bằng cách gán cái Attention Score của nó bằng trừ của cuối cùng. Vậy thì chúng ta sẽ có công thức ở đây.",
      "chunks_retrieved": 9,
      "chunks_used": 9,
      "generation_time_seconds": 19.662642
    },
    {
      "question_id": "sum_004",
      "query": "Multi-Head Self-Attention",
      "summary": "Multi-Head Self-Attention — Tóm tắt ngắn gọn, toàn diện\n\nGiới thiệu\n- Multi‑Head Self‑Attention (MHSA) là cơ chế self‑attention mở rộng dùng trong Transformer, cho phép mỗi token trong cùng một sequence \"tự chú ý\" tới chính nó và các token khác đồng thời qua nhiều head khác nhau. Đây là nền tảng cho khả năng biểu diễn ngữ cảnh và song song hóa trong Transformer.\n\nĐịnh nghĩa & khác biệt với attention thông thường\n- Attention (trong encoder‑decoder): một query từ decoder truy xuất các value từ encoder (giữa hai phần khác nhau).\n- Self‑attention: các token trong cùng một phần (encoder hoặc decoder) làm query/key/value lẫn nhau — tức mỗi token chú ý tới các token khác trong cùng input/output, bao gồm cả chính nó.\n- Multi‑head: chạy nhiều cơ chế self‑attention song song (nhiều head) để thu được các khía cạnh ngữ cảnh khác nhau.\n\nCơ chế hoạt động chính (súc tích)\n- Tại một layer, tất cả token tính attention song song (không phụ thuộc tuần tự), nên có thể batch hóa.\n- Với decoder, cần giới hạn việc nhìn thấy token tương lai để đảm bảo tính tuần tự khi sinh output:\n  - Dùng causal mask (che các kết nối tới các token phía sau) bằng cách gán attention score của các vị trí tương lai một giá trị rất âm (≈ −∞) trước softmax, từ đó không được attend.\n  - Nhờ mask, vẫn có thể tính self‑attention song song trong quá trình huấn luyện mà không vi phạm quy tắc sinh theo thứ tự.\n- Multi‑head tổng hợp kết quả của các head khác nhau thành biểu diễn đầu ra của layer.\n\nƯu điểm\n- Cho phép mô hình học mối quan hệ giữa mọi cặp token (global context).\n- Hỗ trợ song song hóa tính toán across positions => tăng tốc huấn luyện (so với RNN tuần tự).\n- Multi‑head thu được nhiều kiểu tương tác/ngữ cảnh khác nhau.\n\nHạn chế / vấn đề\n- Chi phí tính toán và bộ nhớ tăng theo bình phương độ dài chuỗi: O(T^2) (cần tính attention cho mọi cặp token), thường còn phụ thuộc thêm chiều vector D.\n- Vấn đề vị trí: positional encoding cần xử lý vì self‑attention bản thân không mã vị trí tuyệt đối; vị trí tương đối đôi khi quan trọng nhưng không tự động được nắm bắt.\n- Trong decoder, cần masking cẩn thận để tránh \"nhìn trước\" khi sinh ngôn ngữ.\n\nMối quan hệ với kiến trúc Transformer\n- MHSA là thành phần cốt lõi trong cả encoder và decoder của Transformer.\n- Trong encoder: dùng full self‑attention (các token được tự do attend lẫn nhau).\n- Trong decoder: dùng masked self‑attention để duy trì tính tuần tự, và cross‑attention khi decoder truy xuất encoder outputs.\n\nKey takeaways\n- MHSA = self‑attention + nhiều head, cung cấp biểu diễn ngữ cảnh phong phú và cho phép song song hóa.\n- Cần masking trong decoder để ngăn attending tới tương lai; masking cho phép tính toán song song an toàn.\n- Trade‑off: hiệu năng ngôn ngữ và khả năng song song đổi lấy chi phí tính toán/bộ nhớ O(T^2) và cần cơ chế vị trí thích hợp.",
      "original_text": "Video: [CS431 - Chương 10] Part 3： Cơ chế Self-Attention\nchính vì nó thực hiện song song nên không có cái phép tính nào phụ thuộc với phép tính nào rồi và ở đây thì chúng ta sẽ có cái khái niệm self-attention thì thật ra nó cũng chính là self-attention nhưng mà thay vì chúng ta là cái mối quan hệ giữa encode encoder và decoder đó là cái attention mà trong bài trước thì ở đây đó là cơ chế attention trên cái đoạn encode và encode encoder và encoder hoặc là decoder với decoder tức là mỗi từ nó sẽ chú ý đến nhau trong cùng một cái input hoặc là trong output thì giả sử như đây là nguyên cái giai đoạn encode đó thì các cái từ nó sẽ tự chú ý với nhau trong đó nó cũng có cả chú ý đến chính nó\n\n---\n\nVideo: [CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder\nvề lý thuyết thì decoder nó cũng sẽ tương tự như là encoder tuy nhiên cái output của decoder nó sẽ có một cái vấn đề như thế này đó là decode là một cái quá trình mà chúng ta dạy mã tuần tự dạy mã tuần tự chúng ta không thể nào thực hiện song song được tại vì cái việc song song nó tư đương với cái việc là chúng ta có thể nhìn thấy đáp án ở phía sau tức là tại một cái quá trình decode chúng ta đưa ra cái output tại đây rồi sau đó chúng ta mới đưa ra cái output tại đây đây là lần thứ một đây là lần thứ hai và chúng ta không được phép thấy cái từ ở thứ ba thứ tư thế thì những cái đường màu đỏ ở đây nếu như chúng ta sử dụng self-attention trên cái decode tương tự như là cái self-attention của encode\n\n---\n\nVideo: [CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder\ncái từ ở thứ ba thứ tư thế thì những cái đường màu đỏ ở đây nếu như chúng ta sử dụng self-attention trên cái decode tương tự như là cái self-attention của encode thì những cái đường màu đỏ này nó sẽ vi phạm đó là chúng ta đã nhìn thấy đáp án phía sau đúng không tại vì cái thông tin tại cái vị trí số một này nó nhận được cái thông tin tại cái layer tại cái vị trí này tức là cái từ thứ hai đưa vô rồi từ thứ ba đưa vô đây như vậy nó đã thấy trước đáp án như vậy là không có đáp án không có được phép như vậy vậy thì chúng ta phải bỏ đi các cái cạnh nối màu đỏ này đi chúng ta phải đảm bảo như vậy thì khi đó cái quá trình decode nó mới thật sự là đúng như cái quy tắc của mình đó là chúng ta lần lượt đưa ra các cái dự đoán cho từ từ từ của mình chứ không được phép\n\n---\n\nVideo: [CS431 - Chương 10] Part 3： Cơ chế Self-Attention\nđể tính ra được cái giá trị ở đây chúng ta sẽ phải cần tính trước các giá trị ở layer trước đó ở đây là cái layer số 2 ở đây là layer số 3 đây là layer số 2 và đây là layer số 4 but which is other than in sheep hood thì để tính được cái layer số 3 chúng ta sẽ phải cần thông tin của các layer số 1 và thông tin của layer số 1 thì lại cần thông tin của layer số 2 và ở layer số 2 thì lại cần thông tin của layer số 1 thì tại đây chúng ta thấy là cái trạng thái ẩn này nó sẽ bị phụ thuộc bởi một cái phép tính trước đó đó là đây và các cái phép tính này thì thực hiện song song chính vì nó thực hiện song song nên không có cái phép tính nào phụ thuộc với phép tính nào rồi và ở đây thì chúng ta sẽ có cái khái niệm self-attention thì thật ra nó cũng chính là self-attention\n\n---\n\nVideo: [CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder\nCho cái từ Do. Chúng ta sẽ Ở đây chỗ này là màu trắng. Chỗ này là màu trắng, tức là chúng ta được phép thấy cái từ Start Chúng ta được phép thấy cái từ Start. Nhưng không được phép thấy cái từ Do. Và không được thấy cái từ You, từ Understand. Trong quá trình mà decode cái từ Do. Chúng ta sẽ được thấy cái từ Start. Được thấy cái từ Do, nhưng không được thấy cái từ You, từ Understand. Và trong quá trình mà decode cái từ Understand. Chúng ta sẽ được thấy hết các từ Start, Do, You nhưng không được thấy từ Understand. Được hãy để hnae biểu気 Fish disappear is original. chính là cái mass multi-head cell tension và về công thức tính toán thì cũng rất là đơn giản nếu như cái kỳ nếu như cái kG của mình mà bé hơn Y tức là cái kỳ của mình là những cái từ đã thấy đã thấy thì chúng ta sẽ giữ nguyên cái công thức attention score\n\n---\n\nVideo: [CS431 - Chương 10] Part 3： Cơ chế Self-Attention\nđó thì các cái từ nó sẽ tự chú ý với nhau trong đó nó cũng có cả chú ý đến chính nó chú ý đến chính nó ở đây và chú ý đến những cái từ còn lại trong cái giai đoạn encode của mình hoặc là trong cái giai đoạn decode tức là nó sẽ tự chú ý đến những cái từ trong cái giai đoạn decode của mình đó là cái sự khác biệt giữa cái khái niệm attention và self-attention\n\n---\n\nVideo: [CS431 - Chương 10] Part 6： Một số vấn đề của Transformer\nphần tiếp theo thì chúng ta sẽ cùng lượt qua một số điểm yếu của kiến trúc transformer thì đầu tiên đó là cái chi phí tính toán nó sẽ phải tăng dần theo cái độ dài của chuỗi và cụ thể đó là nó sẽ phụ thuộc với một cái hàm mật 2 theo cái độ dài của chuỗi, tại sao lại như vậy tại vì ở đây chúng ta thấy là cái thao tác self-attention của mình nó sẽ phải đi thực hiện trên tất cả các cái cặp ví dụ cái chuỗi này mà càng dài, chuỗi này là có t t là 1, 2, 3 thì trong cái bước tiếp theo chúng ta sẽ phải đi tính tất cả các cái tính attention trên tất cả các cái từ của mình và chúng ta sẽ phải thực hiện cái việc này trên tất cả các cái cặp như vậy chi phí của mình sẽ là O, T bình phương\n\n---\n\nVideo: [CS431 - Chương 10] Part 3： Cơ chế Self-Attention\ntrong transformer chúng ta sẽ bắt gặp một cái khái niệm dùng rất là thường xuyên đó chính là self-attention tức là cái kỹ thuật tự chú ý thì ở trong cái bài trước chúng ta đã nói về cái khái niệm là attention tức là một cái truy vấn một cái từ truy vấn query của mình ở cái bước decoder nó sẽ truy xuất và tổng hợp nó sẽ truy xuất và tổng hợp thông tin từ các cái tập giá trị value của cái encoder thì như chúng ta thấy là đây là cái giai đoạn là encode và đây là cái giai đoạn decode thì trong cái giai đoạn decode này thì chúng ta sẽ phải lookup chúng ta sẽ phải tra vào tất cả những cái từ ở trong cái giai đoạn encode còn đây là encoder còn đây là cái quá trình là decode thì khi đó là nó gọi là attention tức là cái sự truy vấn của một cái query\n\n---\n\nVideo: [CS431 - Chương 10] Part 6： Một số vấn đề của Transformer\nvà chúng ta sẽ phải thực hiện cái việc này trên tất cả các cái cặp như vậy chi phí của mình sẽ là O, T bình phương chưa kể là chúng ta sẽ phải có thêm cái D là cái số chiều của cái vector này của mình thì đây chính là cái điểm yếu đầu tiên là chi phí tính toán nó sẽ thay đổi theo cái độ dài của cái chuỗi của mình do phải tính trên tất cả các cái cặp tương tác của self-attention và vấn đề tiếp theo đó là vấn đề về biểu diễn vị trí thì trong cái positional emitting thì chúng ta đã từng nhận xét đó là cái tính vị trí tuyệt đối nó sẽ không quan trọng vị trí tuyệt đối nó không quan trọng mà đôi khi cái vị trí tương đối vị trí tương đối trong cái attention nó là quan trọng vị trí tương đối là gì là vị trí giữa một cái từ thứ T với lại những cái từ\n\n---\n\nVideo: [CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder\ncái tính tuần tự này dẫn đến là không có song song không thể song song hóa được không thể tinh sáng song song như vậy thì giải pháp là như thế nào chúng ta biết rằng là self-attention sở dĩ có thể song song hóa được mà chúng ta biết là point of view là não tính tính tính củagoodness Đó là vì cái từ tại thời điểm hiện tại có thể nhìn được những cái từ của tương lai, những cái từ ở đằng sau đó. Đó thì nó mới có thể tính toán song song được. Vậy thì bây giờ làm sao để cái việc mà tính toán của các cái node ở cái layer, các cái trạng thái ẩn của các cái tầng của mình, nó vẫn thực hiện được một cách song song. Nó vẫn thực hiện được một cách song song. Đó chính là chúng ta sẽ sử dụng cơ chế Mesh, Multihead, Self-Attention. Thì chúng ta sẽ che các cái Attention của các cái từ phía sau bằng cách gán cái Attention Score của nó bằng trừ của cuối cùng. Vậy thì chúng ta sẽ có công thức ở đây.",
      "chunks_retrieved": 10,
      "chunks_used": 10,
      "generation_time_seconds": 18.260713
    },
    {
      "question_id": "sum_005",
      "query": "Query, Key, and Value (QKV) Vectors",
      "summary": "Query, Key, Value (QKV) Vectors — Tóm tắt ngắn gọn, toàn diện\n\nGiới thiệu\n- QKV là nhân tố lõi của cơ chế attention/ self-attention trong Transformer. Ý tưởng: dùng một query để \"truy vấn\" tập các key, từ đó lấy trọng số (attention) để tổng hợp các value tương ứng.\n\nĐịnh nghĩa & vai trò\n- Query (Q): vector biểu diễn yêu cầu/truy vấn (tập trung thông tin cần lấy).\n- Key (K): vector biểu diễn chỉ mục/khóa dùng để so khớp với query.\n- Value (V): vector chứa thông tin thực tế được tổng hợp theo trọng số attention.\n- Trong self-attention: mỗi token đồng thời sinh Q, K, V từ cùng embedding và so sánh với tất cả token (kể cả chính nó).\n\nCơ chế hoạt động (các bước chính)\n1. Dựng Q, K, V bằng phép biến đổi tuyến tính từ embedding X:\n   - X_Q = X · W_Q, X_K = X · W_K, X_V = X · W_V\n2. Tính score giữa mọi cặp query và key (dot product):\n   - Scores = X_Q · X_K^T  (cho tất cả cặp query-key)\n3. Chuẩn hóa thành phân phối (attention distribution) bằng softmax:\n   - α = softmax(Scores)\n4. Tổng hợp output cho mỗi query bằng tổng trọng số trên các value:\n   - Output = α · X_V\n5. (Trong thực thi) các head được ghép lại và qua một linear cuối cùng để ra kết quả.\n\nThông tin về vị trí (Positional encoding)\n- Embedding ban đầu không chứa thông tin thứ tự; ta mã hóa vị trí bằng vector p_i (positional encoding) và cộng vào embedding:\n  - X_i' = X_i + p_i\n- Khi cộng p_i vào trước khi nhân với W_Q,W_K,W_V thì Q,K,V sẽ chứa thông tin vị trí.\n\nMulti-head attention\n- Lý do: một từ có nhiều mối quan hệ; nhiều head cho phép học nhiều biểu hiện/quan hệ song song.\n- Mỗi head có bộ W_Q,W_K,W_V riêng, tính attention độc lập, sau đó concat và linearly project.\n\nEncoder–Decoder (cross-attention)\n- Trong decoder, query thường lấy từ trạng thái decoder hiện tại, còn key và value lấy từ output của encoder. Query của decoder truy vấn bộ key/value do encoder cung cấp để thu thập thông tin ngữ cảnh.\n\nMinh họa trực quan / ẩn dụ\n- Tương tự tra cứu: query = từ khoá tìm kiếm, key = chỉ mục trong cơ sở dữ liệu, value = nội dung trả về; attention là cơ chế sắp xếp/đánh giá mức liên quan rồi lấy giá trị tương ứng.\n\nƯu điểm / Nhận xét ngắn gọn\n- Cho phép mô hình nắm bắt mối quan hệ giữa mọi cặp token (long-range dependencies).\n- Multi-head mở rộng khả năng biểu diễn nhiều loại quan hệ cùng lúc.\n- Cần đảm bảo Q và K cùng chiều để tính dot product; positional encoding cần thêm để giữ thông tin thứ tự.\n\nTóm tắt (Key takeaways)\n- Q,K,V là ba phép biến đổi tuyến tính của embedding; attention = softmax(QK^T) · V.\n- Positional encoding được cộng vào embedding để Q/K/V chứa thông tin vị trí.\n- Multi-head dùng nhiều bộ QKV để học nhiều mối quan hệ; decoder dùng cross-attention (query từ decoder, key/value từ encoder).",
      "original_text": "Video: [CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder\nvà vi ngã i k ngã i và qi ngã i nó chính là các cái value key và query cũ rồi chưa có cái thông tin về mặt vector vị trí vi ngã i vi ngã ki và xin lỗi k ngã i và quy ngã i đây chính là những cái vector biểu diễn cho các cái từ cũ của mình cho các cái value key và query cũ của mình và khi chúng ta cộng thêm cái pi thì như vậy cái thông tin vi ki và qi của mình nó sẽ có được cái thông tin về mặt vị trí rồi và chúng ta có thể ở đây thì chúng ta sử dụng cái phép là phép cộng ở đây là chúng ta sử dụng là phép cộng và chúng ta hoàn toàn có thể thực hiện cái phép concat thì vi ngã i của mình đây là vi ngã i chúng ta thực hiện cái phép cộng với lại cái vector biểu diễn của pi\n\n---\n\nVideo: [CS431 - Chương 10] Part 4_2： Kiến trúc Transformer： Bộ Encoder\nmã hóa được cái chỉ số này và đưa vào bên trong cái vector của mình thì ở đây chúng ta giả sử là các cái chỉ số thứ tự y sẽ được mã hóa bằng một cái vector là pi cái chỉ số y sẽ được mã hóa bằng một cái vector tên là pi và pi này thì có d chiều và y của mình sẽ là các cái chỉ số chạy từ 1 cho đến t với t là cái độ dài của cái code hoặc là cái đoạn văn, độ vào của mình thì khi đó các cái vector value, key và query mới của mình thì nó sẽ được tính là bằng vi bằng vi ngã i cộng cho pi tức là chúng ta sẽ có cái sự tham gia của cái thông tin về mặt vị trí pi nó chứa cái thông tin về mặt vị trí của mình và vi ngã i k ngã i và qi ngã i nó chính là các cái value key và query cũ rồi chưa có cái thông tin về mặt vector vị trí\n\n---\n\nVideo: [CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder\nnó sẽ phải ánh xạ từ cái embedding vector đó về cái không gian ánh xạ về cái không gian của query của mình rồi xa nó sẽ nhân với ma trận k để ánh xạ về cái không gian của cái key của mình của cái key của mình và xa nhân với v để ánh xạ về cái không gian của cái value của mình và sang cái bước thứ 2 đó là chúng ta sẽ tính cái attention score giữa cái query và key thì ở trong trường hợp này query và key của mình nó đã có cùng một cái số chiều nó phải đưa về cùng một cái số chiều thì khi đó chúng ta chỉ việc thực hiện cái phép tích vô hướng tích vô hướng giữa một cái query và một cái key thứ chi bất kỳ và chúng ta sẽ trả về là cái relation tức là cái sự liên hệ giữa query và cái key này query thứ y và cái key thứ chi này sau đó chúng ta sẽ chuẩn hóa bước thứ 3 là chúng ta sẽ chuẩn hóa cái giá trị này về cái không gian sát xuất thì thông qua cái hàm softmax\n\n---\n\nVideo: [CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder\nchiếu về cái không gian của query key và value thì mỗi cái từ này nó sẽ đi so với lại các cái từ và thậm chí là nó so với cả chính nó nữa để tính ra cái score xq nhân với xk và triển khai ra thì xq của mình nó chính là x nhân với lại ma trận q rồi xk chính là x nhân với lại ma trận k tất cả chuyển vị thì khi chúng ta triển khai cái chuyển vị nào vào vào bên trong cái dấu hoạt này thì nó sẽ là x chuyển vị đem ra và k chuyển vị đem lên trước như vậy thì cái công thức của ma trận biến đổi x của ma trận attention score của mình đó chính là xq nhân với lại k chuyển vị nhân với x chuyển vị và sang bước thứ 3 chúng ta sẽ tính cái extension distribution với cái hàm softmax thì chúng ta sẽ có cái ma trận a và sang bước số 4 thì chúng ta sẽ tổng hợp là output là bằng cái công thức ở đây\n\n---\n\nVideo: [CS431 - Chương 2] Part 4_3： Kiến trúc Transformer： Bộ Encoder\nNó có đến những hai cái mối quan hệ Và một cách tổng quát thì chúng ta có thể gọi là mối quan hệ đó là 2 mối quan hệ này nè một từ có thể có rất nhiều cái mối quan hệ trong câu do đó chúng ta không sẽ không sử dụng một cái single head attention mà chúng ta sẽ sử dụng multi head thì ở đây chúng ta vẫn sẽ là hệ thống ký hiệu VK và Q tương ứng là value key và query chúng ta đưa qua cái Linear này bản chất đó chính là cái cái phép nhân nhân tuyến tính rồi sau khi chúng ta nhân tuyến tính xong chúng ta thực hiện cái kéo đó rồi đó chúng ta thực hiện cái kéo đó product để mà cuối cùng là chúng ta sẽ nhân với lại tuyến tính để tính ra cái giá trị vector tổng hợp đây chính là cái attention à à à à à à à à à à à à à rồi thì ở bên đây sẽ là cái multi head\n\n---\n\nVideo: [CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder\nsang bước thứ 2 thì chúng ta sẽ tính cái attention score giữa query và key của mình thì ở đây chúng ta sẽ có ma trận là xq nhân với lại xk khi này thì chúng ta sẽ tính là giữa các query giữa các query và các key chúng ta sẽ tính trên một chuỗi tất cả các cái cặp query và key với nhau nhưng mà lưu ý là ở cái bước sell attention này thì query và key của mình nó sẽ có tính là nó sẽ là nó sẽ tên ở đây chúng ta sẽ có các cái vector sau khi chúng ta đã sau khi chúng ta đã chiếu về cái không gian của query key và value thì mỗi cái từ này nó sẽ đi so với lại các cái từ và thậm chí là nó so với cả chính nó nữa để tính ra cái score\n\n---\n\nVideo: [CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder\nquery của mình chứ không phải là key rồi ở đây chính là query đây chính là query còn đây chính là key và value thì cái query này là cái query này chúng ta sẽ đi truy vấn trong cái tập key ở đây để từ đó chúng ta sẽ tổng hợp thông tin rồi thì công thức ở đây là đúng rồi và key của mình nó sẽ lấy từ se se là đến từ encoder là từ encoder rồi se ở đây cũng đến từ encoder và h1 h2 h3 là đến từ encoder và h3 h4 là đến từ encoder rồi thì chúng ta vẫn sẽ có bộ 3 là kvi rồi thì chúng ta vẫn sẽ có bộ 3 là kvi\n\n---\n\nVideo: [CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder\nquery thứ y và cái key thứ chi này sau đó chúng ta sẽ chuẩn hóa bước thứ 3 là chúng ta sẽ chuẩn hóa cái giá trị này về cái không gian sát xuất thì thông qua cái hàm softmax và công thức của softmax ở đây thì chúng ta sẽ có được cái alpha e g chính là cái attention distribution hay là attention score mà chúng ta đã được chuẩn hóa và sang bước số 4 là chúng ta sẽ tính tổng trọng số của các cái value tức là các cái trọng số alpha e g này sẽ nhân với value tương ứng để chúng ta trả kết quả về cái output e tức là output cho cái query thứ y output cho cái query thứ y của mình query thứ y của mình và khi này thì chúng ta sẽ có nếu chúng ta thực hiện trên cái vector, cái dạng vector hóa tức là chúng ta sẽ gom chúng ta sẽ gom\n\n---\n\nVideo: [CS431 - Chương 10] Part 4_1： Kiến trúc Transformer： Bộ Encoder\nnó sẽ đến cái module gọi là shareattention và shareattention nó dựa trên cái cơ chế của attention thế thì attention nó cũng na ná nó cũng tương đương với lại một cái phép truy vấn trong cái bảng dữ liệu của mình có điều nếu như truy vấn trong cái bảng dữ liệu của mình chúng ta có một cái query ở đây chúng ta sẽ tra trong cái cơ sở dữ liệu của mình các cái value thông qua cái chúng ta sẽ sort up dựa trên các cái key để chúng ta lấy thông tin của cái value thì ở đây chúng ta sẽ có các khí niệm là query, key và value thì ở đây chúng ta sẽ hình dung nó liên quan đến một cái ứng dụng trong thực tế đó chính là các cái hệ thống tìm kiếm về multimedia query của mình đó nó chính là các cái keyword khi mà mình tìm kiếm ví dụ như chúng ta muốn tìm kiếm một cái video về chủ đề đó là transformer thì cái keyword của mình sẽ là\n\n---\n\nVideo: [CS431 - Chương 10] Part 5： Kiến trúc Transformer： Bộ Decoder\ntransformer cũng vậy transformer cũng vậy giả sử như chúng ta có s1 s2 cho đến s t thuộc rd tức là cái output của encoder đây chính là cái output của encoder đây là encoder và h1 h2 ht là các cái input decoder tức là chúng ta sẽ có cái ki h i ở đây đây là cái input cho cái quá trình decoder input cho cái quá trình decoder khi đó thì cái cặp các bộ key value và query của mình thì nó sẽ có cái công thức như sau key thì nó sẽ lấy từ se đây là ở đây trong cái công thức này thì xin lỗi ở đây là chúng ta nhầm đây là query query của mình chứ không phải là key rồi ở đây chính là query",
      "chunks_retrieved": 37,
      "chunks_used": 10,
      "generation_time_seconds": 18.477925
    }
  ]
}