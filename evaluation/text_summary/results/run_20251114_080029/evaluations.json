{
  "run_info": {
    "run_id": "run_20251114_080029",
    "timestamp": "20251114_080029",
    "total_questions": 5,
    "successful_evaluations": 5,
    "failed_evaluations": 0,
    "evaluation_model": "gpt-5-mini",
    "threshold": 0.5,
    "n_questions": 10
  },
  "statistics": {
    "overall": {
      "mean": 0.7350639386189258,
      "min": 0.4782608695652174,
      "max": 0.9411764705882353,
      "median": 0.75,
      "std": 0.1508121580311992
    },
    "coverage": {
      "mean": 0.9199999999999999,
      "min": 0.8,
      "max": 1.0
    },
    "alignment": {
      "mean": 0.7417306052855925,
      "min": 0.4782608695652174,
      "max": 0.9411764705882353
    }
  },
  "scores": [
    {
      "question_id": "sum_001",
      "query": "Transformer Architecture",
      "score": 0.7058823529411765,
      "coverage_score": 0.9,
      "alignment_score": 0.7058823529411765,
      "success": true,
      "reason": "The score is 0.71 because the summary is generally faithful (it keeps recommendations like residual connections and scalable layer depth) but includes several unsupported specifics and omits key original details: it asserts per‑layer composition (MHA+FFN and layer norm), claims the decoder mirrors the encoder with masked self‑attention, describes multi‑head splitting/purpose and scaling of heads, and adds limitations (heaviness with many heads) plus extra tasks/usages (summarization, prompting/zero‑shot) that the original did not state. It also fails to mention that the original indicates BERT/GPT were trained on large corpora. These added/unverified details and omissions reduce fidelity to about 0.71.",
      "summary_chars": 3710,
      "original_text_chars": 7973,
      "evaluation_time_seconds": 127.36668700000001
    },
    {
      "question_id": "sum_002",
      "query": "Attention Mechanism",
      "score": 0.9411764705882353,
      "coverage_score": 1.0,
      "alignment_score": 0.9411764705882353,
      "success": true,
      "reason": "The score is 0.94 because the summary is largely faithful and contains no contradictions; it correctly reports that the attention output is used to compute the decoder output (C combined with the decoder state to compute Y_t). However, it adds an unsupported conclusion—that this usage \"improves translation/context matching accuracy\"—which the original text did not state, so a small deduction is warranted. Overall the summary is accurate and well-presented.",
      "summary_chars": 2345,
      "original_text_chars": 9080,
      "evaluation_time_seconds": 87.572304
    },
    {
      "question_id": "sum_003",
      "query": "Self-Attention",
      "score": 0.75,
      "coverage_score": 0.9,
      "alignment_score": 0.75,
      "success": true,
      "reason": "The score is 0.75 because the summary is mostly faithful with no direct contradictions, but it introduces several unsupported assertions (e.g., that multi-head attention is specifically to increase representational capacity, explicit comparisons to RNNs on parallelism/training time, labeling self-attention as explicitly ‘modeling long-range dependencies’, and claiming positional encodings default to absolute) and omits an explicit answer to whether decoding is inherently sequential and cannot be parallelized; these added claims and the missing clarification reduce but do not destroy overall accuracy.",
      "summary_chars": 3032,
      "original_text_chars": 6934,
      "evaluation_time_seconds": 86.070723
    },
    {
      "question_id": "sum_004",
      "query": "Multi-Head Self-Attention",
      "score": 0.4782608695652174,
      "coverage_score": 1.0,
      "alignment_score": 0.4782608695652174,
      "success": false,
      "reason": "The score is 0.48 because the summary keeps the core self-attention points (token-to-token interactions, masking, O(T^2) cost, and use in transformers) but introduces many unsupported specifics: it names and defines MHSA and multiple parallel heads, claims how heads are aggregated and produce different interaction types, and makes causal claims that masking enables fully parallel decoder training and that MHSA speeds training versus RNNs—none of which appear in the original. These added technical and causal details lower fidelity, though the summary remains broadly consistent with the original’s main ideas.",
      "summary_chars": 2803,
      "original_text_chars": 7835,
      "evaluation_time_seconds": 85.667333
    },
    {
      "question_id": "sum_005",
      "query": "Query, Key, and Value (QKV) Vectors",
      "score": 0.8,
      "coverage_score": 0.8,
      "alignment_score": 0.8333333333333334,
      "success": true,
      "reason": "The score is 0.80 because the summary is largely faithful and captures the main idea (multi-head outputs are combined) but adds unsupported specifics and leaves some source-answerable details ambiguous. Extra claims in the summary — e.g., that heads are \"concat then passed through a final linear projection\" and that positional encoding unambiguously makes Q/K/V contain position information — are not stated in the original text. The original also does not specify whether positional vector p_i is added before or after the linear projections, so the summary’s implication about Q/K/V containing position info overreaches. Two concrete questions the source can answer (whether p_i is added to ṽ_i,k̃_i,q̃_i and whether positional vectors p_i have dimension d) are not addressed in the summary. Overall correct in broad strokes but with unsupported/ambiguous details, justifying 0.80.",
      "summary_chars": 2706,
      "original_text_chars": 7841,
      "evaluation_time_seconds": 108.395669
    }
  ]
}