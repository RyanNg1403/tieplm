{
  "run_info": {
    "run_id": "run_20251114_081837",
    "timestamp": "20251114_081837",
    "total_questions": 50,
    "successful_evaluations": 50,
    "failed_evaluations": 0,
    "evaluation_model": "gpt-5-mini",
    "threshold": 0.5,
    "n_questions": 10
  },
  "statistics": {
    "overall": {
      "mean": 0.6921668394422467,
      "min": 0.1,
      "max": 1.0,
      "median": 0.7647058823529411,
      "std": 0.2099306168656
    },
    "coverage": {
      "mean": 0.7897777777777777,
      "min": 0.1,
      "max": 1.0
    },
    "alignment": {
      "mean": 0.7569198995432729,
      "min": 0.15,
      "max": 1.0
    }
  },
  "scores": [
    {
      "question_id": "sum_001",
      "query": "Transformer Architecture",
      "score": 0.7222222222222222,
      "coverage_score": 1.0,
      "alignment_score": 0.7222222222222222,
      "success": true,
      "reason": "The score is 0.72 because the summary captures key facts from the original (that Transformers enable direct interactions and long‑range dependency modeling, and it mentions pretraining, fine‑tuning, prompting, and BERT’s bidirectionality) but introduces several unsupported additions: framing Transformers as an ‘alternative to RNNs’ and claiming they reduce RNN computational costs, attributing stabilization/gradient benefits to residual connections, asserting fewer operations for Transformers vs RNNs, mentioning ‘zero‑shot’ approaches, and likening BERT to Bidirectional RNNs. These extra, unverified claims lower fidelity despite the summary’s generally accurate core points, yielding a fairly high but imperfect score.",
      "summary_chars": 3080,
      "original_text_chars": 7973,
      "evaluation_time_seconds": 119.840563
    },
    {
      "question_id": "sum_002",
      "query": "Attention Mechanism",
      "score": 0.9047619047619048,
      "coverage_score": 1.0,
      "alignment_score": 0.9047619047619048,
      "success": true,
      "reason": "The score is 0.90 because the summary is largely faithful and clear (no contradictions with the original) and correctly conveys that the encoder provides Values = S_i and that attention focuses on relevant source tokens and filters redundancy. However, it makes two minor overstatements: it implies Keys are the same vectors S_i (the original does not confirm this) and it asserts that attention definitively \"improves the quality of predictions\" for seq2seq models (the original only describes focusing/filtering, not a guaranteed improvement). These small extra claims justify a slight deduction.",
      "summary_chars": 2712,
      "original_text_chars": 9080,
      "evaluation_time_seconds": 84.59572
    },
    {
      "question_id": "sum_003",
      "query": "Self-Attention",
      "score": 0.8125,
      "coverage_score": 1.0,
      "alignment_score": 0.8125,
      "success": true,
      "reason": "The score is 0.81 because the summary faithfully conveys the core points (Transformer uses multi-head self-attention, tokens attend across positions, and decoder masking prevents seeing future tokens) with no direct contradictions, but it adds several specifics not present in the original text: it states that each token explicitly serves as query/key/value in self-attention, it frames the O(T^2) pairwise attention as a comparison or alternative to RNN time-step operation, and it claims multi-head attention learns different interaction patterns and that masking is tied to multi-head parallelization. Those unsupported additions reduce strict fidelity, so a high but not perfect score is warranted.",
      "summary_chars": 2998,
      "original_text_chars": 6934,
      "evaluation_time_seconds": 85.783181
    },
    {
      "question_id": "sum_004",
      "query": "Multi-Head Self-Attention",
      "score": 0.8636363636363636,
      "coverage_score": 0.9,
      "alignment_score": 0.8636363636363636,
      "success": true,
      "reason": "The score is 0.86 because the summary is largely faithful and coherent—correctly highlighting the O(T^2) cost and the importance of positional information—but it adds unsupported claims (that self-attention by itself does not encode order and that positional encodings must be used) and combines points into an overstated “major weakness.” It also omits an explicit statement about decoding being inherently sequential. These extra/unjustified assertions and slight omissions lower but do not fatally undermine the summary’s fidelity.",
      "summary_chars": 2790,
      "original_text_chars": 7835,
      "evaluation_time_seconds": 88.122311
    },
    {
      "question_id": "sum_005",
      "query": "Query, Key, and Value (QKV) Vectors",
      "score": 1.0,
      "coverage_score": 1.0,
      "alignment_score": 1.0,
      "success": true,
      "reason": "The score is 1.00 because the summary is fully faithful and accurate to the original text—retaining all key points and tone without adding, omitting, or contradicting information, and presenting the content clearly and concisely.",
      "summary_chars": 2973,
      "original_text_chars": 7841,
      "evaluation_time_seconds": 71.06364000000002
    },
    {
      "question_id": "sum_006",
      "query": "Attention Score Calculation",
      "score": 0.8095238095238095,
      "coverage_score": 1.0,
      "alignment_score": 0.8095238095238095,
      "success": true,
      "reason": "The score is 0.81 because the summary is largely accurate and contains no contradictions with the original text (good fidelity), but it introduces several plausible yet unsupported specifics that reduce its fidelity: it claims a Query equals the decoder state at the current position; that the attention output C is typically combined with the current state to predict the next hidden state; that each multi-head attention head has its own Q, K, V and that heads learn diverse correlations; and that the decoding attention discussion specifically applies to Seq2Seq/machine translation. These extra assertions, while reasonable, were not stated in the original and justify a less-than-perfect score.",
      "summary_chars": 2756,
      "original_text_chars": 9146,
      "evaluation_time_seconds": 100.916294
    },
    {
      "question_id": "sum_007",
      "query": "Attention Distribution (Softmax)",
      "score": 1.0,
      "coverage_score": 1.0,
      "alignment_score": 1.0,
      "success": true,
      "reason": "The score is 1.00 because the summary is fully faithful and faithful to the original: it contains no contradictions or extraneous details and preserves the original’s key points clearly and concisely, warranting a perfect score.",
      "summary_chars": 2472,
      "original_text_chars": 8411,
      "evaluation_time_seconds": 102.19346200000001
    },
    {
      "question_id": "sum_008",
      "query": "Attention Output/Context Vector",
      "score": 0.8,
      "coverage_score": 0.8,
      "alignment_score": 1.0,
      "success": true,
      "reason": "The score is 0.80 because the summary is largely faithful with no contradictions or added content, but it omits two factual details present in the original: that bidirectional encoding with LSTM was used prior to Transformers, and that LSTM is defined as Long Short Term Memory. These omissions reduce completeness though overall accuracy is high.",
      "summary_chars": 2282,
      "original_text_chars": 8532,
      "evaluation_time_seconds": 74.72250599999998
    },
    {
      "question_id": "sum_009",
      "query": "Cross-Attention (Encoder-Decoder Attention)",
      "score": 0.5909090909090909,
      "coverage_score": 1.0,
      "alignment_score": 0.5909090909090909,
      "success": true,
      "reason": "The score is 0.59 because the summary captures the core ideas (dot‑product scoring, normalization to a probability distribution, weighted‑sum aggregation, presence of cross‑attention and positional embeddings) but repeatedly introduces unsupported specifics and precise orderings not present in the original. It asserts softmax as the normalization, names cross‑attention explicitly as multi‑head and places it at a specific decoder step between masked self‑attention and the feed‑forward layer, claims a projection yields a vocabulary distribution, and attributes particular parallelization/learning claims to multi‑head cross‑attention — all extras beyond the source. Overall the summary is largely faithful in substance but contains multiple unwarranted details and some redundancy, justifying a moderate score.",
      "summary_chars": 2239,
      "original_text_chars": 8463,
      "evaluation_time_seconds": 122.07489000000001
    },
    {
      "question_id": "sum_010",
      "query": "Masked Multi-Head Self-Attention",
      "score": 0.6111111111111112,
      "coverage_score": 0.9,
      "alignment_score": 0.6111111111111112,
      "success": true,
      "reason": "The score is 0.61 because the summary captures several core ideas (self-attention, masking, multi-head attention) but contains a direct contradiction—it states a token cannot attend to itself while the original explicitly says tokens can attend to themselves—and adds multiple details not present in the original (specific multi-head split/concat/projection mechanics, claims that heads increase representation diversity, RNN runtime and long-range dependency comparisons, assertions about attention lacking relative position information and positional encodings addressing it, and concrete application claims). It also omits an original point the text could answer about absolute vs. relative positional importance. These factual contradiction(s), unwarranted additions, and omission reduce fidelity, justifying the moderate score.",
      "summary_chars": 3191,
      "original_text_chars": 7835,
      "evaluation_time_seconds": 98.509808
    },
    {
      "question_id": "sum_011",
      "query": "Transformer Encoder Architecture",
      "score": 0.55,
      "coverage_score": 0.9,
      "alignment_score": 0.55,
      "success": true,
      "reason": "The score is 0.55 because the summary gets the high-level Transformer points right but adds several unstated specifics and omits at least one answerable point: it asserts detailed Q/K/V origins and per-token Q/K/V matching in self-attention, claims decoder/self-attention processes in more detail than provided, treats parallelism as a latency reduction, generalizes bidirectionality to all encoders, claims superior long‑term retention versus RNN/LSTM, and states BERT’s ‘Masked Word’ pretraining and combined GPT/BERT training objectives that the original did not explicitly assert; it also fails to state that the original did indicate BERT and GPT are trained self‑supervised. These extra, over‑specific claims and omissions justify a mid‑range score.",
      "summary_chars": 2765,
      "original_text_chars": 8336,
      "evaluation_time_seconds": 129.660171
    },
    {
      "question_id": "sum_012",
      "query": "Transformer Decoder Architecture",
      "score": 0.8823529411764706,
      "coverage_score": 1.0,
      "alignment_score": 0.8823529411764706,
      "success": true,
      "reason": "The score is 0.88 because the summary largely reflects the original but contains two notable errors: it misattributes machine translation as an encoder+decoder example when the original associates it with GPT (decoder-only), and it omits prompting (including zero-shot prompting) as a primary usage method, claiming only fine-tuning/retraining. These are meaningful but limited inaccuracies, so the summary is mostly accurate but not perfect.",
      "summary_chars": 2665,
      "original_text_chars": 8224,
      "evaluation_time_seconds": 108.057706
    },
    {
      "question_id": "sum_013",
      "query": "Positional Encoding/Embedding",
      "score": 0.5,
      "coverage_score": 0.5,
      "alignment_score": 0.5384615384615384,
      "success": true,
      "reason": "The score is 0.50 because the summary mixes correct points with unsupported additions about positional embeddings (it infers that positional encoding is used because self-attention “does not contain temporal order”, that positional vectors are summed into token embeddings to give absolute positions, and that they are used in both encoder and decoder) which the original text does not explicitly state; at the same time the summary omits several concrete, answerable details the original provides (e.g., whether the encoder/decoder are discussed, presence of multi‑head or ‘gross’ attention, and whether a 10,000 denominator is used for positional encoding). These unwarranted inferences plus omissions lower fidelity, so a middling score is appropriate.",
      "summary_chars": 1877,
      "original_text_chars": 2588,
      "evaluation_time_seconds": 108.132244
    },
    {
      "question_id": "sum_014",
      "query": "Feed-Forward Networks in Transformer",
      "score": 0.8,
      "coverage_score": 0.8,
      "alignment_score": 0.8,
      "success": true,
      "reason": "The score is 0.80 because the summary is largely faithful and clearly conveys the lecture’s scope and the parallelization/independence advantages, but it introduces extra unsupported claims (it asserts the task was to summarize “Feed‑Forward Networks in Transformer,” infers “dense connections” and per‑position FFN implementation details not present, and frames attention as explicitly enabling long‑range token connections that weren’t directly stated). It also omits material the original did provide answers for (e.g., whether GPT is suitable for machine translation/automatic content generation and that foundation models use self‑supervised learning when labeled data are unavailable). No direct contradictions and generally good clarity justify a high but not perfect score.",
      "summary_chars": 2493,
      "original_text_chars": 8276,
      "evaluation_time_seconds": 101.75532099999998
    },
    {
      "question_id": "sum_015",
      "query": "BERT Model Architecture",
      "score": 0.6,
      "coverage_score": 0.6,
      "alignment_score": 0.9285714285714286,
      "success": true,
      "reason": "The score is 0.60 because the summary introduced an extra label—calling BERT and GPT “foundation models”—that the original text did not use, and it omitted several concrete details the original did include: use of Keras load_model and cell.model.summary to load and summarize models, the use of MSE as the loss function, Adam as a possible default optimizer, and ResNet50 as a potential pre-trained feature extractor. There are no direct contradictions, but the added label and missing implementation details reduce fidelity.",
      "summary_chars": 1558,
      "original_text_chars": 7868,
      "evaluation_time_seconds": 97.28064599999999
    },
    {
      "question_id": "sum_016",
      "query": "GPT Model Architecture",
      "score": 0.8,
      "coverage_score": 0.8,
      "alignment_score": 0.9411764705882353,
      "success": true,
      "reason": "The score is 0.80 because the summary is largely accurate and contains no direct contradictions, but it adds an unsupported detail by explicitly naming the attention mechanism for GPT (the original only states Transformer/decoder and next-token prediction) and it omits two facts the original does state: that BERT and GPT can be fine-tuned/retrained for downstream tasks and that the 'T' in ChatGPT stands for Transformer. Overall the summary is good but includes one extra claim and misses a couple of answerable points.",
      "summary_chars": 2436,
      "original_text_chars": 9071,
      "evaluation_time_seconds": 93.69920200000001
    },
    {
      "question_id": "sum_017",
      "query": "Vanishing Gradient Problem",
      "score": 0.7894736842105263,
      "coverage_score": 0.8,
      "alignment_score": 0.7894736842105263,
      "success": true,
      "reason": "The score is 0.79 because the summary captures many correct points (e.g., skip connections preserve feature‑map resolution and LSTMs address vanishing gradients) but also introduces inaccurate and unsupported claims and omits some specifics. Contradiction: the summary overstates the impact of skip connections by saying they enabled training of “tens to hundreds of layers,” while the original only gives an example of increasing depth up to about 8 layers and does not claim such extreme depth. Extra information: the summary adds assertions not in the original — that skip/residual connections are directly used for segmentation upsampling, that the same skip concept is applied to deep stacked RNNs, that bidirectional RNNs by themselves address long‑term vanishing or must be combined with LSTM/skip connections, and that LSTMs are specifically applied to language, speech, or time‑series. Missing details/questions: the summary omits whether the original states skip connections allow increasing depth (e.g., up to 8 layers) and whether decoders use unpooling and deconvolution for upsampling. These unsupported additions and omissions justify a good but not perfect score.",
      "summary_chars": 2743,
      "original_text_chars": 8400,
      "evaluation_time_seconds": 108.73422999999998
    },
    {
      "question_id": "sum_018",
      "query": "Computational Cost of Self-Attention",
      "score": 0.7272727272727273,
      "coverage_score": 0.8,
      "alignment_score": 0.7272727272727273,
      "success": true,
      "reason": "The score is 0.73 because the summary correctly highlights the key point that self-attention’s compute cost grows quadratically with sequence length and flags this as a weakness (hence reasonably faithful), but it introduces unsupported claims about large memory growth and that memory scales multiplicatively with heads/layers which the original did not state. It also omits clarifications the original could answer about dependence on D and multi-head/encoder–decoder attention (e.g., whether multi‑head runs multiple attentions and whether decoder queries retrieve encoder values). Overall accurate on the main limitation (justifying a moderate–high score) but reduced fidelity and completeness due to added, unsubstantiated memory claims and missing explanatory details; nevertheless the summary clearly communicates the primary O(T^2) cost concern.",
      "summary_chars": 1879,
      "original_text_chars": 8187,
      "evaluation_time_seconds": 93.360607
    },
    {
      "question_id": "sum_019",
      "query": "Recurrent Neural Networks (RNN) Architecture",
      "score": 0.9166666666666666,
      "coverage_score": 1.0,
      "alignment_score": 0.9166666666666666,
      "success": true,
      "reason": "The score is 0.92 because the summary faithfully conveys the original’s key points (LSTM as a gated variant addressing vanishing gradients and long-term dependencies; emphasis on Neural Machine Translation and early RNN use in NLP) with no contradictions, but it introduces unsupported extra claims — that LSTMs have more parameters/higher computational cost and that RNNs were used for specific tasks like language modeling, generation, and time-series — which were not stated in the source. These plausible but unreferenced additions justify a slight deduction from perfect fidelity.",
      "summary_chars": 3033,
      "original_text_chars": 8251,
      "evaluation_time_seconds": 101.26651000000001
    },
    {
      "question_id": "sum_020",
      "query": "Long Short-Term Memory (LSTM) Architecture",
      "score": 0.3888888888888889,
      "coverage_score": 0.9,
      "alignment_score": 0.3888888888888889,
      "success": false,
      "reason": "The score is 0.39 because the summary adds many unsupported inferences and extra claims not present in the source (e.g., that LSTM was explicitly designed to fix vanishing gradients/long-term dependencies, that it updates cell/hidden states each step to improve gradient flow, explicit advantages over vanilla RNN, its typical use as encoder/decoder with attention, and reasons for its decline after 2015–2016), while omitting at least one concrete detail the original did include (mention of sentiment analysis/text classification as a 1-to-1 task). The summary does correctly identify LSTM as an RNN variant and names its components, but the added unverifiable assertions and omissions reduce fidelity and justify the low score.",
      "summary_chars": 2792,
      "original_text_chars": 8080,
      "evaluation_time_seconds": 96.267284
    },
    {
      "question_id": "sum_021",
      "query": "LSTM Gates (Forget, Input, Output)",
      "score": 0.8,
      "coverage_score": 1.0,
      "alignment_score": 0.8,
      "success": true,
      "reason": "The score is 0.80 because the summary is generally faithful and clear but introduces extra, unsupported details that reduce fidelity: it asserts every gate (including the output gate) uses a sigmoid and that the output gate uses sigmoid (not stated in the original), and it claims implementations require declaring input size based on mentions of padding, pre-trained modules, and library declarations. These unwarranted additions slightly weaken the summary despite overall accuracy.",
      "summary_chars": 2687,
      "original_text_chars": 8578,
      "evaluation_time_seconds": 75.548658
    },
    {
      "question_id": "sum_022",
      "query": "Context Cell/State in LSTM",
      "score": 0.7222222222222222,
      "coverage_score": 0.9,
      "alignment_score": 0.7222222222222222,
      "success": true,
      "reason": "The score is 0.72 because the summary is partially faithful but contains a clear factual error and several unsupported additions: it incorrectly states that LSTM ‘replaces the cell with gates’ while the original explains LSTM augments a context cell with three gates to control remembering/forgetting. The summary also introduces extra details not in the original (specific gate math multiplying new information, a claim about sequence classification, mention of loading pre-trained modules, and an assertion about vanilla RNNs and vanishing gradients). Additionally, the summary fails to answer a question the original does address: whether a Bidirectional variant aggregates forward and backward information.",
      "summary_chars": 2417,
      "original_text_chars": 8457,
      "evaluation_time_seconds": 99.650518
    },
    {
      "question_id": "sum_023",
      "query": "Bidirectional RNN (BiRNN)",
      "score": 0.8,
      "coverage_score": 0.8,
      "alignment_score": 0.8636363636363636,
      "success": true,
      "reason": "The score is 0.80 because the summary is largely accurate (correctly emphasizes bidirectional architectures and LSTM’s role) but introduces unsupported specifics—claiming exact implementation details (two RNNs running in opposite directions with concat/sum of hidden states) and asserting common use of GRU—which the original text did not state; it also omits original-text facts that answerable questions reveal (that the encoder reads the full input and the decoder generates outputs, and the sentiment-analysis example “The movie was terribly exciting”), so it’s good but not fully faithful or complete.",
      "summary_chars": 2637,
      "original_text_chars": 8084,
      "evaluation_time_seconds": 105.312882
    },
    {
      "question_id": "sum_024",
      "query": "Deep Stacked/Multi-layer RNN",
      "score": 0.7,
      "coverage_score": 0.7,
      "alignment_score": 0.9166666666666666,
      "success": true,
      "reason": "The score is 0.70 because the summary captures some correct points (it mentions LSTM, bidirectional ANNs, and stacking/Transformers) but introduces unsupported implications about combining deep stacking with LSTM/bidirectional modules and about resolving long‑term memory that the original does not state. It also omits several original details the reader could expect to find: the attention mechanism, the Deep Visualization Toolbox, and Convolutional Neural Network (CNN) feature‑map discussion, which lowers fidelity and completeness.",
      "summary_chars": 2615,
      "original_text_chars": 8840,
      "evaluation_time_seconds": 106.044242
    },
    {
      "question_id": "sum_025",
      "query": "Sequence-to-Sequence (Seq2Seq) Model",
      "score": 0.4,
      "coverage_score": 0.4,
      "alignment_score": 0.8076923076923077,
      "success": false,
      "reason": "The score is 0.40 because the summary adds multiple unsupported claims absent from the original (introducing/defining Seq2Seq, saying implementations are mainly RNN/LSTM, claiming RNN/LSTM provide longer-dependency advantages, and listing applications like machine translation/text generation/label conversion) while omitting several concrete facts the original text does include (e.g., CNN final output layer has 10 classes; softmax example uses k=4 and output dim=4; linear regression learned θ≈7.7 and 2.97; logistic regression uses sigmoid with use_bias=true; and the linear regression program ran without errors). There are no direct contradictions, but these extraneous additions and key omissions significantly reduce the summary’s fidelity and usefulness.",
      "summary_chars": 3027,
      "original_text_chars": 7786,
      "evaluation_time_seconds": 95.487389
    },
    {
      "question_id": "sum_026",
      "query": "Encoder-Decoder Architecture",
      "score": 0.5,
      "coverage_score": 0.7,
      "alignment_score": 0.5,
      "success": true,
      "reason": "The score is 0.50 because the summary is partially faithful: it preserves the core query/key/value analogy and references the Transformer encoder, but it adds unsupported inferences (treating keys/values as an encoder’s stored info, claiming the idea is used to select/combine encoded information in an encoder–decoder, and asserting a decoder issues queries) that the original text did not state. It also omits several concrete facts the original did include or could answer (the exact video title, that the video is hosted on a YouTube channel, and whether the video contrasts attention with a table-data query). There are no direct contradictions, but these extra claims and omissions reduce fidelity, justifying a mid-range score.",
      "summary_chars": 1238,
      "original_text_chars": 790,
      "evaluation_time_seconds": 59.682447999999994
    },
    {
      "question_id": "sum_027",
      "query": "RNN Handling of Variable Length Sequences",
      "score": 0.6,
      "coverage_score": 0.6,
      "alignment_score": 0.65,
      "success": true,
      "reason": "The score is 0.60 because the summary correctly highlights the main topics (RNN/LSTM, embeddings, padding and seq2seq NMT) but adds several details not present in the original (claims about the summary’s conciseness/completeness, helpers for homogeneous batches, that embeddings+RNN explicitly “help represent tokens effectively”, trade-off discussion for max_review_length, and padded-position handling) and omits several specific factual points the original contains (full forms/acronyms, mention of Keras, Word2Vec, and other explicit details). There are no direct contradictions, so the summary is generally faithful but incomplete and occasionally includes unsupported elaborations, reducing its overall fidelity and completeness.",
      "summary_chars": 2946,
      "original_text_chars": 8745,
      "evaluation_time_seconds": 123.13895800000002
    },
    {
      "question_id": "sum_028",
      "query": "Skip Connection (Residual Module)",
      "score": 0.9,
      "coverage_score": 0.9,
      "alignment_score": 0.9130434782608695,
      "success": true,
      "reason": "The score is 0.90 because the summary is largely faithful and accurate, but it introduced unsupported claims: it treats skip connections/attention as making models easier to visualize or as reducing vanishing gradients—claims the original text did not assert. The original only said attention focuses on encoder positions and can create shortcut-like paths analogous to skip connections; it did not provide evidence that skip connections improve interpretability/visualization or that attention reduces vanishing gradients. Additionally, the summary omitted a factual point the original could answer (whether ResNet was published in 2016). Otherwise the summary is concise and mostly correct.",
      "summary_chars": 3022,
      "original_text_chars": 8999,
      "evaluation_time_seconds": 89.401398
    },
    {
      "question_id": "sum_029",
      "query": "Convolutional Neural Networks (CNN) Introduction",
      "score": 0.7647058823529411,
      "coverage_score": 1.0,
      "alignment_score": 0.7647058823529411,
      "success": true,
      "reason": "The score is 0.76 because the summary largely captures the original’s main points (so there are no direct contradictions and it remains largely faithful) but includes several unsupported inferences that reduce accuracy: it claims pooling “keeps the main/most salient features,” asserts fully-connected networks are “not optimal” for images and gives unmentioned reasons about image dimensionality and sparsity, implies traditional FC networks specifically struggle with images beyond the documented fixed-size input issue, and contrasts RNNs and CNNs as having different objectives/structure — none of which are stated in the original. These extra claims lower fidelity despite overall alignment.",
      "summary_chars": 2655,
      "original_text_chars": 8715,
      "evaluation_time_seconds": 104.69231699999999
    },
    {
      "question_id": "sum_030",
      "query": "Convolution Operation (Tích Chập)",
      "score": 0.8,
      "coverage_score": 0.8,
      "alignment_score": 0.9473684210526315,
      "success": true,
      "reason": "The score is 0.80 because the summary is largely accurate with no contradictions, but it introduces an extra, stronger claim that kernel size, filter count, and block order determine the network’s representational capacity (overstating the original). It also omits concrete details the original gave—e.g., the fully connected layer sizes (120, 84, 10) and that AlexNet used data augmentation—so the summary is mostly faithful but slightly incomplete and slightly overgeneralized.",
      "summary_chars": 2627,
      "original_text_chars": 8548,
      "evaluation_time_seconds": 96.16529399999999
    },
    {
      "question_id": "sum_031",
      "query": "Filter/Kernel in CNNs",
      "score": 0.1,
      "coverage_score": 0.1,
      "alignment_score": 0.3125,
      "success": false,
      "reason": "The score is 0.10 because the summary adds many unsupported assertions (e.g., claims about omitted filter/kernel details, normalizing images to [0,1], CNN-filter equivalences, normalization improving training speed/stability, benchmarking recommendations, and CNN design parameters) while omitting numerous concrete facts present in the original (specific similarity values for 'win' vs 'kai' and 'win' vs 'king', Word2Vec 300‑dim detail, model load time, linear-regression theta values, printed image label, identity-matrix property, and the proposed ReLU fix). It does not introduce direct contradictions, but the heavy inclusion of extra information and the omission of key original facts make the summary unfaithful and unreliable.",
      "summary_chars": 2804,
      "original_text_chars": 8443,
      "evaluation_time_seconds": 119.009578
    },
    {
      "question_id": "sum_032",
      "query": "Feature Maps (Tensor Output)",
      "score": 0.8888888888888888,
      "coverage_score": 0.8888888888888888,
      "alignment_score": 0.9032258064516129,
      "success": true,
      "reason": "The score is 0.89 because the summary faithfully conveys the core points (e.g., two 3×3 convolutions give an effective 5×5 receptive field; VGG uses sequences of 3×3; layers have K filters producing depth-K feature maps) but it adds unsupported or overstated claims about extra nonlinearity and parameter/efficiency benefits and implies quantitative parameter/computation effects the original did not state. It also downplays the original’s caution that stacking can increase computation and omits the simple RGB-channel example the original text provided. Overall mostly accurate but contains extra/unsubstantiated efficiency claims and a few omitted details.",
      "summary_chars": 3630,
      "original_text_chars": 9055,
      "evaluation_time_seconds": 150.906818
    },
    {
      "question_id": "sum_033",
      "query": "Pooling Layer (Max Pooling/Average Pooling)",
      "score": 0.8695652173913043,
      "coverage_score": 1.0,
      "alignment_score": 0.8695652173913043,
      "success": true,
      "reason": "The score is 0.87 because the summary is largely faithful—correctly reporting that pooling reduces spatial size and that weight sharing/parameter reduction can help with overfitting—but it adds plausible yet unsupported claims (it states that pooling itself reduces parameters in the fully connected layers and directly reduces overfitting, and explicitly lists benefits like reduced computational cost and increased small-translation invariance) that are not stated in the original. These added inferences make the summary slightly overreaching, so accuracy is high but not perfect.",
      "summary_chars": 2271,
      "original_text_chars": 7391,
      "evaluation_time_seconds": 94.557971
    },
    {
      "question_id": "sum_034",
      "query": "ReLU Activation Function",
      "score": 0.7894736842105263,
      "coverage_score": 0.9,
      "alignment_score": 0.7894736842105263,
      "success": true,
      "reason": "The score is 0.79 because the summary is largely faithful but adds stronger claims not present in the original (it generalizes AlexNet’s switch to ReLU into a blanket replacement of sigmoid/tanh across modern architectures, states ReLU is universally ‘popular’ and especially faster on very deep/large-data networks, and implies wide/default use in many CNN variants and a broader historical claim about sigmoid/tanh). It also omits a specific implementation detail the original gave: that the activation is set when configuring Conv2D (activation=function). No direct contradictions were present.",
      "summary_chars": 1954,
      "original_text_chars": 8099,
      "evaluation_time_seconds": 90.719088
    },
    {
      "question_id": "sum_035",
      "query": "Sigmoid/Tanh Activation Functions",
      "score": 0.4375,
      "coverage_score": 0.6,
      "alignment_score": 0.4375,
      "success": false,
      "reason": "The score is 0.44 because the summary adds several unsupported or broader claims not present in the original (e.g., referencing a specific CS431 course, asserting sigmoid/tanh were used to ‘introduce nonlinearity’ as a general pedagogical point, claiming ReLU explicitly sped up training or broadly outperforms modern activations, implying widespread use in many classical CNN/RNN examples, and describing vanishing gradients as ‘long dependencies’/many multiplications) while omitting concrete factual details the original did include (the example CNN’s 10 output classes, use of the ADAM optimizer, convolutional filter counts of 6 and 16, and fully connected layer sizes of 120 and 84). There are no direct contradictions, and the summary does capture the high-level idea that early networks used sigmoid/tanh and AlexNet switched to ReLU to avoid vanishing gradients, but the unsupported generalizations and missing architecture/optimizer details justify a below‑average score.",
      "summary_chars": 2242,
      "original_text_chars": 8051,
      "evaluation_time_seconds": 87.748502
    },
    {
      "question_id": "sum_036",
      "query": "Fully Connected (Dense) Layer",
      "score": 0.6666666666666666,
      "coverage_score": 0.7,
      "alignment_score": 0.6666666666666666,
      "success": true,
      "reason": "The score is 0.67 because the summary is generally accurate but adds several specifics not present in the original (e.g., calling the hidden dense layer an MLP, asserting per-neuron full connectivity wording, implying output activation choices and multi-class unit counts as explicit rules, and giving guidance about choosing unit counts and sigmoid-for-binary as a rule). Those extra assertions reduce fidelity. At the same time there are omissions: the summary fails to report details the original does answer (whether RNN/LSTM are discussed, whether Keras Embedding/Dense modules are mentioned, and whether the segments are part of CS431 chapters). There are no direct contradictions, so the summary is partly useful but weakened by added unsupported details and missed answerable points.",
      "summary_chars": 2054,
      "original_text_chars": 7844,
      "evaluation_time_seconds": 114.91541900000001
    },
    {
      "question_id": "sum_037",
      "query": "Weight Sharing in CNNs",
      "score": 0.3,
      "coverage_score": 0.3,
      "alignment_score": 0.5384615384615384,
      "success": false,
      "reason": "The score is 0.30 because the summary contains multiple unsupported or altered specifics (misstating the weight-access API—using layer.get_weights() vs the original cell.model.layer[index].getweight/getweight—claiming computational/memory efficiency and pooling benefits that the original did not assert, and asserting that weight sharing+local connectivity+pooling \"creates the power\" of CNNs) while also omitting many concrete details that the original did include (use of categorical cross-entropy and accuracy, pixel normalization to [0,1], ADAM optimizer, ReLU and identity initialization for RNNs, the Word2Vec similarity example, lin.getweight returning two arrays due to bias separation, and a printed sample label of 7). These added claims and omissions reduce fidelity and justify the low score, although the summary does capture the main CNN topics (convolution, pooling, weight sharing).",
      "summary_chars": 1807,
      "original_text_chars": 8576,
      "evaluation_time_seconds": 111.783587
    },
    {
      "question_id": "sum_038",
      "query": "Local Connectivity in CNNs",
      "score": 0.15,
      "coverage_score": 0.3,
      "alignment_score": 0.15,
      "success": false,
      "reason": "The score is 0.15 because the summary fabricates and generalizes a formal definition and many benefits of a supposed “local connectivity” concept (e.g., that it reduces interaction path length, enables vectorization/GPU parallelism, inherently detects local patterns, or causes/avoids vanishing gradients) that are not stated in the original text, while attributing comparisons and design recommendations (vs. self‑attention/RNNs) that the original does not make. At the same time the summary omits concrete, checkable details the original does include (Word2Vec similarity examples and values, programming guidance, linear regression parameters like θ0≈7.7 and θ1≈2.97, the bias-term convention, and the image geometric‑verification example). These unsupported additions plus missing factual specifics explain the very low score.",
      "summary_chars": 3377,
      "original_text_chars": 8015,
      "evaluation_time_seconds": 157.552002
    },
    {
      "question_id": "sum_039",
      "query": "Overfitting Mitigation",
      "score": 0.84,
      "coverage_score": 0.9,
      "alignment_score": 0.84,
      "success": true,
      "reason": "The score is 0.84 because the summary captures the main points (CNN components, data augmentation, GPUs speeding training) but introduces several plausible yet unsupported claims (that pooling explicitly reduces parameters for the next layer; that parameter reduction specifically \"utilizes\" GPUs for speedup; that AlexNet specifically used GPUs; and the caveat that augmentation \"does not fully replace\" good parameter design) and it omits an answerable detail from the original (whether increasing network depth increases parameter count). These extra/unsubstantiated additions reduce fidelity even though overall coverage is strong.",
      "summary_chars": 2840,
      "original_text_chars": 8551,
      "evaluation_time_seconds": 214.892499
    },
    {
      "question_id": "sum_040",
      "query": "Data Augmentation",
      "score": 0.4,
      "coverage_score": 0.4,
      "alignment_score": 0.47368421052631576,
      "success": false,
      "reason": "The score is 0.40 because the summary adds many unsupported details (claims about scaling, multiple transforms per image, freezing the feature extractor vs. full fine‑tuning, guidance that large‑dataset training depends less on augmentation, per‑epoch/mini‑batch pipeline specifics, validation/test handling, and augmentation drawbacks) that are not stated in the original, even though it does capture some original points (e.g., rotation/noise/brightness and that AlexNet used augmentation). Those extra, ungrounded assertions reduce fidelity, and the summary also fails to answer several concrete factual points the original text does cover (e.g., whether the face‑recognition task distinguishes identity 1 vs 2; logistic regression epochs=5; input features x1 and x2; use of keras.datasets.mnist.load_data; final CNN FC layer sizes 120, 84, 10; and that the 10 output units correspond to labels 0–9). Overall, unsupported additions and omitted factual answers justify a low–moderate score.",
      "summary_chars": 2221,
      "original_text_chars": 8410,
      "evaluation_time_seconds": 109.307956
    },
    {
      "question_id": "sum_041",
      "query": "ResNet Architecture",
      "score": 0.8571428571428571,
      "coverage_score": 1.0,
      "alignment_score": 0.8571428571428571,
      "success": true,
      "reason": "The score is 0.86 because the summary retains the main ideas (residual formula H(x)=convolution(x)+x, skip connections mitigating vanishing gradients, Inception multi-filter approach, and that upsampling can use unpooling/deconvolution) but adds unstated specifics: it mentions batch normalization/activations and exact convolutional-layer counts, asserts an unsupported “~8 layers” figure for deep RNNs, and claims ResNet is combined with Inception and upsampling for segmentation. Those extra, unsupported details slightly reduce fidelity but do not fundamentally misrepresent the core content.",
      "summary_chars": 2315,
      "original_text_chars": 8624,
      "evaluation_time_seconds": 116.710556
    },
    {
      "question_id": "sum_042",
      "query": "MobileNet Architecture",
      "score": 0.9,
      "coverage_score": 0.9,
      "alignment_score": 0.9285714285714286,
      "success": true,
      "reason": "The score is 0.90 because the summary is largely faithful (no contradictions) but includes a small unsupported addition—it claims similarity to the Inception bottleneck while the original only referenced a “bottleneck of Google Linux”—and it fails to answer whether the described attention is different from the query in the video dataset; these minor overreach and omission justify the slight deduction.",
      "summary_chars": 1863,
      "original_text_chars": 2627,
      "evaluation_time_seconds": 102.379282
    },
    {
      "question_id": "sum_043",
      "query": "Depthwise Separable Convolution",
      "score": 0.3,
      "coverage_score": 0.3,
      "alignment_score": 0.6842105263157895,
      "success": false,
      "reason": "The score is 0.30 because the summary inserts several unsupported claims and formulas not present in the original (e.g., equating depthwise separable convolution to a “bottleneck” structure, saying MobileNet specifically targets/optimizes for edge devices, framing DSC as an explicit capacity/accuracy trade-off, and stating general and combined DSC parameter formulas) while omitting precise numeric derivations and many concrete facts the original did provide (so the summary cannot answer multiple specific questions about pooling, filter sizes, FC layer sizes, AlexNet’s use of data augmentation, input image depth, stride-2 pooling effects, and the core CNN operations). There are no direct contradictions with the original, but the added unsupported details and the omissions substantially reduce fidelity.",
      "summary_chars": 2472,
      "original_text_chars": 8957,
      "evaluation_time_seconds": 136.927594
    },
    {
      "question_id": "sum_044",
      "query": "CNN Visualization Techniques",
      "score": 0.6,
      "coverage_score": 0.6,
      "alignment_score": 0.9047619047619048,
      "success": true,
      "reason": "The score is 0.60 because the summary conveys the main CNN visualization and application points and contains no contradictions, but it introduces extra claims not supported by the original (e.g., stating whether the summary is 'concise'/'comprehensive' and implying all localization/classification models are CNN-based) and omits answers the original did provide about: whether CNN inputs can be color or grayscale images; whether the lectures mention the Softmax function; whether object detection applications include autonomous driving (vehicles, pedestrians, obstacles); and whether the detection pipeline is described as region-proposal followed by per-bounding-box classification.",
      "summary_chars": 2375,
      "original_text_chars": 8193,
      "evaluation_time_seconds": 88.121863
    },
    {
      "question_id": "sum_045",
      "query": "Object Detection (YOLO/R-CNN)",
      "score": 0.8928571428571429,
      "coverage_score": 1.0,
      "alignment_score": 0.8928571428571429,
      "success": true,
      "reason": "The score is 0.89 because the summary is largely faithful—correctly reflecting that Faster R-CNN is more accurate and that YOLO evolved to improve detection—while adding a few plausible but unsupported details: explicitly describing Faster R-CNN as a ‘‘generate region proposals then classify/regress’’ two-stage pipeline, asserting later YOLO versions specifically handle scale/small objects/occlusion and increase speed for those reasons, and stating object detection is used as preprocessing for segmentation. These extra inferences slightly reduce fidelity but do not contradict the original, so overall quality is high.",
      "summary_chars": 3112,
      "original_text_chars": 8744,
      "evaluation_time_seconds": 99.420044
    },
    {
      "question_id": "sum_046",
      "query": "Semantic Segmentation (U-Net)",
      "score": 0.76,
      "coverage_score": 1.0,
      "alignment_score": 0.76,
      "success": true,
      "reason": "The score is 0.76 because the summary captures the main points (U‑Net, skip connections, CNN use for localization/segmentation and presence of variants) so it is generally faithful, but it introduces several unsupported or overstated claims not present in the source—e.g., explicit encoder/decoder role wording, asserting a preserved core principle across all U‑Net variants, routine use of the same threshold/bright‑spot method to turn detections into full segmentations, claiming interpolation/thresholding must be “performed correctly” as a primary caveat, and comparing U‑Net as standard versus encoder‑only architectures. These extra inferences reduce fidelity, so the score reflects a moderately high but imperfect summary.",
      "summary_chars": 3110,
      "original_text_chars": 6126,
      "evaluation_time_seconds": 91.859594
    },
    {
      "question_id": "sum_047",
      "query": "Gradient Descent Optimization",
      "score": 0.6,
      "coverage_score": 0.9,
      "alignment_score": 0.6,
      "success": true,
      "reason": "The score is 0.60 because the summary preserves the core ideas (gradient-based training, learning rate α, backpropagation/auto-diff, common issues like initialization/local minima/vanishing gradients, and suggested optimizers) but both omits and adds several unsupported specifics. Missing from the summary were explicit items present in the original (the concrete gradient-update/formula, stopping criteria such as fixed epochs, the link that backprop is derived via the chain rule, and reference to CNN components/architecture). The summary also introduces extra details not stated in the original (the explicit update rule θ ← θ − α∇_θL(θ), explicit statements about α controlling step size and causing oscillation/slow convergence if mis-set, labeling GD as ‘simple/effective’, phrasing of ‘reducing the derivative’, and a comprehensive checklist of dependencies). These omissions and unsupported additions reduce fidelity while retaining the main points, warranting a moderate score.",
      "summary_chars": 2665,
      "original_text_chars": 9185,
      "evaluation_time_seconds": 123.49481899999999
    },
    {
      "question_id": "sum_048",
      "query": "Adam Optimizer",
      "score": 0.6,
      "coverage_score": 0.6,
      "alignment_score": 0.8,
      "success": true,
      "reason": "The score is 0.60 because the summary correctly reflects the recommendation to use Adam in some examples and that libraries handle optimizer defaults, but it overgeneralizes—claiming Adam is the go-to for all listed model types when the original shows SGD for linear/logistic cases. It therefore introduces unsupported extra claims and omits specific details the source contains or can answer (accuracy as metric, learning rate 0.01, calling cell.model.fit, momentum=0.9), reducing fidelity.",
      "summary_chars": 2032,
      "original_text_chars": 7778,
      "evaluation_time_seconds": 99.833408
    },
    {
      "question_id": "sum_049",
      "query": "Binary Cross Entropy Loss",
      "score": 0.9,
      "coverage_score": 0.9,
      "alignment_score": 0.9411764705882353,
      "success": true,
      "reason": "The score is 0.90 because the summary largely matches the original—covering BCE examples (logistic regression, simple nets, RNN/LSTM) and optimizer mentions (Adam/SGD with momentum ≈ 0.9)—but it added an unsupported overgeneralization that BCE is always appropriate for every binary-class model, and it omitted whether the original discussed CNN architectures.",
      "summary_chars": 1761,
      "original_text_chars": 7134,
      "evaluation_time_seconds": 63.117826
    },
    {
      "question_id": "sum_050",
      "query": "Categorical Cross Entropy Loss",
      "score": 0.7,
      "coverage_score": 0.7,
      "alignment_score": 0.8333333333333334,
      "success": true,
      "reason": "The score is 0.70 because the summary captures many correct points but adds unsupported claims and omits some concrete details: it overstates CCE as “widely used” for neural networks despite the original only showing CCE for softmax and CNN and a neural-net example using binary cross-entropy; it claims gradients are small when ŷ_j ≈ 1 (not stated in the original); and it adds that large-loss behavior “needs balancing with optimizer, learning rate, regularization,” which the original did not mention. It also fails to mention specifics the original provides (that binary cross-entropy is used for binary classification, that the neural-network formula uses a sigmoid, and the momentum = 0.9 recommendation). The summary avoids direct contradictions but mixes accurate points with extra, unjustified assertions, justifying a moderately high but imperfect score.",
      "summary_chars": 2146,
      "original_text_chars": 6825,
      "evaluation_time_seconds": 84.605779
    }
  ]
}