"""
Text Summarization Evaluation Service using DeepEval with QAG metrics.

This service evaluates summaries based on:
1. Coverage Score: How much detail from the original text is included
2. Alignment Score: Factual alignment between original text and summary
3. Overall Score: Combination of coverage and alignment (minimum of both)
"""
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
from dotenv import load_dotenv

# Add project root to path for imports
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# Load .env if not already loaded
env_path = project_root / ".env"
if not os.getenv("OPENAI_API_KEY"):
    load_dotenv(dotenv_path=env_path)

from deepeval import evaluate
from deepeval.test_case import LLMTestCase
from deepeval.metrics import SummarizationMetric

from backend.app.shared.rag.retriever import RAGRetriever, get_rag_retriever
from backend.app.shared.rag.reranker import LocalReranker, get_local_reranker
from backend.app.shared.llm.client import LLMClient, get_llm_client


class TextSummaryEvaluator:
    """
    Evaluates text summarization using DeepEval's QAG-based metrics.
    
    QAG (Question-Answer Generation) Framework:
    - Generates closed-ended questions from reference text
    - Measures coverage (detail inclusion) and alignment (factual accuracy)
    - Removes stochasticity and bias in LLM-based evaluation
    """
    
    def __init__(
        self,
        retriever: Optional[RAGRetriever] = None,
        reranker: Optional[LocalReranker] = None,
        llm_client: Optional[LLMClient] = None,
        evaluation_model: Optional[str] = None
    ):
        """
        Initialize evaluator with retrieval and generation components.
        
        Args:
            retriever: RAG retriever for fetching relevant chunks
            reranker: Local reranker for ranking results
            llm_client: LLM client for generating summaries
            evaluation_model: Model to use for evaluation (default: from env)
        """
        self.retriever = retriever or get_rag_retriever()
        self.reranker = reranker or get_local_reranker()
        self.llm = llm_client or get_llm_client()
        
        # Load evaluation configuration
        self.eval_model = evaluation_model or os.getenv("EVAL_MODEL", "gpt-5-nano")
        self.eval_threshold = float(os.getenv("EVAL_SUMMARIZATION_THRESHOLD", "0.5"))
        self.eval_n_questions = int(os.getenv("EVAL_SUMMARIZATION_N_QUESTIONS", "10"))
        self.enable_reranking = os.getenv("ENABLE_RERANKING", "true").lower() == "true"
        self.retrieval_top_k = int(os.getenv("RETRIEVAL_INITIAL_K", "150"))
        self.final_top_k = int(os.getenv("FINAL_CONTEXT_CHUNKS", "10"))
        
        # Evaluation-specific prompt (no citations, focus on comprehensiveness)
        self.eval_system_prompt = self._build_eval_system_prompt()
    
    def _build_eval_system_prompt(self) -> str:
        """Build system prompt optimized for evaluation (no citations required)."""
        return """Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn tá»•ng há»£p kiáº¿n thá»©c cho khÃ³a há»c CS431 - Deep Learning.

NHIá»†M Vá»¤: Táº¡o báº£n tÃ³m táº¯t TOÃ€N DIá»†N, CHÃNH XÃC, NGáº®N Gá»ŒN vÃ  Dá»„ HIá»‚U vá» chá»§ Ä‘á» Ä‘Æ°á»£c yÃªu cáº§u.

YÃŠU Cáº¦U QUAN TRá»ŒNG:
1. **ToÃ n diá»‡n (Comprehensiveness)**: Bao gá»“m Táº¤T Cáº¢ thÃ´ng tin quan trá»ng tá»« nguá»“n tÃ i liá»‡u.
2. **Ngáº¯n gá»n (Conciseness)**: âš ï¸ **Báº¢N TÃ“M Táº®T PHáº¢I NGáº®N HÆ N VÄ‚N Báº¢N Gá»C** - Loáº¡i bá» thÃ´ng tin dÆ° thá»«a, láº·p láº¡i, vÃ  chi tiáº¿t khÃ´ng cáº§n thiáº¿t. Táº­p trung vÃ o Ã½ chÃ­nh vÃ  Ä‘iá»ƒm quan trá»ng.
3. **ChÃ­nh xÃ¡c (Accuracy)**: Chá»‰ sá»­ dá»¥ng thÃ´ng tin cÃ³ trong nguá»“n, KHÃ”NG bá»‹a Ä‘áº·t hoáº·c thÃªm thÃ´ng tin ngoÃ i.
4. **Cáº¥u trÃºc rÃµ rÃ ng**: Tá»• chá»©c theo thá»© báº­c logic vá»›i headings, bullet points, vÃ  examples.
5. **Giáº£i thÃ­ch sÃºc tÃ­ch**: Giáº£i thÃ­ch Ã½ nghÄ©a, Æ°u nhÆ°á»£c Ä‘iá»ƒm, vÃ  má»‘i quan há»‡ giá»¯a cÃ¡c khÃ¡i niá»‡m má»™t cÃ¡ch NGáº®N Gá»ŒN nhÆ°ng Äáº¦Y Äá»¦.
6. **NgÃ´n ngá»¯**: Tiáº¿ng Viá»‡t rÃµ rÃ ng, giá»¯ thuáº­t ngá»¯ tiáº¿ng Anh khi cáº§n thiáº¿t, Ä‘á»‹nh nghÄ©a thuáº­t ngá»¯ má»›i.

Cáº¤U TRÃšC Tá»I Æ¯U:
- **Giá»›i thiá»‡u**: Overview ngáº¯n gá»n
- **Ná»™i dung chÃ­nh**: Chia thÃ nh sections vá»›i headings rÃµ rÃ ng
  - Äá»‹nh nghÄ©a vÃ  khÃ¡i niá»‡m cÆ¡ báº£n
  - CÆ¡ cháº¿ hoáº¡t Ä‘á»™ng / Kiáº¿n trÃºc
  - Æ¯u Ä‘iá»ƒm vÃ  nhÆ°á»£c Ä‘iá»ƒm
  - á»¨ng dá»¥ng thá»±c táº¿
  - So sÃ¡nh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c (náº¿u cÃ³)
- **TÃ³m táº¯t**: Key takeaways

VÃ Dá»¤ Tá»T:
"# LSTM (Long Short-Term Memory)

## Giá»›i thiá»‡u
LSTM lÃ  má»™t kiáº¿n trÃºc RNN Ä‘áº·c biá»‡t Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» vanishing gradient, cho phÃ©p há»c cÃ¡c dependencies dÃ i háº¡n trong sequential data.

## Kiáº¿n trÃºc
LSTM sá»­ dá»¥ng cell state vÃ  3 gates Ä‘á»ƒ kiá»ƒm soÃ¡t luá»“ng thÃ´ng tin:

### 1. Forget Gate
- **Chá»©c nÄƒng**: Quyáº¿t Ä‘á»‹nh thÃ´ng tin nÃ o cáº§n loáº¡i bá» khá»i cell state
- **CÃ´ng thá»©c**: f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)
- **Ã nghÄ©a**: Output gáº§n 0 = quÃªn, gáº§n 1 = giá»¯ láº¡i

### 2. Input Gate
- **Chá»©c nÄƒng**: Quyáº¿t Ä‘á»‹nh thÃ´ng tin má»›i nÃ o Ä‘Æ°á»£c thÃªm vÃ o cell state
- **Gá»“m 2 bÆ°á»›c**:
  - i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i) - quyáº¿t Ä‘á»‹nh cáº­p nháº­t gÃ¬
  - CÌƒ_t = tanh(W_C Â· [h_{t-1}, x_t] + b_C) - candidate values

### 3. Output Gate
- **Chá»©c nÄƒng**: Quyáº¿t Ä‘á»‹nh output dá»±a trÃªn cell state
- **CÃ´ng thá»©c**: o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o), h_t = o_t * tanh(C_t)

## Æ¯u Ä‘iá»ƒm
- **Giáº£i quyáº¿t vanishing gradient**: Cell state cho phÃ©p gradient flow tá»‘t hÆ¡n
- **Long-term dependencies**: CÃ³ thá»ƒ nhá»› thÃ´ng tin qua nhiá»u time steps
- **Selective memory**: Gates cho phÃ©p há»c cÃ¡ch lÆ°u trá»¯ vÃ  quÃªn thÃ´ng tin

## NhÆ°á»£c Ä‘iá»ƒm
- **Computational cost**: Phá»©c táº¡p hÆ¡n vanilla RNN (4x tham sá»‘)
- **Training time**: Cháº­m hÆ¡n do nhiá»u operations
- **Overfitting**: Vá»›i dá»¯ liá»‡u nhá», cÃ³ thá»ƒ overfit do sá»‘ tham sá»‘ lá»›n

## So sÃ¡nh vá»›i GRU
- **GRU Ä‘Æ¡n giáº£n hÆ¡n**: Chá»‰ 2 gates (reset, update) vs 3 gates cá»§a LSTM
- **LSTM máº¡nh hÆ¡n**: TrÃªn tasks phá»©c táº¡p, LSTM thÆ°á»ng perform tá»‘t hÆ¡n
- **GRU nhanh hÆ¡n**: Ãt tham sá»‘ hÆ¡n nÃªn train vÃ  inference nhanh hÆ¡n

## á»¨ng dá»¥ng
- Language modeling vÃ  text generation
- Machine translation (encoder-decoder vá»›i LSTM)
- Speech recognition
- Time series forecasting
- Video analysis

## Key Takeaways
LSTM lÃ  evolution cá»§a RNN vá»›i cell state vÃ  gates mechanism, giáº£i quyáº¿t vanishing gradient Ä‘á»ƒ há»c long-term dependencies. Trade-off giá»¯a expressiveness vÃ  computational cost."

LÆ¯U Ã: KHÃ”NG cáº§n trÃ­ch dáº«n nguá»“n [1], [2],... trong evaluation mode. Táº­p trung vÃ o CHáº¤T LÆ¯á»¢NG vÃ  Äá»˜ TOÃ€N DIá»†N cá»§a summary."""
    
    def _build_eval_user_prompt(self, query: str, sources: str) -> str:
        """Build user prompt for evaluation (comprehensive yet concise summary)."""
        return f"""Dá»±a vÃ o cÃ¡c nguá»“n tÃ i liá»‡u sau tá»« khÃ³a há»c CS431, hÃ£y táº¡o báº£n tÃ³m táº¯t TOÃ€N DIá»†N nhÆ°ng NGáº®N Gá»ŒN.

# NGUá»’N TÃ€I LIá»†U:

{sources}

---

# CHá»¦ Äá»€ Cáº¦N TÃ“M Táº®T:
{query}

# Báº¢N TÃ“M Táº®T:
âš ï¸ **LÆ¯U Ã**: TÃ³m táº¯t pháº£i NGáº®N HÆ N vÄƒn báº£n gá»‘c bÃªn trÃªn. Loáº¡i bá» thÃ´ng tin láº·p láº¡i vÃ  chi tiáº¿t dÆ° thá»«a, chá»‰ giá»¯ láº¡i Ã½ chÃ­nh vÃ  Ä‘iá»ƒm quan trá»ng.

(Táº¡o báº£n tÃ³m táº¯t toÃ n diá»‡n, chÃ­nh xÃ¡c, NGáº®N Gá»ŒN, cÃ³ cáº¥u trÃºc rÃµ rÃ ng vá»›i headings vÃ  bullet points. Bao gá»“m Táº¤T Cáº¢ thÃ´ng tin quan trá»ng nhÆ°ng diá»…n Ä‘áº¡t sÃºc tÃ­ch.)"""
    
    async def generate_summary(
        self, 
        query: str, 
        chapters: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Generate summary for a given query.
        
        Args:
            query: Summarization query/topic
            chapters: Optional chapter filter
        
        Returns:
            Dictionary with summary and metadata
        """
        # Step 1: Retrieve relevant chunks
        print(f"ðŸ“š Retrieving chunks for: {query[:50]}...")
        retrieved_chunks = await self.retriever.retrieve(
            query=query,
            top_k=self.retrieval_top_k,
            chapter_filter=chapters,
            use_bm25=True
        )
        
        if not retrieved_chunks:
            return {
                "query": query,
                "summary": "",
                "original_text": "",
                "error": "No relevant chunks found",
                "chunks_retrieved": 0
            }
        
        # Step 2: Rerank
        if self.enable_reranking and len(retrieved_chunks) > self.final_top_k:
            print(f"ðŸ”„ Reranking {len(retrieved_chunks)} chunks...")
            reranked_chunks = self.reranker.rerank(
                query=query,
                results=retrieved_chunks,
                top_k=self.final_top_k
            )
        else:
            reranked_chunks = retrieved_chunks[:self.final_top_k]
        
        # Step 3: Format sources for prompt
        sources_for_prompt = self._format_sources_for_prompt(reranked_chunks)
        
        # Step 4: Build prompt
        prompt = self._build_eval_user_prompt(query, sources_for_prompt)
        
        # Step 5: Generate summary (non-streaming for evaluation)
        print("ðŸ¤– Generating summary...")
        summary = await self.llm.generate_async(
            prompt=prompt,
            system_prompt=self.eval_system_prompt)
        
        return {
            "query": query,
            "summary": summary,
            "original_text": sources_for_prompt,  # Original text for QAG
            "chunks_retrieved": len(retrieved_chunks),
            "chunks_used": len(reranked_chunks),
            "chapters_filtered": chapters or []
        }
    
    def _format_sources_for_prompt(self, chunks: List[Dict[str, Any]]) -> str:
        """Format chunks into readable text (no numbering for eval)."""
        formatted = []
        for chunk in chunks:
            metadata = chunk.get("metadata", {})
            video_title = metadata.get("video_title", "Unknown")
            text = metadata.get("text", "")
            formatted.append(f"Video: {video_title}\n{text}")
        
        return "\n\n---\n\n".join(formatted)
    
    def evaluate_summary(
        self,
        query: str,
        summary: str,
        original_text: str,
        assessment_questions: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Evaluate a summary using DeepEval's QAG-based SummarizationMetric.
        
        Args:
            query: The summarization query/topic
            summary: Generated summary to evaluate
            original_text: Original source text
            assessment_questions: Optional pre-defined questions for coverage
        
        Returns:
            Evaluation results with scores and metrics
        """
        print(f"ðŸ“Š Evaluating summary for: {query[:50]}...")
        
        # Create test case
        test_case = LLMTestCase(
            input=original_text,
            actual_output=summary
        )
        
        # Create summarization metric with QAG
        metric = SummarizationMetric(
            threshold=self.eval_threshold,
            model=self.eval_model,
            n=self.eval_n_questions,  # Number of questions to generate if assessment_questions not provided
            assessment_questions=assessment_questions,  # Optional custom questions
            verbose_mode=True  # Enable verbose to see question generation
        )
        
        # Evaluate
        metric.measure(test_case)
        
        # Get score breakdown (coverage and alignment scores)
        score_breakdown = getattr(metric, 'score_breakdown', {})
        
        return {
            "query": query,
            "score": metric.score,
            "success": metric.success,
            "reason": metric.reason,
            "coverage_score": score_breakdown.get('Coverage', None),
            "alignment_score": score_breakdown.get('Alignment', None),
            "threshold": self.eval_threshold,
            "n_questions": self.eval_n_questions,
            "evaluation_model": self.eval_model,
            "timestamp": datetime.utcnow().isoformat()
        }
    
    def _format_sources_for_response(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Format sources for response metadata."""
        sources = []
        for idx, chunk in enumerate(chunks, start=1):
            metadata = chunk.get("metadata", {})
            sources.append({
                "index": idx,
                "video_id": metadata.get("video_id", ""),
                "chapter": metadata.get("chapter", ""),
                "video_title": metadata.get("video_title", ""),
                "text_preview": metadata.get("text", "")[:200]
            })
        return sources


def get_text_summary_evaluator() -> TextSummaryEvaluator:
    """Get singleton evaluator instance."""
    return TextSummaryEvaluator()

