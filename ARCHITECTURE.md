# Notebook-LM AI Assistant - Architecture Document

## Project Overview

A video course AI assistant that helps students interact with course content through 4 main capabilities:
1. **Q&A**: Ask questions and get answers with exact video timestamps
2. **Text Summarization**: Get concise summaries on specific topics
3. **Video Summarization**: Summarize content of specific course videos
4. **Quiz Generation**: Generate Yes/No and MCQ quizzes from video content

**Source Material**: YouTube course videos  
**Architecture**: Modular Monolith (Python backend, React frontend)  
**Team Size**: 4 developers working in parallel

---

## Tech Stack

### Core Technologies
- **Backend**: Python with FastAPI (or Flask)
- **Frontend**: React with TypeScript
- **Databases**: 
  - PostgreSQL (video metadata, chapters, timestamps, chat history, quizzes)
  - Vector DB - options: Qdrant/pgvector/Weaviate (embeddings)
- **Transcription**: Whisper API / Deepgram
- **Video Processing**: FFmpeg or OpenCV for keyframe extraction
- **Orchestration**: LangChain / LlamaIndex (or custom implementation)

### Infrastructure
- Docker Compose for local development (Postgres + Vector DB)
- Vector DB data tracked in git (shared embeddings)
- Postgres data NOT in git (schema only)

---

## Architecture: Modular Monolith

### Why Modular Monolith?
âœ… Clear module boundaries for parallel development  
âœ… Shared RAG library easily accessible  
âœ… Simpler development and debugging  
âœ… Single deployment, less operational overhead  
âœ… Can evolve to microservices later if needed

### System Components
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚â”€â”€â”€â”€â–¶â”‚       Backend (FastAPI)          â”‚â”€â”€â”€â”€â–¶â”‚  Databases  â”‚
â”‚  React/TS   â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚             â”‚
â”‚             â”‚     â”‚  â”‚ API Layer (4 endpoints)    â”‚  â”‚     â”‚ PostgreSQL  â”‚
â”‚ - Chat UI   â”‚     â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚     â”‚ Vector DB   â”‚
â”‚ - Task      â”‚     â”‚  â”‚ Core Modules (4 tasks)     â”‚  â”‚     â”‚             â”‚
â”‚   Switcher  â”‚     â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ - Video     â”‚     â”‚  â”‚ Shared Components:         â”‚  â”‚
â”‚   Player    â”‚     â”‚  â”‚ - RAG Library             â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚ - LLM Clients             â”‚  â”‚
                    â”‚  â”‚ - DB Layer                â”‚  â”‚
                    â”‚  â”‚ - Config Manager          â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–²
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Ingestion Pipeline (Separate) â”‚
                    â”‚  - Download YouTube videos     â”‚
                    â”‚  - Transcribe audio           â”‚
                    â”‚  - Extract keyframes          â”‚
                    â”‚  - Generate embeddings        â”‚
                    â”‚  - Store in databases         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Detailed Folder Structure

```
tieplm/
â”œâ”€â”€ frontend/                    # React/TypeScript Web UI
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/         # UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ Chat/          # Chat interface (ChatGPT-like)
â”‚   â”‚   â”‚   â”œâ”€â”€ TaskSwitcher/  # Toggle between 4 tasks
â”‚   â”‚   â”‚   â”œâ”€â”€ VideoPlayer/   # Video with timestamp navigation
â”‚   â”‚   â”‚   â””â”€â”€ shared/        # Reusable components
â”‚   â”‚   â”œâ”€â”€ pages/             # Main pages
â”‚   â”‚   â”œâ”€â”€ services/          # API client services
â”‚   â”‚   â”œâ”€â”€ hooks/             # React hooks
â”‚   â”‚   â”œâ”€â”€ types/             # TypeScript types
â”‚   â”‚   â””â”€â”€ App.tsx
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tsconfig.json
â”‚
â”œâ”€â”€ backend/                     # Python Modular Monolith
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/                # API Routes (one file per task)
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ qa.py          # Q&A endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ text_summary.py # Text summarization endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ video_summary.py # Video summarization endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ quiz.py        # Quiz generation endpoints
â”‚   â”‚   â”‚   â””â”€â”€ health.py      # Health check
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ core/               # Business Logic (module per task)
â”‚   â”‚   â”‚   â”œâ”€â”€ qa/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ service.py     # Q&A orchestration
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ prompts.py    # Task-specific prompts
â”‚   â”‚   â”‚   â”œâ”€â”€ text_summary/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ service.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ prompts.py
â”‚   â”‚   â”‚   â”œâ”€â”€ video_summary/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ service.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ prompts.py
â”‚   â”‚   â”‚   â””â”€â”€ quiz/
â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚       â”œâ”€â”€ service.py
â”‚   â”‚   â”‚       â””â”€â”€ prompts.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ shared/             # Shared Components â­
â”‚   â”‚   â”‚   â”œâ”€â”€ rag/           # Shared RAG Library
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ retriever.py   # Vector search logic
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ reranker.py    # Optional reranking
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ pipeline.py    # RAG orchestration
â”‚   â”‚   â”‚   â”œâ”€â”€ llm/           # LLM Clients
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ client.py      # LLM API wrapper
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ vlm.py         # Vision LLM for video frames
â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings/    # Embedding utilities
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ embedder.py
â”‚   â”‚   â”‚   â”œâ”€â”€ database/      # DB Access Layer
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ postgres.py    # PostgreSQL client
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ vector_db.py   # Vector DB client
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ models.py      # SQLAlchemy models
â”‚   â”‚   â”‚   â””â”€â”€ config/        # Configuration Management
â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚       â”œâ”€â”€ settings.py    # Static configs (Pydantic)
â”‚   â”‚   â”‚       â””â”€â”€ dynamic.py     # DB-backed dynamic configs
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ models/             # Pydantic schemas
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ requests.py    # API request models
â”‚   â”‚   â”‚   â”œâ”€â”€ responses.py   # API response models
â”‚   â”‚   â”‚   â””â”€â”€ entities.py    # Domain entities
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ utils/              # Utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ youtube.py     # YouTube video helpers
â”‚   â”‚   â”‚   â””â”€â”€ timestamps.py  # Timestamp formatting
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ main.py             # FastAPI app entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ ingestion/                   # Standalone Ingestion Pipeline
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ download.py        # Download YouTube videos/audio
â”‚   â”‚   â”œâ”€â”€ transcribe.py      # Whisper/Deepgram transcription
â”‚   â”‚   â”œâ”€â”€ keyframes.py       # Extract keyframes with FFmpeg
â”‚   â”‚   â”œâ”€â”€ embeddings.py      # Generate embeddings
â”‚   â”‚   â””â”€â”€ storage.py         # Store in vector + Postgres DB
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ courses.yaml       # Course structure: chapters, URLs
â”‚   â”œâ”€â”€ main.py                # CLI entry point
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ evaluation/                  # Evaluation Module
â”‚   â”œâ”€â”€ datasets/               # Test datasets (not in git)
â”‚   â”‚   â”œâ”€â”€ qa_eval.json
â”‚   â”‚   â”œâ”€â”€ summary_eval.json
â”‚   â”‚   â”œâ”€â”€ video_eval.json
â”‚   â”‚   â””â”€â”€ quiz_eval.json
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ run_qa_eval.py
â”‚   â”‚   â”œâ”€â”€ run_summary_eval.py
â”‚   â”‚   â”œâ”€â”€ run_video_eval.py
â”‚   â”‚   â””â”€â”€ run_quiz_eval.py
â”‚   â”œâ”€â”€ metrics/                # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ evaluator.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ docker-compose.yml           # Postgres + Vector DB
â”œâ”€â”€ .env.example                 # Environment variables template
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md                    # Project overview
```

---

## Module Ownership (4 Team Members)

### ðŸ‘¤ **Person 1: Frontend + Integration**
**Responsibility**: User interface and API integration
- `frontend/` - Entire React application
- Chat interface (ChatGPT-like)
- Task switcher component (toggle between 4 tasks)
- Video player with timestamp navigation
- API integration layer

**Dependencies**: Needs API contracts from backend team

---

### ðŸ‘¤ **Person 2: Q&A + Text Summarization**
**Responsibility**: First two AI tasks
- `backend/app/api/qa.py` + `backend/app/api/text_summary.py`
- `backend/app/core/qa/` + `backend/app/core/text_summary/`
- Task-specific prompts and orchestration
- Both modules use shared RAG library

**Dependencies**: Shared RAG library from Person 4

---

### ðŸ‘¤ **Person 3: Video Summarization + Quiz Generation**
**Responsibility**: Second two AI tasks
- `backend/app/api/video_summary.py` + `backend/app/api/quiz.py`
- `backend/app/core/video_summary/` + `backend/app/core/quiz/`
- VLM integration for keyframe analysis
- Task-specific prompts and orchestration

**Dependencies**: Shared RAG library and VLM client from Person 4

---

### ðŸ‘¤ **Person 4: Ingestion Pipeline + Shared Infrastructure**
**Responsibility**: Data pipeline and shared components
- `ingestion/` - Entire ingestion pipeline
- `backend/app/shared/` - RAG library, LLM clients, DB layer, config
- Docker setup and database schemas
- Core infrastructure that others depend on

**Dependencies**: None (foundational work)

---

## Key Shared Components

### 1. Shared RAG Library (`backend/app/shared/rag/`)

**Used by**: Q&A, Text Summarization, Quiz Generation

**Common RAG Flow**:
1. Embed user query
2. Search vector database
3. Retrieve relevant chunks with metadata
4. Return results with source info (video URL, timestamps)

**Task-Specific**: Each task uses different prompts and post-processing logic

**Benefits**:
- Code reuse across multiple tasks
- Consistent retrieval behavior
- Easier to optimize and maintain

---

### 2. Configuration System (`backend/app/shared/config/`)

**Two-tier system for easy migration**:

- `settings.py`: Static configurations (hardcoded, from env vars)
- `dynamic.py`: Runtime configurations stored in Postgres

**Migration Path**:
- **Phase 1 (MVP)**: Hardcode prompts in `core/*/prompts.py`
- **Phase 2**: Move prompts to Postgres via `dynamic.py`
- **Phase 3**: Add UI to edit prompts dynamically

**Implementation**: Config loader checks DB first, falls back to static files

---

### 3. Database Layer (`backend/app/shared/database/`)

**PostgreSQL Schema**:
- `videos`: Video metadata (URL, title, duration, course_id)
- `chapters`: Course chapter structure
- `transcripts`: Full transcripts with timestamps
- `chat_history`: User sessions and conversations
- `quiz_questions`: Generated quiz questions and answers
- `dynamic_configs`: Runtime configuration overrides

**Vector DB Schema**:
- Transcript embeddings (chunked by time segments)
- Keyframe descriptions embeddings
- Metadata: video_id, timestamp_start, timestamp_end, chunk_text

---

## Data Flow Examples

### Example 1: Q&A Task
```
User: "What are the benefits of ResNet?"
   â”‚
   â–¼
[Frontend] POST /api/qa
   â”‚
   â–¼
[API Layer] qa.py endpoint
   â”‚
   â–¼
[Core] qa/service.py
   â”œâ”€â–¶ [Shared] embeddings.embedder â†’ Embed query
   â”œâ”€â–¶ [Shared] rag.retriever â†’ Search vector DB
   â”‚   â””â”€â–¶ Returns: [chunk1, chunk2, ...] with (video_id, timestamp)
   â”œâ”€â–¶ [Shared] llm.client â†’ Generate answer
   â”‚   â””â”€â–¶ Prompt from qa/prompts.py
   â–¼
[Response]
{
  "answer": "ResNet introduces skip connections that...",
  "sources": [
    {
      "video_url": "https://youtube.com/watch?v=abc",
      "timestamp": "15:30",
      "chapter": "Deep Learning Architectures"
    }
  ]
}
```

---

### Example 2: Quiz Generation Task
```
User: Select video â†’ "Generate MCQ Quiz"
   â”‚
   â–¼
[Frontend] POST /api/quiz
   â”‚
   â–¼
[API Layer] quiz.py endpoint
   â”‚
   â–¼
[Core] quiz/service.py
   â”œâ”€â–¶ [Shared] database.postgres â†’ Fetch transcript + keyframes
   â”œâ”€â–¶ [Shared] llm.vlm â†’ Analyze keyframes with VLM
   â”œâ”€â–¶ [Shared] llm.client â†’ Generate MCQs
   â”‚   â””â”€â–¶ Prompt from quiz/prompts.py
   â”œâ”€â–¶ [Shared] database.postgres â†’ Store questions
   â–¼
[Response]
{
  "quiz_id": "123",
  "questions": [
    {
      "question": "What type of connection does ResNet use?",
      "options": ["A) Skip", "B) Dense", "C) Recurrent", "D) Pooling"],
      "correct_answer": "A",
      "timestamp": "16:45",
      "video_url": "https://youtube.com/watch?v=abc"
    }
  ]
}
```

---

## Development Workflow

### Phase 1: Setup (Week 1)
1. **All**: Review architecture, assign modules
2. **Person 4**: 
   - Initialize project structure
   - Set up Docker Compose (Postgres + Vector DB)
   - Define database schemas
3. **Person 1**: Initialize React app skeleton
4. **All**: Define API contracts (request/response models)

---

### Phase 2: Foundation (Week 2-3)
1. **Person 4**: 
   - Build ingestion pipeline
   - Implement shared RAG library
   - Set up LLM clients
   - Populate databases with course data
2. **Persons 2 & 3**: Can start working with mocked RAG responses
3. **Person 1**: Build UI components with mocked API responses

---

### Phase 3: Core Development (Week 4-6)
1. **Person 2**: Implement Q&A and Text Summarization modules
2. **Person 3**: Implement Video Summarization and Quiz Generation modules
3. **Person 1**: Complete frontend implementation
4. **Person 4**: Support others, optimize shared components

---

### Phase 4: Integration & Testing (Week 7-8)
1. **All**: Integration testing
2. **All**: Bug fixes and refinements
3. **All**: Set up evaluation module
4. **All**: Run evaluations and optimize

---

## Docker Compose Services

```yaml
services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: tieplm
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    
  vector_db:
    image: qdrant/qdrant  # or alternative (pgvector, weaviate)
    ports:
      - "6333:6333"
    volumes:
      - vector_data:/qdrant/storage

volumes:
  postgres_data:
  vector_data:
```

**Note**: 
- Vector DB data can be committed to git (manageable size)
- Postgres data NOT in git (only schema migrations)
- Each team member runs ingestion pipeline once to populate local DBs

---

## Evaluation Strategy

### End-to-End Task Evaluation

Each task has separate evaluation dataset and script:

1. **Q&A Evaluation** (`evaluation/scripts/run_qa_eval.py`)
   - Metrics: Answer accuracy, source relevance, timestamp precision
   - Dataset: Pre-defined questions with ground truth answers

2. **Text Summarization Evaluation** (`evaluation/scripts/run_summary_eval.py`)
   - Metrics: ROUGE scores, factual consistency, conciseness
   - Dataset: Topics with human-written reference summaries

3. **Video Summarization Evaluation** (`evaluation/scripts/run_video_eval.py`)
   - Metrics: Coverage, coherence, key point extraction
   - Dataset: Videos with human-written summaries

4. **Quiz Evaluation** (`evaluation/scripts/run_quiz_eval.py`)
   - Metrics: Question quality, difficulty distribution, answer correctness
   - Dataset: Manual review of generated quizzes

**Workflow**: 
1. Manually create evaluation datasets
2. Run evaluation scripts that call main system
3. Collect metrics and analyze results
4. Iterate on prompts and RAG strategies

---

## Configuration Migration Example

### Phase 1: Hardcoded Prompts
```python
# backend/app/core/qa/prompts.py
QA_SYSTEM_PROMPT = """
You are a helpful AI assistant for a video course.
Answer questions based on the provided context.
Always cite video sources with timestamps.
"""
```

### Phase 2: DB-backed Prompts
```python
# backend/app/shared/config/dynamic.py
def get_prompt(task: str, prompt_type: str) -> str:
    # Try DB first
    db_prompt = fetch_from_postgres(task, prompt_type)
    if db_prompt:
        return db_prompt
    
    # Fallback to static
    from ..core.qa.prompts import QA_SYSTEM_PROMPT
    return QA_SYSTEM_PROMPT
```

### Phase 3: UI for Editing
```typescript
// frontend/src/components/Admin/PromptEditor.tsx
// Admin interface to edit prompts stored in Postgres
```

---

## Shared Redundancies Identified

### 1. RAG Pipeline
- **Used by**: Q&A, Text Summarization, Quiz Generation
- **Shared**: Embedding, retrieval, vector search
- **Different**: Prompts, post-processing

### 2. Video Metadata Retrieval
- **Used by**: All tasks
- **Shared**: Fetch video info, chapters, timestamps from Postgres

### 3. LLM Client
- **Used by**: All tasks
- **Shared**: API calling, error handling, token management
- **Different**: Prompts and parameters

### 4. Source Citation
- **Used by**: Q&A, Text Summarization, Quiz Generation
- **Shared**: Format video URLs with timestamps

---

## API Endpoints Overview

### Q&A
- `POST /api/qa/ask` - Ask a question
- `GET /api/qa/history` - Get chat history

### Text Summarization
- `POST /api/text-summary/summarize` - Summarize topic
- `POST /api/text-summary/filter` - Get relevant videos for topic

### Video Summarization
- `POST /api/video-summary/summarize` - Summarize specific video
- `GET /api/video-summary/videos` - List available videos

### Quiz Generation
- `POST /api/quiz/generate` - Generate quiz for video
- `GET /api/quiz/{quiz_id}` - Retrieve generated quiz
- `POST /api/quiz/validate` - Validate user answers

### Common
- `GET /api/health` - Health check
- `GET /api/videos` - List all videos with metadata
- `GET /api/chapters` - List course chapters

---

## Environment Variables

```bash
# .env.example

# Database
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=tieplm
POSTGRES_USER=user
POSTGRES_PASSWORD=password

# Vector DB
VECTOR_DB_TYPE=qdrant  # or pgvector, weaviate
VECTOR_DB_HOST=localhost
VECTOR_DB_PORT=6333

# LLM APIs
OPENAI_API_KEY=your_key_here
ANTHROPIC_API_KEY=your_key_here  # optional

# Transcription
WHISPER_API_KEY=your_key_here
# or
DEEPGRAM_API_KEY=your_key_here

# Application
ENVIRONMENT=development
LOG_LEVEL=INFO
```

---

## Next Steps for Team Discussion

### Questions to Discuss:
1. **Tech Stack Finalization**:
   - Vector DB choice: Qdrant, pgvector, or Weaviate?
   - LLM provider: OpenAI, Anthropic, or open-source?
   - Orchestration: LangChain, LlamaIndex, or custom?

2. **Module Assignment**:
   - Confirm 4-person split outlined above
   - Any preferences for specific modules?

3. **Timeline**:
   - Project deadline?
   - Milestone dates for each phase?

4. **Course Content**:
   - How many videos in the course?
   - Average video length?
   - Course structure (chapters/modules)?

5. **Evaluation**:
   - Who will create evaluation datasets?
   - Success criteria for each task?

### Ready to Start?
Once team agrees on architecture:
1. Create initial project structure
2. Set up Docker environment
3. Define detailed API contracts
4. Begin parallel development!

---

**Document Version**: 1.0  
**Last Updated**: November 11, 2025  
**Team Size**: 4 developers  
**Project Type**: University AI Assistant Project