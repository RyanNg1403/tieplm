# Application Ports
BACKEND_PORT=8000
FRONTEND_PORT=3000
VITE_API_URL=http://localhost:8000
# =============================================================================
# API Keys
# =============================================================================
OPENAI_API_KEY=your-openai-key-here

# =============================================================================
# Database Configuration
# =============================================================================

# PostgreSQL
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=tieplm
POSTGRES_USER=tieplm
POSTGRES_PASSWORD=tieplm

# Qdrant Vector Database
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION_NAME=cs431_course_transcripts

# =============================================================================
# Embedding Configuration
# =============================================================================

# Embedding Model
EMBEDDING_PROVIDER=openai
EMBEDDING_MODEL_NAME=text-embedding-3-small
EMBEDDING_DIMENSION=1536

# Embedding Batch Processing
EMBEDDING_BATCH_SIZE=100

# =============================================================================
# LLM Configuration (for contextual chunking and other tasks)
# =============================================================================

# Model Settings
MODEL_PROVIDER=openai
MODEL_NAME=gpt-5-mini

# API Settings
LLM_MAX_RETRIES=3
LLM_TIMEOUT=60
LLM_TEMPERATURE=0.3

# =============================================================================
# Chunking Configuration
# =============================================================================

# Time-Window Chunking
TIME_WINDOW=60
CHUNK_OVERLAP=10

# Contextual Enrichment
CONTEXT_TOKEN_LIMIT=300
ENABLE_CONTEXTUAL_CHUNKING=true

# =============================================================================
# Pipeline Configuration
# =============================================================================

# Logging
LOG_LEVEL=INFO
LOG_DIR=logs

# Processing
MAX_WORKERS=4
ENABLE_PROGRESS_BAR=true

# =============================================================================
# RAG & Retrieval Configuration
# =============================================================================

# Initial Retrieval (before reranking)
RAG_TOP_K_VECTOR=150          # Top K from vector search
RAG_TOP_K_BM25=150            # Top K from BM25 search
RETRIEVAL_SCORE_THRESHOLD=0.5 # Minimum similarity score

# Reranking Configuration
ENABLE_RERANKING=true
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2  # Local cross-encoder model
RERANKER_BATCH_SIZE=32        # Batch size for reranker inference
FINAL_CONTEXT_CHUNKS=10       # Number of chunks to include in LLM prompt (after reranking)

# LLM Response Length
LLM_MAX_COMPLETION_TOKENS=3000

# =============================================================================
# Evaluation Settings
# =============================================================================
EVAL_MODEL=gpt-5-nano
EVAL_SUMMARIZATION_THRESHOLD=0.5
EVAL_SUMMARIZATION_N_QUESTIONS=10

