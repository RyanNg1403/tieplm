# =============================================================================
# Application Ports
# =============================================================================
BACKEND_PORT=8000
FRONTEND_PORT=3000
VITE_API_URL=http://localhost:8000

# =============================================================================
# API Keys
# =============================================================================
OPENAI_API_KEY=your_openai_api_key_here

# =============================================================================
# Database Configuration
# =============================================================================

# PostgreSQL
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=tieplm
POSTGRES_USER=tieplm
POSTGRES_PASSWORD=tieplm

# Qdrant Vector Database
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION_NAME=cs431_course_transcripts

# =============================================================================
# Embedding Configuration
# =============================================================================

# Embedding Model
EMBEDDING_PROVIDER=openai
EMBEDDING_MODEL_NAME=r
EMBEDDING_DIMENSION=1536

# Embedding Batch Processing
EMBEDDING_BATCH_SIZE=100

# =============================================================================
# LLM Configuration
# =============================================================================

# Model Settings
MODEL_PROVIDER=openai
MODEL_NAME=gpt-5-mini
# Note: gpt-5 models don't support temperature or other advanced parameters
# Note: reasoning_effort="minimal" is hardcoded in embedder.py for gpt-5-mini

# API Settings
LLM_MAX_RETRIES=3
LLM_TIMEOUT=60
LLM_MAX_COMPLETION_TOKENS=3000  # Max tokens for LLM response generation

# =============================================================================
# Chunking Configuration (for ingestion pipeline)
# =============================================================================

# Time-Window Chunking
TIME_WINDOW=60
CHUNK_OVERLAP=10

# Contextual Enrichment (for embedding chunks)
CONTEXT_TOKEN_LIMIT=300  # Max tokens for contextual prefix generation (retries: 300->400->500)
ENABLE_CONTEXTUAL_CHUNKING=true

# =============================================================================
# Pipeline Configuration
# =============================================================================

# Logging
LOG_LEVEL=INFO
LOG_DIR=logs

# Processing
MAX_WORKERS=4
ENABLE_PROGRESS_BAR=true

# =============================================================================
# RAG & Retrieval Configuration
# =============================================================================

# Initial Retrieval (before reranking)
RAG_TOP_K_VECTOR=150          # Top K from vector search
RAG_TOP_K_BM25=150            # Top K from BM25 search
RETRIEVAL_SCORE_THRESHOLD=0.5 # Minimum similarity score

# Reranking Configuration
ENABLE_RERANKING=true
RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2  # Local cross-encoder model
RERANKER_TOP_K=10             # Final top K after reranking (for LLM context)
RERANKER_BATCH_SIZE=32        # Batch size for reranker inference

# Final Context Window
FINAL_CONTEXT_CHUNKS=10       # Number of chunks to include in LLM prompt (after reranking)
